---
title: "Replication & Sharding: масштабирование баз данных"
created: 2025-12-22
modified: 2026-02-13
type: deep-dive
status: published
confidence: high
tags:
  - topic/databases
  - replication
  - sharding
  - scaling
  - distributed
  - type/deep-dive
  - level/intermediate
related:
  - "[[databases-overview]]"
  - "[[databases-transactions-acid]]"
  - "[[databases-nosql-comparison]]"
  - "[[architecture-distributed-systems]]"
prerequisites:
  - "[[databases-transactions-acid]]"
  - "[[databases-nosql-comparison]]"
reading_time: 20
difficulty: 6
study_status: not_started
mastery: 0
last_reviewed:
next_review:
---

# Replication & Sharding: масштабирование баз данных

> Когда один сервер не справляется — масштабируемся. Репликация для отказоустойчивости и чтения, шардинг для записи и хранения.

---

## TL;DR

- **Репликация** — копирование данных на несколько серверов (HA + read scaling)
- **Шардинг** — разделение данных между серверами (write scaling + storage)
- **CAP теорема** — выбирай 2 из 3: Consistency, Availability, Partition Tolerance
- **Начни с репликации** — шардинг добавляет сложность, используй когда реально нужен

---

## Часть 1: Интуиция без кода

### Аналогия 1: Библиотека с бестселлером (Репликация)

Представь популярную книгу в библиотеке. Одна копия → очередь из 50 человек.

```
ОДНА КОПИЯ (без репликации):
┌─────────────┐
│ "Война и    │    👤👤👤👤👤👤👤👤👤👤 ← очередь на 2 недели
│   мир"      │
└─────────────┘

ДЕСЯТЬ КОПИЙ (с репликацией):
┌─────────┐ ┌─────────┐ ┌─────────┐
│ Копия 1 │ │ Копия 2 │ │ Копия 3 │ ...
└─────────┘ └─────────┘ └─────────┘
    👤          👤          👤      ← все читают одновременно
```

**Ключевая идея:** Репликация = много **одинаковых** копий для параллельного чтения.

Что происходит при "обновлении" (новое издание)?
- **Synchronous:** Пока ВСЕ копии не заменены, никто не читает
- **Asynchronous:** Новые копии раздают постепенно, кто-то ещё читает старое издание

### Аналогия 2: Телефонный справочник (Шардинг)

Старый телефонный справочник — один огромный том на 1000 страниц. Неудобно искать.

```
БЕЗ ШАРДИНГА:                    С ШАРДИНГОМ:
┌─────────────────┐         ┌────────┐ ┌────────┐ ┌────────┐
│  А - Я          │         │ А - И  │ │ К - С  │ │ Т - Я  │
│  1000 страниц   │   →     │ 333 стр│ │ 333 стр│ │ 334 стр│
│  Один том       │         │ Том 1  │ │ Том 2  │ │ Том 3  │
└─────────────────┘         └────────┘ └────────┘ └────────┘

Ищешь "Петров"?  → Сразу бери Том 2 (partition key = первая буква)
```

**Проблема range-based:** Если фамилии распределены неравномерно?
- Том "А-И": 500 страниц (много Ивановых)
- Том "Т-Я": 150 страниц

### Аналогия 3: Сеть пиццерий (Шардинг vs Репликация)

Пиццерия "Маргарита" расширяется:

```
РЕПЛИКАЦИЯ (одно меню, много точек):
        Центральная кухня (Primary)
              │
    ┌─────────┼─────────┐
    ▼         ▼         ▼
┌───────┐ ┌───────┐ ┌───────┐
│Точка 1│ │Точка 2│ │Точка 3│   ← Все готовят ВСЁ меню
│Центр  │ │Север  │ │ Юг    │   ← Только выдача (read)
└───────┘ └───────┘ └───────┘   ← Заказ нового блюда → центр

ШАРДИНГ (разделение по районам):
┌───────────────┐ ┌───────────────┐ ┌───────────────┐
│   Кухня 1     │ │   Кухня 2     │ │   Кухня 3     │
│   Район А-Г   │ │   Район Д-К   │ │   Район Л-Я   │
│ Свои заказы   │ │ Свои заказы   │ │ Свои заказы   │
└───────────────┘ └───────────────┘ └───────────────┘

Заказ с улицы Ленина? → Район "Л" → Кухня 3
```

**Комбинация в реальности:** Каждая кухня (шард) имеет резервного повара (реплику).

### Аналогия 4: Важное письмо (Sync vs Async репликация)

Отправляешь важный документ:

```
SYNCHRONOUS (заказное с уведомлением):
┌────────┐  Письмо   ┌────────┐  Вручение  ┌────────┐
│Ты      │ ────────▶ │ Почта  │ ─────────▶ │Получ.  │
└────────┘           └────────┘            └────────┘
    ▲                                          │
    │          Уведомление о вручении          │
    └──────────────────────────────────────────┘

Ждёшь подтверждения. Если не дошло — знаешь об этом.

ASYNCHRONOUS (обычное письмо):
┌────────┐  Письмо   ┌────────┐     ???     ┌────────┐
│Ты      │ ────────▶ │ Почта  │ ─ ─ ─ ─ ─▶ │Получ.? │
└────────┘           └────────┘            └────────┘
    │
    └─ "Отправлено!" (но дошло ли?)

Быстро, но письмо может потеряться.
```

**В базах данных:**
- Sync: Транзакция подтверждена = данные точно на реплике
- Async: Транзакция подтверждена = данные в пути, может потеряться при failover

### Аналогия 5: Круглый стол (Consistent Hashing)

Представь круглый стол с 12 местами (как часы). Гости рассаживаются по первой букве фамилии:

```
           12 (Я)
        11      1 (А)
       ╱          ╲
     10            2 (Б)
      │   СТОЛ    │
     9             3 (В)
       ╲          ╱
        8       4 (Г)
           6

Гость "Сидоров" → Хеш("С") = 7 → Садится на место 7
Гость "Иванов"  → Хеш("И") = 4 → Место 4
```

**Что если добавить 13-е место?**
- Обычный хеш: ВСЕ пересаживаются (hash % 13 ≠ hash % 12)
- Consistent hashing: Только гости между 12 и 1 двигаются

```
ДОБАВИЛИ МЕСТО 12.5:
Только гости с мест 12 и 1 смотрят, ближе ли им к 12.5
Остальные не двигаются!

Результат: ~1/N гостей пересаживается (вместо всех)
```

---

## Часть 2: Почему это сложно

### Ошибка 1: Преждевременный шардинг

**СИМПТОМ:** Команда шардирует базу с 10GB данных "на вырост".

**Реальность:**
```
Один PostgreSQL сервер спокойно держит:
• 500GB - 1TB данных
• 10,000+ QPS на чтение
• 1,000+ QPS на запись

Шардинг добавляет:
• Cross-shard запросы (scatter-gather)
• Distributed transactions
• Сложность деплоя × N
• Невозможность простых JOIN
```

**РЕШЕНИЕ:** Правило: сначала вертикальное масштабирование (мощнее сервер), потом репликация (read scaling), и только потом шардинг (когда данные не влезают или writes не успевают).

### Ошибка 2: Неправильный partition key

**СИМПТОМ:** Выбрали `created_at` как partition key для таблицы заказов.

```
Январь:  Шард 1  [████████████] 100% нагрузка (все новые заказы)
Декабрь: Шард 12 [░░░░░░░░░░░░] 0% нагрузка (старые данные)

Результат:
• Hotspot на последнем шарде
• Добавление шардов не помогает
• Один шард = bottleneck всей системы
```

**РЕШЕНИЕ:** Выбирай partition key с высокой кардинальностью и равномерным распределением:
- ✅ `user_id` — миллионы уникальных, равномерно
- ✅ `tenant_id` — для SaaS
- ❌ `status` — 3-5 значений, неравномерно
- ❌ `timestamp` — hotspot на последнем диапазоне

### Ошибка 3: Игнорирование replication lag

**СИМПТОМ:** После записи сразу читают с реплики → получают старые данные.

```python
# Пользователь меняет email
update_email(user_id=123, email="new@mail.com")  # → Primary

# Сразу показываем профиль (читаем с реплики)
profile = get_profile(user_id=123)  # → Replica (lag 100ms)
print(profile.email)  # "old@mail.com" — WTF?!

# Пользователь: "Не сохранилось!"
```

**РЕШЕНИЕ:**
1. **Read-your-writes:** После записи читай с Primary
2. **Session stickiness:** Весь сеанс пользователя на одном сервере
3. **Мониторинг lag:** Алерт если > 1 секунды
4. **Synchronous replica:** Для критичных данных

### Ошибка 4: Split-brain без fencing

**СИМПТОМ:** После network partition — два Primary, данные разошлись.

```
СЦЕНАРИЙ:
                    Network
Primary ─ ─ ─ ─ ─ partition ─ ─ ─ ─ ─ Replica
   │                                      │
   ▼                                      ▼
Продолжает                          Промоутится
принимать writes                    в Primary
   │                                      │
   ▼                                      ▼
user_id=1: {"balance": 100}        user_id=1: {"balance": 200}

Партиция восстановилась:
ЧЬИ ДАННЫЕ ПРАВИЛЬНЫЕ?!
```

**РЕШЕНИЕ:**
- **STONITH** (Shoot The Other Node In The Head): Принудительное отключение
- **Quorum:** Primary нужно большинство голосов (3 узла → нужно 2)
- **Lease-based:** Primary арендует роль на 30 сек, должен продлевать
- **Witness node:** Арбитр в третьем датацентре

### Ошибка 5: Cross-shard запросы в критичном пути

**СИМПТОМ:** Страница загружается 5 секунд из-за scatter-gather по 16 шардам.

```
Запрос: SELECT * FROM orders WHERE status = 'pending' LIMIT 10

БЕЗ ШАРДИНГА:
┌─────────┐
│ 1 query │ → 10ms
└─────────┘

С ШАРДИНГОМ (partition key = user_id, запрос по status):
┌────────┐ ┌────────┐ ┌────────┐     ┌────────┐
│Shard 1 │ │Shard 2 │ │Shard 3 │ ... │Shard 16│
└───┬────┘ └───┬────┘ └───┬────┘     └───┬────┘
    │          │          │              │
    └──────────┴──────────┴──────────────┘
                    │
              Coordinator
              (merge + sort)
                    │
                 500ms (×50 overhead!)
```

**РЕШЕНИЕ:**
- Дизайн под partition key: `WHERE user_id = ? AND status = 'pending'`
- Денормализация: Дублировать данные для разных access patterns
- Отдельная таблица: Global `pending_orders` без шардинга
- CQRS: Read models оптимизированы под запросы

### Ошибка 6: Забыли про rebalancing

**СИМПТОМ:** Добавили шарды, но данные остались на старых.

```
ДО: 4 шарда, hash % 4
user_id=1:  hash=1001, 1001 % 4 = 1 → Shard 1
user_id=99: hash=9999, 9999 % 4 = 3 → Shard 3

ПОСЛЕ: 8 шардов, hash % 8
user_id=1:  hash=1001, 1001 % 8 = 1 → Shard 1 ✓ (повезло)
user_id=99: hash=9999, 9999 % 8 = 7 → Shard 7 ✗ (данные на 3!)

Без миграции: 50% запросов идут на пустые шарды!
```

**РЕШЕНИЕ:**
- **Consistent hashing:** Минимизирует перемещение (~1/N данных)
- **Virtual nodes:** Каждый физический шард = много виртуальных
- **Online rebalancing:** Vitess, Citus делают это автоматически
- **Планируй заранее:** Начни с большего числа виртуальных шардов

---

## Часть 3: Ментальные модели

### Модель 1: Дерево решений масштабирования

```
                    Нужно масштабирование?
                           │
              ┌────────────┴────────────┐
              │                         │
        Проблема в                Проблема в
        ЧТЕНИИ?                   ЗАПИСИ/ХРАНЕНИИ?
              │                         │
              ▼                         ▼
        РЕПЛИКАЦИЯ              Один сервер
        (read replicas)          достаточно?
              │                         │
              │                    ┌────┴────┐
              │                   Да         Нет
              │                    │          │
              │               Вертикальное   ШАРДИНГ
              │               масштабирование
              │
              ▼
        Проблема в latency
        для удалённых юзеров?
              │
              ▼
        GEO-РЕПЛИКАЦИЯ
        (реплики в разных регионах)
```

### Модель 2: Спектр consistency-latency

```
         STRONG                                     EVENTUAL
        CONSISTENCY                                CONSISTENCY
            │                                           │
            ▼                                           ▼
┌───────────┬───────────┬───────────┬───────────┬───────────┐
│  Sync     │  Quorum   │  Quorum   │  Async    │  Async    │
│  W=ALL    │  W=N/2+1  │  W=1,R=N  │  W=1,R=1  │  eventual │
│  R=1      │  R=N/2+1  │           │           │           │
├───────────┼───────────┼───────────┼───────────┼───────────┤
│           │           │           │           │           │
│  Latency: │  Latency: │  Latency: │  Latency: │  Latency: │
│  HIGH     │  MEDIUM   │  MIXED    │  LOW      │  LOWEST   │
│           │           │           │           │           │
│  Пример:  │  Пример:  │  Пример:  │  Пример:  │  Пример:  │
│  Финансы  │  E-comm   │  Logging  │  Метрики  │  Соц.сети │
│           │           │  (write)  │           │           │
└───────────┴───────────┴───────────┴───────────┴───────────┘
```

### Модель 3: Quorum как пересечение множеств

```
N = 5 реплик: [A] [B] [C] [D] [E]

ЗАПИСЬ W=3:                    ЧТЕНИЕ R=3:
┌─────────────────┐            ┌─────────────────┐
│  Записали на:   │            │  Читаем с:      │
│  [A] [B] [C]    │            │  [C] [D] [E]    │
└─────────────────┘            └─────────────────┘
        │                              │
        └──────────┬───────────────────┘
                   │
                   ▼
            ПЕРЕСЕЧЕНИЕ: [C]

W + R = 3 + 3 = 6 > N = 5
→ Гарантировано есть хотя бы 1 общая реплика!
→ [C] содержит свежие данные
→ STRONG CONSISTENCY

Если W=2, R=2 → W+R=4 < N=5 → НЕТ гарантии пересечения!
┌───────┐     ┌───────┐
│[A][B] │     │[D][E] │  ← Нет общих → может читать stale
└───────┘     └───────┘
```

### Модель 4: Эволюция архитектуры

```
СТАДИЯ 1: Один сервер
┌───────────────┐
│   PostgreSQL  │     < 100GB, < 1000 QPS
│   Single      │     Просто, надёжно
└───────────────┘

СТАДИЯ 2: Primary + Replicas
┌───────────────┐
│    Primary    │──┬──▶ Replica 1 (reads)
│    (writes)   │  └──▶ Replica 2 (reads)
└───────────────┘
                        < 500GB, read-heavy workload
                        HA, read scaling

СТАДИЯ 3: Шардинг + Репликация
┌─────────────────────────────────────────────┐
│  ┌─────────┐  ┌─────────┐  ┌─────────┐     │
│  │ Shard 1 │  │ Shard 2 │  │ Shard 3 │     │
│  │ P → R   │  │ P → R   │  │ P → R   │     │
│  └─────────┘  └─────────┘  └─────────┘     │
└─────────────────────────────────────────────┘

                        > 1TB, write-heavy, scale-out

СТАДИЯ 4: Global Distribution
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│ US Region    │  │ EU Region    │  │ APAC Region  │
│ Shard 1-10   │◀─│ Shard 1-10   │─▶│ Shard 1-10   │
│ + Replicas   │  │ + Replicas   │  │ + Replicas   │
└──────────────┘  └──────────────┘  └──────────────┘

                        Global users, low latency everywhere
```

### Модель 5: Trade-off треугольник

```
                    CONSISTENCY
                        ▲
                       ╱ ╲
                      ╱   ╲
                     ╱     ╲
                    ╱       ╲
                   ╱  IDEAL  ╲
                  ╱  (нельзя) ╲
                 ╱             ╲
                ╱───────────────╲
               ╱                 ╲
              ▼                   ▼
         LATENCY ◀───────────▶ AVAILABILITY

CP: Жертвуем Availability
• При partition отказываем в обслуживании
• Примеры: PostgreSQL, MongoDB

AP: Жертвуем Consistency
• При partition возвращаем stale данные
• Примеры: Cassandra, DynamoDB

CA: Невозможно в распределённой системе
• Partition БУДЕТ (сеть ненадёжна)
• Это только для single-node

ВЫБОР ЗАВИСИТ ОТ USE CASE:
• Банк: CP (лучше отказ, чем неверный баланс)
• Twitter: AP (лучше показать старый твит, чем ничего)
```

---

## Терминология

| Термин | Значение |
|--------|----------|
| **Primary/Master** | Сервер, принимающий записи |
| **Replica/Slave** | Копия данных, только для чтения |
| **Synchronous replication** | Запись подтверждается после записи на реплику |
| **Asynchronous replication** | Запись подтверждается сразу, реплика догоняет |
| **Failover** | Переключение на реплику при падении primary |
| **Shard** | Часть данных на отдельном сервере |
| **Partition Key** | Ключ для распределения данных по шардам |
| **Rebalancing** | Перераспределение данных при добавлении шардов |
| **Consistent Hashing** | Алгоритм распределения с минимальным rebalancing |
| **Quorum** | Минимальное число узлов для подтверждения операции |

---

## Репликация: зачем и как

```
┌─────────────────────────────────────────────────────────────────┐
│                    ЗАЧЕМ РЕПЛИКАЦИЯ                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. HIGH AVAILABILITY (HA)                                      │
│     Primary падает → failover на replica                        │
│     Downtime: секунды вместо часов                              │
│                                                                 │
│  2. READ SCALING                                                │
│     Читай с реплик → разгрузи primary                          │
│     10 реплик = 10x throughput чтения                          │
│                                                                 │
│  3. GEOGRAPHIC DISTRIBUTION                                     │
│     Реплика в Европе → меньше latency для EU пользователей     │
│                                                                 │
│  4. BACKUP/ANALYTICS                                            │
│     Отдельная реплика для бэкапов / тяжёлых отчётов           │
│     Не влияет на production                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Архитектуры репликации

```
┌─────────────────────────────────────────────────────────────────┐
│                 АРХИТЕКТУРЫ РЕПЛИКАЦИИ                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. PRIMARY-REPLICA (Master-Slave)                              │
│                                                                 │
│     ┌──────────┐                                                │
│     │ Primary  │ ◀── Writes                                     │
│     │ (Master) │                                                │
│     └────┬─────┘                                                │
│          │ WAL Stream                                           │
│     ┌────┴────┬────────────┐                                    │
│     ▼         ▼            ▼                                    │
│  ┌──────┐ ┌──────┐    ┌──────┐                                 │
│  │Replica│ │Replica│    │Replica│ ◀── Reads                     │
│  └──────┘ └──────┘    └──────┘                                 │
│                                                                 │
│  2. MULTI-PRIMARY (Master-Master)                               │
│                                                                 │
│     ┌──────────┐        ┌──────────┐                           │
│     │ Primary 1│ ◀────▶ │ Primary 2│                           │
│     └──────────┘        └──────────┘                           │
│          ▲                   ▲                                  │
│       Writes              Writes                                │
│                                                                 │
│     ⚠️ Сложность: конфликты при concurrent writes              │
│                                                                 │
│  3. CASCADING REPLICATION                                       │
│                                                                 │
│     Primary ──▶ Replica 1 ──▶ Replica 2 ──▶ Replica 3          │
│                                                                 │
│     Уменьшает нагрузку на Primary                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Sync vs Async репликация

```
┌─────────────────────────────────────────────────────────────────┐
│              SYNCHRONOUS vs ASYNCHRONOUS                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  SYNCHRONOUS:                                                   │
│  ┌────────┐   1. Write   ┌────────┐                            │
│  │ Client │ ──────────▶  │Primary │                            │
│  └────────┘              └───┬────┘                            │
│       ▲                      │ 2. Replicate                    │
│       │                      ▼                                  │
│       │ 4. ACK          ┌────────┐                             │
│       └──────────────── │Replica │ 3. Write + ACK              │
│                         └────────┘                             │
│                                                                 │
│  ✅ Данные гарантированно на replica                            │
│  ❌ Latency выше (ждём replica)                                 │
│  ❌ Если replica недоступна → writes блокируются               │
│                                                                 │
│  ─────────────────────────────────────────────                 │
│                                                                 │
│  ASYNCHRONOUS:                                                  │
│  ┌────────┐   1. Write   ┌────────┐                            │
│  │ Client │ ──────────▶  │Primary │                            │
│  └────────┘              └───┬────┘                            │
│       ▲                      │ 3. Replicate (async)            │
│       │ 2. ACK (сразу!)     ▼                                  │
│       └──────────────── ┌────────┐                             │
│                         │Replica │                             │
│                         └────────┘                             │
│                                                                 │
│  ✅ Низкая latency                                              │
│  ✅ Primary не зависит от replica                               │
│  ❌ Replication lag — replica может отставать                  │
│  ❌ При failover возможна потеря данных                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### PostgreSQL Streaming Replication

```bash
# postgresql.conf на Primary
wal_level = replica
max_wal_senders = 10
synchronous_standby_names = 'replica1'  # для sync

# pg_hba.conf — разрешить репликацию
host replication replicator 192.168.1.0/24 md5

# На Replica
pg_basebackup -h primary -D /var/lib/postgresql/data -U replicator -Fp -Xs -P

# postgresql.conf на Replica
primary_conninfo = 'host=primary port=5432 user=replicator'
```

```sql
-- Проверить статус репликации
SELECT client_addr, state, sent_lsn, write_lsn, replay_lsn,
       pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes
FROM pg_stat_replication;

-- Проверить lag в секундах
SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS lag_seconds;
```

---

## Шардинг: когда один сервер мал

```
┌─────────────────────────────────────────────────────────────────┐
│                    ЗАЧЕМ ШАРДИНГ                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Репликация НЕ РЕШАЕТ:                                         │
│  • Запись всё ещё идёт на один Primary                         │
│  • Данные не помещаются на один сервер                         │
│                                                                 │
│  Шардинг:                                                       │
│  • Данные разделены между серверами                            │
│  • Каждый сервер обрабатывает часть writes                     │
│  • Горизонтальное масштабирование                              │
│                                                                 │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐              │
│  │ Shard 1 │ │ Shard 2 │ │ Shard 3 │ │ Shard 4 │              │
│  │ A-F     │ │ G-L     │ │ M-R     │ │ S-Z     │              │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘              │
│                                                                 │
│  Users A-F → Shard 1                                           │
│  Users G-L → Shard 2                                           │
│  ...                                                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Стратегии шардинга

```
┌─────────────────────────────────────────────────────────────────┐
│                 СТРАТЕГИИ ШАРДИНГА                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. RANGE-BASED                                                 │
│     user_id 1-1000 → Shard 1                                   │
│     user_id 1001-2000 → Shard 2                                │
│                                                                 │
│     ✅ Простой range queries                                    │
│     ❌ Hotspots (новые users все на последнем шарде)           │
│                                                                 │
│  2. HASH-BASED                                                  │
│     shard = hash(user_id) % num_shards                         │
│                                                                 │
│     ✅ Равномерное распределение                                │
│     ❌ Сложный rebalancing при добавлении шардов               │
│     ❌ Range queries требуют scatter-gather                     │
│                                                                 │
│  3. CONSISTENT HASHING                                          │
│     Виртуальное кольцо с узлами                                │
│                                                                 │
│          Shard A                                                │
│            ╱ ╲                                                  │
│           ╱   ╲                                                 │
│     Shard D───────Shard B                                      │
│           ╲   ╱                                                 │
│            ╲ ╱                                                  │
│          Shard C                                                │
│                                                                 │
│     ✅ Минимальный rebalancing (только соседи)                  │
│     ✅ Используется в Cassandra, DynamoDB                       │
│                                                                 │
│  4. DIRECTORY-BASED                                             │
│     Отдельный сервис знает где какие данные                    │
│                                                                 │
│     ✅ Гибкость                                                  │
│     ❌ Single point of failure (lookup service)                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Выбор partition key

```
┌─────────────────────────────────────────────────────────────────┐
│              ВЫБОР PARTITION KEY                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Хороший partition key:                                        │
│  ✅ Высокая кардинальность (много уникальных значений)          │
│  ✅ Равномерное распределение (без hotspots)                    │
│  ✅ Соответствует access patterns (запросы в одном шарде)      │
│                                                                 │
│  Примеры:                                                       │
│  ─────────                                                      │
│  E-commerce: user_id                                           │
│    • Заказы пользователя на одном шарде                        │
│    • Нет cross-shard для "мои заказы"                          │
│                                                                 │
│  SaaS: tenant_id                                               │
│    • Данные компании на одном шарде                            │
│    • Изоляция tenants                                          │
│                                                                 │
│  ❌ Плохой выбор: timestamp (все новые на одном шарде)          │
│  ❌ Плохой выбор: status (мало значений, неравномерно)          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Проблемы шардинга

```sql
-- ПРОБЛЕМА 1: Cross-shard queries

-- ❌ Запрос по всем шардам (scatter-gather)
SELECT * FROM orders WHERE status = 'pending';
-- Нужно запросить ВСЕ шарды и объединить результаты

-- ✅ Запрос в одном шарде
SELECT * FROM orders WHERE user_id = 123 AND status = 'pending';
-- user_id — partition key, запрос на одном шарде


-- ПРОБЛЕМА 2: Cross-shard joins

-- ❌ Join между данными на разных шардах
SELECT u.name, o.total
FROM users u JOIN orders o ON u.id = o.user_id
WHERE o.created_at > '2025-01-01';
-- users и orders могут быть на разных шардах!

-- ✅ Денормализация — хранить user_name в orders
SELECT user_name, total FROM orders WHERE created_at > '2025-01-01';


-- ПРОБЛЕМА 3: Global sequences

-- ❌ Auto-increment не работает глобально
-- Shard 1: id = 1, 2, 3
-- Shard 2: id = 1, 2, 3  -- коллизия!

-- ✅ UUID
INSERT INTO orders (id, ...) VALUES (gen_random_uuid(), ...);

-- ✅ Snowflake ID (Twitter)
-- 64-bit: timestamp + datacenter + machine + sequence
```

### Инструменты для шардинга

| PostgreSQL | MySQL | Managed |
|------------|-------|---------|
| Citus | Vitess | AWS Aurora |
| pg_partman | ProxySQL | Google Spanner |
| Postgres-XL | MySQL Cluster | CockroachDB |

---

## CAP теорема на практике

```
┌─────────────────────────────────────────────────────────────────┐
│                 CAP: ПРАКТИЧЕСКИЙ ВЫБОР                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  В распределённой системе PARTITION TOLERANCE обязателен       │
│  (сеть БУДЕТ разрываться)                                      │
│                                                                 │
│  Выбор: Consistency vs Availability                            │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ CP (Consistency + Partition Tolerance)                  │   │
│  │                                                         │   │
│  │ При network partition:                                  │   │
│  │ • Отказываемся отвечать если не уверены в консистентности│  │
│  │ • Лучше ошибка, чем неправильные данные                │   │
│  │                                                         │   │
│  │ Примеры: PostgreSQL, MongoDB, HBase, Redis Cluster     │   │
│  │ Use case: финансы, inventory, критичные данные         │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ AP (Availability + Partition Tolerance)                 │   │
│  │                                                         │   │
│  │ При network partition:                                  │   │
│  │ • Продолжаем отвечать с локальными данными             │   │
│  │ • Eventual consistency — данные сойдутся позже         │   │
│  │                                                         │   │
│  │ Примеры: Cassandra, DynamoDB, CouchDB                  │   │
│  │ Use case: социальные сети, логи, IoT, аналитика        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Quorum для баланса C и A

```
┌─────────────────────────────────────────────────────────────────┐
│                      QUORUM                                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  N = число реплик                                              │
│  W = число реплик для подтверждения записи                     │
│  R = число реплик для чтения                                   │
│                                                                 │
│  Правило: W + R > N → strong consistency                       │
│                                                                 │
│  Пример (N=3):                                                 │
│  ┌────────────────────────────────────────────────────────┐    │
│  │ W=3, R=1: Все должны записать, читаем с любой         │    │
│  │           Strong consistency, slow writes              │    │
│  │                                                        │    │
│  │ W=1, R=3: Быстрая запись, но читаем со всех           │    │
│  │           Strong consistency, slow reads               │    │
│  │                                                        │    │
│  │ W=2, R=2: Баланс (majority quorum)                    │    │
│  │           Популярный выбор                            │    │
│  │                                                        │    │
│  │ W=1, R=1: Быстро, но eventual consistency             │    │
│  │           Может читать stale данные                   │    │
│  └────────────────────────────────────────────────────────┘    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Failover стратегии

```
┌─────────────────────────────────────────────────────────────────┐
│                    FAILOVER СТРАТЕГИИ                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. MANUAL FAILOVER                                             │
│     DBA вручную промоутит replica в primary                    │
│     ❌ Долго (минуты-часы)                                      │
│     ✅ Контроль                                                  │
│                                                                 │
│  2. AUTOMATIC FAILOVER                                          │
│     Patroni/PgBouncer/ProxySQL автоматически переключают       │
│     ✅ Быстро (секунды)                                         │
│     ⚠️ Риск split-brain                                        │
│                                                                 │
│  3. SPLIT-BRAIN PROBLEM                                         │
│     ┌────────┐     Network      ┌────────┐                     │
│     │Primary │ ─ ─ partition ─ ─│Replica │                     │
│     └────────┘                  └────────┘                     │
│          │                           │                          │
│     Оба думают, что они Primary!                               │
│     Writes на обоих → конфликт данных                          │
│                                                                 │
│     Решение: Fencing (STONITH — Shoot The Other Node In Head)  │
│     Отключить старый Primary принудительно                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Связи

- [[databases-overview]] — карта раздела
- [[databases-transactions-acid]] — транзакции в распределённых системах
- [[databases-backup-recovery]] — бэкапы и DR
- [[architecture-distributed-systems]] — распределённые системы

---

## Источники

- [PostgreSQL Streaming Replication](https://www.postgresql.org/docs/current/warm-standby.html)
- [Citus Documentation](https://docs.citusdata.com/)
- "Designing Data-Intensive Applications" by Martin Kleppmann — Chapters 5-6
- [Jepsen: Distributed Systems Analysis](https://jepsen.io/)

---

---

## Проверь себя

> [!question]- Почему асинхронная репликация используется чаще синхронной, несмотря на риск потери данных?
> Синхронная репликация ждёт подтверждения от реплики перед COMMIT — увеличивает latency на каждую запись (сетевой round-trip). При сбое реплики мастер блокируется. Асинхронная не ждёт — мастер работает с максимальной скоростью. Потеря данных при failover минимальна (секунды). Компромисс: semi-synchronous — ждёт хотя бы одну реплику.

> [!question]- Когда горизонтальное масштабирование (шардинг) оправдано, а когда лучше масштабировать вертикально?
> Вертикальное (больше RAM/CPU/SSD) — дешевле, проще, работает до определённого предела (~1TB данных, ~10K WPS). Шардинг нужен когда: один сервер не справляется с записью, данные не помещаются на один сервер, нужна geo-distribution. Шардинг усложняет: cross-shard joins, distributed transactions, rebalancing. Для 95% проектов вертикальное + read replicas достаточно.

> [!question]- Почему выбор shard key — самое важное решение при шардинге?
> Shard key определяет распределение данных. Плохой key (например, дата) создаёт «горячие» шарды: все новые записи идут в один шард. Хороший key (user_id с hash) — равномерное распределение. Изменить shard key после — очень дорого (перемещение всех данных). Ключ должен обеспечивать равномерность И позволять запросы без cross-shard scatter.

> [!question]- Чем отличается master-slave от multi-master репликации?
> Master-slave: один мастер принимает записи, реплики — только чтение. Простая, предсказуемая, но мастер — single point of failure (failover нужен). Multi-master: несколько узлов принимают записи — нет SPOF, но конфликты записи требуют разрешения (last-write-wins, CRDT). Multi-master сложнее и используется для geo-distributed систем.

---

## Ключевые карточки

Что такое репликация и зачем она нужна?
?
Копирование данных на несколько серверов. Цели: отказоустойчивость (failover при сбое мастера), масштабирование чтения (read replicas), снижение latency (реплика ближе к пользователю). Не заменяет бэкапы.

Чем отличается шардинг от партиционирования?
?
Партиционирование — разделение таблицы на части на ОДНОМ сервере (по дате, диапазону). Шардинг — распределение данных на РАЗНЫЕ серверы. Шардинг = партиционирование + распределённая система. Партиционирование проще и решает 80% задач.

Что такое Consistent Hashing?
?
Алгоритм, где серверы и данные размещены на виртуальном кольце (0-2^32). Данные идут на ближайший по кольцу сервер. При добавлении/удалении узла перемещается ~1/N данных вместо всех. Используется в Cassandra, DynamoDB.

Что такое replication lag?
?
Задержка между записью на мастере и появлением данных на реплике. При async — миллисекунды-секунды. Проблема: read-your-writes — пользователь записал на мастере, но читает с реплики устаревшие данные. Решение: sticky sessions, causal consistency.

Что такое split-brain при failover?
?
Ситуация когда два узла считают себя мастером (при сетевом разделении). Оба принимают записи — данные расходятся. Предотвращение: fencing (STONITH), quorum-based election, shared storage. Критичная проблема для HA-кластеров.

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Следующий шаг | [[databases-backup-recovery]] | Бэкапы и disaster recovery |
| Углубиться | [[database-internals-complete]] | WAL streaming, consensus протоколы |
| Углубиться | [[cloud-databases-complete]] | Managed репликация в AWS/GCP |
| Смежная тема | [[architecture-distributed-systems]] | CAP, consensus, Raft/Paxos |
| Обзор | [[databases-overview]] | Вернуться к карте раздела |

---

*Обновлено: 2026-01-09 — добавлены педагогические секции (5 аналогий: библиотека/репликация, телефонный справочник/шардинг, пиццерии, письмо/sync-async, круглый стол/consistent hashing; 6 типичных ошибок с решениями; 5 ментальных моделей масштабирования)*
