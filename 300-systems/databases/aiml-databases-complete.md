---
title: "AI/ML Databases: Vector Databases & Embeddings"
type: guide
status: published
tags:
  - topic/databases
  - type/guide
  - level/intermediate
related:
  - "[[vector-databases-guide]]"
  - "[[embeddings-complete-guide]]"
  - "[[nosql-databases-complete]]"
modified: 2026-02-13
prerequisites:
  - "[[nosql-databases-complete]]"
reading_time: 24
difficulty: 7
study_status: not_started
mastery: 0
last_reviewed:
next_review:
---

# AI/ML Databases: Vector Databases & Embeddings

> ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ³Ğ°Ğ¹Ğ´ Ğ¿Ğ¾ vector databases Ğ´Ğ»Ñ RAG, semantic search Ğ¸ AI-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹

**Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ:** Advanced
**Ğ’Ñ€ĞµĞ¼Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ:** 45 Ğ¼Ğ¸Ğ½ÑƒÑ‚
**ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ:** 2025-12-30

---

## Prerequisites

| Ğ¢ĞµĞ¼Ğ°            | Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ğ¾                                 | Ğ“Ğ´Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ                         |
| --------------- | ------------------------------------------- | ----------------------------------- |
| **Embeddings**  | ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ | [[embeddings-complete-guide]]       |
| **LLM basics**  | Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ GPT, RAG, Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹                 | [[llm-fundamentals]]                |
| **Python**      | ĞšĞ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Python                       | [[python-basics]]                   |
| **Ğ‘Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…** | ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ², Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²                | [[databases-fundamentals-complete]] |

### Ğ”Ğ»Ñ ĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»

| Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ | Ğ§Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ |
|---------|--------------|
| **ML Engineer** | Ğ’Ñ‹Ğ±Ğ¾Ñ€ vector DB, chunking strategies, production deployment |
| **Backend Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº** | Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ vector search Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ |
| **AI Architect** | Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, hybrid search, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ |

---

## Ğ¢ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ

> ğŸ’¡ **Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ:**
>
> **Vector Database** = Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°, Ğ³Ğ´Ğµ ĞºĞ½Ğ¸Ğ³Ğ¸ ÑÑ‚Ğ¾ÑÑ‚ **Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ**, Ğ° Ğ½Ğµ Ğ¿Ğ¾ Ğ°Ğ»Ñ„Ğ°Ğ²Ğ¸Ñ‚Ñƒ. "ĞĞ°Ğ¹Ğ´Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞµ Ğ½Ğ° ÑÑ‚Ñƒ ĞºĞ½Ğ¸Ğ³Ñƒ" Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ĞºĞ½Ğ¸Ğ³Ğ¸ Ñ€ÑĞ´Ğ¾Ğ¼.

| Ğ¢ĞµÑ€Ğ¼Ğ¸Ğ½ | Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ | ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ |
|--------|----------|----------|
| **Vector/Embedding** | Ğ§Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°/Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ | **GPS-ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ ÑĞ¼Ñ‹ÑĞ»Ğ°** â€” Ğ³Ğ´Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ |
| **Similarity Search** | ĞŸĞ¾Ğ¸ÑĞº Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² | **ĞĞ°Ğ¹Ğ´Ğ¸ ĞºĞ½Ğ¸Ğ³Ğ¸ Ñ€ÑĞ´Ğ¾Ğ¼** â€” Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğµ = ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ |
| **ANN** | Approximate Nearest Neighbors | **ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº** â€” Ğ½Ğµ 100% Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹, Ğ½Ğ¾ Ğ² 1000x Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ |
| **HNSW** | Hierarchical Navigable Small World graph | **ĞšĞ°Ñ€Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¾** â€” Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²ÑĞ·Ğ¸ |
| **IVF** | Inverted File Index | **Ğ”ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ¹Ğ¾Ğ½Ñ‹** â€” ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ğ¹Ğ´Ğ¸ Ñ€Ğ°Ğ¹Ğ¾Ğ½, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¸Ñ‰Ğ¸ Ğ² Ğ½Ñ‘Ğ¼ |
| **Cosine Similarity** | ĞœĞµÑ€Ğ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ (ÑƒĞ³Ğ¾Ğ» Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸) | **Ğ£Ğ³Ğ¾Ğ» Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€ĞµĞ»ĞºĞ°Ğ¼Ğ¸** â€” ÑĞ¼Ğ¾Ñ‚Ñ€ÑÑ‚ Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ = Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸ |
| **Dimension** | Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° (768, 1536, 3072) | **ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº** â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ = Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, Ğ½Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğµ |
| **Chunking** | Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ | **ĞĞ°Ñ€ĞµĞ·ĞºĞ° ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹** â€” ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ³Ğ»Ğ°Ğ²Ğ° = Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ |
| **Hybrid Search** | Vector + keyword search | **Ğ”Ğ²Ğ° Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°Ñ€Ñ** â€” Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼ |
| **Reranking** | ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² | **Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ** â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ top-100 |
| **Quantization** | Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² | **JPEG Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞµĞ»** â€” Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¼ĞµÑÑ‚Ğ°, Ñ‡ÑƒÑ‚ÑŒ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ |
| **Metadata Filtering** | Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼ | **Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ½Ğ¸Ğ³Ğ¸ 2024 Ğ³Ğ¾Ğ´Ğ°** â€” ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ similarity |

---

## Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ

1. [Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² Vector Databases](#Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ-Ğ²-vector-databases)
2. [ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Embeddings](#ĞºĞ°Ğº-Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚-embeddings)
3. [ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°](#Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹-Ğ¿Ğ¾Ğ¸ÑĞºĞ°)
4. [Pinecone](#pinecone)
5. [Milvus](#milvus)
6. [Qdrant](#qdrant)
7. [Weaviate](#weaviate)
8. [Chroma](#chroma)
9. [FAISS](#faiss)
10. [pgvector](#pgvector)
11. [Chunking Strategies](#chunking-strategies)
12. [Hybrid Search](#hybrid-search)
13. [LangChain Integration](#langchain-integration)
14. [Security & Access Control](#security--access-control)
15. [Production Best Practices](#production-best-practices)
16. [Cost Analysis](#cost-analysis)
17. [Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ](#Ğ²Ñ‹Ğ±Ğ¾Ñ€-Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ)

---

## Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² Vector Databases

### Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Vector Database?

Vector database â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ´Ğ»Ñ **Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² (embeddings)**, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ similarity search.

```
Traditional DB:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT *    â”‚ â†’ Exact match
â”‚ WHERE id=1  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Vector DB:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Find similar to     â”‚ â†’ Approximate nearest neighbors
â”‚ [0.1, 0.3, ..., 0.8]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ñ‹ Vector Databases?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Documents â†’ Chunks â†’ Embeddings â†’ Vector DB                 â”‚
â”‚                                        â†“                     â”‚
â”‚  Query â†’ Embedding â†’ Similarity Search â†’ Context â†’ LLM      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Use Cases:**
- **RAG (Retrieval-Augmented Generation):** ĞŸĞ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ LLM
- **Semantic Search:** ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ, Ğ½Ğµ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
- **Recommendation Systems:** ĞŸĞ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹, ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚
- **Image Search:** ĞŸĞ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
- **Anomaly Detection:** ĞŸĞ¾Ğ¸ÑĞº outliers Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

### Ğ Ñ‹Ğ½Ğ¾Ğº Vector Databases

```
Market Growth:
2024: $1.73B
2032: $10.6B (projected)

GitHub Stars (Dec 2025):
Milvus:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35,000+
Qdrant:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 9,000+
Weaviate:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8,000+
Chroma:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 6,000+
```

---

## ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Embeddings

### Embedding = Vector Representation

```
Text: "The cat sat on the mat"
          â†“
    Embedding Model
          â†“
Vector: [0.12, -0.34, 0.56, ..., 0.78]
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              1536 or 3072 dimensions
```

### Embedding Models Comparison

| Model | Dimensions | Quality | Speed | Cost |
|-------|------------|---------|-------|------|
| text-embedding-3-large | 3072 | Best | Slow | $$$ |
| text-embedding-3-small | 1536 | Good | Fast | $ |
| BGE-large-en | 1024 | Very Good | Medium | Free |
| all-MiniLM-L6-v2 | 384 | OK | Very Fast | Free |

### OpenAI Embeddings

```python
from openai import OpenAI

client = OpenAI()

# Create embedding
response = client.embeddings.create(
    model="text-embedding-3-large",
    input="Your text here",
    dimensions=3072  # Can reduce: 256, 512, 1024, 1536
)

embedding = response.data[0].embedding
print(f"Dimensions: {len(embedding)}")  # 3072
```

**Matryoshka Representation:**
```python
# Full embedding
full = client.embeddings.create(
    model="text-embedding-3-large",
    input="text",
    dimensions=3072
)

# Truncated (still useful!)
small = client.embeddings.create(
    model="text-embedding-3-large",
    input="text",
    dimensions=512  # 6x less storage
)
```

### Sentence Transformers (Open Source)

```python
from sentence_transformers import SentenceTransformer

# Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embeddings
sentences = ["This is a sentence", "Another sentence"]
embeddings = model.encode(sentences)

print(embeddings.shape)  # (2, 384)
```

### Similarity Metrics

```
Cosine Similarity:
cos(A, B) = (A Â· B) / (||A|| Ã— ||B||)
Range: [-1, 1]
Best for: Text embeddings (normalized)

Dot Product:
A Â· B = Î£(ai Ã— bi)
Range: unbounded
Best for: Pre-normalized vectors (OpenAI)

Euclidean Distance (L2):
âˆšÎ£(ai - bi)Â²
Range: [0, âˆ)
Best for: Image embeddings, spatial data
```

**Python Example:**

```python
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def dot_product(a, b):
    return np.dot(a, b)

def euclidean_distance(a, b):
    return np.linalg.norm(a - b)

# Compare
vec1 = np.array([0.1, 0.2, 0.3])
vec2 = np.array([0.15, 0.25, 0.35])

print(f"Cosine: {cosine_similarity(vec1, vec2):.4f}")  # ~0.9986
print(f"Dot:    {dot_product(vec1, vec2):.4f}")        # ~0.185
print(f"L2:     {euclidean_distance(vec1, vec2):.4f}")  # ~0.0866
```

---

## ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°

### ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ½Ğµ Brute Force?

```
Brute Force:
- Compare query to ALL vectors
- O(n Ã— d) complexity
- 1M vectors Ã— 1536 dims = 1.5B operations per query
- Too slow for production
```

### HNSW (Hierarchical Navigable Small World)

```
                    Layer 2 (sparse)
                    â—‹â”€â”€â”€â”€â”€â”€â”€â—‹
                   /         \
                  /           \
              Layer 1 (medium)
              â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â—‹
             /|   |   |   |\
            / |   |   |   | \
       Layer 0 (dense - all nodes)
       â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹â”€â—‹

Search: Start from top, navigate down
```

**Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- **Pros:** High recall (>99%), logarithmic complexity
- **Cons:** Memory-intensive, slow index build
- **Parameters:**
  - `M`: Connections per node (16-64)
  - `ef_construction`: Build quality (100-500)
  - `ef_search`: Search quality/speed trade-off

```python
# Qdrant HNSW config
from qdrant_client.models import HnswConfigDiff

collection_config = {
    "hnsw_config": HnswConfigDiff(
        m=16,                    # Connections per node
        ef_construct=100,        # Build quality
        full_scan_threshold=10000  # When to use brute force
    )
}
```

### IVF (Inverted File Index)

```
       Cluster 1    Cluster 2    Cluster 3
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
       â”‚ â—‹ â—‹ â—‹ â”‚    â”‚ â—‹ â—‹ â—‹ â”‚    â”‚ â—‹ â—‹ â—‹ â”‚
       â”‚ â—‹ â—‹   â”‚    â”‚ â—‹ â—‹ â—‹ â”‚    â”‚ â—‹ â—‹   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜
           â†‘            â†‘            â†‘
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                  Query: Find nearest cluster(s)
                         then search within
```

**Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- **Pros:** Scalable, less memory than HNSW
- **Cons:** Lower recall, requires training
- **Parameters:**
  - `nlist`: Number of clusters (sqrt(n) to 4*sqrt(n))
  - `nprobe`: Clusters to search (1-nlist)

### Product Quantization (PQ)

```
Original vector: [0.12, 0.34, 0.56, 0.78, ...]  (1536 floats = 6KB)
                          â†“ Quantize
Compressed:      [code1, code2, code3, ...]     (192 bytes = 32x smaller)
```

**Trade-off:** 4-16x compression, slight accuracy loss

### Algorithm Selection

| Algorithm | Memory | Speed | Recall | Best For |
|-----------|--------|-------|--------|----------|
| HNSW | High | Fast | 99%+ | Production, quality-critical |
| IVF | Medium | Medium | 95%+ | Large datasets |
| IVF+PQ | Low | Medium | 90%+ | Billion-scale |
| DiskANN | Low | Medium | 95%+ | Larger than RAM |

---

## Pinecone

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PINECONE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        Fully managed, cloud-native                   â”‚
â”‚ Architecture: Serverless (Jan 2024) or Pods               â”‚
â”‚ Best for:    Zero-ops production, fast time-to-market     â”‚
â”‚ Pricing:     Usage-based (storage + RU + WU)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Serverless vs Pods

| Aspect | Serverless | Pods |
|--------|------------|------|
| Scaling | Automatic | Manual |
| Pricing | Usage-based | Per-pod/minute |
| Cost at idle | Near zero | Pod baseline |
| Setup | Instant | Configure capacity |

**Recommendation:** Serverless for most new projects

### Quick Start

```python
from pinecone import Pinecone, ServerlessSpec

# Initialize
pc = Pinecone(api_key="YOUR_API_KEY")

# Create serverless index
pc.create_index(
    name="my-index",
    dimension=1536,
    metric="cosine",
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)

# Connect
index = pc.Index("my-index")

# Upsert vectors
index.upsert(
    vectors=[
        {
            "id": "doc1",
            "values": [0.1, 0.2, ...],  # 1536 dims
            "metadata": {"title": "Document 1", "category": "tech"}
        }
    ]
)

# Query
results = index.query(
    vector=[0.1, 0.2, ...],
    top_k=5,
    include_metadata=True,
    filter={"category": {"$eq": "tech"}}
)
```

### Pricing (Dec 2025)

```
Serverless:
â”œâ”€â”€ Storage:     $0.33/GB/month
â”œâ”€â”€ Read Units:  $8.25/million
â”œâ”€â”€ Write Units: $2.00/million
â””â”€â”€ Free:        $100 credits to start

Pods:
â”œâ”€â”€ Starter:     $0.00 (1 pod, limited)
â”œâ”€â”€ Standard:    $0.0833/hour/pod
â””â”€â”€ Enterprise:  Custom pricing
```

---

## Milvus

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MILVUS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        Open-source, distributed                      â”‚
â”‚ Architecture: Cloud-native, compute/storage separation    â”‚
â”‚ Best for:    Billion-scale, GPU acceleration, full controlâ”‚
â”‚ Managed:     Zilliz Cloud                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Milvus Cluster                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Proxy      â”‚   Query Node â”‚   Data Node  â”‚   Index Node  â”‚
â”‚   (routing)  â”‚   (search)   â”‚   (storage)  â”‚   (indexing)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
       â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  etcd        â”‚              â”‚  MinIO/S3     â”‚
â”‚  (metadata)  â”‚              â”‚  (storage)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Index Types

| Index | Type | GPU | Best For |
|-------|------|-----|----------|
| FLAT | Brute force | No | <10K vectors |
| IVF_FLAT | Clustering | No | General use |
| IVF_SQ8 | Quantized | No | Memory-constrained |
| IVF_PQ | Compressed | No | Billion-scale |
| HNSW | Graph | No | High recall |
| GPU_IVF_FLAT | GPU | Yes | Fast ingestion |
| GPU_IVF_PQ | GPU | Yes | Large + fast |
| DiskANN | SSD | No | Larger than RAM |

### Quick Start

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# Connect
connections.connect("default", host="localhost", port="19530")

# Define schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536)
]
schema = CollectionSchema(fields, "Document embeddings")

# Create collection
collection = Collection("documents", schema)

# Insert
data = [
    ["doc1", "doc2"],  # texts
    [[0.1, 0.2, ...], [0.3, 0.4, ...]]  # embeddings
]
collection.insert(data)

# Create index
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {"M": 16, "efConstruction": 256}
}
collection.create_index("embedding", index_params)

# Load and search
collection.load()
results = collection.search(
    data=[[0.1, 0.2, ...]],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 64}},
    limit=5
)
```

### Zilliz Cloud Pricing

```
Serverless:
â”œâ”€â”€ Compute:  $4/million vCUs
â”œâ”€â”€ Storage:  $0.02/GB/hour
â””â”€â”€ Free tier available

Dedicated:
â”œâ”€â”€ Starter:    $99/month
â”œâ”€â”€ Standard:   $299/month
â””â”€â”€ Enterprise: Custom
```

---

## Qdrant

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QDRANT                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        Open-source, written in Rust                  â”‚
â”‚ Architecture: Single binary, HNSW-based                   â”‚
â”‚ Best for:    Performance, complex filtering, edge         â”‚
â”‚ Managed:     Qdrant Cloud                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **Rust:** Memory safety, no GC pauses
- **HNSW with Quantization:** High recall + compression
- **Powerful Filtering:** Complex boolean queries
- **Multi-tenancy:** Flexible sharding
- **ACID Transactions:** Data integrity

### Quick Start

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Connect
client = QdrantClient(host="localhost", port=6333)

# Create collection
client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=1536,
        distance=Distance.COSINE
    )
)

# Insert points
client.upsert(
    collection_name="documents",
    points=[
        PointStruct(
            id=1,
            vector=[0.1, 0.2, ...],
            payload={"title": "Doc 1", "category": "tech"}
        )
    ]
)

# Search with filtering
results = client.search(
    collection_name="documents",
    query_vector=[0.1, 0.2, ...],
    query_filter={
        "must": [
            {"key": "category", "match": {"value": "tech"}}
        ]
    },
    limit=5
)
```

### Advanced Filtering

```python
from qdrant_client.models import Filter, FieldCondition, MatchValue, Range

# Complex filter
filter = Filter(
    must=[
        FieldCondition(
            key="category",
            match=MatchValue(value="tech")
        ),
        FieldCondition(
            key="year",
            range=Range(gte=2023)
        )
    ],
    must_not=[
        FieldCondition(
            key="status",
            match=MatchValue(value="draft")
        )
    ]
)

results = client.search(
    collection_name="documents",
    query_vector=query_embedding,
    query_filter=filter,
    limit=10
)
```

### Quantization for Memory Savings

```python
from qdrant_client.models import ScalarQuantizationConfig, ScalarType

# Enable scalar quantization (4x memory reduction)
client.update_collection(
    collection_name="documents",
    quantization_config=ScalarQuantizationConfig(
        type=ScalarType.INT8,
        quantile=0.99,
        always_ram=True
    )
)
```

---

## Weaviate

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WEAVIATE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        Open-source, modular                          â”‚
â”‚ Architecture: GraphQL API, plugin modules                  â”‚
â”‚ Best for:    Hybrid search, RAG, multimodal               â”‚
â”‚ Managed:     Weaviate Cloud                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **Hybrid Search:** Vector + BM25 in one query
- **Modules:** Vectorizers, generators, rankers
- **GraphQL API:** Flexible queries
- **Generative Search:** Built-in RAG

### Quick Start

```python
import weaviate
from weaviate.classes.init import Auth

# Connect to Cloud
client = weaviate.connect_to_weaviate_cloud(
    cluster_url="YOUR_URL",
    auth_credentials=Auth.api_key("YOUR_KEY")
)

# Create collection with vectorizer
client.collections.create(
    name="Document",
    vectorizer_config=[
        weaviate.classes.config.Configure.Vectorizer.text2vec_openai()
    ],
    generative_config=weaviate.classes.config.Configure.Generative.openai()
)

# Add data (auto-vectorized!)
collection = client.collections.get("Document")
collection.data.insert({
    "title": "My Document",
    "content": "This is the content..."
})

# Hybrid search
results = collection.query.hybrid(
    query="machine learning",
    alpha=0.5,  # 0=BM25, 1=vector
    limit=5
)

# Generative search (RAG)
results = collection.generate.near_text(
    query="What is ML?",
    grouped_task="Summarize these documents",
    limit=3
)
```

### Hybrid Search Deep Dive

```python
# Alpha controls blend
#   alpha=0.0  â†’  Pure BM25 (keyword)
#   alpha=0.5  â†’  Equal blend (default)
#   alpha=1.0  â†’  Pure vector (semantic)

# Keyword-focused
results = collection.query.hybrid(
    query="python tutorial",
    alpha=0.2,  # More keyword weight
    limit=10
)

# Semantic-focused
results = collection.query.hybrid(
    query="how to learn programming",
    alpha=0.8,  # More vector weight
    limit=10
)
```

### Fusion Algorithms

```
Ranked Fusion (default):
- Combines ranks from both searches
- Simple, works well generally

Relative Score Fusion:
- Normalizes scores before combining
- Better when score magnitudes differ
```

---

## Chroma

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CHROMA                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        Open-source, local-first                      â”‚
â”‚ Architecture: Embedded or client-server                    â”‚
â”‚ Best for:    Prototyping, LangChain, development          â”‚
â”‚ Status:      Production features evolving                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Start

```python
import chromadb
from chromadb.utils import embedding_functions

# In-memory (development)
client = chromadb.Client()

# Persistent (production-like)
client = chromadb.PersistentClient(path="./chroma_db")

# With OpenAI embeddings
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="YOUR_KEY",
    model_name="text-embedding-3-small"
)

# Create collection
collection = client.create_collection(
    name="documents",
    embedding_function=openai_ef,
    metadata={"hnsw:space": "cosine"}
)

# Add documents (auto-embedded!)
collection.add(
    documents=["Document 1 text", "Document 2 text"],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}],
    ids=["id1", "id2"]
)

# Query
results = collection.query(
    query_texts=["What is machine learning?"],
    n_results=5,
    where={"source": "doc1"}
)
```

### LangChain Integration

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load and split documents
loader = TextLoader("document.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
splits = text_splitter.split_documents(documents)

# Create vector store
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)

# Retrieve
retriever = vectorstore.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance
    search_kwargs={"k": 5}
)

docs = retriever.invoke("What is the main topic?")
```

---

## FAISS

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FAISS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        Library (not a database)                      â”‚
â”‚ By:          Facebook AI Research                          â”‚
â”‚ Architecture: In-memory, GPU support                       â”‚
â”‚ Best for:    Research, custom solutions, benchmarks        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **GPU Acceleration:** CUDA support
- **Multiple Indexes:** Flat, IVF, HNSW, PQ
- **No Server:** Just a library
- **Battle-tested:** Used in production at Meta

### Quick Start

```python
import faiss
import numpy as np

# Create random embeddings
d = 1536  # dimension
nb = 10000  # database size
np.random.seed(42)
xb = np.random.random((nb, d)).astype('float32')

# Build index
index = faiss.IndexFlatL2(d)  # Brute-force L2
index.add(xb)

print(f"Total vectors: {index.ntotal}")

# Search
k = 5  # top-k results
xq = np.random.random((1, d)).astype('float32')  # query

D, I = index.search(xq, k)
print(f"Distances: {D}")
print(f"Indices: {I}")
```

### Index Types

```python
# 1. Flat (brute-force, exact)
index = faiss.IndexFlatL2(d)

# 2. IVF (clustering)
nlist = 100  # number of clusters
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFFlat(quantizer, d, nlist)
index.train(xb)  # Required!
index.add(xb)

# 3. HNSW
index = faiss.IndexHNSWFlat(d, 32)  # 32 = M parameter
index.add(xb)

# 4. IVF + PQ (compressed)
m = 8  # bytes per vector
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)
index.train(xb)
index.add(xb)
```

### GPU Acceleration

```python
# Single GPU
res = faiss.StandardGpuResources()
gpu_index = faiss.index_cpu_to_gpu(res, 0, index)

# Multiple GPUs
gpu_index = faiss.index_cpu_to_all_gpus(index)
```

### LangChain Integration

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# Create from documents
vectorstore = FAISS.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings()
)

# Save/Load
vectorstore.save_local("faiss_index")
loaded = FAISS.load_local(
    "faiss_index",
    OpenAIEmbeddings(),
    allow_dangerous_deserialization=True
)
```

---

## pgvector

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     pgvector                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type:        PostgreSQL extension                          â”‚
â”‚ Best for:    Existing Postgres users, ACID + vectors       â”‚
â”‚ Performance: Competitive with specialized DBs (2025)       â”‚
â”‚ Limitation:  Requires PostgreSQL expertise                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation

```sql
-- Enable extension
CREATE EXTENSION vector;

-- Create table
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(1536)
);

-- Insert
INSERT INTO documents (content, embedding)
VALUES ('Sample text', '[0.1, 0.2, ...]');
```

### Index Types

```sql
-- HNSW (recommended)
CREATE INDEX ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- IVFFlat (faster build, lower recall)
CREATE INDEX ON documents
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

### Queries

```sql
-- Nearest neighbors (cosine)
SELECT id, content, embedding <=> '[0.1, ...]' AS distance
FROM documents
ORDER BY embedding <=> '[0.1, ...]'
LIMIT 5;

-- With filtering
SELECT id, content
FROM documents
WHERE category = 'tech'
ORDER BY embedding <=> '[0.1, ...]'
LIMIT 5;
```

### Python Integration

```python
import psycopg2
from pgvector.psycopg2 import register_vector

conn = psycopg2.connect("dbname=mydb")
register_vector(conn)

cur = conn.cursor()

# Search
embedding = [0.1, 0.2, ...]
cur.execute("""
    SELECT id, content, embedding <=> %s AS distance
    FROM documents
    ORDER BY embedding <=> %s
    LIMIT 5
""", (embedding, embedding))

results = cur.fetchall()
```

### Performance (2025 Benchmarks)

```
pgvectorscale vs Qdrant (50M vectors, 99% recall):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ pgvectorscale â”‚ 471 QPS â”‚ â† 11.4x faster
â”‚ Qdrant        â”‚  41 QPS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Both achieve sub-100ms p99 latency
```

---

## Chunking Strategies

### Why Chunking Matters

```
Document (5000 tokens)
        â†“
  Chunking Strategy
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1  â”‚ Chunk 2  â”‚ Chunk 3  â”‚ ...  â”‚
â”‚ 400 tok  â”‚ 400 tok  â”‚ 400 tok  â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
  Embeddings (one per chunk)
        â†“
  Vector Database
```

### Recommended Settings

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Starting point (most projects)
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # ~400-512 tokens
    chunk_overlap=50,    # 10-20% overlap
    separators=["\n\n", "\n", ". ", " ", ""]
)

chunks = splitter.split_documents(documents)
```

### Strategy Comparison

| Strategy | Recall | Precision | Best For |
|----------|--------|-----------|----------|
| Fixed size | Good | Medium | General use |
| Semantic | Best | Good | Quality-critical |
| Page-level | Good | Good | Structured docs |
| No overlap | Medium | Best | Storage-constrained |

### NVIDIA 2024 Benchmark Results

```
Accuracy by Strategy:
Page-level:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.648 (winner)
Semantic 400:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.620
Recursive 200:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   0.580
```

### Advanced: Contextual Chunking (Anthropic 2024)

```python
# Idea: Add context to each chunk before embedding

def add_context(document, chunk):
    """Use LLM to add document context to chunk"""
    prompt = f"""
    Document: {document[:1000]}

    Chunk: {chunk}

    Add a brief context sentence before this chunk.
    """
    context = llm.invoke(prompt)
    return f"{context}\n\n{chunk}"

# Each chunk now includes document-level context
contextualized_chunks = [
    add_context(full_doc, chunk)
    for chunk in chunks
]
```

---

## Hybrid Search

### Why Hybrid?

```
Query: "Python 3.12 new features"

Vector Search (semantic):
âœ“ "Latest Python version improvements"
âœ— "JavaScript ES2024 features"

BM25 Search (keyword):
âœ“ Documents containing "Python 3.12"
âœ— "Latest Python version improvements" (no exact match)

Hybrid (combined):
âœ“ Best of both worlds
```

### Reciprocal Rank Fusion (RRF)

```
RRF Score = Î£ 1/(k + rank)

Example (k=60):
Document A: rank 1 in vector, rank 5 in BM25
  RRF = 1/(60+1) + 1/(60+5) = 0.016 + 0.015 = 0.031

Document B: rank 3 in vector, rank 2 in BM25
  RRF = 1/(60+3) + 1/(60+2) = 0.016 + 0.016 = 0.032 â† Winner
```

### Implementation with Weaviate

```python
# Hybrid search
results = collection.query.hybrid(
    query="Python tutorial",
    alpha=0.5,  # Equal weight
    limit=10,
    fusion_type="relative_score"  # or "ranked"
)

# Adjust for use case
# Legal docs: alpha=0.3 (more keyword)
# Creative search: alpha=0.8 (more semantic)
```

### Implementation with Qdrant

```python
from qdrant_client import models

# Requires BM25 pre-indexing
results = client.search(
    collection_name="documents",
    query_vector=query_embedding,
    query_filter=models.Filter(
        must=[
            models.FieldCondition(
                key="text",
                match=models.MatchText(text="Python")
            )
        ]
    ),
    limit=10
)
```

---

## LangChain Integration

### Unified Interface

```python
from langchain_community.vectorstores import (
    Chroma, FAISS, Pinecone, Qdrant, Weaviate, Milvus, PGVector
)
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# All follow same pattern:
# vectorstore = VectorStore.from_documents(docs, embeddings)
# retriever = vectorstore.as_retriever()
# docs = retriever.invoke("query")
```

### Complete RAG Pipeline

```python
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# 1. Vector store
vectorstore = Chroma.from_documents(docs, embeddings)

# 2. Retriever
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20}
)

# 3. LLM
llm = ChatOpenAI(model="gpt-4o")

# 4. Prompt
prompt = ChatPromptTemplate.from_template("""
Answer based on the following context:

{context}

Question: {input}
""")

# 5. Chain
document_chain = create_stuff_documents_chain(llm, prompt)
retrieval_chain = create_retrieval_chain(retriever, document_chain)

# 6. Invoke
result = retrieval_chain.invoke({"input": "What is the main topic?"})
print(result["answer"])
```

### Search Types

```python
# 1. Similarity (default)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# 2. MMR (Maximum Marginal Relevance) - reduces redundancy
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,  # Fetch more, then diversify
        "lambda_mult": 0.5  # Balance relevance/diversity
    }
)

# 3. Similarity with threshold
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 5,
        "score_threshold": 0.7  # Only highly relevant
    }
)
```

---

## Security & Access Control

### Key Challenges

```
Security Risks:
â”œâ”€â”€ Embedding Inversion Attack
â”‚   â””â”€â”€ Reconstruct original data from embeddings
â”œâ”€â”€ All-or-Nothing Access
â”‚   â””â”€â”€ API keys grant full access
â”œâ”€â”€ Multi-tenant Data Leakage
â”‚   â””â”€â”€ User A sees User B's data
â””â”€â”€ Missing Encryption
    â””â”€â”€ Data at rest/in transit exposed
```

### RBAC Implementation (Qdrant)

```python
from qdrant_client import QdrantClient
import jwt

# Generate JWT with permissions
payload = {
    "sub": "user123",
    "access": [
        {"collection": "public_docs", "access": "r"},
        {"collection": "user123_private", "access": "rw"}
    ]
}
token = jwt.encode(payload, SECRET_KEY, algorithm="HS256")

# Connect with token
client = QdrantClient(
    url="https://your-qdrant-instance.com",
    api_key=token  # JWT as API key
)

# User can only access permitted collections
```

### Multi-tenancy Patterns

```python
# Pattern 1: Collection per tenant
client.create_collection(f"tenant_{tenant_id}")

# Pattern 2: Payload filtering
client.upsert(
    collection_name="shared",
    points=[
        PointStruct(
            id=1,
            vector=[...],
            payload={"tenant_id": "tenant123"}
        )
    ]
)

# Search with mandatory filter
results = client.search(
    collection_name="shared",
    query_vector=[...],
    query_filter={"must": [{"key": "tenant_id", "match": {"value": current_tenant}}]}
)
```

### Best Practices Checklist

```
[ ] Enable TLS/HTTPS for all connections
[ ] Use short-lived tokens (JWT with expiry)
[ ] Implement RBAC at collection/document level
[ ] Encrypt data at rest (if supported)
[ ] Audit access logs
[ ] Regular security assessments
[ ] Compliance verification (SOC2, HIPAA, etc.)
```

---

## Production Best Practices

### Scaling Strategies

```
Horizontal Scaling:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Load Balancer                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Replica 1    â”‚ â”‚   Replica 2   â”‚
     â”‚  (reads)      â”‚ â”‚   (reads)     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Primary     â”‚
     â”‚   (writes)    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sharding:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard 1    â”‚ â”‚  Shard 2    â”‚ â”‚  Shard 3    â”‚
â”‚  (A-H)      â”‚ â”‚  (I-P)      â”‚ â”‚  (Q-Z)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Optimization

```python
# 1. Batch operations
# BAD: Individual upserts
for doc in documents:
    client.upsert(collection_name="docs", points=[doc])

# GOOD: Batch upsert
client.upsert(collection_name="docs", points=documents)

# 2. Vector compression
client.update_collection(
    collection_name="docs",
    quantization_config=ScalarQuantizationConfig(
        type=ScalarType.INT8,  # 4x memory reduction
        quantile=0.99,
        always_ram=True
    )
)

# 3. Pre-filtering
# Instead of searching all vectors, filter first
results = client.search(
    collection_name="docs",
    query_vector=[...],
    query_filter={"must": [{"key": "category", "match": {"value": "tech"}}]},
    limit=10
)
```

### Monitoring Metrics

```
Key Metrics to Track:
â”œâ”€â”€ Latency
â”‚   â”œâ”€â”€ p50 (median)
â”‚   â”œâ”€â”€ p99 (critical!)  â† Focus here
â”‚   â””â”€â”€ p999
â”œâ”€â”€ Throughput
â”‚   â”œâ”€â”€ Queries per second
â”‚   â””â”€â”€ Inserts per second
â”œâ”€â”€ Recall
â”‚   â””â”€â”€ Accuracy vs brute-force
â”œâ”€â”€ Resources
â”‚   â”œâ”€â”€ Memory usage
â”‚   â”œâ”€â”€ CPU utilization
â”‚   â””â”€â”€ Disk I/O
â””â”€â”€ Errors
    â”œâ”€â”€ Timeout rate
    â””â”€â”€ Error rate
```

### Production Checklist

```
[ ] High availability (replicas)
[ ] Automatic failover
[ ] Backup strategy
[ ] Monitoring & alerting
[ ] Load testing completed
[ ] Security audit passed
[ ] Documentation updated
[ ] Runbook for incidents
```

---

## Cost Analysis

### Break-Even: Managed vs Self-Hosted

```
Break-even point:
~80-100 million queries/month
~100 million vectors with high query volume

Below threshold â†’ Managed (Pinecone, Zilliz, Qdrant Cloud)
Above threshold â†’ Self-hosted (Milvus, Qdrant OSS)
```

### Cost Comparison Table

| Provider | Free Tier | Entry Paid | Enterprise |
|----------|-----------|------------|------------|
| Pinecone | $100 credits | ~$70/mo | Custom |
| Qdrant Cloud | 1GB free | ~$100/mo | Custom |
| Zilliz (Milvus) | Free tier | $89/mo | Custom |
| Weaviate | $25/mo | ~$50/mo | Custom |
| Self-hosted | - | $200-500/mo* | - |

*Self-hosted includes compute, storage, operations

### Hidden Costs (Self-Hosted)

```
Self-hosted TCO:
â”œâ”€â”€ Compute (EC2, GKE, etc.)
â”œâ”€â”€ Storage (SSD for HNSW)
â”œâ”€â”€ Dependencies
â”‚   â”œâ”€â”€ Kafka/Pulsar (Milvus WAL)
â”‚   â”œâ”€â”€ etcd (metadata)
â”‚   â””â”€â”€ Kubernetes
â”œâ”€â”€ Operations
â”‚   â”œâ”€â”€ Monitoring (Prometheus, Grafana)
â”‚   â”œâ”€â”€ Backups
â”‚   â””â”€â”€ Updates
â””â”€â”€ Engineering time
    â””â”€â”€ Often the biggest cost!
```

### Recommendations

```
Startup/POC:
â†’ Managed (Pinecone, Chroma Cloud)
â†’ Minimize ops, focus on product

Growth stage (<100M vectors):
â†’ Managed with cost monitoring
â†’ Consider Qdrant Cloud for cost-effectiveness

Scale (>100M vectors, high QPS):
â†’ Self-hosted (Milvus, Qdrant OSS)
â†’ Dedicated infrastructure team
```

---

## Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ

### Decision Flowchart

```
Start
  â”‚
  â–¼
Already using PostgreSQL? â”€â”€Yesâ”€â”€â–º pgvector
  â”‚
  No
  â”‚
  â–¼
Need zero ops? â”€â”€Yesâ”€â”€â–º Pinecone Serverless
  â”‚
  No
  â”‚
  â–¼
Billion-scale + GPU? â”€â”€Yesâ”€â”€â–º Milvus/Zilliz
  â”‚
  No
  â”‚
  â–¼
Complex filtering needed? â”€â”€Yesâ”€â”€â–º Qdrant
  â”‚
  No
  â”‚
  â–¼
Need hybrid search? â”€â”€Yesâ”€â”€â–º Weaviate
  â”‚
  No
  â”‚
  â–¼
Prototyping/LangChain? â”€â”€Yesâ”€â”€â–º Chroma
  â”‚
  No
  â”‚
  â–¼
Custom/Research? â”€â”€Yesâ”€â”€â–º FAISS
```

### Summary Table

| Solution | Best For | Pros | Cons |
|----------|----------|------|------|
| Pinecone | Zero-ops production | Managed, fast | Cost at scale |
| Milvus | Billion-scale | GPU, indexes | Complex ops |
| Qdrant | Performance | Rust, filtering | Smaller community |
| Weaviate | Hybrid search | Modules, RAG | Resource heavy |
| Chroma | Prototyping | Simple, LangChain | Not production-ready |
| FAISS | Research | GPU, flexible | Not a database |
| pgvector | Postgres users | ACID, existing | Requires tuning |

### Final Recommendations

```
For most RAG projects:
1. Development: Chroma
2. Production: Pinecone or Qdrant Cloud
3. Scale: Milvus self-hosted

Embedding models:
1. Quality: text-embedding-3-large
2. Cost-effective: text-embedding-3-small
3. Open source: BGE-large-en

Chunking:
1. Start: 400-512 tokens, 10-20% overlap
2. Test with your data
3. Consider semantic chunking for quality
```

---

## ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹

### ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ
- [Pinecone Docs](https://docs.pinecone.io/)
- [Milvus Docs](https://milvus.io/docs)
- [Qdrant Docs](https://qdrant.tech/documentation/)
- [Weaviate Docs](https://weaviate.io/developers/weaviate)
- [Chroma Docs](https://docs.trychroma.com/)
- [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)
- [pgvector](https://github.com/pgvector/pgvector)

### LangChain Integration
- [LangChain Vector Stores](https://python.langchain.com/docs/integrations/vectorstores/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)

### Benchmarks
- [Qdrant Benchmarks](https://qdrant.tech/benchmarks/)
- [ANN Benchmarks](https://ann-benchmarks.com/)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

## Ğ¡Ğ²ÑĞ·ÑŒ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ñ‚ĞµĞ¼Ğ°Ğ¼Ğ¸

[[vector-databases-guide]] â€” Ğ­Ñ‚Ğ¾Ñ‚ Ğ³Ğ°Ğ¹Ğ´ Ğ¿Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ±Ğ°Ğ·Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‘Ñ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² embeddings, similarity search Ğ¸ ANN-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹. Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑĞµÑ‚ ÑÑ‚Ñƒ Ñ‚ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (Pinecone, Milvus, Qdrant, Weaviate) Ğ¸ production-Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ vector-databases-guide Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¾Ñ‚ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.

[[embeddings-complete-guide]] â€” ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ³Ğ°Ğ¹Ğ´ Ğ¿Ğ¾ embeddings Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚, ĞºĞ°Ğº ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ· Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ embeddings (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸) Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ vector database Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ chunking-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ» ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµÑ€ĞµĞºĞ²Ğ¸Ğ·Ğ¸Ñ‚Ğ¾Ğ¼.

[[nosql-databases-complete]] â€” NoSQL-Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (MongoDB, Redis, Cassandra) Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‚ÑÑ Ñ vector databases. ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Document DB Ğ¸ Key-Value stores Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ NoSQL Ñ vector-Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ (MongoDB Atlas Vector Search), Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ vector DB. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ñ‹.

[[databases-fundamentals-complete]] â€” Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹, Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¸, CAP-Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ğ°) Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹ Ğ¸ Ğº vector databases: HNSW â€” ÑÑ‚Ğ¾ Ñ‚Ğ¸Ğ¿ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°, metadata filtering Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ, Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼ĞµĞ¶Ğ´Ñƒ managed Ğ¸ self-hosted Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ñ‚ĞµĞ¼Ğ¸ Ğ¶Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸. ĞŸĞ¾Ğ»ĞµĞ·ĞµĞ½ ĞºĞ°Ğº Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.

## Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞµ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ

- Kleppmann M. (2017). *Designing Data-Intensive Applications*. â€” Ğ“Ğ»Ğ°Ğ²Ñ‹ Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ… Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ´Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ vector databases Ğ¸ trade-offs Ğ¼ĞµĞ¶Ğ´Ñƒ consistency, availability Ğ¸ performance.
- Petrov A. (2019). *Database Internals*. â€” Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ (B-Tree, LSM-Tree), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ANN-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ (HNSW, IVF) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ¸ ĞºĞ°ĞºĞ¸Ğµ trade-offs Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚.
- Redmond E., Wilson J.R. (2012). *Seven Databases in Seven Weeks*. â€” ĞĞ±Ğ·Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (document, graph, key-value), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ´Ğ»Ñ AI/ML-Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° vector DB Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ°, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ‘Ğ”.

---

---

## ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ ÑĞµĞ±Ñ

> [!question]- ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ´Ğ»Ñ RAG Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ pgvector Ğ² PostgreSQL, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ vector database?
> pgvector Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ embeddings Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ similarity search Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² PostgreSQL, Ñ€ÑĞ´Ğ¾Ğ¼ Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ < 10 Ğ¼Ğ»Ğ½ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ PostgreSQL â€” pgvector Ğ¿Ñ€Ğ¾Ñ‰Ğµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ. ĞÑ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ vector DB Ğ½ÑƒĞ¶Ğ½Ğ° Ğ¿Ñ€Ğ¸: > 100 Ğ¼Ğ»Ğ½ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ñ‡ (multi-tenancy, hybrid search), Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° PostgreSQL Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ.

> [!question]- Ğ§ĞµĞ¼ HNSW Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ IVF Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ°ĞºĞ¾Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ?
> HNSW (Hierarchical Navigable Small World): Ğ³Ñ€Ğ°Ñ„ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸, Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ recall (>95%), Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ½Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ RAM (Ğ²ĞµÑÑŒ Ğ³Ñ€Ğ°Ñ„ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸). IVF (Inverted File Index): ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ + Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ğ¼, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ recall. HNSW Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ñ…. IVF Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ RAM.

> [!question]- ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ chunking-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹?
> Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ chunks â€” Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ). Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ â€” Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ ÑĞ¼Ñ‹ÑĞ»Ğ° (Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°). ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°: 200-500 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ñ overlap 10-20%. Semantic chunking (Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²) Ğ»ÑƒÑ‡ÑˆĞµ fixed-size. ĞŸĞ»Ğ¾Ñ…Ğ¾Ğ¹ chunking Ğ½ĞµĞ»ÑŒĞ·Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ¼.

> [!question]- ĞšĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ vector database Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ keyword search (BM25)?
> BM25 Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ² (ĞºĞ¾Ğ´Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ID, Ğ¸Ğ¼ĞµĞ½Ğ°), known-item search (Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ·Ğ½Ğ°ĞµÑ‚ Ñ‡Ñ‚Ğ¾ Ğ¸Ñ‰ĞµÑ‚), ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹. Vector search Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ), multimodal (Ñ‚ĞµĞºÑÑ‚ + Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ), ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ â€” hybrid search (BM25 + vector Ñ reranking).

---

## ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸

Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Vector Database?
?
Ğ‘Ğ”, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° embeddings (Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ â€” similarity search (Ğ½Ğ°Ğ¹Ñ‚Ğ¸ K Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²). Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ RAG, semantic search, Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹: Pinecone, Milvus, Qdrant, Weaviate.

Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ HNSW Ğ¸Ğ½Ğ´ĞµĞºÑ?
?
Hierarchical Navigable Small World â€” Ğ³Ñ€Ğ°Ñ„ Ğ´Ğ»Ñ ANN (Approximate Nearest Neighbor) Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞœĞ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°: Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ â€” Ğ³Ñ€ÑƒĞ±Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, Ğ½Ğ¸Ğ¶Ğ½Ğ¸Ğµ â€” Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹. Recall > 95%, O(log N) Ğ¿Ğ¾Ğ¸ÑĞº. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² Pinecone, Qdrant, pgvector. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ trade-off: RAM vs ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾.

Ğ§ĞµĞ¼ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ cosine similarity, L2 Ğ¸ dot product?
?
Cosine â€” ÑƒĞ³Ğ¾Ğ» Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ (Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ñ‹), ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. L2 (Euclidean) â€” Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² = cosine. Dot product â€” Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ magnitude-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ embeddings.

Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ pgvector?
?
Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ PostgreSQL Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². Ğ¢Ğ¸Ğ¿ vector(1536), Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ <=> (cosine), <-> (L2). Ğ˜Ğ½Ğ´ĞµĞºÑÑ‹: ivfflat (IVF), hnsw. ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾: Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ‘Ğ”, Ğ¿Ñ€Ğ¸Ğ²Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ SQL, Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ < 10 Ğ¼Ğ»Ğ½ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ².

Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ hybrid search Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ RAG?
?
ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ keyword search (BM25) Ğ¸ vector search (semantic). BM25 Ğ»Ğ¾Ğ²Ğ¸Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ, vector â€” ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· reciprocal rank fusion Ğ¸Ğ»Ğ¸ cross-encoder reranking. Ğ›ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ñ‡ĞµĞ¼ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.

Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ chunking Ğ² RAG?
?
Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ embedding Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸ĞµĞ¹. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: fixed-size (Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼), semantic (Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ²/ÑĞµĞºÑ†Ğ¸Ğ¹), recursive (Ğ¿Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸). Overlap 10-20% Ğ¼ĞµĞ¶Ğ´Ñƒ chunks. Ğ Ğ°Ğ·Ğ¼ĞµÑ€ 200-500 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² â€” Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.

---

## ĞšÑƒĞ´Ğ° Ğ´Ğ°Ğ»ÑŒÑˆĞµ

| ĞĞ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ | ĞšÑƒĞ´Ğ° | Ğ—Ğ°Ñ‡ĞµĞ¼ |
|-------------|------|-------|
| Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³ | [[vector-databases-guide]] | Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ embeddings Ğ¸ similarity search |
| Ğ£Ğ³Ğ»ÑƒĞ±Ğ¸Ñ‚ÑŒÑÑ | [[embeddings-complete-guide]] | ĞšĞ°Ğº ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ embeddings |
| Ğ£Ğ³Ğ»ÑƒĞ±Ğ¸Ñ‚ÑŒÑÑ | [[rag-advanced-techniques]] | ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ RAG-Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ |
| Ğ¡Ğ¼ĞµĞ¶Ğ½Ğ°Ñ Ñ‚ĞµĞ¼Ğ° | [[nosql-databases-complete]] | MongoDB Atlas Vector Search, Redis Vector |
| Ğ¡Ğ¼ĞµĞ¶Ğ½Ğ°Ñ Ñ‚ĞµĞ¼Ğ° | [[llm-fundamentals]] | ĞÑĞ½Ğ¾Ğ²Ñ‹ LLM, Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ |
| ĞĞ±Ğ·Ğ¾Ñ€ | [[databases-overview]] | Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ Ğº ĞºĞ°Ñ€Ñ‚Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ° |

---

*Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: 2025-12-30*
*ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ deep research (22+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²)*
