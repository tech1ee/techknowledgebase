---
title: "Roadmap AI Engineer для новичка"
created: 2025-11-24
modified: 2025-11-24
type: tutorial
status: published
confidence: high
sources_verified: true
tags:
  - topic/ai-ml
  - ai-ml/career
  - ai-ml/learning
  - type/tutorial
  - level/intermediate
related:
  - "[[ai-engineering-intro]]"
  - "[[rag-and-prompt-engineering]]"
  - "[[ai-engineer-tech-stack]]"
reading_time: 10
difficulty: 4
study_status: not_started
mastery: 0
last_reviewed:
next_review:
---

# Roadmap AI Engineer для новичка

Если уже разработчик: 2-3 месяца. Если базовый Python: 4-6 месяцев. Если с нуля: 6-12 месяцев. Математика не критична на старте. Python + LLM API + RAG = минимально жизнеспособный навык.

---

## Терминология

| Термин | Значение |
|--------|----------|
| **LLM API** | Вызов модели через HTTP (OpenAI, Anthropic) |
| **RAG** | Поиск в своих данных + генерация ответа |
| **Embedding** | Числовой вектор, представляющий текст |
| **Vector DB** | База для хранения и поиска embeddings |
| **LangChain** | Фреймворк для построения AI-цепочек |
| **Fine-tuning** | Дообучение модели на своих данных |
| **Inference** | Запуск модели для получения ответа |
| **Agent** | LLM + инструменты + цикл рассуждений |

---

## Честный дисклеймер

*Я не буду обещать "стань AI Engineer за 30 дней". Реалистичные сроки:*
- *Если уже разработчик: 2-3 месяца*
- *Если базовый Python: 4-6 месяцев*
- *Если с нуля: 6-12 месяцев*

*И это при 10-20 часах в неделю. Можно быстрее, но качество пострадает.*

---

## Этап 0: Оценка стартовой позиции

Ответь себе честно:

**Знаешь Python?**
- Нет → добавь 1-2 месяца на основы
- Базово → ок, можно начинать
- Хорошо → пропускай Python-этап

**Есть опыт разработки?**
- Нет → будет сложнее, но возможно
- Да → большой плюс, быстрее освоишь

**Знаешь математику (линал, статистика)?**
- Нет → для AI Engineering на старте не критично
- Да → поможет понимать как работают модели

---

## Этап 1: Фундамент (4-6 недель)

### Python (если нужен)

*Почему Python, а не JavaScript/Go/что угодно ещё?* Потому что 95% AI-инструментов написаны на Python. Все библиотеки, все туториалы, все примеры — Python. Можно ли на другом? Можно. Нужно ли? Нет.

**Что реально нужно знать:**

```python
# 1. Базовый синтаксис — переменные, циклы, условия
# Если не понимаешь этот код — начни с основ
users = ["Alice", "Bob", "Charlie"]
for user in users:
    if len(user) > 4:
        print(f"Long name: {user}")

# 2. Функции и классы — потому что весь AI-код структурирован так
def get_embedding(text: str) -> list[float]:
    """Превращает текст в вектор чисел"""
    # тут будет вызов API
    pass

# 3. Работа с файлами — будешь читать документы, логи, конфиги
with open("data.json", "r") as f:
    data = json.load(f)

# 4. pip и virtual environments — иначе утонешь в конфликтах версий
# pip install openai
# python -m venv myenv
# source myenv/bin/activate

# 5. Jupyter Notebooks — стандарт для экспериментов с AI
# Установи: pip install jupyter
# Запусти: jupyter notebook
```

**Ключевое:** Не нужно становиться Python-гуру. Нужно уметь читать код, писать простые скрипты, и гуглить ошибки. 2-3 недели интенсива достаточно.

**Ресурсы:**
- [Python для начинающих на Stepik](https://stepik.org/course/58852) — на русском, бесплатно
- [Automate the Boring Stuff](https://automatetheboringstuff.com/) — практические проекты, бесплатно

### Основы ML (обзорно)

*Важно: тебе НЕ нужно уметь обучать модели. Нужно понимать базовые концепции, чтобы знать, что происходит "под капотом".*

**Модель и inference — что это вообще?**

```
Обучение (training):
Данные → [Алгоритм учится] → Модель (файл с числами)

Использование (inference):
Вопрос → [Модель] → Ответ

AI Engineer работает с inference — берёт готовую модель и строит приложения.
ML Engineer работает с training — создаёт саму модель.
```

**Embeddings — это магия RAG**

Представь, что каждое слово или предложение можно превратить в точку в пространстве. Похожие по смыслу тексты окажутся рядом.

```
"кошка" → [0.2, 0.8, 0.1, ...]
"котёнок" → [0.21, 0.79, 0.12, ...]  ← близко к "кошка"!
"автомобиль" → [0.9, 0.1, 0.7, ...]  ← далеко от "кошка"
```

Это позволяет искать "похожие документы" — основа [[rag-and-prompt-engineering|RAG]].

**Нейросети на пальцах**

Нейросеть — это функция с миллиардами настраиваемых чисел (параметров). Во время обучения эти числа подбираются так, чтобы функция давала правильные ответы.

GPT-4 — это ~1.7 триллиона таких чисел. Никто не понимает, что каждое из них делает. Но в совокупности — модель "понимает" язык.

**Ресурсы:**
- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) — лучшие визуализации, посмотри хотя бы первые 2 видео
- Andrew Ng ML курс — классика, но необязательно весь. Первые 2 недели достаточно

---

## Этап 2: LLM и Prompt Engineering (3-4 недели)

*Это сердце профессии. Если освоишь это хорошо — уже можешь делать полезные вещи.*

### Как работают LLM — без которого никуда

**Токенизация — почему "привет" и "пРиВеТ" это разное**

LLM не видит буквы. Она видит токены — кусочки текста. Одно слово может быть одним токеном или несколькими.

```
"Hello" → [15339]                    # 1 токен
"Привет" → [9501, 32761, 18849]     # 3 токена!

Вот почему русский текст стоит дороже — больше токенов.
Вот почему редкие слова модель понимает хуже — она видит их как набор частей.
```

**Context window — память модели**

Модель "помнит" только то, что влезает в контекст.

```
GPT-3.5: 16k токенов ≈ 12 страниц текста
GPT-4o: 128k токенов ≈ 100 страниц
Claude 3: 200k токенов ≈ 150 страниц

Всё, что не влезло — модель не видит. Просто не существует для неё.
```

*Пример проблемы:* Ты даёшь модели документ на 200 страниц. У GPT-3.5 влезет только первые 12. Остальное — обрезано. Модель ответит на основе начала документа и проигнорирует конец.

**Temperature — креативность vs предсказуемость**

```
Temperature 0.0: "Столица России — Москва"
                 (всегда одинаковый ответ)

Temperature 1.0: "Столица России — Москва, величественный город..."
                 (может добавить что-то неожиданное)

Temperature 2.0: "Столица России — сердце бескрайних просторов..."
                 (может нести чушь)

Для кода и фактов: temperature = 0
Для креатива: temperature = 0.7-0.9
```

**System prompt — характер модели**

Это инструкция в начале разговора, которая определяет "кто" эта модель.

```python
# Без system prompt — дефолтный помощник
"Как сварить яйцо?" → длинный ответ со всеми деталями

# С system prompt
system = "Отвечай как уставший шеф-повар. Максимум 2 предложения."
"Как сварить яйцо?" → "Кипяток, яйцо, 7 минут. Всё."
```

### Prompt Engineering — искусство задавать вопросы

**Zero-shot:** Просто спрашиваешь без примеров.
```
"Переведи на английский: Привет мир"
```

**Few-shot:** Даёшь примеры, модель учится на лету.
```
"кошка" → "cat"
"собака" → "dog"
"яблоко" → ?
```

**Chain-of-Thought:** Просишь думать вслух. Критично для математики и логики.
```
"Решай пошагово. У Маши 5 яблок..."
→ Модель: "Шаг 1: ... Шаг 2: ... Ответ: ..."
```

Подробнее: [[rag-and-prompt-engineering]]

### Работа с API — первый код

```python
# Первое, что ты напишешь как AI Engineer
from openai import OpenAI

client = OpenAI(api_key="sk-...")  # Ключ из dashboard.openai.com

response = client.chat.completions.create(
    model="gpt-4o-mini",  # Дешёвая модель для экспериментов
    messages=[
        {"role": "system", "content": "Ты помощник программиста"},
        {"role": "user", "content": "Что такое рекурсия в 1 предложении?"}
    ],
    temperature=0.0
)

print(response.choices[0].message.content)
# "Рекурсия — это функция, которая вызывает саму себя."
```

**Rate limits и ошибки — будут всегда**

```python
import time
from openai import RateLimitError

def safe_call(prompt):
    for attempt in range(3):
        try:
            return client.chat.completions.create(...)
        except RateLimitError:
            time.sleep(2 ** attempt)  # 1, 2, 4 секунды
    raise Exception("API недоступен")
```

### Практика

```python
# Первый проект: простой чат-бот
from openai import OpenAI

client = OpenAI()

messages = [
    {"role": "system", "content": "Ты — помощник программиста"},
]

while True:
    user_input = input("Ты: ")
    messages.append({"role": "user", "content": user_input})

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
    )

    assistant_message = response.choices[0].message.content
    print(f"AI: {assistant_message}")
    messages.append({"role": "assistant", "content": assistant_message})
```

**Ресурсы:**
- [OpenAI Cookbook](https://cookbook.openai.com/) — официальные примеры
- [Prompt Engineering Guide](https://www.promptingguide.ai/) — бесплатно, на русском тоже
- [DeepLearning.AI: ChatGPT Prompt Engineering](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) — бесплатный курс

---

## Этап 3: RAG (4-5 недель)

### Что изучить

1️⃣ **Embeddings**
- Как текст превращается в вектор
- Модели эмбеддингов (OpenAI, sentence-transformers)

2️⃣ **Векторные базы данных**
- Chroma (начни с неё — проще всего)
- Основные операции: добавление, поиск

3️⃣ **RAG pipeline**
- Загрузка документов
- Chunking стратегии
- Retrieval + Generation

### Практика: RAG чат-бот

```python
# Минимальный RAG
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 1. Загружаем документ
loader = TextLoader("knowledge_base.txt")
documents = loader.load()

# 2. Разбиваем на chunks
text_splitter = CharacterTextSplitter(chunk_size=1000)
docs = text_splitter.split_documents(documents)

# 3. Создаём векторную БД
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(docs, embeddings)

# 4. Создаём RAG chain
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=db.as_retriever()
)

# 5. Задаём вопрос
result = qa.run("Какая политика отпусков?")
```

**Ресурсы:**
- [LangChain документация](https://python.langchain.com/docs/)
- [LlamaIndex документация](https://docs.llamaindex.ai/)
- [RAG from Scratch (YouTube)](https://www.youtube.com/watch?v=sVcwVQRHIc8) — LangChain tutorial

---

## Этап 4: Проекты для портфолио (4-6 недель)

### Проект 1: Q&A по документации
- Загрузи документацию любой библиотеки
- Сделай чат-бот, который отвечает на вопросы

### Проект 2: AI-ассистент для конкретной задачи
- Помощник для написания кода
- Анализатор резюме
- Генератор контента для соцсетей

### Проект 3: Что-то с агентами
- Агент, который ищет в интернете
- Multi-step task solver
- Автоматизация workflow

### Что важно для портфолио

1️⃣ **Deployed проект** — не просто код на GitHub, а работающий сервис
2️⃣ **README с описанием** — что делает, как запустить, какие технологии
3️⃣ **End-to-end ownership** — от идеи до деплоя

*По словам работающих AI-инженеров, именно end-to-end проекты выделяют кандидатов на собеседованиях.*

---

## Этап 5: Продвинутые темы (ongoing)

После базы можно углубляться:

- **Fine-tuning** — LoRA, QLoRA
- **AI-агенты** — AutoGen, CrewAI, LangGraph
- **Evaluation** — как измерять качество LLM-приложений
- **MLOps для LLM** — мониторинг, A/B тесты
- **Multimodal** — работа с изображениями, аудио

---

## Где искать работу

### Позиции для старта

- **AI/LLM Engineer** — прямое попадание
- **Backend Engineer (AI team)** — войти через backend
- **Solutions Engineer (AI продукт)** — если хорош в коммуникации
- **AI Product Engineer** — на стыке продукта и технологий

### Где искать

- LinkedIn (фильтр по AI/ML)
- Y Combinator Jobs (стартапы)
- Remote OK (удалёнка)
- Telegram-каналы с вакансиями

### Что спрашивают на собеседованиях

1️⃣ **Системный дизайн RAG-системы**
2️⃣ **Prompt engineering задачи**
3️⃣ **Как бы ты решил X с помощью LLM**
4️⃣ **Опыт с конкретными инструментами**
5️⃣ **Live coding (Python)**

---

## Реалистичные ожидания

### Что будет легко
- Базовые API-вызовы
- Простые RAG-системы
- Prompt engineering для типовых задач

### Что будет сложно
- Debugging LLM-приложений (модель "чёрный ящик")
- Evaluation качества (как понять, что работает хорошо?)
- Production-grade системы (latency, costs, reliability)

### Типичные ошибки новичков

| Ошибка | Почему плохо | Как избежать |
|--------|--------------|--------------|
| Слишком много теории | Нет практического опыта | 70% время на проекты |
| Учить всё подряд | Распыление | Фокус на одном стеке |
| Игнорировать деплой | Нет portfolio | Деплоить каждый проект |
| Ждать идеального момента | Не начнёшь никогда | Начни с малого сегодня |

---

## Полезные ресурсы

### Курсы
- [freeCodeCamp AI Engineering](https://www.freecodecamp.org/news/ai-engineering-roadmap/) — бесплатно
- [DeepLearning.AI short courses](https://www.deeplearning.ai/short-courses/) — бесплатные мини-курсы
- [roadmap.sh/ai-engineer](https://roadmap.sh/ai-engineer) — интерактивный roadmap

### Сообщества
- r/LocalLLaMA — open-source модели
- r/MachineLearning — общее ML
- AI Discord серверы
- Telegram-каналы по AI

### Практика
- [LangChain Cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)
- [OpenAI Cookbook](https://cookbook.openai.com/)
- Kaggle competitions (для ML-части)

---

## Actionable план на первую неделю

- [ ] Зарегистрироваться на OpenAI, получить API key
- [ ] Написать первый скрипт с API-вызовом
- [ ] Пройти 1-2 урока Prompt Engineering Guide
- [ ] Установить LangChain, запустить первый пример
- [ ] Присоединиться к 1-2 сообществам

---

## Связи

- Что это за профессия: [[ai-engineering-intro]]
- Главные техники: [[rag-and-prompt-engineering]]
- Какие инструменты учить: [[ai-engineer-tech-stack]]
- Как учиться эффективно: [[learning-complex-things]]
- Docker для деплоя проектов: [[docker-for-developers]]

---

## Источники

- [roadmap.sh: AI Engineer](https://roadmap.sh/ai-engineer) — проверено 2025-11-24
- [freeCodeCamp: AI Engineering Roadmap](https://www.freecodecamp.org/news/ai-engineering-roadmap/) — проверено 2025-11-24
- [DataCamp: AI Developer Roadmap](https://www.datacamp.com/blog/ai-developer-roadmap) — проверено 2025-11-24
- [Analytics Vidhya: Roadmap to AI Engineer 2025](https://www.analyticsvidhya.com/blog/2024/04/roadmap-to-become-an-ai-engineer/) — проверено 2025-11-24

---

**Последняя верификация**: 2025-11-24
**Уровень достоверности**: high

---

*Проверено: 2026-01-09*

---

## Проверь себя

> [!question]- Почему AI Engineer работает преимущественно с inference, а не с training, и как это влияет на необходимый набор навыков?
> AI Engineer берёт уже обученные модели и строит приложения поверх них (inference), тогда как ML Engineer создаёт сами модели (training). Это принципиально меняет стек: AI Engineer'у критичнее знать API-интеграцию, prompt engineering и RAG, чем линейную алгебру и backpropagation. Глубокое понимание математики помогает, но не является входным барьером — можно быть продуктивным, зная как правильно вызывать модель и обрабатывать результат.

> [!question]- У тебя документ на 180 страниц и доступ к GPT-3.5 (16k токенов) и Claude 3 (200k токенов). Какую стратегию выберешь для каждой модели и почему?
> Для Claude 3 — можно загрузить весь документ целиком в контекст (180 страниц ~ 135k токенов, влезает в 200k). Для GPT-3.5 — нужна RAG-стратегия: разбить документ на chunks, создать embeddings, хранить в векторной БД и подавать в контекст только релевантные фрагменты. Прямая загрузка в GPT-3.5 обрежет документ до ~12 страниц, и модель ответит только по началу, проигнорировав 93% содержимого.

> [!question]- Новичок использует temperature=1.5 для генерации SQL-запросов. К каким проблемам это приведёт?
> Высокая temperature увеличивает "креативность" модели — она выбирает менее вероятные токены. Для SQL это катастрофа: модель может генерировать синтаксически некорректные запросы, выдумывать несуществующие функции, путать имена таблиц. Для кода и фактических задач нужна temperature=0 (детерминированный, предсказуемый ответ). Temperature выше 0.7 уместна только для творческих задач — написание текстов, брейншторм идей.

> [!question]- В чём ключевое отличие few-shot от chain-of-thought промптинга, и когда каждый подход эффективнее?
> Few-shot учит модель формату через примеры ("вот вход — вот выход"), что эффективно для задач классификации, перевода, форматирования. Chain-of-thought просит модель рассуждать пошагово, что критично для задач с логикой, математикой и многоступенчатым анализом. На практике: few-shot — когда важен формат ответа, chain-of-thought — когда важна правильность рассуждения. Часто их комбинируют: примеры с пошаговыми рассуждениями.

---

## Ключевые карточки

AI Engineer vs ML Engineer — в чём главное отличие?
?
AI Engineer работает с inference (использует готовые модели через API для построения приложений). ML Engineer работает с training (создаёт и обучает сами модели). AI Engineer'у нужнее навыки интеграции, prompt engineering и RAG, чем глубокая математика.

Что такое embedding и зачем он нужен в RAG?
?
Embedding — числовой вектор, представляющий текст в многомерном пространстве. Похожие по смыслу тексты получают близкие векторы. В RAG это позволяет находить релевантные документы по семантическому сходству, а не по точному совпадению ключевых слов.

Почему русский текст обходится дороже при работе с LLM API?
?
LLM работают с токенами, а не с символами. Русские слова разбиваются на большее количество токенов, чем английские (например, "Привет" = 3 токена, "Hello" = 1 токен). Оплата идёт за количество токенов, поэтому тот же объём текста на русском стоит в 2-3 раза дороже.

Что такое context window и какую проблему он создаёт?
?
Context window — максимальное количество токенов, которые модель "видит" одновременно (GPT-3.5: 16k, GPT-4o: 128k, Claude 3: 200k). Всё, что не влезает — обрезается. Модель отвечает только на основе того, что попало в контекст. Решение для больших документов — RAG (подача только релевантных фрагментов).

Temperature 0 vs Temperature 1 — когда что использовать?
?
Temperature 0 — детерминированный ответ, всегда одинаковый. Использовать для кода, SQL, фактов, классификации. Temperature 0.7-1.0 — вариативный ответ с элементом "креативности". Использовать для написания текстов, брейншторма, генерации идей. Выше 1.0 — обычно бессмысленно, модель начинает "галлюцинировать".

RAG pipeline — из каких шагов состоит?
?
1) Загрузка документов (document loading). 2) Разбиение на chunks (chunking/splitting). 3) Создание embeddings для каждого chunk. 4) Сохранение в векторную БД. 5) При запросе: поиск релевантных chunks по семантическому сходству (retrieval). 6) Передача найденных chunks + вопроса в LLM для генерации ответа (generation).

Три типа проектов для портфолио AI Engineer — какие и почему именно они?
?
1) Q&A по документации — демонстрирует владение RAG pipeline. 2) AI-ассистент для конкретной задачи — показывает умение решать реальные проблемы. 3) Проект с агентами — демонстрирует продвинутые навыки (multi-step reasoning, tool use). Ключевое: каждый проект должен быть задеплоен (не просто код на GitHub), с README и end-to-end ownership.

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Углубить основы | [[ai-engineering-intro]] | Детальный разбор профессии AI Engineer, роли и компетенции |
| Освоить RAG | [[rag-advanced-techniques]] | Продвинутые техники RAG: re-ranking, hybrid search, evaluation |
| Prompt Engineering | [[prompt-engineering-masterclass]] | Системное изучение техник промптинга от базовых до продвинутых |
| Агенты | [[ai-agents-advanced]] | Следующий уровень: автономные агенты, tool use, планирование |
| Деплой AI-проектов | [[docker-for-developers]] | Контейнеризация для деплоя портфолио-проектов (кросс-домен: DevOps) |
| Стратегия обучения | [[learning-complex-things]] | Как эффективно осваивать сложные темы (кросс-домен: когнитивистика) |
| Портфолио и карьера | [[portfolio-strategy]] | Как правильно оформить AI-проекты для поиска работы (кросс-домен: карьера) |
