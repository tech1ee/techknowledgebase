---
title: "Tutorial: Production-Ready RAG Chatbot —Å –Ω—É–ª—è"
tags:
  - topic/ai-ml
  - rag
  - tutorial
  - langchain
  - chromadb
  - qdrant
  - openai
  - project
  - production
  - type/tutorial
  - level/intermediate
category: ai-ml
level: practical
created: 2025-01-15
updated: 2026-02-13
reading_time: 82
difficulty: 5
study_status: not_started
mastery: 0
last_reviewed:
next_review:
related:
  - [rag-advanced-techniques]]
  - "[[embeddings-complete-guide]]"
  - "[[vector-databases-guide]"
status: published
---

# Tutorial: Production-Ready RAG Chatbot —Å –Ω—É–ª—è

> –ü–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é AI-—á–∞—Ç–±–æ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∞—à–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –û—Ç –∑–∞–≥—Ä—É–∑–∫–∏ PDF –¥–æ production-–¥–µ–ø–ª–æ—è –∑–∞ 2-3 —á–∞—Å–∞.

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **Python 3.10+** | –í–µ—Å—å –∫–æ–¥ –Ω–∞ Python, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **LLM –æ—Å–Ω–æ–≤—ã** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —á—Ç–æ —Ç–∞–∫–æ–µ LLM, —Ç–æ–∫–µ–Ω—ã, –ø—Ä–æ–º–ø—Ç—ã | [[llm-fundamentals]] |
| **REST API** | –†–∞–±–æ—Ç–∞ —Å OpenAI/Cohere API | [[ai-api-integration]] |
| **–ë–∞–∑–æ–≤—ã–π async/await** | –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | Python asyncio docs |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫** | ‚úÖ –î–∞ | –û—Ç–ª–∏—á–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ AI |
| **Intermediate** | ‚úÖ –î–∞ | –§–æ–∫—É—Å –Ω–∞ best practices |
| **Advanced** | ‚úÖ –î–∞ | Production-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ evaluation |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **RAG (Retrieval-Augmented Generation)** = LLM –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–µ –∏–∑ –≥–æ–ª–æ–≤—ã, –∞ —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ç–≤–æ–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ö–∞–∫ Google + ChatGPT –≤–º–µ—Å—Ç–µ.

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **RAG** | –ü–æ–∏—Å–∫ + –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–∏, –ø–æ—Ç–æ–º –æ—Ç–≤–µ—á–∞–π | **–°—Ç—É–¥–µ–Ω—Ç –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ —Å –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–º** ‚Äî –∏—â–µ—Ç –æ—Ç–≤–µ—Ç, –ø–æ—Ç–æ–º —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç |
| **Embedding** | –¢–µ–∫—Å—Ç ‚Üí —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä (—Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª) | **GPS-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è —Å–º—ã—Å–ª–∞** ‚Äî –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã —Ä—è–¥–æ–º |
| **Vector Database** | –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ | **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å GPS-–Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π** ‚Äî –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–Ω–∏–≥–∏ |
| **Chunk** | –ö—É—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–±—ã—á–Ω–æ 200-500 —Å–ª–æ–≤) | **–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑ –∫–Ω–∏–≥–∏** ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∏–º –Ω–∞ —á–∞—Å—Ç–∏ |
| **Chunking** | –ü—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ chunks | **–ù–∞—Ä–µ–∑–∫–∞ –ø–∏—Ü—Ü—ã** ‚Äî –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç —Ä–µ–∂–µ–º –Ω–∞ –∫—É—Å–∫–∏ |
| **Hybrid Search** | Vector + keyword –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–µ | **–°–º—ã—Å–ª + —Ç–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞** ‚Äî –∏—â–µ–º –∏ –ø–æ —Å–º—ã—Å–ª—É, –∏ –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º |
| **BM25** | –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ | **Ctrl+F –Ω–∞ —Å—Ç–µ—Ä–æ–∏–¥–∞—Ö** ‚Äî —É–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º |
| **Reranking** | –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ | **–í—Ç–æ—Ä–æ–µ –º–Ω–µ–Ω–∏–µ** ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —á—Ç–æ –≤–∞–∂–Ω–µ–µ |
| **Faithfulness** | –û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö (–Ω–µ –≤—ã–¥—É–º–∫–∞) | **–ß–µ—Å—Ç–Ω–æ—Å—Ç—å** ‚Äî LLM –≥–æ–≤–æ—Ä–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –≤–∏–¥–µ–ª –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö |
| **RAGAS** | –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ RAG —Å–∏—Å—Ç–µ–º | **–û—Ü–µ–Ω–∫–∞ 1-10** ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞ |

---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ / –ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç

**–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é:** —É –≤–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ 500+ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –ø–æ–ª–∏—Ç–∏–∫, FAQ. –°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ —Ç—Ä–∞—Ç—è—Ç —á–∞—Å—ã –Ω–∞ –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –í—ã —Ö–æ—Ç–∏—Ç–µ —á–∞—Ç–±–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–≥–Ω–æ–≤–µ–Ω–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –æ—Ç–≤–µ—Ç—ã.

**–ü—Ä–æ–±–ª–µ–º–∞ —Å –æ–±—ã—á–Ω—ã–º–∏ LLM:**
- ChatGPT –Ω–µ –∑–Ω–∞–µ—Ç –≤–∞—à–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- Fine-tuning –¥–æ—Ä–æ–≥–æ–π ($10-100k+) –∏ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç "–≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä–æ–≤–∞—Ç—å" ‚Äî –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–∫—Ç—ã

**RAG —Ä–µ—à–∞–µ—Ç —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã:**
- –ú–æ–¥–µ–ª—å –∏—â–µ—Ç –≤ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ‚Äî –ø—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
- –ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ü–æ –¥–∞–Ω–Ω—ã–º 2025 –≥–æ–¥–∞, **70% enterprise AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç RAG** ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/07/silent-killers-of-production-rag/))

**–ö–æ–º—É –ø–æ–¥–æ–π–¥—ë—Ç —ç—Ç–æ—Ç tutorial:**
- Backend/fullstack —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç—è—Ç –¥–æ–±–∞–≤–∏—Ç—å AI –≤ –ø—Ä–æ–¥—É–∫—Ç
- Data engineers, —Å—Ç—Ä–æ—è—â–∏–º knowledge management —Å–∏—Å—Ç–µ–º—ã
- –°—Ç–∞—Ä—Ç–∞–ø–∞–º, –∫–æ—Ç–æ—Ä—ã–º –Ω—É–∂–µ–Ω MVP —á–∞—Ç–±–æ—Ç–∞ –∑–∞ –≤—ã—Ö–æ–¥–Ω—ã–µ

---

## –ß—Ç–æ –ø–æ—Å—Ç—Ä–æ–∏–º

> –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π production-ready RAG chatbot —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∞—à–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –°—Ç–µ–∫: LangChain + Qdrant/ChromaDB + OpenAI + Streamlit + Hybrid Search + Reranking + Observability.

**–í—Ä–µ–º—è:** 2-3 —á–∞—Å–∞
**–£—Ä–æ–≤–µ–Ω—å:** Intermediate
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –†–∞–±–æ—Ç–∞—é—â–∏–π chatbot —Å best practices 2025 –≥–æ–¥–∞

### –ß–µ–º—É –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å

1. **Indexing Pipeline** ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ PDF/MD/DOCX, chunking, embeddings
2. **Vector Search** ‚Äî –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –∫–æ–≥–¥–∞ –µ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
3. **Hybrid Search** ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è vector + keyword –¥–ª—è +20% recall
4. **Reranking** ‚Äî Cross-encoder –¥–ª—è +25% precision
5. **RAG Chain** ‚Äî prompt engineering –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
6. **Evaluation** ‚Äî RAGAS –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
7. **Production Deploy** ‚Äî Docker, observability, —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏

### Prerequisities

- Python 3.10+
- OpenAI API key (–∏–ª–∏ –¥—Ä—É–≥–æ–π LLM provider)
- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ Python async/await
- (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Cohere API key –¥–ª—è reranking

### –°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—É—Å–∫–∞

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | –° –æ–ø–ª–∞—Ç–æ–π |
|-----------|-----------|-----------|
| Vector DB | ChromaDB (local), Qdrant (local) | Pinecone ($70/mo), Qdrant Cloud ($25/mo) |
| Embeddings | ‚Äî | OpenAI text-embedding-3: ~$0.02/1M tokens |
| LLM | ‚Äî | GPT-4o-mini: ~$0.15/1M tokens, GPT-4o: ~$2.50/1M tokens |
| Reranking | ‚Äî | Cohere: $1/1000 searches |

**–¢–∏–ø–∏—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–ª—è MVP:** $5-20/–º–µ—Å—è—Ü –ø—Ä–∏ 1000 –∑–∞–ø—Ä–æ—Å–∞—Ö/–¥–µ–Ω—å

---

## –ü–æ—á–µ–º—É RAG, –∞ –Ω–µ Fine-tuning?

RAG (Retrieval-Augmented Generation) —Ä–µ—à–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é –ø—Ä–æ–±–ª–µ–º—É LLM: –º–æ–¥–µ–ª–∏ –∑–Ω–∞—é—Ç —Ç–æ–ª—å–∫–æ —Ç–æ, –Ω–∞ —á—ë–º –æ–±—É—á–∞–ª–∏—Å—å. –ü–æ –¥–∞–Ω–Ω—ã–º 2025 –≥–æ–¥–∞, **70% enterprise AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç RAG** ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/07/silent-killers-of-production-rag/)).

| –ü–æ–¥—Ö–æ–¥ | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |
|--------|-------|--------|
| **Fine-tuning** | –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –±—ã—Å—Ç—Ä—ã–π inference | –î–æ—Ä–æ–≥–æ, –¥–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç, —Å–ª–æ–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å |
| **RAG** | –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –¥–µ—à–µ–≤–ª–µ | –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ retrieval |

**RAG –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç –∫–æ–≥–¥–∞:**
- –î–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è
- –ù—É–∂–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –í–∞–∂–Ω–∞ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RAG-—Å–∏—Å—Ç–µ–º—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Production RAG Architecture 2025                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ   Documents  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Chunking   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Embeddings  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  (.pdf,.md)  ‚îÇ    ‚îÇ  (Semantic/  ‚îÇ    ‚îÇ  (text-emb-  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ   Recursive) ‚îÇ    ‚îÇ   3-large)   ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                                                  ‚îÇ                       ‚îÇ
‚îÇ                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                             ‚îÇ                    ‚ñº                    ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ           ‚îÇ  Vector DB   ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ           ‚îÇ(Qdrant/Chroma‚îÇ              ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    User      ‚îÇ          ‚îÇ    ‚îÇ         HYBRID SEARCH         ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Question   ‚îÇ          ‚îÇ    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ    ‚îÇ  ‚îÇ Vector  ‚îÇ    ‚îÇ  BM25   ‚îÇ   ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ    ‚îÇ  ‚îÇ Search  ‚îÇ    ‚îÇ Keyword ‚îÇ   ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ         ‚ñº                   ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ    ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇQuery Embed + ‚îÇ          ‚îÇ    ‚îÇ              ‚ñº                ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Expansion    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ    ‚îÇ    ‚îÇ   RRF Fusion ‚îÇ           ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ                ‚ñº                        ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ        ‚îÇ   Reranker   ‚îÇ                 ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ        ‚îÇ (Cohere/BGE) ‚îÇ                 ‚îÇ  ‚îÇ
‚îÇ                             ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ  ‚îÇ
‚îÇ                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                              ‚ñº                           ‚îÇ
‚îÇ                                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ                                      ‚îÇ  Top-K Docs  ‚îÇ                    ‚îÇ
‚îÇ                                      ‚îÇ   Context    ‚îÇ                    ‚îÇ
‚îÇ                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                              ‚îÇ                           ‚îÇ
‚îÇ                                              ‚ñº                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                        LLM Generation                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   System    ‚îÇ + ‚îÇ   Context   ‚îÇ + ‚îÇ   Query     ‚îÇ ‚îÄ‚îÄ‚ñ∂ Answer   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Prompt    ‚îÇ   ‚îÇ   (docs)    ‚îÇ   ‚îÇ             ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                              ‚îÇ                           ‚îÇ
‚îÇ                                              ‚ñº                           ‚îÇ
‚îÇ                                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ                                      ‚îÇ   Answer +   ‚îÇ                    ‚îÇ
‚îÇ                                      ‚îÇ   Citations  ‚îÇ                    ‚îÇ
‚îÇ                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º

1. **Indexing (–æ—Ñ–ª–∞–π–Ω):**
   - –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ chunks
   - –ö–∞–∂–¥—ã–π chunk –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ embedding (–≤–µ–∫—Ç–æ—Ä 1536/3072 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)
   - –í–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ vector database —Å metadata

2. **Retrieval (—Ä–∞–Ω—Ç–∞–π–º):**
   - –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ embedding
   - Hybrid search: vector similarity + keyword BM25
   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —á–µ—Ä–µ–∑ Reciprocal Rank Fusion
   - Reranker –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç top-N –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

3. **Generation (—Ä–∞–Ω—Ç–∞–π–º):**
   - –¢–æ–ø-K —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö chunks –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ LLM –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
   - LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–õ–¨–ö–û —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
   - –û—Ç–≤–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

---

## Step 1: Setup –ø—Ä–æ–µ–∫—Ç–∞

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
rag-chatbot/
‚îú‚îÄ‚îÄ app.py                 # Streamlit UI
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ chunking.py        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ chunking
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py     # Vector DB operations
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py       # Hybrid retrieval + reranking
‚îÇ   ‚îú‚îÄ‚îÄ chain.py           # RAG chain
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py      # RAGAS metrics
‚îú‚îÄ‚îÄ documents/             # –í–∞—à–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
‚îú‚îÄ‚îÄ data/                  # Vector DB storage
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_rag.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ .env
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# requirements.txt
# Core
langchain>=0.3.0
langchain-openai>=0.2.0
langchain-community>=0.3.0
langchain-qdrant>=0.2.0

# Vector Databases
qdrant-client>=1.12.0
chromadb>=0.5.0

# Embeddings & Reranking
cohere>=5.0.0
sentence-transformers>=3.0.0

# Document Processing
pypdf>=4.0.0
unstructured>=0.15.0
python-docx>=1.0.0
tiktoken>=0.7.0

# Text Processing
rank-bm25>=0.2.2

# Web UI
streamlit>=1.38.0

# Evaluation
ragas>=0.2.0

# Observability
langfuse>=2.0.0

# Utils
python-dotenv>=1.0.0
pydantic>=2.0.0
```

```bash
pip install -r requirements.txt
```

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# .env
OPENAI_API_KEY=sk-...

# Optional: –¥–ª—è reranking
COHERE_API_KEY=...

# Optional: –¥–ª—è Qdrant Cloud
QDRANT_URL=https://xxx.qdrant.io
QDRANT_API_KEY=...

# Optional: –¥–ª—è observability
LANGFUSE_PUBLIC_KEY=...
LANGFUSE_SECRET_KEY=...
```

---

## Step 2: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
# rag/config.py
"""
–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã.

–ü–æ—á–µ–º—É –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏?
- –õ–µ–≥–∫–æ –º–µ–Ω—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞
- –†–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏ –¥–ª—è dev/staging/prod
- –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ñ–æ–ª—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
"""
from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Literal


class RAGConfig(BaseSettings):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞."""

    # === Chunking ===
    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 256-512 —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º 2025
    # https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025
    chunk_size: int = Field(
        default=512,
        description="–†–∞–∑–º–µ—Ä chunk –≤ —Ç–æ–∫–µ–Ω–∞—Ö. 256-512 –¥–ª—è factoid queries, 1024+ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö"
    )
    chunk_overlap: int = Field(
        default=50,
        description="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É chunks. 10-20% –æ—Ç chunk_size —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö"
    )

    # === Embeddings ===
    embedding_model: str = Field(
        default="text-embedding-3-large",
        description="OpenAI –º–æ–¥–µ–ª—å. 'large' (3072 dim) —Ç–æ—á–Ω–µ–µ, 'small' (1536 dim) –¥–µ—à–µ–≤–ª–µ"
    )

    # === Vector Store ===
    vectorstore_type: Literal["qdrant", "chroma"] = Field(
        default="qdrant",
        description="Qdrant –¥–ª—è production (Rust, –±—ã—Å—Ç—Ä—ã–π), Chroma –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤"
    )
    collection_name: str = "documents"

    # === Retrieval ===
    retrieval_k: int = Field(
        default=20,
        description="–°–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑–≤–ª–µ–∫–∞—Ç—å –¥–æ reranking. –ë–æ–ª—å—à–µ = –ª—É—á—à–µ recall, –Ω–æ –¥–æ—Ä–æ–∂–µ"
    )
    final_k: int = Field(
        default=5,
        description="–°–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ LLM –ø–æ—Å–ª–µ reranking"
    )
    use_hybrid_search: bool = Field(
        default=True,
        description="Hybrid search —É–ª—É—á—à–∞–µ—Ç recall –Ω–∞ ~20% –ø–æ –±–µ–Ω—á–º–∞—Ä–∫–∞–º"
    )
    hybrid_alpha: float = Field(
        default=0.7,
        description="–ë–∞–ª–∞–Ω—Å vector/keyword. 0.7 = 70% semantic, 30% keyword"
    )

    # === Reranking ===
    use_reranking: bool = Field(
        default=True,
        description="Reranking —É–ª—É—á—à–∞–µ—Ç precision –Ω–∞ 25-35% (Cohere benchmarks)"
    )
    rerank_model: str = Field(
        default="rerank-english-v3.0",
        description="Cohere rerank –º–æ–¥–µ–ª—å. rerank-v3 –∏–ª–∏ rerank-multilingual-v3.0"
    )

    # === LLM ===
    llm_model: str = Field(
        default="gpt-4o",
        description="gpt-4o –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞, gpt-4o-mini –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏/—Å—Ç–æ–∏–º–æ—Å—Ç–∏"
    )
    llm_temperature: float = Field(
        default=0.0,
        description="0.0 –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, 0.3-0.7 –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏"
    )

    class Config:
        env_file = ".env"
        env_prefix = "RAG_"


# Singleton instance
config = RAGConfig()
```

---

## Step 3: Document Loader

```python
# rag/document_loader.py
"""
–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
- PDF (—Å OCR –¥–ª—è —Å–∫–∞–Ω–æ–≤)
- Markdown
- TXT
- DOCX
- HTML

–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º:
1. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
2. –í—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π loader
3. –î–æ–∫—É–º–µ–Ω—Ç –ø–∞—Ä—Å–∏—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç + metadata
4. Metadata –æ–±–æ–≥–∞—â–∞–µ—Ç—Å—è (source, file_type, page_number)
"""
from pathlib import Path
from typing import List, Optional, Dict, Any
import logging

from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    UnstructuredHTMLLoader,
    Docx2txtLoader,
)
from langchain.schema import Document

logger = logging.getLogger(__name__)


class DocumentLoader:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        loader = DocumentLoader()
        docs = loader.load_directory("./documents")
    """

    # –ú–∞–ø–ø–∏–Ω–≥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π –Ω–∞ loaders
    # –ö–∞–∂–¥—ã–π loader —É–º–µ–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ —Å–≤–æ–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
    LOADERS = {
        ".pdf": PyPDFLoader,
        ".txt": TextLoader,
        ".md": UnstructuredMarkdownLoader,
        ".html": UnstructuredHTMLLoader,
        ".docx": Docx2txtLoader,
    }

    def __init__(self, extra_metadata: Optional[Dict[str, Any]] = None):
        """
        Args:
            extra_metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                           (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"project": "docs-v2"})
        """
        self.extra_metadata = extra_metadata or {}

    def load_file(self, file_path: str) -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª.

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            –°–ø–∏—Å–æ–∫ Document (–¥–ª—è PDF - –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)

        Raises:
            ValueError: –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        """
        path = Path(file_path)
        suffix = path.suffix.lower()

        if suffix not in self.LOADERS:
            supported = ", ".join(self.LOADERS.keys())
            raise ValueError(
                f"–§–æ—Ä–º–∞—Ç {suffix} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. "
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ: {supported}"
            )

        loader_class = self.LOADERS[suffix]
        loader = loader_class(str(path))

        try:
            documents = loader.load()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
            raise

        # –û–±–æ–≥–∞—â–∞–µ–º metadata
        for doc in documents:
            doc.metadata.update({
                "source": path.name,
                "source_path": str(path.absolute()),
                "file_type": suffix,
                **self.extra_metadata
            })

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω: {path.name} ({len(documents)} —á–∞—Å—Ç–µ–π)")
        return documents

    def load_directory(
        self,
        directory: str,
        recursive: bool = True,
        exclude_patterns: Optional[List[str]] = None
    ) -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

        Args:
            directory: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            recursive: –ò—Å–∫–∞—Ç—å –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
            exclude_patterns: –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["*_draft.md"])

        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        all_docs = []
        dir_path = Path(directory)
        exclude_patterns = exclude_patterns or []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
        pattern = "**/*" if recursive else "*"

        for file_path in dir_path.glob(pattern):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if not file_path.is_file():
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            if file_path.suffix.lower() not in self.LOADERS:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º exclude patterns
            if any(file_path.match(pat) for pat in exclude_patterns):
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω (exclude): {file_path.name}")
                continue

            try:
                docs = self.load_file(str(file_path))
                all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {file_path}: {e}")
                continue

        logger.info(
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {directory}"
        )
        return all_docs
```

---

## Step 4: Chunking - –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

```python
# rag/chunking.py
"""
–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ chunks.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π chunking –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ 9%!
https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025

–û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
1. RecursiveCharacter - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç (88-90% recall)
2. Semantic - –ª—É—á—à–∏–π recall (+9%), –Ω–æ –¥–æ—Ä–æ–∂–µ
3. Sentence - –¥–ª—è Q&A –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""
from typing import List, Literal, Optional
from abc import ABC, abstractmethod
import logging

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document

from .config import config

logger = logging.getLogger(__name__)


class BaseChunker(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è chunking —Å—Ç—Ä–∞—Ç–µ–≥–∏–π."""

    @abstractmethod
    def split(self, documents: List[Document]) -> List[Document]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ chunks."""
        pass


class RecursiveChunker(BaseChunker):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π chunking - –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô –î–ï–§–û–õ–¢.

    –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:
    1. –ü—ã—Ç–∞–µ—Ç—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º (\\n\\n)
    2. –ï—Å–ª–∏ chunk —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - –ø–æ —Å—Ç—Ä–æ–∫–∞–º (\\n)
    3. –ó–∞—Ç–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (. )
    4. –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ - –ø–æ —Å–ª–æ–≤–∞–º –∏ —Å–∏–º–≤–æ–ª–∞–º

    –≠—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ.

    –¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
    - 85-90% recall –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö
    - –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–±–µ–∑ API –≤—ã–∑–æ–≤–æ–≤)
    """

    def __init__(
        self,
        chunk_size: int = config.chunk_size,
        chunk_overlap: int = config.chunk_overlap,
    ):
        """
        Args:
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä chunk –≤ —Å–∏–º–≤–æ–ª–∞—Ö.
                       512 —Å–∏–º–≤–æ–ª–æ–≤ ~ 128 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
                       –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: 400-512 —Ç–æ–∫–µ–Ω–æ–≤
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É chunks.
                          10-20% –æ—Ç chunk_size —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        # –ò–µ—Ä–∞—Ä—Ö–∏—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π - –æ—Ç –∫—Ä—É–ø–Ω—ã—Ö –∫ –º–µ–ª–∫–∏–º
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=[
                "\n\n",    # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã (—Å–∞–º—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π)
                "\n",      # –°—Ç—Ä–æ–∫–∏
                ". ",      # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                ", ",      # –ß–∞—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                " ",       # –°–ª–æ–≤–∞
                ""         # –°–∏–º–≤–æ–ª—ã (–∫—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π)
            ],
            # –í–∞–∂–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –Ω–∞—á–∞–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            add_start_index=True,
        )

    def split(self, documents: List[Document]) -> List[Document]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ chunks.

        –ö–∞–∂–¥—ã–π chunk –ø–æ–ª—É—á–∞–µ—Ç:
        - page_content: —Ç–µ–∫—Å—Ç chunk
        - metadata: –≤—Å–µ –ø–æ–ª—è —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ + start_index
        """
        chunks = self.splitter.split_documents(documents)

        # –î–æ–±–∞–≤–ª—è–µ–º chunk_id –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i
            chunk.metadata["chunk_size"] = len(chunk.page_content)

        logger.info(
            f"Recursive chunking: {len(documents)} docs -> {len(chunks)} chunks"
        )
        return chunks


class SemanticChunker(BaseChunker):
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π chunking - –õ–£–ß–®–ò–ô RECALL, –Ω–æ –¥–æ—Ä–æ–∂–µ.

    –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:
    1. –ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ embedding
    2. –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
    3. –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ > threshold - —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–∞ chunk

    –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
    - –î–æ +9% recall –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å recursive
    - –ù–æ: —Ç—Ä–µ–±—É–µ—Ç embedding –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–¥–æ—Ä–æ–≥–æ!)

    –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
    - –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–µ—è–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    - –í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É
    - –ë—é–¥–∂–µ—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç
    """

    def __init__(
        self,
        breakpoint_threshold_type: Literal[
            "percentile", "standard_deviation", "interquartile"
        ] = "percentile",
        breakpoint_threshold_amount: float = 95,
    ):
        """
        Args:
            breakpoint_threshold_type: –ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã
                - "percentile": —Ä–∞–∑—Ä—ã–≤ –≤ —Ç–æ–ø-N% —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
                - "standard_deviation": —Ä–∞–∑—Ä—ã–≤ > N —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
                - "interquartile": —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥
            breakpoint_threshold_amount: –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
                –î–ª—è percentile: 95 = —Ç–æ–ª—å–∫–æ —Ç–æ–ø 5% —Ä–∞–∑—Ä—ã–≤–æ–≤ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ embedding –º–æ–¥–µ–ª—å, —á—Ç–æ –∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        embeddings = OpenAIEmbeddings(model=config.embedding_model)

        self.splitter = SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type=breakpoint_threshold_type,
            breakpoint_threshold_amount=breakpoint_threshold_amount,
        )

    def split(self, documents: List[Document]) -> List[Document]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏."""
        chunks = self.splitter.split_documents(documents)

        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i
            chunk.metadata["chunking_method"] = "semantic"

        logger.info(
            f"Semantic chunking: {len(documents)} docs -> {len(chunks)} chunks"
        )
        return chunks


def get_chunker(
    strategy: Literal["recursive", "semantic"] = "recursive"
) -> BaseChunker:
    """
    –§–∞–±—Ä–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è chunker.

    Args:
        strategy:
            - "recursive": –±—ã—Å—Ç—Ä—ã–π, —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è —Å—Ç–∞—Ä—Ç–∞)
            - "semantic": –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –¥–æ—Ä–æ–∂–µ

    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:
    –ù–∞—á–Ω–∏—Ç–µ —Å recursive. –ï—Å–ª–∏ recall < 85%, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ semantic.
    –†–∞–∑–Ω–∏—Ü–∞ –≤ 3-5% recall —Ä–µ–¥–∫–æ –æ–ø—Ä–∞–≤–¥—ã–≤–∞–µ—Ç 10x —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    if strategy == "recursive":
        return RecursiveChunker()
    elif strategy == "semantic":
        return SemanticChunker()
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
```

---

## Step 5: Vector Store - –í—ã–±–æ—Ä –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Vector Databases (2025)

| Database | Best For | Performance | Cost |
|----------|----------|-------------|------|
| **ChromaDB** | –ü—Ä–æ—Ç–æ—Ç–∏–ø—ã, < 10M vectors | ~20ms p50 –Ω–∞ 100k | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ |
| **Qdrant** | Production, filtering | Rust-fast, sub-50ms | Open-source |
| **Pinecone** | Managed production | sub-50ms at scale | $$$ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ù–∞—á–Ω–∏—Ç–µ —Å ChromaDB –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞, –º–∏–≥—Ä–∏—Ä—É–π—Ç–µ –Ω–∞ Qdrant –¥–ª—è production.

–ò—Å—Ç–æ—á–Ω–∏–∫: [Firecrawl Vector DB Comparison 2025](https://www.firecrawl.dev/blog/best-vector-databases-2025)

```python
# rag/vectorstore.py
"""
Vector Store –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Qdrant –∏ ChromaDB.

–ü–æ–¥ –∫–∞–ø–æ—Ç–æ–º:
1. Embeddings: —Ç–µ–∫—Å—Ç -> –≤–µ–∫—Ç–æ—Ä (OpenAI text-embedding-3)
2. Indexing: HNSW –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ approximate nearest neighbor search
3. Storage: –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫–µ –∏–ª–∏ –≤ –æ–±–ª–∞–∫–µ
"""
from typing import List, Optional, Dict, Any, Tuple
from abc import ABC, abstractmethod
import logging

from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from langchain_qdrant import QdrantVectorStore
from langchain_community.vectorstores import Chroma
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

from .config import config

logger = logging.getLogger(__name__)


class BaseVectorStore(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –¥–ª—è vector store."""

    @abstractmethod
    def add_documents(self, documents: List[Document]) -> List[str]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ store."""
        pass

    @abstractmethod
    def similarity_search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict] = None
    ) -> List[Document]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        pass

    @abstractmethod
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """–ü–æ–∏—Å–∫ —Å relevance scores."""
        pass


class QdrantStore(BaseVectorStore):
    """
    Qdrant Vector Store - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production.

    –ü–æ—á–µ–º—É Qdrant:
    - –ù–∞–ø–∏—Å–∞–Ω –Ω–∞ Rust (–±—ã—Å—Ç—Ä—ã–π!)
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - Scalar quantization (4-8x —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ hybrid search –∏–∑ –∫–æ—Ä–æ–±–∫–∏

    –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://qdrant.tech/documentation/
    """

    def __init__(
        self,
        collection_name: str = config.collection_name,
        url: Optional[str] = None,
        api_key: Optional[str] = None,
        path: str = "./data/qdrant",
    ):
        """
        Args:
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            url: URL Qdrant Cloud (–µ—Å–ª–∏ None - –ª–æ–∫–∞–ª—å–Ω—ã–π)
            api_key: API key –¥–ª—è Qdrant Cloud
            path: –ü—É—Ç—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        # Embeddings –º–æ–¥–µ–ª—å
        # text-embedding-3-large: 3072 dimensions, –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ
        # text-embedding-3-small: 1536 dimensions, –¥–µ—à–µ–≤–ª–µ
        self.embeddings = OpenAIEmbeddings(
            model=config.embedding_model
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_dim = 3072 if "large" in config.embedding_model else 1536

        # –ö–ª–∏–µ–Ω—Ç Qdrant
        if url:
            # Cloud mode
            self.client = QdrantClient(url=url, api_key=api_key)
        else:
            # Local mode —Å persistence
            self.client = QdrantClient(path=path)

        self.collection_name = collection_name
        self._ensure_collection()

        # LangChain wrapper
        self.vectorstore = QdrantVectorStore(
            client=self.client,
            collection_name=collection_name,
            embedding=self.embeddings,
        )

    def _ensure_collection(self):
        """–°–æ–∑–¥–∞—ë—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        collections = self.client.get_collections().collections
        exists = any(c.name == self.collection_name for c in collections)

        if not exists:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.embedding_dim,
                    distance=Distance.COSINE,  # Cosine similarity
                ),
            )
            logger.info(f"Created collection: {self.collection_name}")

    def add_documents(self, documents: List[Document]) -> List[str]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å embeddings."""
        ids = self.vectorstore.add_documents(documents)
        logger.info(f"Added {len(ids)} documents to Qdrant")
        return ids

    def similarity_search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict] = None
    ) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            filter: –§–∏–ª—å—Ç—Ä—ã –ø–æ metadata (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"file_type": ".pdf"})
        """
        return self.vectorstore.similarity_search(
            query, k=k, filter=filter
        )

    def similarity_search_with_score(
        self,
        query: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        """–ü–æ–∏—Å–∫ —Å cosine similarity scores (0-1, –±–æ–ª—å—à–µ = –ª—É—á—à–µ)."""
        return self.vectorstore.similarity_search_with_score(query, k=k)

    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        info = self.client.get_collection(self.collection_name)
        return {
            "name": self.collection_name,
            "vectors_count": info.vectors_count,
            "points_count": info.points_count,
            "status": info.status,
        }


class ChromaStore(BaseVectorStore):
    """
    ChromaDB - –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è.

    –ü–ª—é—Å—ã:
    - Zero setup (embedded mode)
    - –ü—Ä–æ—Å—Ç–æ–π API
    - –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è < 10M vectors

    –ú–∏–Ω—É—Å—ã:
    - –ù–µ –¥–ª—è production scale
    - –ú–µ–Ω–µ–µ –∑—Ä–µ–ª—ã–π —á–µ–º Qdrant/Pinecone
    """

    def __init__(
        self,
        collection_name: str = config.collection_name,
        persist_directory: str = "./data/chroma",
    ):
        self.embeddings = OpenAIEmbeddings(model=config.embedding_model)

        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory=persist_directory,
        )
        self.collection_name = collection_name

    def add_documents(self, documents: List[Document]) -> List[str]:
        ids = self.vectorstore.add_documents(documents)
        logger.info(f"Added {len(ids)} documents to Chroma")
        return ids

    def similarity_search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[Dict] = None
    ) -> List[Document]:
        return self.vectorstore.similarity_search(query, k=k, filter=filter)

    def similarity_search_with_score(
        self,
        query: str,
        k: int = 5
    ) -> List[Tuple[Document, float]]:
        return self.vectorstore.similarity_search_with_score(query, k=k)


def get_vectorstore(
    store_type: str = config.vectorstore_type
) -> BaseVectorStore:
    """–§–∞–±—Ä–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è vector store."""
    if store_type == "qdrant":
        return QdrantStore()
    elif store_type == "chroma":
        return ChromaStore()
    else:
        raise ValueError(f"Unknown store type: {store_type}")
```

---

## Step 6: Hybrid Search + Reranking

```python
# rag/retriever.py
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π retriever —Å Hybrid Search –∏ Reranking.

Hybrid Search –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç:
- Vector search (semantic similarity) - –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª
- BM25 (keyword search) - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤

–†–µ–∑—É–ª—å—Ç–∞—Ç: recall +20% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —á–∏—Å—Ç—ã–º vector search.
https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking

Reranking (Cohere/BGE):
- Cross-encoder –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–∞—Ä—ã (query, document)
- –¢–æ—á–Ω–µ–µ —á–µ–º bi-encoder embeddings
- –ù–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –ø–æ—ç—Ç–æ–º—É –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ top-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
- –£–ª—É—á—à–∞–µ—Ç precision –Ω–∞ 25-35%
"""
from typing import List, Optional, Tuple
import logging

from langchain.schema import Document
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from rank_bm25 import BM25Okapi
import cohere

from .vectorstore import BaseVectorStore, get_vectorstore
from .config import config

logger = logging.getLogger(__name__)


class HybridRetriever:
    """
    Hybrid Retriever = Vector Search + BM25 + Reranking.

    Pipeline:
    1. Parallel: vector search (top-K) + BM25 search (top-K)
    2. Fusion: Reciprocal Rank Fusion –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    3. Reranking: Cohere rerank –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    4. Return: top-N –ª—É—á—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    –ü—Ä–∏–º–µ—Ä:
        retriever = HybridRetriever(documents)
        results = retriever.retrieve("What is RAG?", k=5)
    """

    def __init__(
        self,
        documents: Optional[List[Document]] = None,
        vectorstore: Optional[BaseVectorStore] = None,
        use_hybrid: bool = config.use_hybrid_search,
        use_reranking: bool = config.use_reranking,
        hybrid_alpha: float = config.hybrid_alpha,
    ):
        """
        Args:
            documents: –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è BM25 (–Ω—É–∂–Ω—ã –µ—Å–ª–∏ use_hybrid=True)
            vectorstore: Vector store (—Å–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –µ—Å–ª–∏ None)
            use_hybrid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å hybrid search
            use_reranking: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å reranking
            hybrid_alpha: –í–µ—Å vector search (1.0 = —Ç–æ–ª—å–∫–æ vector, 0.0 = —Ç–æ–ª—å–∫–æ BM25)
        """
        self.use_hybrid = use_hybrid
        self.use_reranking = use_reranking
        self.hybrid_alpha = hybrid_alpha

        # Vector store
        self.vectorstore = vectorstore or get_vectorstore()

        # BM25 retriever (–¥–ª—è hybrid search)
        self.bm25_retriever = None
        if use_hybrid and documents:
            self._init_bm25(documents)

        # Cohere reranker
        self.reranker = None
        if use_reranking:
            self._init_reranker()

    def _init_bm25(self, documents: List[Document]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç BM25 retriever.

        BM25 (Best Matching 25) - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞:
        - TF (Term Frequency): —á–∞—Å—Ç–æ—Ç–∞ —Ç–µ—Ä–º–∏–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        - IDF (Inverse Document Frequency): —Ä–µ–¥–∫–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞ –≤ –∫–æ—Ä–ø—É—Å–µ
        - Length normalization: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–∞–¥ vector search:
        - –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, –∫–æ–¥—ã, –∏–º–µ–Ω–∞)
        - –ù–µ —Ç—Ä–µ–±—É–µ—Ç GPU/API
        - Interpretable
        """
        self.bm25_retriever = BM25Retriever.from_documents(
            documents,
            k=config.retrieval_k,
        )
        logger.info(f"BM25 initialized with {len(documents)} documents")

    def _init_reranker(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Cohere Reranker.

        –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç cross-encoder reranking:
        1. Bi-encoder (embeddings): encode(query) –∏ encode(doc) –æ—Ç–¥–µ–ª—å–Ω–æ
           - –ë—ã—Å—Ç—Ä–æ, –Ω–æ —Ç–µ—Ä—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É query –∏ doc
        2. Cross-encoder (reranker): encode(query + doc) –≤–º–µ—Å—Ç–µ
           - –ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –≤–∏–¥–∏—Ç –≤—Å–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏
           - –ü–æ—ç—Ç–æ–º—É –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫ top-N –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º

        Cohere Rerank 3.5:
        - 32K context window
        - +25% precision –Ω–∞ challenging queries
        - –î–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ AWS Bedrock
        """
        try:
            self.reranker = CohereRerank(
                model=config.rerank_model,
                top_n=config.final_k,
            )
            logger.info(f"Cohere reranker initialized: {config.rerank_model}")
        except Exception as e:
            logger.warning(f"Cohere reranker init failed: {e}. Continuing without reranking.")
            self.use_reranking = False

    def retrieve(
        self,
        query: str,
        k: Optional[int] = None,
        filter: Optional[dict] = None,
    ) -> List[Document]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (default: config.final_k)
            filter: –§–∏–ª—å—Ç—Ä—ã –ø–æ metadata

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        k = k or config.final_k

        # Step 1: Initial retrieval
        if self.use_hybrid and self.bm25_retriever:
            docs = self._hybrid_retrieve(query, filter)
        else:
            docs = self._vector_retrieve(query, filter)

        # Step 2: Reranking
        if self.use_reranking and self.reranker and len(docs) > 0:
            docs = self._rerank(query, docs)

        # Step 3: Limit to k
        return docs[:k]

    def _vector_retrieve(
        self,
        query: str,
        filter: Optional[dict] = None
    ) -> List[Document]:
        """Pure vector search."""
        return self.vectorstore.similarity_search(
            query,
            k=config.retrieval_k,
            filter=filter,
        )

    def _hybrid_retrieve(
        self,
        query: str,
        filter: Optional[dict] = None
    ) -> List[Document]:
        """
        Hybrid search —Å Reciprocal Rank Fusion.

        RRF Score = sum(1 / (k + rank_i)) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ retriever
        –≥–¥–µ k = 60 (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è)

        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–Ω–≥–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å scores.
        """
        # Vector retriever
        vector_retriever = self.vectorstore.vectorstore.as_retriever(
            search_kwargs={"k": config.retrieval_k}
        )

        # Ensemble —Å –≤–µ—Å–∞–º–∏
        # alpha = 0.7 –æ–∑–Ω–∞—á–∞–µ—Ç 70% –≤–µ—Å–∞ vector, 30% –≤–µ—Å–∞ BM25
        ensemble = EnsembleRetriever(
            retrievers=[vector_retriever, self.bm25_retriever],
            weights=[self.hybrid_alpha, 1 - self.hybrid_alpha],
        )

        docs = ensemble.invoke(query)
        logger.debug(f"Hybrid search returned {len(docs)} documents")
        return docs

    def _rerank(
        self,
        query: str,
        documents: List[Document]
    ) -> List[Document]:
        """
        Reranking —Å Cohere.

        Cross-encoder –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞–∂–¥—É—é –ø–∞—Ä—É (query, document)
        –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç relevance score.
        """
        if not documents:
            return []

        try:
            # Cohere rerank —á–µ—Ä–µ–∑ LangChain
            compressed_docs = self.reranker.compress_documents(
                documents=documents,
                query=query,
            )
            logger.debug(f"Reranking: {len(documents)} -> {len(compressed_docs)} documents")
            return list(compressed_docs)
        except Exception as e:
            logger.warning(f"Reranking failed: {e}. Returning original docs.")
            return documents

    def retrieve_with_scores(
        self,
        query: str,
        k: Optional[int] = None,
    ) -> List[Tuple[Document, float]]:
        """Retrieval —Å relevance scores (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ UI)."""
        k = k or config.final_k
        return self.vectorstore.similarity_search_with_score(query, k=k)
```

---

## Step 7: RAG Chain

```python
# rag/chain.py
"""
RAG Chain - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç.

–î–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ (–ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ LangChain 2025):
1. RAG Agent: LLM —Ä–µ—à–∞–µ—Ç –∫–æ–≥–¥–∞ –∏—Å–∫–∞—Ç—å (–≥–∏–±–∫–æ, 2+ –≤—ã–∑–æ–≤–∞ LLM)
2. RAG Chain: –≤—Å–µ–≥–¥–∞ –∏—â–µ—Ç (–±—ã—Å—Ç—Ä–æ, 1 –≤—ã–∑–æ–≤ LLM)

–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º Chain –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏.
"""
from typing import List, Dict, Any, Optional, Generator
import logging

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

from .retriever import HybridRetriever
from .config import config

logger = logging.getLogger(__name__)


# === PROMPT ENGINEERING ===
# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç! –ü–ª–æ—Ö–æ–π –ø—Ä–æ–º–ø—Ç = –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏.

SYSTEM_PROMPT = """You are a precise assistant that answers questions based ONLY on the provided context.

## STRICT RULES:
1. Answer ONLY using information from the context below
2. If the context doesn't contain the answer, say: "I don't have enough information to answer this question based on the available documents."
3. NEVER make up information or use knowledge outside the context
4. Cite sources using [1], [2] etc. format
5. Be concise but comprehensive
6. Use the same language as the question

## Context:
{context}

## Question: {question}

## Answer:"""

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
ANALYTICAL_PROMPT = """You are an expert analyst. Analyze the provided context to answer the question thoroughly.

## Instructions:
1. Use ONLY the provided context
2. Structure your answer with clear sections
3. Cite sources with [1], [2] format
4. If information is incomplete, state what's missing
5. Provide actionable insights where appropriate

## Context:
{context}

## Question: {question}

## Analysis:"""


class RAGChain:
    """
    Production RAG Chain —Å best practices.

    Features:
    - Hybrid retrieval + reranking
    - Structured prompts —Å citations
    - Streaming support
    - Source tracking
    """

    def __init__(
        self,
        retriever: HybridRetriever,
        model_name: str = config.llm_model,
        temperature: float = config.llm_temperature,
        prompt_type: str = "default",
    ):
        """
        Args:
            retriever: Configured HybridRetriever
            model_name: OpenAI model (gpt-4o, gpt-4o-mini)
            temperature: 0.0 –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, 0.3+ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
            prompt_type: "default" –∏–ª–∏ "analytical"
        """
        self.retriever = retriever

        # LLM
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            # Streaming –¥–ª—è –ª—É—á—à–µ–≥–æ UX
            streaming=True,
        )

        # Prompt template
        template = ANALYTICAL_PROMPT if prompt_type == "analytical" else SYSTEM_PROMPT
        self.prompt = ChatPromptTemplate.from_template(template)

        # Output parser
        self.output_parser = StrOutputParser()

    def _format_docs(self, docs: List[Document]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM.

        –§–æ—Ä–º–∞—Ç:
        [1] Source: filename.pdf (page 5)
        Content here...

        [2] Source: another.md
        More content...
        """
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "Unknown")
            page = doc.metadata.get("page", "")
            page_info = f" (page {page})" if page else ""

            formatted.append(
                f"[{i}] Source: {source}{page_info}\n{doc.page_content}"
            )

        return "\n\n---\n\n".join(formatted)

    def query(self, question: str) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç RAG query.

        Returns:
            {
                "question": str,
                "answer": str,
                "sources": List[{
                    "content": str,
                    "source": str,
                    "score": float (if available)
                }]
            }
        """
        # Step 1: Retrieve
        docs = self.retriever.retrieve(question)

        if not docs:
            return {
                "question": question,
                "answer": "I couldn't find any relevant information in the documents.",
                "sources": []
            }

        # Step 2: Format context
        context = self._format_docs(docs)

        # Step 3: Generate
        chain = self.prompt | self.llm | self.output_parser
        answer = chain.invoke({
            "context": context,
            "question": question,
        })

        # Step 4: Format sources
        sources = [
            {
                "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                "source": doc.metadata.get("source", "Unknown"),
                "page": doc.metadata.get("page"),
                "chunk_id": doc.metadata.get("chunk_id"),
            }
            for doc in docs
        ]

        return {
            "question": question,
            "answer": answer,
            "sources": sources,
        }

    def stream(self, question: str) -> Generator[str, None, None]:
        """
        Streaming –æ—Ç–≤–µ—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ UX.

        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
            for chunk in chain.stream("What is RAG?"):
                print(chunk, end="", flush=True)
        """
        docs = self.retriever.retrieve(question)

        if not docs:
            yield "I couldn't find any relevant information in the documents."
            return

        context = self._format_docs(docs)
        chain = self.prompt | self.llm | self.output_parser

        for chunk in chain.stream({
            "context": context,
            "question": question,
        }):
            yield chunk

    async def aquery(self, question: str) -> Dict[str, Any]:
        """Async –≤–µ—Ä—Å–∏—è query –¥–ª—è –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π."""
        docs = self.retriever.retrieve(question)

        if not docs:
            return {
                "question": question,
                "answer": "I couldn't find any relevant information.",
                "sources": []
            }

        context = self._format_docs(docs)
        chain = self.prompt | self.llm | self.output_parser

        answer = await chain.ainvoke({
            "context": context,
            "question": question,
        })

        sources = [
            {
                "content": doc.page_content[:300] + "...",
                "source": doc.metadata.get("source", "Unknown"),
            }
            for doc in docs
        ]

        return {
            "question": question,
            "answer": answer,
            "sources": sources,
        }
```

---

## Step 8: Evaluation —Å RAGAS

```python
# rag/evaluation.py
"""
Evaluation RAG —Å–∏—Å—Ç–µ–º—ã —Å RAGAS metrics.

RAGAS (Retrieval Augmented Generation Assessment) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.
https://docs.ragas.io/

–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
1. Faithfulness - –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ? (anti-hallucination)
2. Answer Relevancy - –æ—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤–æ–ø—Ä–æ—Å—É?
3. Context Precision - –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω?
4. Context Recall - –≤—Å–µ –Ω—É–∂–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–π–¥–µ–Ω—ã?

–¢–∏–ø–∏—á–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ (2025):
- Faithfulness > 0.9 (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
- Answer Relevancy > 0.85
- Context Precision > 0.7
- Context Recall > 0.7
"""
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class EvaluationSample:
    """–û–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –¥–ª—è evaluation."""
    question: str
    answer: str
    contexts: List[str]  # Retrieved contexts
    ground_truth: Optional[str] = None  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)


class RAGEvaluator:
    """
    Evaluator –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        evaluator = RAGEvaluator()

        samples = [
            EvaluationSample(
                question="What is RAG?",
                answer="RAG is...",
                contexts=["RAG stands for..."],
                ground_truth="RAG is Retrieval Augmented Generation..."
            )
        ]

        results = evaluator.evaluate(samples)
        print(results)
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAGAS."""
        try:
            from ragas import evaluate
            from ragas.metrics import (
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
            )
            self.evaluate_fn = evaluate
            self.metrics = [
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
            ]
            self._available = True
        except ImportError:
            logger.warning("RAGAS not installed. Run: pip install ragas")
            self._available = False

    def evaluate(
        self,
        samples: List[EvaluationSample],
        metrics: Optional[List[str]] = None,
    ) -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ RAG —Å–∏—Å—Ç–µ–º—ã.

        Args:
            samples: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            metrics: –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—Ç—å (default: –≤—Å–µ)

        Returns:
            {"faithfulness": 0.92, "answer_relevancy": 0.88, ...}
        """
        if not self._available:
            return self._fallback_evaluate(samples)

        from datasets import Dataset

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ RAGAS —Ñ–æ—Ä–º–∞—Ç
        data = {
            "question": [s.question for s in samples],
            "answer": [s.answer for s in samples],
            "contexts": [s.contexts for s in samples],
        }

        # Ground truth –Ω—É–∂–µ–Ω –¥–ª—è context_recall
        if all(s.ground_truth for s in samples):
            data["ground_truth"] = [s.ground_truth for s in samples]

        dataset = Dataset.from_dict(data)

        # Evaluate
        results = self.evaluate_fn(
            dataset=dataset,
            metrics=self.metrics,
        )

        return dict(results)

    def _fallback_evaluate(
        self,
        samples: List[EvaluationSample]
    ) -> Dict[str, float]:
        """
        –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑ RAGAS (–¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏).

        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–∞–∑–æ–≤—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏:
        - –û—Ç–≤–µ—Ç –Ω–µ –ø—É—Å—Ç–æ–π
        - –û—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        - –û—Ç–≤–µ—Ç –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–π –¥–ª–∏–Ω—ã
        """
        scores = {
            "answer_not_empty": 0.0,
            "uses_context": 0.0,
            "reasonable_length": 0.0,
        }

        for sample in samples:
            # Check non-empty
            if sample.answer.strip():
                scores["answer_not_empty"] += 1

            # Check uses context words
            context_words = set()
            for ctx in sample.contexts:
                context_words.update(ctx.lower().split())
            answer_words = set(sample.answer.lower().split())
            overlap = len(context_words & answer_words)
            if overlap > 5:
                scores["uses_context"] += 1

            # Check reasonable length
            if 50 < len(sample.answer) < 2000:
                scores["reasonable_length"] += 1

        # Normalize
        n = len(samples)
        return {k: v / n for k, v in scores.items()}


def create_test_dataset(
    questions: List[str],
    ground_truths: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    """
    –°–æ–∑–¥–∞—ë—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è evaluation.

    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —Ç–µ—Å—Ç–æ–≤:
    1. –í–∫–ª—é—á–∞–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –≤–æ–ø—Ä–æ—Å–æ–≤ (factoid, analytical, comparison)
    2. –î–æ–±–∞–≤–ª—è–π—Ç–µ edge cases (–≤–æ–ø—Ä–æ—Å—ã –±–µ–∑ –æ—Ç–≤–µ—Ç–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö)
    3. –ú–∏–Ω–∏–º—É–º 20-50 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    4. –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    dataset = []
    for i, q in enumerate(questions):
        item = {"question": q}
        if ground_truths and i < len(ground_truths):
            item["ground_truth"] = ground_truths[i]
        dataset.append(item)
    return dataset
```

---

## Step 9: Streamlit UI

```python
# app.py
"""
Streamlit UI –¥–ª—è RAG Chatbot.

–ó–∞–ø—É—Å–∫: streamlit run app.py
"""
import os
from pathlib import Path
import logging

import streamlit as st
from dotenv import load_dotenv

from rag.document_loader import DocumentLoader
from rag.chunking import get_chunker
from rag.vectorstore import get_vectorstore
from rag.retriever import HybridRetriever
from rag.chain import RAGChain
from rag.config import config

# Setup
load_dotenv()
logging.basicConfig(level=logging.INFO)

# Page config
st.set_page_config(
    page_title="RAG Chatbot",
    page_icon="[BOT]",
    layout="wide"
)


# === Session State Initialization ===
def init_session_state():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏."""
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = get_vectorstore()

    if "documents" not in st.session_state:
        st.session_state.documents = []

    if "rag_chain" not in st.session_state:
        st.session_state.rag_chain = None


init_session_state()


# === Sidebar: Document Management ===
with st.sidebar:
    st.title("[DOC] Document Management")

    # Upload
    uploaded_files = st.file_uploader(
        "Upload documents",
        type=["pdf", "txt", "md", "docx"],
        accept_multiple_files=True
    )

    if uploaded_files:
        if st.button("Process Documents", type="primary"):
            with st.spinner("Processing documents..."):
                loader = DocumentLoader()
                chunker = get_chunker("recursive")

                # Temp save
                temp_dir = Path("./temp_uploads")
                temp_dir.mkdir(exist_ok=True)

                all_chunks = []
                for file in uploaded_files:
                    file_path = temp_dir / file.name
                    with open(file_path, "wb") as f:
                        f.write(file.getbuffer())

                    # Load and chunk
                    docs = loader.load_file(str(file_path))
                    chunks = chunker.split(docs)
                    all_chunks.extend(chunks)

                    st.write(f"- {file.name}: {len(chunks)} chunks")

                # Add to vectorstore
                st.session_state.vectorstore.add_documents(all_chunks)
                st.session_state.documents.extend(all_chunks)

                # Initialize RAG chain
                retriever = HybridRetriever(
                    documents=st.session_state.documents,
                    vectorstore=st.session_state.vectorstore,
                )
                st.session_state.rag_chain = RAGChain(retriever)

                st.success(f"Processed {len(all_chunks)} chunks from {len(uploaded_files)} files")

                # Cleanup
                for file in temp_dir.glob("*"):
                    file.unlink()

    st.divider()

    # Stats
    try:
        if hasattr(st.session_state.vectorstore, 'get_stats'):
            stats = st.session_state.vectorstore.get_stats()
            st.metric("Documents in DB", stats.get("vectors_count", 0))
    except Exception:
        st.metric("Documents loaded", len(st.session_state.documents))

    # Settings
    st.subheader("[GEAR] Settings")

    retrieval_k = st.slider(
        "Number of sources",
        min_value=1,
        max_value=10,
        value=config.final_k,
        help="How many documents to retrieve"
    )

    use_hybrid = st.checkbox(
        "Hybrid Search",
        value=config.use_hybrid_search,
        help="Combine vector + keyword search"
    )

    use_rerank = st.checkbox(
        "Reranking",
        value=config.use_reranking,
        help="Use Cohere reranker for better precision"
    )

    if st.button("Clear Chat"):
        st.session_state.messages = []
        st.rerun()


# === Main Chat Interface ===
st.title("[BOT] RAG Chatbot")
st.caption("Ask questions about your documents")

# Check if system is ready
if not st.session_state.rag_chain:
    st.info("Please upload documents in the sidebar to get started.")

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # Show sources for assistant messages
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("[DOC] Sources"):
                for i, source in enumerate(message["sources"], 1):
                    st.markdown(f"**[{i}] {source['source']}**")
                    st.caption(source["content"])

# Chat input
if prompt := st.chat_input("Ask a question..."):
    if not st.session_state.rag_chain:
        st.warning("Please upload documents first!")
    else:
        # Add user message
        st.session_state.messages.append({
            "role": "user",
            "content": prompt
        })

        with st.chat_message("user"):
            st.markdown(prompt)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                result = st.session_state.rag_chain.query(prompt)

                st.markdown(result["answer"])

                # Show sources
                if result["sources"]:
                    with st.expander("[DOC] Sources"):
                        for i, source in enumerate(result["sources"], 1):
                            st.markdown(f"**[{i}] {source['source']}**")
                            st.caption(source["content"])

        # Save to history
        st.session_state.messages.append({
            "role": "assistant",
            "content": result["answer"],
            "sources": result["sources"],
        })
```

---

## Step 10: –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ –∫–∞–∫ –∏—Ö –∏–∑–±–µ–∂–∞—Ç—å

### 80% RAG –ø—Ä–æ–µ–∫—Ç–æ–≤ —Ç–µ—Ä–ø—è—Ç –Ω–µ—É–¥–∞—á—É. –í–æ—Ç –ø–æ—á–µ–º—É:

–ü–æ –¥–∞–Ω–Ω—ã–º [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/07/silent-killers-of-production-rag/), –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ enterprise RAG –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–µ –¥–æ—Ö–æ–¥—è—Ç –¥–æ production. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:

| –ü—Ä–æ–±–ª–µ–º–∞ | –°–∏–º–ø—Ç–æ–º—ã | –†–µ—à–µ–Ω–∏–µ |
|----------|----------|---------|
| **–ü–ª–æ—Ö–æ–π chunking** | –û—Ç–≤–µ—Ç—ã –Ω–µ—Ç–æ—á–Ω—ã–µ, –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è | –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ recursive/semantic chunking, 256-512 —Ç–æ–∫–µ–Ω–æ–≤ |
| **–¢–æ–ª—å–∫–æ vector search** | –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã | Hybrid search (vector + BM25) |
| **–ù–µ—Ç reranking** | –ú–Ω–æ–≥–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ | –î–æ–±–∞–≤—å—Ç–µ Cohere/BGE reranker |
| **–ú–æ–Ω–æ–ª–∏—Ç–Ω–∞—è knowledge base** | –†–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–±—É—é—Ç —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π | –†–∞–∑–¥–µ–ª–∏—Ç–µ –ø–æ –¥–æ–º–µ–Ω–∞–º, –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –æ—Ç–¥–µ–ª—å–Ω–æ |
| **Vendor lock-in** | –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –æ–¥–Ω–æ–≥–æ LLM/API | –ê–±—Å—Ç—Ä–∞–∫—Ü–∏–∏, fallback –Ω–∞ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ |
| **–ù–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞** | –ù–µ –∑–Ω–∞–µ—Ç–µ –∫–æ–≥–¥–∞ —Å–∏—Å—Ç–µ–º–∞ –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç | LangFuse/LangSmith —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ |
| **–£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ** | Knowledge base –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π re-indexing pipeline |

### Debugging checklist:

```python
# –ß–µ–∫-–ª–∏—Å—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ RAG —Å–∏—Å—Ç–µ–º—ã

def diagnose_rag_issues(rag_chain, test_questions):
    """
    –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ç–∏–ø–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º.
    """
    issues = []

    for q in test_questions:
        result = rag_chain.query(q)

        # 1. –ü—É—Å—Ç–æ–π retrieval
        if not result["sources"]:
            issues.append({
                "type": "empty_retrieval",
                "question": q,
                "fix": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ: –¥–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã? Embeddings —Å–æ–≤–ø–∞–¥–∞—é—Ç?"
            })
            continue

        # 2. Irrelevant sources
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–≤–µ—Ä–∫—É relevance

        # 3. Hallucination (–æ—Ç–≤–µ—Ç –Ω–µ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö)
        # –¢—Ä–µ–±—É–µ—Ç RAGAS evaluation

        # 4. Incomplete answer
        if len(result["answer"]) < 50:
            issues.append({
                "type": "short_answer",
                "question": q,
                "fix": "–í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∏–π prompt"
            })

    return issues
```

---

## Step 11: Production Deployment

### Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  rag-app:
    build: .
    ports:
      - "8501:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - COHERE_API_KEY=${COHERE_API_KEY}
      - RAG_VECTORSTORE_TYPE=qdrant
      - QDRANT_URL=http://qdrant:6333
    depends_on:
      - qdrant
    volumes:
      - ./documents:/app/documents

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

volumes:
  qdrant_data:
```

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create directories
RUN mkdir -p documents data

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

### –ó–∞–ø—É—Å–∫

```bash
# Development
streamlit run app.py

# Production —Å Docker
docker-compose up -d

# –û—Ç–∫—Ä–æ–µ—Ç—Å—è: http://localhost:8501
```

---

## 8 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä RAG (2025)

–ü–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º [Humanloop](https://humanloop.com/blog/rag-architectures):

1. **Simple RAG** - –±–∞–∑–æ–≤—ã–π retrieval + generation
2. **RAG with Memory** - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
3. **Branched RAG** - –≤—ã–±–∏—Ä–∞–µ—Ç –Ω—É–∂–Ω—ã–π source –ø–æ —Ç–∏–ø—É –≤–æ–ø—Ä–æ—Å–∞
4. **HyDE** - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
5. **Adaptive RAG** - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é retrieval
6. **Corrective RAG (CRAG)** - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–∏—Å–∫
7. **Self-RAG** - –º–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç –∫–æ–≥–¥–∞ –∏ —á—Ç–æ –∏—Å–∫–∞—Ç—å
8. **Agentic RAG** - –∞–≥–µ–Ω—Ç—ã –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É—é—Ç —Å–ª–æ–∂–Ω—ã–µ multi-step –∑–∞–ø—Ä–æ—Å—ã

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ù–∞—á–Ω–∏—Ç–µ —Å Simple RAG + Hybrid Search + Reranking. –≠—Ç–æ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç 90% use cases.

---

## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (RAGAS)

–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ [RAGAS Documentation](https://docs.ragas.io/en/latest/concepts/metrics/):

| –ú–µ—Ç—Ä–∏–∫–∞ | –ß—Ç–æ –∏–∑–º–µ—Ä—è–µ—Ç | Target |
|---------|-------------|--------|
| **Faithfulness** | –û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (anti-hallucination) | > 0.9 |
| **Answer Relevancy** | –û—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤–æ–ø—Ä–æ—Å—É | > 0.85 |
| **Context Precision** | –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã | > 0.7 |
| **Context Recall** | –í—Å–µ –Ω—É–∂–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–π–¥–µ–Ω—ã | > 0.7 |

```python
# –ü—Ä–∏–º–µ—Ä evaluation
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

results = evaluate(
    dataset=test_dataset,
    metrics=[faithfulness, answer_relevancy]
)
print(results)
# {"faithfulness": 0.92, "answer_relevancy": 0.88}
```

---

## Observability

–î–ª—è production –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

**LangFuse** (open-source, self-hosted):
- Tracing –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
- Latency breakdown (retrieval vs generation)
- Cost tracking
- A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤

```python
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LangFuse
from langfuse.callback import CallbackHandler

langfuse_handler = CallbackHandler()

chain.invoke(
    {"question": "What is RAG?"},
    config={"callbacks": [langfuse_handler]}
)
```

**LangSmith** (managed, LangChain team):
- –ì–ª—É–±–æ–∫–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangChain
- Production monitoring
- Debugging tools

–ò—Å—Ç–æ—á–Ω–∏–∫–∏: [LangFuse](https://langfuse.com/blog/2025-10-28-rag-observability-and-evals), [LangSmith](https://www.metacto.com/blogs/what-is-langsmith-a-comprehensive-guide-to-llm-observability)

---

## Production Checklist

–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –≤ production —É–±–µ–¥–∏—Ç–µ—Å—å:

### Indexing Pipeline
- [ ] –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫ (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã)
- [ ] Chunk size –æ–ø—Ç–∏–º–∞–ª–µ–Ω (256-512 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è factoid, 1024+ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏)
- [ ] Chunk overlap 10-20% —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
- [ ] Metadata –≤–∫–ª—é—á–∞–µ—Ç source, page, timestamp

### Retrieval Quality
- [ ] Hybrid search –≤–∫–ª—é—á—ë–Ω (vector + BM25)
- [ ] Reranking –≤–∫–ª—é—á—ë–Ω (Cohere –∏–ª–∏ BGE)
- [ ] –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 20+ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–æ–∑–¥–∞–Ω
- [ ] Context Precision > 0.7
- [ ] Context Recall > 0.7

### Generation Quality
- [ ] –ü—Ä–æ–º–ø—Ç —è–≤–Ω–æ –∑–∞–ø—Ä–µ—â–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
- [ ] Faithfulness > 0.9 (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ —á–µ—Ä–µ–∑ RAGAS)
- [ ] Answer Relevancy > 0.85
- [ ] –û—Ç–≤–µ—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—Ç citations [1], [2]

### Operations
- [ ] Observability –Ω–∞—Å—Ç—Ä–æ–µ–Ω (LangFuse/LangSmith)
- [ ] Latency < 3s –Ω–∞ p95
- [ ] Error handling –¥–ª—è API failures
- [ ] Rate limiting –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [ ] Costs –º–æ–Ω–∏—Ç–æ—Ä—è—Ç—Å—è

### Security
- [ ] API keys –≤ environment variables
- [ ] –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥ —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ—Ç—Å—è
- [ ] PII –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ª–æ–≥–∞—Ö

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

–ü–æ—Å–ª–µ –æ—Å–≤–æ–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ RAG, –∏–∑—É—á–∏—Ç–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:

```
–≠—Ç–æ—Ç tutorial (Simple RAG)
        ‚îÇ
        ‚îú‚îÄ‚îÄ –£–ª—É—á—à–µ–Ω–∏–µ Retrieval
        ‚îÇ   ‚îú‚îÄ‚îÄ [[rag-advanced-techniques]] ‚Üí HyDE, Query Expansion
        ‚îÇ   ‚îú‚îÄ‚îÄ [[embeddings-complete-guide]] ‚Üí Fine-tuning embeddings
        ‚îÇ   ‚îî‚îÄ‚îÄ [[vector-databases-guide]] ‚Üí Scaling, production
        ‚îÇ
        ‚îú‚îÄ‚îÄ –£–ª—É—á—à–µ–Ω–∏–µ Generation
        ‚îÇ   ‚îú‚îÄ‚îÄ [[prompt-engineering-masterclass]] ‚Üí Structured outputs
        ‚îÇ   ‚îî‚îÄ‚îÄ [[ai-cost-optimization]] ‚Üí –£–º–µ–Ω—å—à–µ–Ω–∏–µ costs
        ‚îÇ
        ‚îî‚îÄ‚îÄ Production
            ‚îú‚îÄ‚îÄ [[ai-observability-monitoring]] ‚Üí Monitoring, alerts
            ‚îî‚îÄ‚îÄ [[ai-devops-deployment]] ‚Üí Kubernetes, autoscaling
```

**–°–ª–µ–¥—É—é—â–∏–π —É—Ä–æ–≤–µ–Ω—å:**
- **Agentic RAG** ‚Äî LLM —Å–∞–º —Ä–µ—à–∞–µ—Ç –∫–æ–≥–¥–∞ –∏ —á—Ç–æ –∏—Å–∫–∞—Ç—å
- **Multi-modal RAG** ‚Äî –ø–æ–∏—Å–∫ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ –≤–∏–¥–µ–æ
- **Self-RAG** ‚Äî –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤–æ–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### Tutorials & Guides
- [LangChain RAG Tutorial](https://docs.langchain.com/oss/python/langchain/rag) - Official LangChain documentation
- [Botpress: How to Build a RAG Chatbot in 2025](https://botpress.com/blog/build-rag-chatbot)
- [n8n: Build a Custom Knowledge RAG Chatbot](https://blog.n8n.io/rag-chatbot/)

### Architecture & Best Practices
- [Humanloop: 8 RAG Architectures](https://humanloop.com/blog/rag-architectures)
- [orq.ai: RAG Architecture Explained](https://orq.ai/blog/rag-architecture)
- [Analytics Vidhya: Enterprise RAG Failures](https://www.analyticsvidhya.com/blog/2025/07/silent-killers-of-production-rag/)
- [Label Studio: Seven RAG Pitfalls](https://labelstud.io/blog/seven-ways-your-rag-system-could-be-failing-and-how-to-fix-them/)

### Chunking Strategies
- [Firecrawl: Best Chunking Strategies for RAG in 2025](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025)
- [DataCamp: Chunking Strategies](https://www.datacamp.com/blog/chunking-strategies)
- [Weaviate: Chunking Strategies](https://weaviate.io/blog/chunking-strategies-for-rag)

### Vector Databases
- [Firecrawl: Best Vector Databases 2025](https://www.firecrawl.dev/blog/best-vector-databases-2025)
- [Medium: Vector Database Comparison](https://medium.com/tech-ai-made-easy/vector-database-comparison-pinecone-vs-weaviate-vs-qdrant-vs-faiss-vs-milvus-vs-chroma-2025-15bf152f891d)

### Hybrid Search & Reranking
- [Superlinked: Optimizing RAG with Hybrid Search & Reranking](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
- [Weaviate: Hybrid Search Explained](https://weaviate.io/blog/hybrid-search-explained)
- [Cohere Rerank](https://cohere.com/rerank)
- [Analytics Vidhya: Top Rerankers for RAG](https://www.analyticsvidhya.com/blog/2025/06/top-rerankers-for-rag/)

### Evaluation
- [RAGAS Documentation](https://docs.ragas.io/en/stable/)
- [FutureAGI: RAG Evaluation Metrics 2025](https://futureagi.com/blogs/rag-evaluation-metrics-2025)
- [Cohorte: Evaluating RAG Systems in 2025](https://www.cohorte.co/blog/evaluating-rag-systems-in-2025-ragas-deep-dive-giskard-showdown-and-the-future-of-context)

### Observability
- [LangFuse: RAG Observability](https://langfuse.com/blog/2025-10-28-rag-observability-and-evals)
- [Langflow: LLM Observability Explained](https://www.langflow.org/blog/llm-observability-explained-feat-langfuse-langsmith-and-langwatch)
- [Firecrawl: Best LLM Observability Tools](https://www.firecrawl.dev/blog/best-llm-observability-tools)

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏

- [[rag-advanced-techniques]] - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ RAG —Ç–µ—Ö–Ω–∏–∫–∏ (Agentic RAG, Self-RAG)
- [[embeddings-complete-guide]] - Embeddings –º–æ–¥–µ–ª–∏ –∏ fine-tuning
- [[vector-databases-guide]] - –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ vector databases
- [[ai-observability-monitoring]] - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ AI —Å–∏—Å—Ç–µ–º
- [[langchain-masterclass]] - –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ LangChain

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ö–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è production-ready RAG chatbot?
> Document loader + chunker, embedding model, vector store (Qdrant/ChromaDB), retrieval pipeline (query -> search -> rerank), LLM –¥–ª—è generation, chat memory (conversation history), –∏ frontend (Streamlit/Gradio –∏–ª–∏ API). –î–ª—è production –¥–æ–±–∞–≤–∏—Ç—å: evaluation, caching, rate limiting, observability.

> [!question]- –ö–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å conversation memory –≤ RAG chatbot?
> Short-term: –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (window memory). Long-term: summarization –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π. –î–ª—è RAG: chat history –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è reformulation –∑–∞–ø—Ä–æ—Å–∞ (standalone question), —á—Ç–æ–±—ã retrieval —Ä–∞–±–æ—Ç–∞–ª –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –ü—Ä–∏–º–µ—Ä: "–ê —á—Ç–æ –Ω–∞—Å—á—ë—Ç —Ü–µ–Ω—ã?" -> "–ö–∞–∫–æ–≤–∞ —Ü–µ–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∞ X, —É–ø–æ–º—è–Ω—É—Ç–æ–≥–æ —Ä–∞–Ω–µ–µ?"

> [!question]- –ö–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ RAG chatbot –∏ –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?
> Retrieval: precision@k, recall@k, MRR. Generation: faithfulness (–Ω–µ—Ç hallucination), relevance (–æ—Ç–≤–µ—Ç –ø–æ —Ç–µ–º–µ), completeness. End-to-end: answer correctness vs ground truth. User metrics: satisfaction score, follow-up rate. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: RAGAS framework –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏.

> [!question]- –ö–∞–∫–∏–µ –æ—à–∏–±–∫–∏ —á–∞—Å—Ç–æ –¥–æ–ø—É—Å–∫–∞—é—Ç –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ RAG chatbot?
> –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ/–º–∞–ª–µ–Ω—å–∫–∏–µ chunks, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ overlap, –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ metadata –ø—Ä–∏ retrieval, –Ω–µ—Ç reranking (–ø–æ–ª–∞–≥–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ embedding similarity), –Ω–µ—Ç evaluation pipeline, –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ fallback –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –≤–Ω–µ scope –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ò–∑ –∫–∞–∫–∏—Ö —ç—Ç–∞–ø–æ–≤ —Å–æ—Å—Ç–æ–∏—Ç RAG pipeline?
?
Indexing: load documents -> chunk -> embed -> store in vector DB. Retrieval: user query -> embed -> similarity search -> (optional) rerank -> top-k chunks. Generation: retrieved chunks + query -> LLM prompt -> answer. Evaluation: automated metrics + human feedback.

–ß—Ç–æ —Ç–∞–∫–æ–µ reranking –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω –≤ RAG?
?
–ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ retrieved –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é cross-encoder –º–æ–¥–µ–ª–∏ (Cohere Rerank, bge-reranker). Embedding search (bi-encoder) –±—ã—Å—Ç—Ä—ã–π –Ω–æ approximate. Reranker —Ç–æ—á–Ω–µ–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç relevance –ø–∞—Ä—ã (query, document). –¢–∏–ø–∏—á–Ω—ã–π flow: retrieve 20 -> rerank -> top 5.

–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å vector store –¥–ª—è RAG chatbot?
?
–ü—Ä–æ—Ç–æ—Ç–∏–ø: ChromaDB (in-memory, –ø—Ä–æ—Å—Ç–æ–π). Production: Qdrant (Rust, –±—ã—Å—Ç—Ä—ã–π, self-hosted), Pinecone (managed, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π). PostgreSQL –ø—Ä–æ–µ–∫—Ç: pgvector. –ö—Ä–∏—Ç–µ—Ä–∏–∏: –º–∞—Å—à—Ç–∞–± –¥–∞–Ω–Ω—ã—Ö, hosting preference, filtering capabilities, –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å.

–ß—Ç–æ —Ç–∞–∫–æ–µ hybrid search –∏ –∫–æ–≥–¥–∞ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?
?
–ö–æ–º–±–∏–Ω–∞—Ü–∏—è semantic search (embeddings) –∏ keyword search (BM25). Semantic –Ω–∞—Ö–æ–¥–∏—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, keyword --- —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤. Hybrid –¥–∞—ë—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –≤–∞–∂–Ω—ã –∏ —Å–º—ã—Å–ª, –∏ —Ç–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã. Qdrant –∏ Weaviate –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –Ω–∞—Ç–∏–≤–Ω–æ.

–ö–∞–∫ –¥–µ–ø–ª–æ–∏—Ç—å RAG chatbot –≤ production?
?
Backend: FastAPI/Flask —Å async, vector store –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å. Frontend: Streamlit (–±—ã—Å—Ç—Ä–æ), React (–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π UI). Infra: Docker Compose –¥–ª—è dev, Kubernetes –¥–ª—è production. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: LangSmith/Langfuse –¥–ª—è traces, Prometheus –¥–ª—è infra. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: semantic cache –¥–ª—è —á–∞—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[tutorial-document-qa]] | –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è Document Q&A —Å–∏—Å—Ç–µ–º–∞ |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[rag-advanced-techniques]] | –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ RAG |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[api-design]] | –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API –¥–ª—è chatbot |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
