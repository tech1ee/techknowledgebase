---
title: "Mobile AI/ML: Complete Guide to On-Device Inference (2024-2025)"
date: 2025-12-29
type: guide
status: published
tags:
  - mobile
  - topic/ai-ml
  - on-device
  - npu
  - quantization
  - production
  - type/guide
  - level/intermediate
modified: 2026-02-13
reading_time: 24
difficulty: 6
study_status: not_started
mastery: 0
last_reviewed:
next_review:
related:
  - "[[ai-ml-overview-v2]]"
  - "[[local-llms-self-hosting]]"
  - "[[android-overview]]"
  - "[[ios-development]]"
  - "[[llm-inference-optimization]]"
---

# Mobile AI/ML: Complete Guide to On-Device Inference

Comprehensive guide covering SDKs, hardware acceleration, mobile-optimized models, quantization techniques, and production patterns for on-device AI/ML as of late 2025.

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ ML** | –ß—Ç–æ —Ç–∞–∫–æ–µ –º–æ–¥–µ–ª–∏, inference | [[ai-ml-overview-v2]] |
| **Mobile Development** | iOS –∏–ª–∏ Android —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ | [[android-overview]], iOS docs |
| **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ trade-offs | [[llm-inference-optimization]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **Mobile Developer** | ‚úÖ –î–∞ | –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è AI –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è |
| **AI/ML Engineer** | ‚úÖ –î–∞ | On-device deployment |
| **Product Manager** | ‚úÖ –î–∞ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π |
| **–ù–æ–≤–∏—á–æ–∫ –≤ ML** | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ | –°–Ω–∞—á–∞–ª–∞ [[ai-ml-overview-v2]] |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **On-Device AI** = AI —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä—è–º–æ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ, –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ API

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **On-Device Inference** | –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ | **–û—Ñ–ª–∞–π–Ω –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä** ‚Äî –≤—Å—ë —Å—á–∏—Ç–∞–µ—Ç —Å–∞–º —Ç–µ–ª–µ—Ñ–æ–Ω |
| **NPU** | Neural Processing Unit | **AI-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä** ‚Äî —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —á–∏–ø –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π |
| **TOPS** | Trillions of Operations Per Second | **–ú–æ—â–Ω–æ—Å—Ç—å NPU** ‚Äî —á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –±—ã—Å—Ç—Ä–µ–µ AI |
| **LiteRT** | Lite Runtime (–±—ã–≤—à–∏–π TensorFlow Lite) | **–õ—ë–≥–∫–∏–π –¥–≤–∏–∂–æ–∫** ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö |
| **ExecuTorch** | PyTorch –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ | **PyTorch –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ** ‚Äî –ø—Ä–∏–≤—ã—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –º–æ–±–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç |
| **MLX** | Apple ML framework | **ML –¥–ª—è iPhone/Mac** ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ Apple Silicon |
| **Quantization** | –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏ | **–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞** ‚Äî –º–æ–¥–µ–ª—å –º–µ–Ω—å—à–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ |
| **GGUF** | –§–æ—Ä–º–∞—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π | **–§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞** ‚Äî –∫–∞–∫ MP3 –¥–ª—è –º—É–∑—ã–∫–∏, GGUF –¥–ª—è –º–æ–¥–µ–ª–µ–π |
| **tok/s** | –¢–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É | **–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞** ‚Äî —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞ —Å–µ–∫—É–Ω–¥—É |

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [SDK Comparison](#sdk-comparison)
3. [Hardware Acceleration](#hardware-acceleration)
4. [Mobile-Optimized Models](#mobile-optimized-models)
5. [Quantization Techniques](#quantization-techniques)
6. [Production Patterns](#production-patterns)
7. [Platform Integration](#platform-integration)
8. [Decision Guide](#decision-guide)
9. [References](#references)

---

## Executive Summary

### State of Mobile AI in 2025

Mobile AI has matured from experimental to production-ready:

| Metric | 2023 | 2025 |
|--------|------|------|
| LLM on device | Experimental | Production (Gemma 3n, Llama 3.2) |
| NPU TOPS | 15-20 | 35-73 |
| Model size | 1-2GB max | 4-8GB viable |
| Inference speed | 5-10 tok/s | 20-45 tok/s |

### Key Developments (2024-2025)

1. **LiteRT** (Sep 2024) ‚Äî TensorFlow Lite rebranded, multi-framework support
2. **ExecuTorch 1.0** (Oct 2024) ‚Äî Native PyTorch deployment, 50KB runtime
3. **Gemma 3n** (2025) ‚Äî 2-4GB effective memory, multimodal
4. **NNAPI Deprecated** ‚Äî Migration to GPU delegates and NPU SDKs
5. **On-device LoRA** ‚Äî Fine-tuning now possible on mobile GPUs

---

## SDK Comparison

### Overview Matrix

| SDK | Platforms | Format | NPU | GPU | Best For |
|-----|-----------|--------|-----|-----|----------|
| **LiteRT** | Android, iOS, Web | .tflite | Via delegates | Yes | Cross-platform, general |
| **CoreML** | iOS/macOS | .mlmodel | ANE | Metal | iOS-exclusive |
| **ONNX Runtime** | All | .onnx | Via EPs | Yes | Framework agnostic |
| **MediaPipe** | Android, iOS, Web | .task | Via TFLite | Yes | Pre-built tasks |
| **ExecuTorch** | Android, iOS, Embedded | .pte | QNN, CoreML | Vulkan, Metal | PyTorch native |

### LiteRT (TensorFlow Lite)

**Status:** Production-ready, actively developed
**Rebrand:** TensorFlow Lite ‚Üí LiteRT (September 2024)

```kotlin
// Android: LiteRT inference
val interpreter = Interpreter(loadModelFile("model.tflite"))
interpreter.run(inputBuffer, outputBuffer)
```

**Key Features:**
- Multi-framework: PyTorch, JAX, TensorFlow, Keras
- Minimal runtime: ~300KB
- Hardware delegates: GPU, NNAPI (deprecated), CoreML, Hexagon

**Delegates:**
| Delegate | Platform | Acceleration |
|----------|----------|--------------|
| GPU | Android/iOS | OpenGL, Metal |
| CoreML | iOS | Apple Neural Engine |
| Hexagon | Qualcomm | DSP/NPU |
| XNNPACK | All | CPU optimization |

### CoreML

**Status:** iOS-exclusive, best for Apple devices

```swift
// iOS: CoreML inference
let config = MLModelConfiguration()
config.computeUnits = .all  // Use ANE
let model = try MyModel(configuration: config)
let prediction = try model.prediction(input: inputData)
```

**Optimization Tips:**
1. Use `.all` compute units (best performance)
2. Weight palettization (1-8 bits) for ANE
3. W8A8 quantization for A17 Pro/M4
4. EnumeratedShapes over dynamic inputs

### ONNX Runtime Mobile

**Status:** Cross-platform, framework agnostic

```java
// Android: ONNX Runtime
OrtEnvironment env = OrtEnvironment.getEnvironment();
OrtSession.SessionOptions opts = new OrtSession.SessionOptions();
opts.addXnnpack();  // CPU optimization
OrtSession session = env.createSession(modelPath, opts);
```

**Execution Providers:**
| EP | Platform | Status |
|----|----------|--------|
| CPU | All | Default |
| XNNPACK | Android/iOS | Best for float |
| CoreML | iOS | Stable |
| NNAPI | Android | **DEPRECATED** |
| QNN | Qualcomm | New recommended |

**Known Issue:** Operator coverage gaps ‚Äî some models need CPU fallback for unsupported ops.

### MediaPipe

**Status:** Best for pre-built ML tasks, experimental for LLM

**Supported Tasks:**
- Vision: Object detection, segmentation, pose
- Audio: Speech recognition
- Text: Classification
- GenAI: LLM inference (experimental)

```kotlin
// Android: MediaPipe object detection
val detector = ObjectDetector.createFromOptions(context, options)
val results = detector.detect(image)
```

**LLM API (Experimental):**
- Models: Gemma 3n, Gemma 2B, Phi-2
- Features: Multimodal, LoRA adapters
- Devices: Pixel 8+, Samsung S23+ recommended

### ExecuTorch

**Status:** Production-ready (v1.0 Oct 2024), PyTorch native

```python
# Export model for mobile
import torch
from executorch.exir import to_edge

model = MyModel()
exported = torch.export.export(model, example_inputs)
edge_program = to_edge(exported)
et_program = edge_program.to_executorch()
```

**Key Advantages:**
- No format conversion needed
- 50KB base runtime
- 12+ hardware backends
- Production-proven at Meta

**Backends:**
| Backend | Vendor | Acceleration |
|---------|--------|--------------|
| XNNPACK | ARM | CPU |
| CoreML | Apple | ANE |
| MPS | Apple | Metal GPU |
| QNN | Qualcomm | Hexagon NPU |
| Vulkan | Cross-platform | GPU |
| NeuroPilot | MediaTek | APU |

---

## Hardware Acceleration

### NPU Comparison Matrix

| SoC | NPU | TOPS | LLM Speed | Power Efficiency |
|-----|-----|------|-----------|------------------|
| Apple A18 Pro | Neural Engine | 35 | ~33 tok/s (8B) | Best |
| Snapdragon 8 Gen 3 | Hexagon | 73 | 15 tok/s (10B) | Excellent |
| MediaTek D9300+ | APU 790 | 46 | 22 tok/s (7B) | Very Good |
| Samsung Exynos 2400 | Custom | 17 | - | Good |
| Google Tensor G4 | Edge TPU | ~15 | 45 tok/s (3.5B) | Good |

### Apple Neural Engine

**Optimization for ANE:**

```swift
// Best practices for CoreML on ANE
let config = MLModelConfiguration()

// 1. Use all compute units
config.computeUnits = .all

// 2. For specific ANE targeting
config.computeUnits = .cpuAndNeuralEngine  // Skip GPU

// 3. Fixed shapes for better optimization
let constraint = MLImageConstraint(
    pixelsWide: 224,
    pixelsHigh: 224
)
```

**Key Techniques:**
1. **Weight palettization** ‚Äî Best for ANE (1-8 bits)
2. **W8A8 quantization** ‚Äî int8-int8 compute path on A17+
3. **EnumeratedShapes** ‚Äî Fixed inputs run faster
4. **Avoid custom ops** ‚Äî Fall back to CPU

### Qualcomm Hexagon NPU

**QNN Integration:**

```kotlin
// Android: LiteRT with QNN delegate
dependencies {
    implementation("com.qualcomm.qnn:qnn-runtime:2.34.0")
    implementation("com.qualcomm.qnn:qnn-litert-delegate:2.34.0")
}

// Usage
val delegate = QnnDelegate(options)
interpreter.modifyGraphWithDelegate(delegate)
```

**Performance (llama.cpp QNN backend):**
- 7-10x improvement for quantized LLM inference
- Optimized for MULMAT operations

### NPU vs GPU Power Consumption

| Metric | NPU | GPU |
|--------|-----|-----|
| Peak power | 35W | 75W |
| Power savings | 35-70% less | Baseline |
| Latency | Sub-ms | Higher |
| Batch throughput | Lower | Higher |

**When to Use:**
- **NPU:** LLM inference, sustained workloads, battery-critical
- **GPU:** Batch processing, training, graphics-heavy

---

## Mobile-Optimized Models

### LLM Models

| Model | Params | Memory | Performance | Multimodal |
|-------|--------|--------|-------------|------------|
| Gemma 3n E2B | 5B (2B eff) | ~2GB | 1300+ Elo | Yes |
| Gemma 3n E4B | 8B (4B eff) | ~3GB | GPT-4.1-nano | Yes |
| Llama 3.2 1B Q4 | 1B | ~600MB | Basic | No |
| Llama 3.2 3B Q4 | 3B | ~1.5GB | Good | No |
| Phi-3-mini | 3.8B | ~2GB | 69% MMLU | No |
| Qwen3-0.6B | 0.6B | ~400MB | Strong | No |

### Gemma 3n (Google, 2025)

**Architecture: MatFormer**
- Nested smaller models within larger
- Per-Layer Embeddings (PLE)
- Selective parameter activation

**Features:**
- Multimodal: text, image, audio, video
- 140+ languages
- E2B: 2GB RAM, E4B: 3GB RAM

### Llama 3.2 Quantized (Meta, Oct 2024)

**Quantization Techniques:**
1. QAT with LoRA (QLoRA)
2. SpinQuant (post-training)

**Performance:**
| Metric | Improvement |
|--------|-------------|
| Decode speed | 2-4x |
| Prefill speed | 5x |
| Model size | -56% |
| Memory | -41% |

**Device Testing:**
- OnePlus 12: Full support
- Samsung S24+: 1B and 3B
- Samsung S22: 1B only

### Vision Models

| Model | Size | Task | SDK |
|-------|------|------|-----|
| MobileViT | ~6M | Classification | CoreML, TFLite |
| EfficientNet-Lite | 5-25M | Classification | TFLite |
| YOLO Nano/Small | 3-10M | Detection | CoreML, TFLite |
| MobileSAM | ~10M | Segmentation | ExecuTorch |

### Audio Models

| Model | Size | Task | Latency |
|-------|------|------|---------|
| Whisper Tiny | 39M | STT | Real-time |
| Whisper Base | 74M | STT | Real-time |
| OpenWakeWord | ~10M | Wake word | <50ms |

---

## Quantization Techniques

### GGUF Format

**Naming Convention:**
```
Q + [bits] + [type] + [variant]

Q4_0    ‚Äî 4-bit legacy
Q4_K_M  ‚Äî 4-bit K-quant medium (recommended)
Q5_K_M  ‚Äî 5-bit K-quant medium
Q8_0    ‚Äî 8-bit legacy
```

### Recommended Formats

| Device | RAM | Format | Use Case |
|--------|-----|--------|----------|
| iPhone 15 Pro+ | 8GB | Q5_K_M | Quality priority |
| iPhone 13/14 | 6GB | Q4_K_M | General |
| High-end Android | 12GB+ | Q5_K_M/Q8_0 | Quality |
| Mid-range Android | 8GB | Q4_K_M | General |
| Entry-level | 4-6GB | Q4_K_M (small) | Basic |

### Quality vs Size Trade-offs

| Format | Compression | Quality Loss | Recommendation |
|--------|-------------|--------------|----------------|
| FP16 | 1x | 0% | Development |
| Q8_0 | 2x | ~1% | Quality-critical |
| Q6_K | 2.7x | ~2% | Good balance |
| Q5_K_M | 3.2x | ~3% | Mac/iPad sweet spot |
| Q4_K_M | 4x | ~5% | Best for iPhone |
| Q2_K | 8x | ~20%+ | Not recommended |

### K-Quant vs Legacy

**K-Quant Advantages:**
- Adaptive precision by layer importance
- Super blocks reduce memory overhead
- Better quality at same bit width

### Mobile Quantization Best Practices

1. **Start with Q4_K_M** ‚Äî Best balance for mobile
2. **Test on real device** ‚Äî Simulator differs
3. **Measure latency** ‚Äî Not just accuracy
4. **Consider memory** ‚Äî Leave room for app
5. **Profile power** ‚Äî Battery drain matters

---

## Production Patterns

### Hybrid Cloud+Edge Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Request      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Confidence Check   ‚îÇ
‚îÇ  (threshold: 0.8)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ           ‚îÇ
   High        Low
     ‚îÇ           ‚îÇ
     ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇOn-Device‚îÇ ‚îÇ Cloud   ‚îÇ
‚îÇInference‚îÇ ‚îÇ Fallback‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fallback Strategies:**
- Confidence-based: prediction < threshold
- Out-of-domain: input doesn't match training
- Timeout-based: local inference too slow

### Model Caching

**Cache Types:**
| Type | Purpose | Implementation |
|------|---------|----------------|
| Model Cache | Loaded models | Keep warm |
| KV Cache | LLM context | Token chunks |
| Prompt Cache | Repeated queries | Hash lookup |
| Semantic Cache | Similar queries | Embedding |

**KV Cache Optimization:**
- 2 orders of magnitude latency reduction
- Token-level granularity
- LCTRU eviction policy

### Model Preloading

```kotlin
// Android: Background preloading
class MyApplication : Application() {
    override fun onCreate() {
        super.onCreate()
        CoroutineScope(Dispatchers.IO).launch {
            ModelManager.preload("model.tflite")
        }
    }
}
```

```swift
// iOS: Async preloading
class AppDelegate: UIApplicationDelegate {
    func application(...) -> Bool {
        Task.detached {
            try await ModelManager.shared.preload()
        }
        return true
    }
}
```

### OTA Model Updates

**A/B Partition Strategy:**
```
Partition A (active, v1.0)
Partition B (staging, v1.1)
          ‚Üì
     Validation
          ‚Üì
    Success ‚Üí Swap A‚ÜîB
    Failure ‚Üí Rollback
```

**Best Practices:**
1. Staged rollouts: 1% ‚Üí 10% ‚Üí 100%
2. Automatic rollback on failure
3. Version compatibility checks
4. Device capability tracking

### Privacy & Federated Learning

**Industry Adoption:**
- Google: Gboard, "Hey Google"
- Apple: Siri, Photos
- Meta: AI assistants with FL-DP

**Privacy Techniques:**
| Technique | Protection |
|-----------|------------|
| Differential Privacy | Individual data |
| Secure Aggregation | Model updates |
| Local-only Inference | No data transfer |

### Observability

**Key Metrics:**
| Category | Metrics |
|----------|---------|
| Performance | Latency, throughput |
| Accuracy | Confidence, corrections |
| Resource | Memory, battery, CPU/GPU |
| Health | Drift, staleness, version |

**Tools:**
- Arize AI: LLM observability
- Dynatrace: A/B testing
- Custom: OpenTelemetry integration

---

## Platform Integration

### Android (Kotlin)

```kotlin
// LiteRT setup
class AIService(context: Context) {
    private val interpreter: Interpreter

    init {
        val options = Interpreter.Options().apply {
            addDelegate(GpuDelegate())
            setNumThreads(4)
        }
        interpreter = Interpreter(loadModel(context), options)
    }

    fun predict(input: FloatArray): FloatArray {
        val output = FloatArray(outputSize)
        interpreter.run(input, output)
        return output
    }
}
```

### iOS (Swift)

```swift
// CoreML setup
class AIService {
    private var model: MLModel?

    func load() async throws {
        let config = MLModelConfiguration()
        config.computeUnits = .all
        model = try await MyModel.load(configuration: config)
    }

    func predict(input: MLFeatureProvider) throws -> MLFeatureProvider {
        return try model!.prediction(from: input)
    }
}
```

### Flutter

```dart
// TensorFlow Lite Flutter
import 'package:tflite_flutter/tflite_flutter.dart';

class AIService {
  late Interpreter _interpreter;

  Future<void> load() async {
    _interpreter = await Interpreter.fromAsset('model.tflite');
  }

  List<double> predict(List<double> input) {
    var output = List<double>.filled(outputSize, 0);
    _interpreter.run(input, output);
    return output;
  }
}
```

### React Native

```javascript
// react-native-fast-tflite
import { loadModel } from 'react-native-fast-tflite';

const model = await loadModel('model.tflite');
const output = model.run([inputData]);
```

### Kotlin Multiplatform

```kotlin
// Shared code with expect/actual
expect class AIService() {
    suspend fun load()
    fun predict(input: FloatArray): FloatArray
}

// Android actual
actual class AIService {
    private lateinit var interpreter: Interpreter
    // TFLite implementation
}

// iOS actual
actual class AIService {
    private var model: MLModel? = null
    // CoreML implementation
}
```

---

## Decision Guide

### SDK Selection Flowchart

```
Need mobile AI?
‚îú‚îÄ‚îÄ iOS only?
‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí CoreML (best ANE)
‚îÇ
‚îú‚îÄ‚îÄ Android only?
‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí LiteRT (best ecosystem)
‚îÇ
‚îú‚îÄ‚îÄ Cross-platform?
‚îÇ   ‚îú‚îÄ‚îÄ PyTorch source? ‚Üí ExecuTorch
‚îÇ   ‚îú‚îÄ‚îÄ Max compatibility? ‚Üí ONNX Runtime
‚îÇ   ‚îî‚îÄ‚îÄ Pre-built tasks? ‚Üí MediaPipe
‚îÇ
‚îî‚îÄ‚îÄ LLM specifically?
    ‚îú‚îÄ‚îÄ Production? ‚Üí ExecuTorch/CoreML
    ‚îî‚îÄ‚îÄ Prototype? ‚Üí MediaPipe LLM API
```

### Model Selection by Device

| Device Class | Max Memory | Recommended Model |
|--------------|------------|-------------------|
| iPhone 15 Pro+ | 4-6GB | Gemma 3n E4B, Phi-3 |
| iPhone 13/14 | 2-3GB | Gemma 3n E2B, Llama 3.2 3B |
| Flagship Android | 6-8GB | Most models |
| Mid-range Android | 3-4GB | Llama 3.2 3B, Qwen3-1.7B |
| Entry Android | 1-2GB | Qwen3-0.6B, Llama 3.2 1B |

### Quantization Selection

```
Quality priority?
‚îú‚îÄ‚îÄ YES ‚Üí Q8_0 or Q5_K_M
‚îî‚îÄ‚îÄ NO (size/speed priority)
    ‚îú‚îÄ‚îÄ iPhone ‚Üí Q4_K_M
    ‚îú‚îÄ‚îÄ High-end Android ‚Üí Q5_K_M
    ‚îî‚îÄ‚îÄ Mid-range ‚Üí Q4_K_M
```

### Hardware Acceleration Selection

```
Model type?
‚îú‚îÄ‚îÄ Quantized (INT8/4)
‚îÇ   ‚îî‚îÄ‚îÄ NPU (best efficiency)
‚îÇ
‚îú‚îÄ‚îÄ Float32
‚îÇ   ‚îî‚îÄ‚îÄ GPU or XNNPACK
‚îÇ
‚îú‚îÄ‚îÄ LLM
‚îÇ   ‚îî‚îÄ‚îÄ NPU (sustained low power)
‚îÇ
‚îî‚îÄ‚îÄ Batch processing
    ‚îî‚îÄ‚îÄ GPU (throughput)
```

---

## References

### Official Documentation

- [LiteRT (TensorFlow Lite)](https://ai.google.dev/edge/litert)
- [CoreML](https://developer.apple.com/documentation/coreml)
- [ONNX Runtime Mobile](https://onnxruntime.ai/docs/tutorials/mobile/)
- [MediaPipe](https://ai.google.dev/edge/mediapipe)
- [ExecuTorch](https://pytorch.org/executorch/)

### Research Papers

- [Gemma 3n Architecture](https://ai.google.dev/gemma/docs/gemma-3n)
- [Llama 3.2 Quantization](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- [On-Device Language Models Survey](https://arxiv.org/html/2409.00088v1)

### Research Reports (This Guide)

- [Mobile AI SDKs Research](../docs/research/2025-12-29-mobile-ai-sdks.md)
- [NPU Hardware Acceleration](../docs/research/2025-12-29-mobile-npu-acceleration.md)
- [Mobile Models & Quantization](../docs/research/2025-12-29-mobile-models-quantization.md)
- [Production Mobile AI Patterns](../docs/research/2025-12-29-mobile-ai-production.md)

---

*Last updated: 2025-12-29*
*Research depth: 100+ sources across 4 deep research sessions*

---

## –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏

**[[ai-ml-overview-v2]]** ‚Äî –û–±—â–∏–π –æ–±–∑–æ—Ä AI/ML Engineering –¥–∞—ë—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–æ–±–∏–ª—å–Ω—ã–π AI. On-device inference ‚Äî —ç—Ç–æ –æ–¥–Ω–∞ –∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π deployment, –Ω–∞—Ä—è–¥—É —Å cloud API –∏ self-hosted —Ä–µ—à–µ–Ω–∏—è–º–∏. –ü–æ–Ω–∏–º–∞–Ω–∏–µ –æ–±—â–µ–π –∫–∞—Ä—Ç–∏–Ω—ã –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –æ —Ç–æ–º, –∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤—ã–Ω–æ—Å–∏—Ç—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –∞ –∫–∞–∫–∏–µ –æ—Å—Ç–∞–≤–∏—Ç—å –≤ –æ–±–ª–∞–∫–µ, —É—á–∏—Ç—ã–≤–∞—è trade-offs –º–µ–∂–¥—É latency, privacy, —Å—Ç–æ–∏–º–æ—Å—Ç—å—é –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º.

**[[llm-inference-optimization]]** ‚Äî –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ –º–æ–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–µ—à–∞—é—Ç –æ–¥–Ω—É –∏ —Ç—É –∂–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É: memory bandwidth bottleneck. –¢–µ—Ö–Ω–∏–∫–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ (AWQ, GPTQ, FP8), –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤ —Å–µ—Ä–≤–µ—Ä–Ω–æ–º –≥–∞–π–¥–µ, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö GGUF –∏ K-quant. KV cache management, –æ–ø–∏—Å–∞–Ω–Ω—ã–π —á–µ—Ä–µ–∑ PagedAttention –Ω–∞ —Å–µ—Ä–≤–µ—Ä–∞—Ö, –∏–º–µ–µ—Ç –º–æ–±–∏–ª—å–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏ —Å –µ—â—ë –±–æ–ª–µ–µ –∂—ë—Å—Ç–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –ø–∞–º—è—Ç–∏.

**[[android-overview]]** ‚Äî Android ‚Äî –æ–¥–Ω–∞ –∏–∑ –¥–≤—É—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è on-device AI. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LiteRT (TensorFlow Lite), ExecuTorch –∏ QNN delegate –¥–ª—è Qualcomm Hexagon NPU –æ–ø–∏—Å–∞–Ω–∞ –≤ –¥–∞–Ω–Ω–æ–º –º–∞—Ç–µ—Ä–∏–∞–ª–µ. –ì–∞–π–¥ –ø–æ Android –¥–∞—ë—Ç –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã: lifecycle management, background work, permissions ‚Äî –≤—Å—ë, —á—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–±–∏–ª—å–Ω–æ–≥–æ AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —á—Ç–µ–Ω–∏–µ

- **Huyen, C. (2022). *Designing Machine Learning Systems*. O'Reilly.** ‚Äî –†–∞–∑–¥–µ–ª—ã –æ–± edge deployment, model compression –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ inference –Ω–∞–ø—Ä—è–º—É—é –ø—Ä–∏–º–µ–Ω–∏–º—ã –∫ –º–æ–±–∏–ª—å–Ω–æ–º—É AI. –ö–Ω–∏–≥–∞ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏, pruning –∏ knowledge distillation —Å —Å–∏—Å—Ç–µ–º–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è.

- **Goodfellow, I., Bengio, Y. & Courville, A. (2016). *Deep Learning*. MIT Press.** ‚Äî –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–≥–æ, –ø–æ—á–µ–º—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç (–∏–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç) –∏ –∫–∞–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.

- **Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.** ‚Äî –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è trade-offs –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ (Q4_K_M vs Q8_0) –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤.

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ö–∞–∫–∏–µ SDK –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è on-device AI –Ω–∞ Android –∏ iOS?
> Android: MediaPipe (Google, —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π), TensorFlow Lite (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π), ExecuTorch (Meta, PyTorch models). iOS: Core ML (Apple native), Create ML (–æ–±—É—á–µ–Ω–∏–µ), MLX (Apple Silicon optimized). –ö—Ä–æ—Å—Å-–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ: ONNX Runtime, MediaPipe. –î–ª—è LLM: llama.cpp —á–µ—Ä–µ–∑ C++ binding.

> [!question]- –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç –¥–ª—è on-device AI –∏ –∫–∞–∫ –∏—Ö –æ–±—Ö–æ–¥–∏—Ç—å?
> –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (RAM), latency (CPU/NPU), battery drain, thermal throttling. –û–±—Ö–æ–¥—ã: –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (INT8/INT4), model pruning, knowledge distillation (–º–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —É –±–æ–ª—å—à–æ–π), NPU delegation, –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (–ø—Ä–æ—Å—Ç–æ–µ on-device, —Å–ª–æ–∂–Ω–æ–µ –≤ –æ–±–ª–∞–∫–µ).

> [!question]- –ß—Ç–æ —Ç–∞–∫–æ–µ NPU –∏ –∫–∞–∫ –æ–Ω —É—Å–∫–æ—Ä—è–µ—Ç AI –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö?
> Neural Processing Unit --- —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–∏–ø –¥–ª—è ML inference. –ù–∞ iPhone: Apple Neural Engine (16-core), –Ω–∞ Android: Qualcomm Hexagon, Google Tensor TPU, Samsung NPU. –£—Å–∫–æ—Ä–µ–Ω–∏–µ 5-10x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å CPU, –ø—Ä–∏ –º–µ–Ω—å—à–µ–º —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏. –î–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ Core ML (iOS) –∏–ª–∏ NNAPI (Android).

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ö–∞–∫–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö?
?
TFLite (.tflite) –¥–ª—è TensorFlow Lite, Core ML (.mlmodel/.mlpackage) –¥–ª—è iOS, ONNX (.onnx) –∫—Ä–æ—Å—Å-–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π, ExecuTorch (.pte) –¥–ª—è PyTorch Mobile, –∏ GGUF –¥–ª—è llama.cpp. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–µ–∂–¥—É —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ —á–µ—Ä–µ–∑ ONNX –∫–∞–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π.

–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π?
?
–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –∏–∑ float32 –≤ int8/int4, —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –≤ 2-4x –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ inference. Post-training quantization: –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–æ—â–µ). Quantization-aware training: –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Ç–æ—á–Ω–µ–µ). –î–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö --- INT8 —Å—Ç–∞–Ω–¥–∞—Ä—Ç, INT4 –¥–ª—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

–ß—Ç–æ —Ç–∞–∫–æ–µ MediaPipe –∏ –∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –æ–Ω —Ä–µ—à–∞–µ—Ç?
?
Google framework –¥–ª—è on-device ML: object detection, face mesh, pose estimation, hand tracking, text classification, LLM inference. –ö—Ä–æ—Å—Å-–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π (Android, iOS, Web, Python). –í–∫–ª—é—á–∞–µ—Ç pre-trained –º–æ–¥–µ–ª–∏ –∏ pipeline –¥–ª—è custom –∑–∞–¥–∞—á.

–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å on-device AI vs cloud API?
?
On-device: offline access, real-time (camera, audio), privacy-sensitive data, –Ω–∏–∑–∫–∞—è latency (<50ms). Cloud: —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ (GPT-4 level), –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏, —Ä–µ–¥–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã. –ì–∏–±—Ä–∏–¥–Ω—ã–π: simple tasks on-device, complex tasks in cloud. –†–µ—à–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç UX –∏ connectivity.

–ö–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å battery consumption –¥–ª—è on-device AI?
?
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å NPU/GPU delegation –≤–º–µ—Å—Ç–æ CPU, batch inference (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞—á–∫–∞–º–∏), throttle —á–∞—Å—Ç–æ—Ç—É inference, model caching (–Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ), –∏ background processing —Å low priority. iOS: Energy Gauges –≤ Xcode, Android: Battery Historian.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[local-llms-self-hosting]] | –ó–∞–ø—É—Å–∫ LLM –ª–æ–∫–∞–ª—å–Ω–æ |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[llm-inference-optimization]] | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è inference |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[android-app-startup-performance]] | –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
