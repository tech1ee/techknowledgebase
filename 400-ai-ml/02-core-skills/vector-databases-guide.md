---
title: "Vector Databases: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
created: 2025-12-24
updated: 2026-02-13
author: AI Assistant
reading_time: 75
difficulty: 6
study_status: not_started
mastery: 0
last_reviewed:
next_review:
level: intermediate-advanced
type: guide
topics:
  - vector-database
  - embeddings
  - similarity-search
  - RAG
  - HNSW
  - IVF
  - PQ
  - DiskANN
  - hybrid-search
  - production
status: published
tags:
  - topic/ai-ml
  - type/guide
  - level/intermediate
related:
  - "[[embeddings-complete-guide]]"
  - "[[rag-advanced-techniques]]"
  - "[[aiml-databases-complete]]"
---

# Vector Databases: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **Embeddings** | –ö–∞–∫ —Ç–µ–∫—Å—Ç –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä | [[embeddings-complete-guide]] |
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ SQL** | –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –ë–î | [[databases-fundamentals-complete]] |
| **Python** | –ü—Ä–∏–º–µ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ | –õ—é–±–æ–π –∫—É—Ä—Å Python |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ AI** | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ | –°–Ω–∞—á–∞–ª–∞ [[embeddings-complete-guide]] |
| **Backend Developer** | ‚úÖ –î–∞ | –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è vector search –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è |
| **AI/ML Engineer** | ‚úÖ –î–∞ | RAG, semantic search, similarity |
| **DevOps** | ‚úÖ –î–∞ | Deployment –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Vector Database** = –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞ "–ø–æ —Å–º—ã—Å–ª—É", –∞ –Ω–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Vector/Embedding** | –ß–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞ | **–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã** ‚Äî –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç = —Ç–æ—á–∫–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–º—ã—Å–ª–æ–≤ |
| **Similarity Search** | –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ | **–ù–∞–π—Ç–∏ —Å–æ—Å–µ–¥–µ–π** ‚Äî –∫—Ç–æ –±–ª–∏–∂–µ –≤—Å–µ–≥–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ |
| **Cosine Similarity** | –ú–µ—Ä–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (—É–≥–æ–ª –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏) | **–£–≥–æ–ª –∑—Ä–µ–Ω–∏—è** ‚Äî –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ = –ø–æ—Ö–æ–∂–µ |
| **HNSW** | Hierarchical Navigable Small World | **–ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π** ‚Äî –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–æ—Å–µ–¥–µ–π —Å–æ—Å–µ–¥–µ–π |
| **ANN** | Approximate Nearest Neighbors | **–ü—Ä–∏–º–µ—Ä–Ω—ã–π –ø–æ–∏—Å–∫** ‚Äî –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —Ç–æ—á–Ω—ã–π, –Ω–æ –Ω–µ 100% |
| **Index** | –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ | **–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ** ‚Äî –Ω–µ —á–∏—Ç–∞–µ–º –≤—Å—ë, —Å—Ä–∞–∑—É –∫ –Ω—É–∂–Ω–æ–º—É |
| **Hybrid Search** | Vector + keyword –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–µ | **–î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞** ‚Äî –∏ –ø–æ —Å–º—ã—Å–ª—É, –∏ –ø–æ —Å–ª–æ–≤–∞–º |
| **Recall** | –î–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ | **–ü–æ–ª–Ω–æ—Ç–∞** ‚Äî —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–≥–æ –Ω–∞—à–ª–∏ –∏–∑ –≤—Å–µ–≥–æ –Ω—É–∂–Ω–æ–≥–æ |

---

## –ù–∞—á–Ω–µ–º —Å –∏–Ω—Ç—É–∏—Ü–∏–∏

–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —É —Ç–µ–±—è –µ—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–æ—Ç–æ –∫–æ—Ç–µ–Ω–∫–∞ –∏ —Ö–æ—á–µ—Ç –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ö–∞–∫ –±—ã —Ç—ã —ç—Ç–æ —Å–¥–µ–ª–∞–ª?

–ü–µ—Ä–≤–∞—è –º—ã—Å–ª—å: "–°—Ä–∞–≤–Ω—é –∫–∞–∂–¥—É—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —Å –∑–∞–ø—Ä–æ—Å–æ–º". –ù–æ –º–∏–ª–ª–∏–æ–Ω —Å—Ä–∞–≤–Ω–µ–Ω–∏–π –∑–∞–π–º–µ—Ç —Å–µ–∫—É–Ω–¥—ã –∏–ª–∏ –¥–∞–∂–µ –º–∏–Ω—É—Ç—ã. –ê –µ—Å–ª–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –º–∏–ª–ª–∏–∞—Ä–¥?

–í—Ç–æ—Ä–∞—è –º—ã—Å–ª—å: "–ú–æ–∂–µ—Ç, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å —Ñ–æ—Ç–æ –ø–æ –ø–∞–ø–∫–∞–º?" –û—Ç–ª–∏—á–Ω–∞—è –∏–¥–µ—è! –ù–æ –ø–æ –∫–∞–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º? –¶–≤–µ—Ç? –û–±—ä–µ–∫—Ç—ã? –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ? –ò –∫–∞–∫ –∏—Å–∫–∞—Ç—å "–ø–æ—Ö–æ–∂–∏–µ", –∞ –Ω–µ "—Ç–æ—á–Ω–æ —Ç–∞–∫–∏–µ –∂–µ"?

–ò–º–µ–Ω–Ω–æ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç **vector databases** -- —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ö—Ä–∞–Ω—è—Ç –Ω–µ –ø—Ä–æ—Å—Ç–æ –¥–∞–Ω–Ω—ã–µ, –∞ –∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ "–æ—Ç–ø–µ—á–∞—Ç–∫–∏" (embeddings) –∏ —É–º–µ—é—Ç –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ –æ—Ç–ø–µ—á–∞—Ç–∫–∏ —Å—Ä–µ–¥–∏ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.

---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ

### –ü—Ä–æ–±–ª–µ–º–∞: keyword search –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª

| –°–∏–º–ø—Ç–æ–º | –ü—Ä–∏—á–∏–Ω–∞ | –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è |
|---------|---------|-------------|
| –ü–æ–∏—Å–∫ "–º–∞—à–∏–Ω–∞" –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç "–∞–≤—Ç–æ–º–æ–±–∏–ª—å" | Keyword matching –∏—â–µ—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ |
| RAG –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç | BM25 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è TF-IDF, –Ω–µ —Å–µ–º–∞–Ω—Ç–∏–∫—É | LLM –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É–µ—Ç |
| –ü–æ–∏—Å–∫ –ø–æ –º–∏–ª–ª–∏–æ–Ω–∞–º –∑–∞–Ω–∏–º–∞–µ—Ç —Å–µ–∫—É–Ω–¥—ã | Brute-force O(n) –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å | UX –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º |
| –°–ª–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç | Exact match –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç "–ø–æ—Ö–æ–∂–µ—Å—Ç—å" | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç |

### –ö–∞–∫ vector databases —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã

| –ó–∞–¥–∞—á–∞ | –ë–µ–∑ vector DB | –° vector DB |
|--------|---------------|-------------|
| **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** | Keyword + —Å–∏–Ω–æ–Ω–∏–º—ã –≤—Ä—É—á–Ω—É—é | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–º—ã—Å–ª–∞ |
| **RAG retrieval** | BM25 fails –Ω–∞ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∞—Ö | 95%+ recall –Ω–∞ semantic queries |
| **Latency –Ω–∞ –º–∏–ª–ª–∏–æ–Ω–∞—Ö** | –°–µ–∫—É–Ω–¥—ã (brute-force) | <50ms (HNSW, ANN) |
| **–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã** | Category matching | Semantic similarity |
| **Multi-modal search** | –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —Ç–µ–∫—Å—Ç–∞/–∫–∞—Ä—Ç–∏–Ω–æ–∫ | –ï–¥–∏–Ω—ã–π embedding space |

---

## –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–ü–æ—á–µ–º—É –æ–±—ã—á–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç](#–ø–æ—á–µ–º—É-–æ–±—ã—á–Ω—ã–µ-–±–∞–∑—ã-–¥–∞–Ω–Ω—ã—Ö-–Ω–µ-–ø–æ–¥—Ö–æ–¥—è—Ç)
2. [–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç vector databases](#–∫–∞–∫-—Ä–∞–±–æ—Ç–∞—é—Ç-vector-databases)
3. [–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: –º–∞–≥–∏—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞](#–∞–ª–≥–æ—Ä–∏—Ç–º—ã-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)
4. [–ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è: –∫–∞–∫—É—é –≤—ã–±—Ä–∞—Ç—å](#–º–µ—Ç—Ä–∏–∫–∏-—Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è)
5. [–û–±–∑–æ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö 2025](#–æ–±–∑–æ—Ä-–±–∞–∑-–¥–∞–Ω–Ω—ã—Ö-2025)
6. [pgvector: –∫–æ–≥–¥–∞ PostgreSQL –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ](#pgvector)
7. [Hybrid Search: –ª—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤](#hybrid-search)
8. [Reranking: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](#reranking)
9. [Embedding Models: –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏](#embedding-models)
10. [Chunking Strategies: –∫–∞–∫ —Ä–∞–∑–±–∏–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã](#chunking-strategies)
11. [Multi-tenancy: –∏–∑–æ–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö](#multi-tenancy)
12. [Production Best Practices](#production-best-practices)
13. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–∏—Å—Ç–æ—Ä–∏–∏)
14. [–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —Å–≤–æ—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö](#–∫–∞–∫-–≤—ã–±—Ä–∞—Ç—å-–±–∞–∑—É-–¥–∞–Ω–Ω—ã—Ö)
15. [–ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è](#–ø—Ä–æ–≤–µ—Ä—å-—Å–µ–±—è)

---

## –ü–æ—á–µ–º—É –æ–±—ã—á–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç

### –ü—Ä–æ–±–ª–µ–º–∞ –Ω–∞ –ø–∞–ª—å—Ü–∞—Ö

–î–æ–ø—É—Å—Ç–∏–º, —Ç—ã –æ–ø–∏—Å–∞–ª –∫–∞–∂–¥—É—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é –≤ –≤–∏–¥–µ –≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ 1536 —á–∏—Å–µ–ª (—ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä embedding –æ—Ç OpenAI). –ö–∞–∂–¥–æ–µ —á–∏—Å–ª–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –∫–∞–∫—É—é-—Ç–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è -- —è—Ä–∫–æ—Å—Ç—å, –Ω–∞–ª–∏—á–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É.

–¢–µ–ø–µ—Ä—å —É —Ç–µ–±—è 10 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–∞–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –ß—Ç–æ–±—ã –Ω–∞–π—Ç–∏ 10 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∫ –∑–∞–ø—Ä–æ—Å—É, –Ω—É–∂–Ω–æ:

```
10,000,000 –≤–µ–∫—Ç–æ—Ä–æ–≤ x 1536 –∏–∑–º–µ—Ä–µ–Ω–∏–π = 15.36 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –æ–ø–µ—Ä–∞—Ü–∏–π
```

–î–∞–∂–µ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –∂–µ–ª–µ–∑–µ —ç—Ç–æ –∑–∞–π–º–µ—Ç —Å–µ–∫—É–Ω–¥—ã. –ê –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∂–¥–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç.

### –ü–æ—á–µ–º—É SQL –Ω–µ –ø–æ–º–æ–∂–µ—Ç

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ç–æ—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:
- "–ù–∞–π–¥–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å ID = 12345" -- —ç—Ç–æ –ª–µ–≥–∫–æ, –µ—Å—Ç—å –∏–Ω–¥–µ–∫—Å—ã
- "–ù–∞–π–¥–∏ –≤—Å–µ –∑–∞–∫–∞–∑—ã –∑–∞ –≤—á–µ—Ä–∞" -- —Ç–æ–∂–µ –ø—Ä–æ—Å—Ç–æ, –¥–∞—Ç–∞ -- —ç—Ç–æ —á–∏—Å–ª–æ

–ù–æ "–Ω–∞–π–¥–∏ –ø–æ—Ö–æ–∂–∏–µ" -- —ç—Ç–æ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–∞—è –∑–∞–¥–∞—á–∞. B-tree –∏–Ω–¥–µ–∫—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç SQL –±—ã—Å—Ç—Ä—ã–º, —Ä–∞–±–æ—Ç–∞—é—Ç —Å —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –í–µ–∫—Ç–æ—Ä –∏–∑ 1536 –∏–∑–º–µ—Ä–µ–Ω–∏–π –Ω–µ–ª—å–∑—è "—É–ø–æ—Ä—è–¥–æ—á–∏—Ç—å" –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–º —Å–º—ã—Å–ª–µ.

### –†–µ—à–µ–Ω–∏–µ: –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫

–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∫–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä (—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫, –∏–ª–∏ exact search), vector databases –∏—Å–ø–æ–ª—å–∑—É—é—Ç **Approximate Nearest Neighbor (ANN)** -- –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.

–ò–¥–µ—è –ø—Ä–æ—Å—Ç–∞: –º—ã –≥–æ—Ç–æ–≤—ã –ø–æ—Ç–µ—Ä—è—Ç—å 1-5% —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–¥–∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 1000 —Ä–∞–∑. –í–º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö 10 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤, —É–º–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Ç–æ–ª—å–∫–æ ~10,000 -- –∏ –Ω–∞—Ö–æ–¥—è—Ç 95-99% —Ç–µ—Ö –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

–≠—Ç–æ –∫–∞–∫ –∏—Å–∫–∞—Ç—å –∫–Ω–∏–≥—É –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ: –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∂–¥—É—é –ø–æ–ª–∫—É (—Ç–æ—á–Ω–æ, –Ω–æ –¥–æ–ª–≥–æ), –∞ –º–æ–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –ø–æ–π—Ç–∏ –≤ –Ω—É–∂–Ω—ã–π —Ä–∞–∑–¥–µ–ª, –ø–æ—Ç–æ–º –Ω–∞ –Ω—É–∂–Ω—É—é –ø–æ–ª–∫—É (–±—ã—Å—Ç—Ä–æ –∏ –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ).

---

## –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç vector databases

### –ê–Ω–∞—Ç–æ–º–∏—è vector database

–ü—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–≥—Ä—É–∂–∞—Ç—å—Å—è –≤ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –¥–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä–µ–º—Å—è, –∏–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç —Ç–∏–ø–∏—á–Ω–∞—è vector database. –≠—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ "—Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤" -- —ç—Ç–æ —Å–ª–æ–∂–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ª–æ—è–º–∏.

**API Layer** -- —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞. –¢—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—à—å –∑–∞–ø—Ä–æ—Å—ã (–≤—Å—Ç–∞–≤–∏—Ç—å –≤–µ–∫—Ç–æ—Ä, –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ, —É–¥–∞–ª–∏—Ç—å), –∞ —ç—Ç–æ—Ç —Å–ª–æ–π –∏—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç. –û–±—ã—á–Ω–æ —ç—Ç–æ REST API, gRPC –∏–ª–∏ SDK –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤.

**Index Layer** -- —Å–µ—Ä–¥—Ü–µ —Å–∏—Å—Ç–µ–º—ã. –ó–¥–µ—Å—å –∂–∏–≤—É—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç –ø–æ–∏—Å–∫ –±—ã—Å—Ç—Ä—ã–º. –ò–º–µ–Ω–Ω–æ –∑–¥–µ—Å—å —Ä–∞–±–æ—Ç–∞—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã HNSW, IVF –∏ –¥—Ä—É–≥–∏–µ, –æ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –ø–æ–≥–æ–≤–æ—Ä–∏–º –¥–∞–ª—å—à–µ.

**Storage Layer** -- –≥–¥–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤–µ–∫—Ç–æ—Ä—ã. –ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è memory-mapping (mmap), —á—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞–Ω–Ω—ã–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–º–µ—â–∞—é—Ç—Å—è –≤ RAM.

**Metadata Layer** -- –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ–∫—Ç–æ—Ä–∞—Ö. –ö–æ–≥–¥–∞ —Ç—ã –∏—â–µ—à—å –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ç–µ–±–µ —á–∞—Å—Ç–æ –Ω—É–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å: "—Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü", "—Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'". Metadata –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ç–æ –¥–µ–ª–∞—Ç—å.

**Distributed Layer** -- –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ, –æ–¥–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç. –≠—Ç–æ—Ç —Å–ª–æ–π –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É —Å–µ—Ä–≤–µ—Ä–∞–º–∏) –∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é (–∫–æ–ø–∏–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏).

### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç vector database –æ—Å–æ–±–µ–Ω–Ω–æ–π

–û–±—ã—á–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: "–î–∞–π –º–Ω–µ –∑–∞–ø–∏—Å—å —Å ID = 123".

Vector database –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: "–î–∞–π –º–Ω–µ –∑–∞–ø–∏—Å–∏, –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —ç—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä".

–≠—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∏ –¥–ª—è –∏—Ö —Ä–µ—à–µ–Ω–∏—è –Ω—É–∂–Ω—ã —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.

---

## –ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

–¢–µ–ø–µ—Ä—å —Å–∞–º–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ -- –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç vector databases –±—ã—Å—Ç—Ä—ã–º–∏. –û—Å–Ω–æ–≤–Ω—ã–µ: HNSW, IVF, PQ –∏ DiskANN. –ö–∞–∂–¥—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ-—Å–≤–æ–µ–º—É.

### HNSW: –∫–∞—Ä—Ç–∞ –º–µ—Ç—Ä–æ —Å —ç–∫—Å–ø—Ä–µ—Å—Å-—Å—Ç–∞–Ω—Ü–∏—è–º–∏

**Hierarchical Navigable Small World** -- —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤ 2025 –≥–æ–¥—É. –ï–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç Pinecone, Qdrant, Weaviate, pgvector.

#### –ê–Ω–∞–ª–æ–≥–∏—è

–ü—Ä–µ–¥—Å—Ç–∞–≤—å –∫–∞—Ä—Ç—É –º–µ—Ç—Ä–æ –±–æ–ª—å—à–æ–≥–æ –≥–æ—Ä–æ–¥–∞. –ï—Å—Ç—å –æ–±—ã—á–Ω—ã–µ —Å—Ç–∞–Ω—Ü–∏–∏, –≥–¥–µ –ø–æ–µ–∑–¥–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ. –ê –µ—Å—Ç—å —ç–∫—Å–ø—Ä–µ—Å—Å-–ª–∏–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç –º–µ–ª–∫–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏ –∏ –±—ã—Å—Ç—Ä–æ –¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Ç–µ–±—è –≤ –Ω—É–∂–Ω—ã–π —Ä–∞–π–æ–Ω.

HNSW —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫ –∂–µ. –û–Ω —Å—Ç—Ä–æ–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ "—Å–ª–æ–µ–≤" –≥—Ä–∞—Ñ–∞:

- **–í–µ—Ä—Ö–Ω–∏–π —Å–ª–æ–π (Layer 3)** -- —Ä–µ–¥–∫–∏–µ "—ç–∫—Å–ø—Ä–µ—Å—Å-—Å—Ç–∞–Ω—Ü–∏–∏". –í—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–∑–ª–æ–≤, —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–ª–∏–Ω–Ω—ã–º–∏ —Ä–µ–±—Ä–∞–º–∏. –ó–¥–µ—Å—å –º—ã –±—ã—Å—Ç—Ä–æ "–ø—Ä—ã–≥–∞–µ–º" –≤ –Ω—É–∂–Ω—ã–π —Ä–∞–π–æ–Ω.

- **–°—Ä–µ–¥–Ω–∏–µ —Å–ª–æ–∏ (Layer 1-2)** -- –±–æ–ª—å—à–µ —Å—Ç–∞–Ω—Ü–∏–π, –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–µ—Ä–µ—Ö–æ–¥—ã. –£—Ç–æ—á–Ω—è–µ–º –ø–æ–∑–∏—Ü–∏—é.

- **–ù–∏–∂–Ω–∏–π —Å–ª–æ–π (Layer 0)** -- –≤—Å–µ —Å—Ç–∞–Ω—Ü–∏–∏. –ó–¥–µ—Å—å –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã, –∏ –º—ã –∏—â–µ–º —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç —Å—Ä–µ–¥–∏ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.

#### –ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–∏—Å–∫

1. –ù–∞—á–∏–Ω–∞–µ–º —Å –≤–µ—Ä—Ö–Ω–µ–≥–æ —Å–ª–æ—è. –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π —É–∑–µ–ª –∫ –Ω–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.
2. –°–ø—É—Å–∫–∞–µ–º—Å—è –Ω–∞ —Å–ª–æ–π –Ω–∏–∂–µ, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–π–¥–µ–Ω–Ω—ã–π —É–∑–µ–ª –∫–∞–∫ —Å—Ç–∞—Ä—Ç–æ–≤—É—é —Ç–æ—á–∫—É.
3. –ü–æ–≤—Ç–æ—Ä—è–µ–º, –ø–æ–∫–∞ –Ω–µ –¥–æ–π–¥–µ–º –¥–æ –Ω–∏–∂–Ω–µ–≥–æ —Å–ª–æ—è.
4. –ù–∞ –Ω–∏–∂–Ω–µ–º —Å–ª–æ–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º top-k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.

–°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ -- O(log n). –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –¥–ª—è 10 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ—Ä–Ω–æ 23 —à–∞–≥–∞ –≤–º–µ—Å—Ç–æ 10 –º–∏–ª–ª–∏–æ–Ω–æ–≤.

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã HNSW: –¥–µ—Ç–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

–£ HNSW –µ—Å—Ç—å —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞, –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏—Ö -- —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É "—Ä–∞–±–æ—Ç–∞–µ—Ç" –∏ "—Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ".

**M (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –Ω–∞ —É–∑–µ–ª)** -- —Å–∫–æ–ª—å–∫–æ "—Å–æ—Å–µ–¥–µ–π" –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –∫–∞–∂–¥—ã–π —É–∑–µ–ª –≤ –≥—Ä–∞—Ñ–µ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏. –ë–æ–ª—å—à–µ —Å–≤—è–∑–µ–π = –ª—É—á—à–µ recall –∏ —Ç–æ—á–Ω–æ—Å—Ç—å (–ø–æ–∏—Å–∫ –∏–º–µ–µ—Ç –±–æ–ª—å—à–µ –ø—É—Ç–µ–π –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è), –Ω–æ –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –≤—Å—Ç–∞–≤–∫–∞.

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ M:
- **M = 5-12** -- –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –Ω–∏–∑–∫–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é
- **M = 12-48** -- –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤
- **M = 48-64** -- –¥–ª—è –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (word embeddings, face descriptors)
- **M = 2-100** -- –æ–±—â–∏–π —Ä–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω

–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: –ø—Ä–∏–º–µ—Ä–Ω–æ M * 8-10 –±–∞–π—Ç –Ω–∞ —Ö—Ä–∞–Ω–∏–º—ã–π —ç–ª–µ–º–µ–Ω—Ç.

**ef_construction (—Ä–∞–∑–º–µ—Ä –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏)** -- –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤-—Å–æ—Å–µ–¥–µ–π, –∏—Å—Å–ª–µ–¥—É–µ–º—ã—Ö –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ —É–∑–ª–∞ –≤ –≥—Ä–∞—Ñ. –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 400 vs 200) –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—É –Ω–∞—Ö–æ–¥–∏—Ç—å –±–æ–ª–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –≥—Ä–∞—Ñ—É –∏ –ª—É—á—à–µ–º—É recall. –û–¥–Ω–∞–∫–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è.

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- –ù–∞—á–Ω–∏—Ç–µ —Å **ef_construction = 100-200** –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤
- –î–ª—è –≤—ã—Å–æ–∫–æ–≥–æ recall –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **ef_construction = 400-500**
- –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ recall –ø—Ä–∏ ef = ef_construction –º–µ–Ω—å—à–µ 0.9, –µ—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è

**ef_search (—Ä–∞–∑–º–µ—Ä –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ)** -- —Ä–∞–∑–º–µ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–ø–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –≤–æ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞. –ë–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–µ–¥–µ—Ç –∫ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–º—É, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É. –ó–Ω–∞—á–µ–Ω–∏–µ ef_search –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω—å—à–µ k (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å–æ—Å–µ–¥–µ–π).

–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
- **ef_search = 100**: ~85% recall, ~1ms latency
- **ef_search = 500**: ~98% recall, ~5ms latency

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**

| –°—Ü–µ–Ω–∞—Ä–∏–π | M | ef_construction | ef_search |
|----------|---|-----------------|-----------|
| Real-time —Å–∏—Å—Ç–µ–º–∞ | 12 | 200 | 100 |
| –í—ã—Å–æ–∫–∏–π recall | 24 | 400 | 500 |
| –ë–∞–ª–∞–Ω—Å | 16 | 128 | 128 |
| –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π recall | 64 | 256 | 256 |

```python
# –ü—Ä–∏–º–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ HNSW –≤ Qdrant
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, HnswConfigDiff

client = QdrantClient(host="localhost", port=6333)

client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=1536,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å OpenAI embeddings
        distance=Distance.COSINE
    ),
    hnsw_config=HnswConfigDiff(
        m=16,                    # –ó–æ–ª–æ—Ç–∞—è —Å–µ—Ä–µ–¥–∏–Ω–∞
        ef_construct=100,        # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        full_scan_threshold=10000  # –î–æ 10K –∑–∞–ø–∏—Å–µ–π –¥–µ–ª–∞–µ–º brute force
    )
)
```

### IVF: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏ –ø–æ —Ç–µ–º–∞–º

**Inverted File Index** -- –±–æ–ª–µ–µ —Å—Ç–∞—Ä—ã–π, –Ω–æ –≤—Å–µ –µ—â–µ –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥. –≠—Ç–æ partition-based –∏–Ω–¥–µ–∫—Å, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ö—Ä–∞–Ω–µ–Ω–∏–∏, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç HNSW, –∫–æ—Ç–æ—Ä—ã–π in-memory.

#### –ê–Ω–∞–ª–æ–≥–∏—è

–ü—Ä–µ–¥—Å—Ç–∞–≤—å –±–æ–ª—å—à—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –∏—Å–∫–∞—Ç—å –∫–Ω–∏–≥—É —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø–æ–ª–æ–∫, —Ç—ã —Å–Ω–∞—á–∞–ª–∞ –∏–¥–µ—à—å –≤ –Ω—É–∂–Ω—ã–π —Ä–∞–∑–¥–µ–ª: "–•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", –ø–æ—Ç–æ–º "–§–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞", –ø–æ—Ç–æ–º "–ù–∞—É—á–Ω–∞—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞". –ò —Ç–æ–ª—å–∫–æ —Ç–∞–º –∏—â–µ—à—å —Å—Ä–µ–¥–∏ —Å–æ—Ç–Ω–∏ –∫–Ω–∏–≥ –≤–º–µ—Å—Ç–æ –º–∏–ª–ª–∏–æ–Ω–∞.

IVF –¥–µ–ª–∞–µ—Ç —Ç–æ –∂–µ —Å–∞–º–æ–µ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏. –û–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –ø–æ–º–æ—â—å—é k-means, –∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –±–ª–∏–∂–µ –≤—Å–µ–≥–æ –∫ –∑–∞–ø—Ä–æ—Å—É.

#### –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

**–≠—Ç–∞–ø 1: –û–±—É—á–µ–Ω–∏–µ (Clustering Phase).** –ë–µ—Ä–µ–º –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã (–∏–ª–∏ –∏—Ö –≤—ã–±–æ—Ä–∫—É) –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º k-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é. –ü–æ–ª—É—á–∞–µ–º N —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ (—Ü–µ–Ω—Ç—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤). –ö–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä "–ø—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç—Å—è" –∫ –±–ª–∏–∂–∞–π—à–µ–º—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É.

K-means -- –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: —Å–Ω–∞—á–∞–ª–∞ —Å–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è K —Ç–æ—á–µ–∫ –∫–∞–∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã, –∑–∞—Ç–µ–º –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤—Å–µ —Ç–æ—á–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—é—Ç—Å—è –±–ª–∏–∂–∞–π—à–µ–º—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É, –∏ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –¥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞.

**–≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫.** –ü–æ–ª—É—á–∏–≤ –∑–∞–ø—Ä–æ—Å:
1. –ù–∞—Ö–æ–¥–∏–º nprobe –±–ª–∏–∂–∞–π—à–∏—Ö —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ –∫ –∑–∞–ø—Ä–æ—Å—É
2. –ò—â–µ–º —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —ç—Ç–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
3. –í–æ–∑–≤—Ä–∞—â–∞–µ–º top-k

**–î–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞:**

**nlist** -- –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –û–±—ã—á–Ω–æ –±–µ—Ä—É—Ç –∫–æ—Ä–µ–Ω—å –∏–∑ —á–∏—Å–ª–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤.
- –î–ª—è 1 –º–∏–ª–ª–∏–æ–Ω–∞ -- –æ–∫–æ–ª–æ 1000 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
- –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –∫–∞–∂–¥–æ–º, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–æ–ª—å—à–µ–≥–æ —á–∏—Å–ª–∞ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤

**nprobe** -- —Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∏ –ø–æ–∏—Å–∫–µ.
- –ë–æ–ª—å—à–µ = –ª—É—á—à–µ recall, –º–µ–¥–ª–µ–Ω–Ω–µ–µ –ø–æ–∏—Å–∫
- –û–±—ã—á–Ω–æ 10-100
- –î–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å 1M –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ 1000 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, nprobe=10 –æ–∑–Ω–∞—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É ~1% –¥–∞–Ω–Ω—ã—Ö

### HNSW vs IVF: –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | HNSW | IVF |
|----------------|------|-----|
| **–¢–∏–ø —Å—Ç—Ä—É–∫—Ç—É—Ä—ã** | –ì—Ä–∞—Ñ | –ö–ª–∞—Å—Ç–µ—Ä—ã |
| **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** | 3x –ª—É—á—à–µ —á–µ–º IVFFlat | –ë–∞–∑–æ–≤–∞—è |
| **–ü–∞–º—è—Ç—å** | –ú–Ω–æ–≥–æ (—Ö—Ä–∞–Ω–∏—Ç —Å–≤—è–∑–∏) | –ú–∞–ª–æ (–æ—Å–æ–±–µ–Ω–Ω–æ —Å PQ) |
| **–í—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è** | –î–æ–ª–≥–æ–µ | –ë—ã—Å—Ç—Ä–æ–µ |
| **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è** | –û—Ç–ª–∏—á–Ω–æ | –ü–ª–æ—Ö–æ (–Ω—É–∂–µ–Ω rebuild) |
| **Recall** | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π | –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ |
| **Filtered search** | –ú–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω | –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω |
| **–í—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** | –•–æ—Ä–æ—à–æ –¥–µ—Ä–∂–∏—Ç | –î–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç (curse of dimensionality) |
| **–°—Ç–æ–∏–º–æ—Å—Ç—å (AWS –ø—Ä–∏–º–µ—Ä)** | ~$75/—á–∞—Å | ~$11/—á–∞—Å (—Å IVFPQ) |

**–í—ã–±–∏—Ä–∞–π HNSW**, –µ—Å–ª–∏:
- –î–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è (–Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —É–¥–∞–ª–µ–Ω–∏—è)
- –ù—É–∂–µ–Ω –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π recall
- –ü–∞–º—è—Ç—å –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞
- –í—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ embeddings

**–í—ã–±–∏—Ä–∞–π IVF**, –µ—Å–ª–∏:
- –î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è batch'–∞–º–∏ (—Ä–∞–∑ –≤ –¥–µ–Ω—å, —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é)
- –ü–∞–º—è—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞
- –ù—É–∂–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (pre-filtering)
- –û–≥—Ä–æ–º–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –±—é–¥–∂–µ—Ç–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏

### PQ: —Å–∂–∞—Ç–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

**Product Quantization** —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–∞–º—è—Ç–∏. HNSW –∏ IVF —É—Å–∫–æ—Ä—è—é—Ç –ø–æ–∏—Å–∫, –Ω–æ —Å–∞–º–∏ –≤–µ–∫—Ç–æ—Ä—ã –≤—Å–µ –µ—â–µ –∑–∞–Ω–∏–º–∞—é—Ç –º–Ω–æ–≥–æ –º–µ—Å—Ç–∞. –í–µ–∫—Ç–æ—Ä –∏–∑ 1536 float32 -- —ç—Ç–æ 6 KB. –ú–∏–ª–ª–∏–∞—Ä–¥ —Ç–∞–∫–∏—Ö -- 6 TB —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–µ–∫—Ç–æ—Ä—ã!

#### –ê–Ω–∞–ª–æ–≥–∏—è

–ö–æ–≥–¥–∞ —Ç—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ—à—å —Ñ–æ—Ç–æ –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–µ, –æ–Ω–æ —Å–∂–∏–º–∞–µ—Ç—Å—è. –î–∞, –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ–º–Ω–æ–≥–æ –ø–∞–¥–∞–µ—Ç, –Ω–æ —Ñ–∞–π–ª —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤ 10 —Ä–∞–∑ –º–µ–Ω—å—à–µ. PQ –¥–µ–ª–∞–µ—Ç —Ç–æ –∂–µ —Å–∞–º–æ–µ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏.

#### –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

1. **–†–∞–∑–±–∏–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä –Ω–∞ —á–∞—Å—Ç–∏ (—Å—É–±–≤–µ–∫—Ç–æ—Ä—ã).** –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–µ–∫—Ç–æ—Ä –∏–∑ 128 –∏–∑–º–µ—Ä–µ–Ω–∏–π —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ 8 —á–∞—Å—Ç–µ–π –ø–æ 16 –∏–∑–º–µ—Ä–µ–Ω–∏–π.

2. **–î–ª—è –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ —Å–æ–∑–¥–∞–µ–º "—Å–ª–æ–≤–∞—Ä—å" (codebook)** –∏–∑ 256 —Ç–∏–ø–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. –≠—Ç–æ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é k-means.

3. **–ö–æ–¥–∏—Ä—É–µ–º:** –í–º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å—É–±–≤–µ–∫—Ç–æ—Ä–∞ —Ö—Ä–∞–Ω–∏–º –∏–Ω–¥–µ–∫—Å –±–ª–∏–∂–∞–π—à–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å–ª–æ–≤–∞—Ä—è (1 byte!).

4. **–†–µ–∑—É–ª—å—Ç–∞—Ç:** –í–µ—Å—å –≤–µ–∫—Ç–æ—Ä –≤–º–µ—Å—Ç–æ 512 bytes (128 x 4) —Ç–µ–ø–µ—Ä—å –∑–∞–Ω–∏–º–∞–µ—Ç 8 bytes (8 –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ 1 byte).

**–°–∂–∞—Ç–∏–µ –≤ 64 —Ä–∞–∑–∞!** PQ –º–æ–∂–µ—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–∂–∞—Ç–∏–µ –¥–æ 64x, —Ç–æ–≥–¥–∞ –∫–∞–∫ –æ–±—ã—á–Ω–∞—è scalar quantization -- –¥–æ 32x.

#### –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

- **–ú–∞—Å—Å–∏–≤–Ω–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è:** –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å–∂–∞—Ç–∏–µ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å 95%
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:** –µ—Å–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–±–∏—Ç –Ω–∞ n —á–∞—Å—Ç–µ–π, –æ–Ω –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è n —á–∏—Å–ª–∞–º–∏
- **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ:** –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ k –æ–±—ã—á–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å—Ç–µ–ø–µ–Ω—å—é 2 –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —Ç—Ä–µ–±—É—è n * log(k) –±–∏—Ç –Ω–∞ –≤–µ–∫—Ç–æ—Ä

#### Trade-offs

PQ -- lossy compression, —á–∞—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–µ—Ä—è–µ—Ç—Å—è:
- Recall –ø–∞–¥–∞–µ—Ç –Ω–∞ 1-5%
- –†–∞—Å—Å—Ç–æ—è–Ω–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–º–∏
- –î–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è rescoring —Å –ø–æ–ª–Ω—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã PQ:**
- **m** -- –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É–±–≤–µ–∫—Ç–æ—Ä–æ–≤ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ m)
- **code_size** -- –±–∏—Ç—ã –Ω–∞ —Å—É–±–≤–µ–∫—Ç–æ—Ä (–æ–±—ã—á–Ω–æ 8)
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: code_size = 8, –∑–∞—Ç–µ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ m –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –ø–∞–º—è—Ç–∏ –∏ recall

### IVF_PQ: –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥

IVF_PQ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç IVF –¥–ª—è –≥—Ä—É–±–æ–≥–æ —Å—É–∂–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ PQ –¥–ª—è —Å–∂–∞—Ç–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤:

1. IVF –±—ã—Å—Ç—Ä–æ —Å—É–∂–∞–µ—Ç –æ–±–ª–∞—Å—Ç—å –ø–æ–∏—Å–∫–∞ –¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
2. PQ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –±–æ–ª—å—à–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –ø–∞–º—è—Ç–∏
3. –ò—Ç–æ–≥: –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ –ø–∞–º—è—Ç–∏

**Benchmark:** IVFPQ+HNSW –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Å–µ–≥–æ 154 MB -- —ç—Ç–æ –≤ 15 —Ä–∞–∑ –º–µ–Ω—å—à–µ —á–µ–º HNSW alone!

### Binary Quantization: 40x —É—Å–∫–æ—Ä–µ–Ω–∏–µ

Binary Quantization (BQ) -- –µ—â–µ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è, –ø–æ—è–≤–∏–≤—à–∏–π—Å—è –≤ 2024 –≥–æ–¥—É.

#### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç

–í—Å–µ —á–∏—Å–ª–∞ –±–æ–ª—å—à–µ –Ω—É–ª—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è 1, –æ—Å—Ç–∞–ª—å–Ω—ã–µ -- 0. –ö–∞–∂–¥–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è 1 –±–∏—Ç–æ–º –≤–º–µ—Å—Ç–æ 32.

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- **32x —Å–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏:** 100K OpenAI Ada-002 –≤–µ–∫—Ç–æ—Ä–æ–≤: 900 MB -> 128 MB
- **–î–æ 40x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞** –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é Hamming distance (XOR –æ–ø–µ—Ä–∞—Ü–∏–∏)

#### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

Binary quantization —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è:
- **–í—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤** (>=1024 dimensions)
- **–¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è** –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

–†–µ–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- OpenAI text-embedding-ada-002 (1536d): 0.98 recall@100 —Å 4x oversampling
- Cohere embed-english-v2.0 (4096d): 0.98 recall@50 —Å 2x oversampling
- mxbai-embed-large-v1: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç >96% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ 32x —Å–∂–∞—Ç–∏–∏

**–ü–æ–¥–¥–µ—Ä–∂–∫–∞:**
- Qdrant: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è BQ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
- OpenSearch 2.17+: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ 1, 2, –∏–ª–∏ 4 –±–∏—Ç –Ω–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–µ
- Weaviate: BQ –¥–ª—è flat –∏ HNSW –∏–Ω–¥–µ–∫—Å–æ–≤
- pgvector 0.7+: binary_quantize —Ñ—É–Ω–∫—Ü–∏—è

### DiskANN: –º–∏–ª–ª–∏–∞—Ä–¥ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ

**DiskANN** -- –ø—Ä–æ—Ä—ã–≤ –æ—Ç Microsoft Research –¥–ª—è billion-scale –ø–æ–∏—Å–∫–∞ –Ω–∞ SSD.

#### –ü—Ä–æ–±–ª–µ–º–∞, –∫–æ—Ç–æ—Ä—É—é —Ä–µ—à–∞–µ—Ç DiskANN

HNSW –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 95% recall –ø—Ä–∏ <5ms –¥–ª—è 100M –≤–µ–∫—Ç–æ—Ä–æ–≤, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç ~500GB RAM. –ù–∞ 1B –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–æ–∏–º–æ—Å—Ç—å RAM –≤ –æ–±–ª–∞–∫–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç $10k/–º–µ—Å—è—Ü.

#### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç

DiskANN –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º **Vamana** –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞:
1. –ì—Ä–∞—Ñ –∏ –ø–æ–ª–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Ö—Ä–∞–Ω—è—Ç—Å—è –Ω–∞ SSD
2. –í –ø–∞–º—è—Ç–∏ —Ç–æ–ª—å–∫–æ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ embeddings –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
3. –¢–æ—á–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å SSD

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
- –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç 5-10x –±–æ–ª—å—à–µ —Ç–æ—á–µ–∫ –Ω–∞ –º–∞—à–∏–Ω—É —á–µ–º DRAM-based —Ä–µ—à–µ–Ω–∏—è
- 1 –º–∏–ª–ª–∏–∞—Ä–¥ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å 95% accuracy –ø—Ä–∏ 5ms latency
- –¢—Ä–µ–±—É–µ—Ç 15-50x –º–µ–Ω—å—à–µ RAM —á–µ–º HNSW
- Latency ~10-20ms –Ω–∞ NVMe SSD (–ø—Ä–∏–µ–º–ª–µ–º–æ –¥–ª—è –º–Ω–æ–≥–∏—Ö use cases)

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:**
- –û–≥—Ä–æ–º–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –±—é–¥–∂–µ—Ç–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
- –ö–æ–≥–¥–∞ HNSW —Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥ –ø–æ –ø–∞–º—è—Ç–∏
- Single-node deployments —Å –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤

---

## –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è

–í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω -- –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ–π, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ embedding –º–æ–¥–µ–ª–∏.

### Cosine Similarity

**–ß—Ç–æ –∏–∑–º–µ—Ä—è–µ—Ç:** –£–≥–æ–ª –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –∏—Ö –¥–ª–∏–Ω—É.

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- **–¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ NLP** -- —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è similarity
- –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∞ –Ω–µ –º–∞–≥–Ω–∏—Ç—É–¥–∞
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã

**–ü—Ä–∏–º–µ—Ä:** all-MiniLM-L6-v2 –æ–±—É—á–µ–Ω–∞ —Å cosine similarity -- –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ—ë –∂–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞.

### Dot Product (Inner Product)

**–ß—Ç–æ –∏–∑–º–µ—Ä—è–µ—Ç:** –ò –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∏ –º–∞–≥–Ω–∏—Ç—É–¥—É –≤–µ–∫—Ç–æ—Ä–æ–≤.

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- –ú–Ω–æ–≥–∏–µ LLM –æ–±—É—á–µ–Ω—ã —Å dot product (–Ω–∞–ø—Ä–∏–º–µ—Ä, msmarco-bert-base-dot-v5)
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –≥–¥–µ –º–∞–≥–Ω–∏—Ç—É–¥–∞ = –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ
- –ö–æ–≥–¥–∞ –æ–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç —Å —Ç–µ–º –∂–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º, –Ω–æ –±–æ–ª—å—à–µ–π –º–∞–≥–Ω–∏—Ç—É–¥–æ–π, "–ª—É—á—à–µ"

**–í–∞–∂–Ω–æ:** –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã, cosine –∏ dot product —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã.

### Euclidean Distance

**–ß—Ç–æ –∏–∑–º–µ—Ä—è–µ—Ç:** –ê–±—Å–æ–ª—é—Ç–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- –ú–æ–¥–µ–ª–∏ –±–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π loss function
- LSH (Locality Sensitive Hashing) –º–µ—Ç–æ–¥—ã
- K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
- Anomaly detection -- –ª–æ–≤–∏–º —Ç–æ—á–∫–∏ –Ω–∞ –±–æ–ª—å—à–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏
- –ö–æ–≥–¥–∞ –º–∞–≥–Ω–∏—Ç—É–¥–∞ –Ω–µ—Å–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ counts/measures

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

```python
# –ü—Ä–∞–≤–∏–ª–æ: match metric to training
distance_map = {
    "text-embedding-3-small": "cosine",
    "text-embedding-3-large": "cosine",
    "msmarco-bert-base-dot-v5": "dot",
    "all-MiniLM-L6-v2": "cosine",
    "cohere-embed-v3": "cosine",
}
```

---

## –û–±–∑–æ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö 2025

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –ø–æ–Ω–∏–º–∞–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –¥–∞–≤–∞–π –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ benchmarks.

### Pinecone: "–ü—Ä–æ—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç"

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Serverless (2024-2025)

Pinecone Serverless –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–ª –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É vector database:
- **Compute-storage separation** -- —ç–ª–∞—Å—Ç–∏—á–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- **Vector clustering –Ω–∞ blob storage** –¥–ª—è low-latency –ø–æ–∏—Å–∫–∞
- **Multi-tenant compute layer** —Å usage-based billing
- **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–∏–ª–ª–∏–æ–Ω–æ–≤ namespaces** –Ω–∞ –∏–Ω–¥–µ–∫—Å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è agentic workloads (2025):
- Adaptive indexing —Å log-structured merge trees
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ bursty, –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã—Ö query patterns

#### –¶–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (2024)

| –ü–ª–∞–Ω | –°—Ç–æ–∏–º–æ—Å—Ç—å | –í–∫–ª—é—á–µ–Ω–æ |
|------|-----------|----------|
| Starter (free) | $0 | 2GB storage, 2M write units/mo, 1M read units/mo |
| Standard | $50/mo minimum | $6/M writes, $24/M reads |
| Serverless | Pay-as-you-go | $0.33/GB storage, $8.25/M reads, $2/M writes |
| Enterprise | Custom | Dedicated, compliance |

#### –î–ª—è –∫–æ–≥–æ Pinecone

- –°—Ç–∞—Ä—Ç–∞–ø—ã –±–µ–∑ DevOps
- Enterprise —Å –±—é–¥–∂–µ—Ç–æ–º, –≥–¥–µ –≤—Ä–µ–º—è –≤–∞–∂–Ω–µ–µ –¥–µ–Ω–µ–≥
- Production –∑–∞ –Ω–µ–¥–µ–ª—é, –∞ –Ω–µ –∑–∞ –º–µ—Å—è—Ü

```python
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="YOUR_API_KEY")

# Serverless -- –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
pc.create_index(
    name="semantic-search",
    dimension=1536,
    metric="cosine",
    spec=ServerlessSpec(cloud="aws", region="us-east-1")
)
```

### Qdrant: –º–æ—â—å Rust –¥–ª—è –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏—Å—Ç–æ–≤

#### Benchmark –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–°–æ–≥–ª–∞—Å–Ω–æ benchmark-–∞–º 2024:
- **Highest RPS –∏ lowest latencies** –≤ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
- **4x RPS gains** –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
- **Sub-10ms p50** –Ω–∞ 1M-scale –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
- SOC 2 Type II —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è (2024)

#### –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

**Pre-filtering:** –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –í–û –í–†–ï–ú–Ø vector search, –∞ –Ω–µ –ø–æ—Å–ª–µ. –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å filtered queries.

**Quantization –∏–∑ –∫–æ—Ä–æ–±–∫–∏:** Scalar, binary, product -- –≤—ã–±–∏—Ä–∞–π –ø–æ–¥ —Å—Ü–µ–Ω–∞—Ä–∏–π.

**Hybrid Cloud (2024):** –ü–µ—Ä–≤—ã–π managed vector database, –¥–µ–ø–ª–æ—è—â–∏–π—Å—è –≤ –ª—é–±–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏ (on-prem, air-gapped).

#### –¶–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ

- Free 1GB cluster
- $0.014/hour hybrid cloud
- Custom private cloud pricing

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, Range

client = QdrantClient(host="localhost", port=6333)

# Pre-filtering -- —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –î–û vector search
results = client.search(
    collection_name="articles",
    query_vector=query_embedding,
    query_filter=Filter(
        must=[
            FieldCondition(key="published_date", range=Range(gte="2025-01-01")),
            FieldCondition(key="category", match={"value": "technology"})
        ]
    ),
    limit=10
)
```

### Weaviate: –ª—É—á—à–∏–π Hybrid Search

#### –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

Weaviate —Å–æ—á–µ—Ç–∞–µ—Ç vector search, object storage –∏ inverted index:
- **–ù–∞—Ç–∏–≤–Ω—ã–π hybrid search** -- –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ–∑–∂–µ, –∞ –∑–∞–ª–æ–∂–µ–Ω —Å –Ω–∞—á–∞–ª–∞
- **BM25F** -- –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π BM25 –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
- **GraphQL API** -- –º–æ—â–Ω—ã–π –∏ –≥–∏–±–∫–∏–π
- **–ú–æ–¥—É–ª—å–Ω—ã–µ vectorizers** -- OpenAI, Cohere, local models

#### Fusion Algorithms

**Relative Score Fusion** (default from v1.24):
- –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç scores –æ—Ç –æ–±–æ–∏—Ö –ø–æ–∏—Å–∫–æ–≤
- Highest = 1, lowest = 0, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ

**Ranked Fusion** (RRF):
- –†–∞–Ω–≥-based scoring
- –ú–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –∞–±—Å–æ–ª—é—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º

#### Alpha Parameter

```python
# alpha = 0: —Ç–æ–ª—å–∫–æ BM25
# alpha = 0.5: –±–∞–ª–∞–Ω—Å
# alpha = 1: —Ç–æ–ª—å–∫–æ vector

result = client.query.get("Document", ["content"])\
    .with_hybrid(query="machine learning", alpha=0.5)\
    .with_limit(10).do()
```

#### Scalability

- **50,000+ active tenants** per node
- **1 million tenants** –Ω–∞ 20-node –∫–ª–∞—Å—Ç–µ—Ä–µ
- GDPR-compliant deletes —á–µ—Ä–µ–∑ tenant isolation

### Milvus: GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–ª—è –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤

#### GPU Acceleration (2024)

Milvus 2.4 —Å NVIDIA CAGRA (RAPIDS cuVS):
- **–î–æ 50x –±—ã—Å—Ç—Ä–µ–µ** —á–µ–º CPU-based HNSW
- **21x speedup** –¥–ª—è index building vs CPU
- **Index building:** 56 –º–∏–Ω—É—Ç –Ω–∞ 8 DGX H100 vs ~6.22 –¥–Ω–µ–π –Ω–∞ CPU

#### GPU Index Types

- **GPU_CAGRA:** Graph-based, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è GPU
- **GPU_IVF_FLAT:** Partition-based –¥–ª—è GPU
- **GPU_BRUTE_FORCE:** –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π recall = 1

#### –ö–æ–≥–¥–∞ GPU –∏–º–µ–µ—Ç —Å–º—ã—Å–ª

GPU vector search –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–Ω–∏–∂–∞–µ—Ç latency:
- –ù—É–∂–µ–Ω **–≤—ã—Å–æ–∫–∏–π QPS** (—Å–æ—Ç–Ω–∏-—Ç—ã—Å—è—á–∏) –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
- GPU memory –º–µ–Ω—å—à–µ CPU RAM –∏ –¥–æ—Ä–æ–∂–µ
- –õ—É—á—à–µ –¥–ª—è batch processing –∏ high-throughput

### ChromaDB: –±—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Ç–æ—Ç–∏–ø

#### –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

**–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è:**
- RAG –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º (PoC)
- –û–±—É—á–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- –î–æ ~10-50 –º–∏–ª–ª–∏–æ–Ω–æ–≤ embeddings

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- Single-node -- –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ
- HNSW index –≤ RAM -- memory-hungry
- –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ—Ç–æ–∫ –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å/–ø–∏—Å–∞—Ç—å –≤ –∏–Ω–¥–µ–∫—Å
- Library mode –∏–º–µ–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å Gunicorn workers

```python
import chromadb

client = chromadb.Client()  # In-memory
collection = client.create_collection("my_collection")

collection.add(
    documents=["Doc about ML", "Doc about DBs"],
    ids=["doc1", "doc2"]
)
```

### –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ 2025

| –ö—Ä–∏—Ç–µ—Ä–∏–π | Pinecone | Qdrant | Weaviate | Milvus | Chroma | pgvector |
|----------|----------|--------|----------|--------|--------|----------|
| **Open Source** | –ù–µ—Ç | –î–∞ | –î–∞ | –î–∞ | –î–∞ | –î–∞ |
| **–°–ª–æ–∂–Ω–æ—Å—Ç—å —Å—Ç–∞—Ä—Ç–∞** | –õ–µ–≥–∫–æ | –°—Ä–µ–¥–Ω–µ | –°—Ä–µ–¥–Ω–µ | –°–ª–æ–∂–Ω–æ | –õ–µ–≥–∫–æ | –õ–µ–≥–∫–æ |
| **Self-hosting** | –ù–µ—Ç | –î–∞ | –î–∞ | –î–∞ | –î–∞ | –î–∞ |
| **–ú–∞—Å—à—Ç–∞–±** | –ú–∏–ª–ª–∏–∞—Ä–¥—ã | –°–æ—Ç–Ω–∏ M | –°–æ—Ç–Ω–∏ M | –ú–∏–ª–ª–∏–∞—Ä–¥—ã | –î–µ—Å—è—Ç–∫–∏ M | 50-100M |
| **Latency (p50, 1M)** | <20ms | <10ms | 20-50ms | <10ms | 10-50ms | 10-50ms |
| **GPU Support** | –ù–µ—Ç | –ù–µ—Ç | –ù–µ—Ç | –î–∞ | –ù–µ—Ç | –ù–µ—Ç |
| **Hybrid Search** | –î–∞ | –î–∞ | –õ—É—á—à–∏–π | –î–∞ | –ù–µ—Ç | –û–≥—Ä–∞–Ω–∏—á–µ–Ω |
| **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è** | Post | Pre | Post | Pre | –ë–∞–∑–æ–≤–∞—è | SQL |
| **Compliance** | SOC2/HIPAA | SOC2 | SOC2/HIPAA | - | - | - |

---

## pgvector

### –ö–æ–≥–¥–∞ PostgreSQL –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ

–û—Å–æ–±–∞—è —Å–∏—Ç—É–∞—Ü–∏—è: —Ç—ã —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å PostgreSQL. –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã –±—ç–∫–∞–ø—ã, —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥. –ö–æ–º–∞–Ω–¥–∞ –∑–Ω–∞–µ—Ç psql.

**pgvector** -- —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ PostgreSQL, –¥–æ–±–∞–≤–ª—è—é—â–µ–µ vector search –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É.

### –ü–æ—á–µ–º—É —ç—Ç–æ —á–∞—Å—Ç–æ –ª—É—á—à–∏–π –≤—ã–±–æ—Ä

**–í—Å—ë –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ:**

```sql
-- –ù–∞–π–¥–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ò –ø–æ–∫–∞–∂–∏ –∞–≤—Ç–æ—Ä–∞
SELECT d.content, u.name as author,
       1 - (d.embedding <=> query_embedding) AS similarity
FROM documents d
JOIN users u ON d.author_id = u.id
ORDER BY d.embedding <=> query_embedding
LIMIT 10;
```

–° Pinecone –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.

**ACID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏:** –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ embedding -- –∞—Ç–æ–º–∞—Ä–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è.

**–ó–Ω–∞–∫–æ–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:** psql, pgAdmin, DBeaver, Datagrip, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.

### pgvector 0.8.0 (–ù–æ—è–±—Ä—å 2024): Major Update

**Iterative Index Scans** -- –≥–ª–∞–≤–Ω–∞—è –Ω–æ–≤–∏–Ω–∫–∞:
- –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "overfiltering" -- –∫–æ–≥–¥–∞ —Ñ–∏–ª—å—Ç—Ä—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ–º –æ–∂–∏–¥–∞–ª–æ—Å—å
- –í–∫–ª—é—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ `hnsw.iterative_scan` –∏ `ivfflat.iterative_scan`
- –î–≤–∞ —Ä–µ–∂–∏–º–∞: relaxed_order –∏ strict_order

**–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
- **–î–æ 5.7x —É–ª—É—á—à–µ–Ω–∏–µ** –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö query patterns vs 0.7.4
- **–î–æ 9x –±—ã—Å—Ç—Ä–µ–µ** –Ω–∞ Aurora PostgreSQL
- **100x –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** –≤ filtered queries
- E-commerce –∑–∞–ø—Ä–æ—Å—ã: 120ms -> 70ms

**pgvector 0.7.0 (–ê–ø—Ä–µ–ª—å 2024):**
- **halfvec:** 2-byte floats (–¥–æ 4000 dimensions)
- **sparsevec:** –¥–æ 1000 nonzero dimensions
- **bit vectors:** –¥–æ 64000 dimensions
- **Parallel HNSW build**
- **binary_quantize** —Ñ—É–Ω–∫—Ü–∏—è

```sql
-- –£—Å—Ç–∞–Ω–æ–≤–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –¢–∞–±–ª–∏—Ü–∞ —Å embeddings
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(1536),
    metadata JSONB
);

-- HNSW –∏–Ω–¥–µ–∫—Å
CREATE INDEX ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Iterative scan –¥–ª—è –ª—É—á—à–∏—Ö filtered results (0.8.0)
SET hnsw.iterative_scan = relaxed_order;
```

### pgvector vs Pinecone Benchmark (2024)

–ù–∞ 50M Cohere embeddings (ANN-benchmarks fork):
- **28x lower p95 latency** —Å pgvector + pgvectorscale
- **16x higher throughput** at 99% recall
- Comparison was at storage-optimized tier

### –ö–æ–≥–¥–∞ pgvector –Ω–µ —Ö–≤–∞—Ç–∏—Ç

- **> 50-100M –≤–µ–∫—Ç–æ—Ä–æ–≤** -- PostgreSQL –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–ª—è —Ç–∞–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞
- **Distributed cluster** -- pgvector –Ω–∞ –æ–¥–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ
- **Hybrid search –∫—Ä–∏—Ç–∏—á–µ–Ω** -- –º–µ–Ω–µ–µ —ç–ª–µ–≥–∞–Ω—Ç–Ω–æ —á–µ–º Weaviate

---

## Hybrid Search

### –ü–æ—á–µ–º—É –æ–¥–Ω–æ–≥–æ vector search –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ

–¢—ã –ø–æ—Å—Ç—Ä–æ–∏–ª RAG-—Å–∏—Å—Ç–µ–º—É. –†–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ -- –¥–æ –∑–∞–ø—Ä–æ—Å–∞ "SKU-12345".

**Vector search** –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª, –Ω–æ "SKU-12345" -- —Ç–æ—á–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä. Vector search –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –¥—Ä—É–≥–∏–µ SKU, "–ø–æ—Ö–æ–∂–∏–µ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É".

**–î—Ä—É–≥–æ–π –ø—Ä–∏–º–µ—Ä:** "–ù–∞–π–¥–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ –ò–≤–∞–Ω–æ–≤–∞" -- vector search –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –ü–µ—Ç—Ä–æ–≤–∞ (–æ–±–∞ —Ä—É—Å—Å–∫–∏–µ —Ñ–∞–º–∏–ª–∏–∏, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏).

### –†–µ—à–µ–Ω–∏–µ: Hybrid Search = Vector + BM25

**BM25** -- —É–ª—É—á—à–µ–Ω–Ω—ã–π TF-IDF —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ –¥–ª–∏–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è.

**Vector Search** -- –ø–æ–Ω–∏–º–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–í–º–µ—Å—Ç–µ –ø–æ–∫—Ä—ã–≤–∞—é—Ç –æ–±–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Hybrid Search

1. –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å
2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º vector search –∏ BM25
3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–µ—Ä–µ–∑ fusion algorithm (RRF)
4. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) reranking –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º top-k

### Reciprocal Rank Fusion (RRF)

–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è. –í–∞–∂–µ–Ω —Ä–∞–Ω–≥, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π score.

```python
def reciprocal_rank_fusion(results_lists: list, k: int = 60) -> list:
    """
    –§–æ—Ä–º—É–ª–∞: score(doc) = sum(1 / (k + rank_in_list))
    k=60 -- —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ä–∞–Ω–≥–∞–º–∏.
    """
    doc_scores = {}
    for results in results_lists:
        for rank, (doc_id, _) in enumerate(results, start=1):
            if doc_id not in doc_scores:
                doc_scores[doc_id] = 0
            doc_scores[doc_id] += 1 / (k + rank)
    return sorted(doc_scores.items(), key=lambda x: -x[1])

# doc2 –≤—ã—Å–æ–∫–æ –≤ –û–ë–û–ò–• —Å–ø–∏—Å–∫–∞—Ö -> –±—É–¥–µ—Ç –ø–µ—Ä–≤—ã–º
```

**–ü–æ—á–µ–º—É RRF –ª—É—á—à–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è scores?** Scores –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã. BM25 score 0.8 –∏ vector similarity 0.8 -- —Ä–∞–∑–Ω—ã–µ –≤–µ—â–∏. –†–∞–Ω–≥–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–æ–∂–Ω–æ.

### Alpha Parameter

- `alpha = 0` -- —Ç–æ–ª—å–∫–æ BM25
- `alpha = 0.5` -- –±–∞–ª–∞–Ω—Å (default)
- `alpha = 1` -- —Ç–æ–ª—å–∫–æ vector

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∫–æ–¥–∞–º–∏:** alpha = 0.3-0.4
- **–•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã:** alpha = 0.7-0.8
- **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π:** alpha = 0.5
- –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö!

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

```python
# Weighted score combination
# FinalScore = (VectorScore * 0.5) + (KeywordScore * 0.3) + (RecencyScore * 0.2)

def hybrid_search(query: str, weights: dict = None):
    weights = weights or {"vector": 0.5, "keyword": 0.3, "recency": 0.2}

    vector_results = vector_search(query)
    bm25_results = keyword_search(query)

    # Normalize scores to 0-1
    vector_scores = normalize(vector_results)
    bm25_scores = normalize(bm25_results)

    # Combine
    combined = {}
    for doc_id, score in vector_scores.items():
        combined[doc_id] = score * weights["vector"]
    for doc_id, score in bm25_scores.items():
        combined[doc_id] = combined.get(doc_id, 0) + score * weights["keyword"]

    return sorted(combined.items(), key=lambda x: -x[1])
```

---

## Reranking

### –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Reranking

–î–∞–∂–µ –ø–æ—Å–ª–µ hybrid search —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å. **Cross-Encoder Reranker** —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –∫–∞–∂–¥—É—é –ø–∞—Ä—É (query, document) –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å.

### Bi-Encoder vs Cross-Encoder

**Bi-Encoder (embedding models):**
- –ë—ã—Å—Ç—Ä—ã–π, scalable
- Query –∏ document –∫–æ–¥–∏—Ä—É—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
- –ú–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–π

**Cross-Encoder:**
- –ú–µ–¥–ª–µ–Ω–Ω—ã–π, –Ω–µ scalable –¥–ª—è –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞
- Query –∏ document –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ —á–µ—Ä–µ–∑ transformer
- –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π (attention across query and document)

**–†–µ—à–µ–Ω–∏–µ:** Bi-Encoder –¥–ª—è retrieval top-100, Cross-Encoder –¥–ª—è rerank top-10.

### Production Reranking

```python
import cohere

co = cohere.Client("YOUR_API_KEY")

def rerank(query: str, documents: list[str], top_n: int = 5):
    """
    –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –ø–æ–¥–Ω–∏–º–∞–µ—Ç precision –Ω–∞ 5-15%.
    """
    results = co.rerank(
        query=query,
        documents=documents,
        model="rerank-english-v3.0",
        top_n=top_n
    )
    return [(r.document.text, r.relevance_score) for r in results.results]
```

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏

–ò–∑ MTEB Leaderboard (reranking task):
- **BAAI/bge-reranker-large** (278M params) -- –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- **Xenova/ms-marco-MiniLM-L-6-v2** -- –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π
- **Cohere rerank-v3.0** -- managed API

### Cross-Encoders vs LLM-based Rerankers

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 2024 –ø–æ–∫–∞–∑–∞–ª–æ: cross-encoders –æ—Å—Ç–∞—é—Ç—Å—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–º–∏ —Å LLM-based rerankers –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ inference.

### Latency Considerations

Cross-encoder inference –¥–ª—è 100 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –∑–∞–º–µ—Ç–Ω–æ–µ –≤—Ä–µ–º—è. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- –î–µ—Ä–∂–∏—Ç–µ reranking window –Ω–µ–±–æ–ª—å—à–∏–º (10-50 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- –ö—ç—à–∏—Ä—É–π—Ç–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ queries
- –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ async reranking –¥–ª—è UX

---

## Embedding Models

### –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å

**MTEB Leaderboard** -- –∫–ª—é—á–µ–≤–æ–π —Ä–µ—Å—É—Ä—Å:
- 58 datasets, 112 —è–∑—ã–∫–æ–≤
- –§–∏–ª—å—Ç—Ä—ã –ø–æ task (retrieval, classification, clustering)
- –§–∏–ª—å—Ç—Ä—ã –ø–æ —è–∑—ã–∫—É –∏ –¥–æ–º–µ–Ω—É

### –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

**Embedding Dimensions:**
- –ë–æ–ª—å—à–µ != –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ
- –ë–æ–ª—å—à–∏–µ dimensions –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –±–æ–ª—å—à–µ –Ω—é–∞–Ω—Å–æ–≤
- –ú–µ–Ω—å—à–∏–µ -- –±—ã—Å—Ç—Ä–µ–µ inference, –º–µ–Ω—å—à–µ storage
- –ë–∞–ª–∞–Ω—Å: 768-1536 –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ cases

**Max Tokens:**
- –ü—Ä–µ–¥–µ–ª —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ embedding
- RAG chunks –æ–±—ã—á–Ω–æ 200-500 tokens
- 512 tokens –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ cases
- –ï—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å 8192+ tokens –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

### OpenAI Models Comparison

| –ú–æ–¥–µ–ª—å | Dimensions | MTEB Avg | MIRACL | –¶–µ–Ω–∞/1M tokens |
|--------|------------|----------|--------|----------------|
| text-embedding-ada-002 | 1536 | 61.0% | 31.4% | –£—Å—Ç–∞—Ä–µ–≤—à–∞—è |
| text-embedding-3-small | 1536 | 62.3% | 44.0% | $0.02 |
| text-embedding-3-large | 3072 | 64.6% | 54.9% | $0.13 |

**text-embedding-3-large –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- Matryoshka Representation Learning -- –º–æ–∂–Ω–æ truncate dimensions
- 256-dimensional version –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω—ã–π ada-002!
- –õ—É—á—à–∞—è multilingual –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### OpenAI vs Cohere

| –ê—Å–ø–µ–∫—Ç | OpenAI 3-large | Cohere Embed v3 |
|--------|----------------|-----------------|
| **Accuracy (nDCG@10)** | 0.811 | 0.686-0.781 |
| **–¶–µ–Ω–∞/1M tokens** | $1.30 | $0.50 (1024d) |
| **Storage cost** | 4x –±–æ–ª—å—à–µ | –ú–µ–Ω—å—à–µ |
| **Typo tolerance** | –í—ã—Å–æ–∫–∞—è | –ù–∏–∂–µ |
| **Multilingual** | –•–æ—Ä–æ—à–∞—è | 100+ —è–∑—ã–∫–æ–≤ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **Cohere** -- –¥–ª—è multilingual, budget-conscious, open-domain RAG
- **OpenAI 3-large** -- –¥–ª—è semantic search, classification, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å

### –í–∞–∂–Ω–æ: Upgradability

–ü—Ä–∏ —Å–º–µ–Ω–µ embedding –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ **–ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ**. –ü—Ä–æ–µ–∫—Ç–∏—Ä—É–π—Ç–µ —Å–∏—Å—Ç–µ–º—É —Å —ç—Ç–∏–º —É—á–µ—Ç–æ–º!

---

## Chunking Strategies

### –ü–æ—á–µ–º—É Chunking –∫—Ä–∏—Ç–∏—á–µ–Ω

**Chunking -- arguably —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä –¥–ª—è RAG performance.** –ü–ª–æ—Ö–∏–µ chunks = –ø–ª–æ—Ö–æ–π retrieval, –¥–∞–∂–µ —Å –∏–¥–µ–∞–ª—å–Ω—ã–º retriever.

**–î–≤–µ –ø—Ä–∏—á–∏–Ω—ã chunking:**
1. Embedding models –∏–º–µ—é—Ç context windows -- excess tokens truncated
2. Chunks –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –¥–ª—è search

### –†–∞–∑–º–µ—Ä Chunk

> "–ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä embeddings wildly –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ query, –ø–æ–ª—É—á–∏—Ç–µ lower similarity score."

–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–∞–∑–º–µ—Ä—ã:
- **200-500 tokens** –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ RAG
- **Overlap 50-100 tokens** –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

#### 1. Fixed-Size Chunking
```python
# –ü—Ä–æ—Å—Ç–æ–π, –Ω–æ –≥—Ä—É–±—ã–π –ø–æ–¥—Ö–æ–¥
chunks = [text[i:i+500] for i in range(0, len(text), 500)]
```
**–ü–ª—é—Å—ã:** –ü—Ä–æ—Å—Ç–æ—Ç–∞, consistent sizes
**–ú–∏–Ω—É—Å—ã:** –†–∞–∑—Ä—ã–≤–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è/–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —Ç–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É

#### 2. Recursive Chunking (LangChain style)
1. –°–Ω–∞—á–∞–ª–∞ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º/—Å–µ–∫—Ü–∏—è–º
2. –ï—Å–ª–∏ chunk > limit, split –¥–∞–ª—å—à–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
3. –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë > limit, split –ø–æ —Å–ª–æ–≤–∞–º

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " "]
)
```

#### 3. Semantic Chunking
- Split –ø–æ semantic boundaries
- Merge consecutive similar segments
- –ë–æ–ª–µ–µ coherent chunks

#### 4. LLM-Based Chunking
- LLM —Ä–µ—à–∞–µ—Ç –≥–¥–µ split
- –ú–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å context/summaries
- –°–∞–º—ã–π –º–æ—â–Ω—ã–π, –Ω–æ –¥–æ—Ä–æ–≥–æ–π

### Contextual Retrieval (Anthropic, 2024)

Anthropic –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥:
1. –ü—Ä–æ–º–ø—Ç–∏–º Claude —Å –ø–æ–ª–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º + chunk
2. Claude –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç contextualized description
3. Description prepends –∫ chunk –ø–µ—Ä–µ–¥ embedding
4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç high-level meaning –≤ –∫–∞–∂–¥–æ–º chunk

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** Document cached –≤ prompt –¥–ª—è –≤—Å–µ—Ö chunks.

### Pre-Chunking vs Post-Chunking

**Pre-Chunking (—Å—Ç–∞–Ω–¥–∞—Ä—Ç):**
- Chunks —Å–æ–∑–¥–∞—é—Ç—Å—è –∑–∞—Ä–∞–Ω–µ–µ
- –ë—ã—Å—Ç—Ä—ã–π retrieval
- –¢—Ä–µ–±—É–µ—Ç upfront —Ä–µ—à–µ–Ω–∏–π –æ —Ä–∞–∑–º–µ—Ä–µ

**Post-Chunking:**
- Embedding —Ü–µ–ª—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- Chunking at query time –¥–ª—è retrieved docs
- Caching –¥–µ–ª–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã

1. **Match chunk size to embedding model training** -- –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
2. **–°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ metadata** -- link –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, timestamps
3. **Test different strategies** –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö
4. **Consider domain** -- –∫–æ–¥ vs –ø—Ä–æ–∑–∞ —Ç—Ä–µ–±—É—é—Ç —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤

---

## Multi-tenancy

### –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ –∏–∑–æ–ª—è—Ü–∏—è

Multi-tenancy -- –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–¥–µ –æ–¥–∏–Ω instance –æ–±—Å–ª—É–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ customers —Å –∏–∑–æ–ª—è—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö.

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- Data privacy –º–µ–∂–¥—É tenants
- Performance isolation (no noisy neighbors)
- GDPR compliance (right to be forgotten)

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–∑–æ–ª—è—Ü–∏–∏

#### 1. Namespace/Partition Level (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```python
# Pinecone namespaces
index.upsert(vectors=vectors, namespace="tenant_123")
index.query(vector=query, namespace="tenant_123", top_k=10)
```

**–ü–ª—é—Å—ã:** –ü—Ä–æ—Å—Ç–æ—Ç–∞, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —Ö–æ—Ä–æ—à–∞—è –∏–∑–æ–ª—è—Ü–∏—è
**–ú–∏–Ω—É—Å—ã:** –ù–µ–ª—å–∑—è query across tenants

#### 2. Metadata Filtering

```python
# Filter by tenant_id
results = client.search(
    collection="shared",
    query_vector=query,
    filter={"tenant_id": "tenant_123"}
)
```

**–ü–ª—é—Å—ã:** Flexibility, cross-tenant queries possible
**–ú–∏–Ω—É—Å—ã:** –ú–µ–Ω—å—à–∞—è –∏–∑–æ–ª—è—Ü–∏—è, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ performance issues

#### 3. Database/Collection Level

–ö–∞–∂–¥—ã–π tenant –∏–º–µ–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é database/collection.

**–ü–ª—é—Å—ã:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è
**–ú–∏–Ω—É—Å—ã:** Resource overhead, scalability limits

### Platform-Specific Implementations

**Weaviate:** –î–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è per-tenant –æ—Ç–¥–µ–ª—å–Ω–æ
- –ë—ã—Å—Ç—Ä—ã–µ deletes (GDPR compliance)
- Dedicated high-performance index per tenant
- 50,000+ active tenants per node

**Milvus:** Database, collection, –∏ partition-level strategies

**Pinecone:** Namespaces -- —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥

### Best Practices

1. **Design namespace architecture** -- isolated namespaces per tenant
2. **Implement RBAC** -- access controls per tenant
3. **Monitor per-tenant metrics** -- latency, error rates
4. **Balance isolation vs efficiency** -- strict SLAs may need dedicated resources

---

## Production Best Practices

### Indexing & Query Optimization

1. **Choose index type wisely:**
   - HNSW –¥–ª—è high-recall, dynamic data
   - IVF –¥–ª—è batch updates, memory constraints
   - PQ/BQ –¥–ª—è memory optimization

2. **Batch operations:**
   - Reduce network overhead
   - Bulk inserts –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã

3. **Metadata filtering:**
   - Filter FIRST, then vector search
   - –°—É–∂–∞–µ—Ç search space

### RAG Pipeline Best Practices

1. **Hybrid retrieval:**
   - Vector + BM25 (–∏–ª–∏ SPLADE)
   - RRF fusion

2. **Smart chunking:**
   - 200-500 tokens —Å overlap
   - Preserve semantic boundaries

3. **Re-ranking:**
   - Cross-encoder –¥–ª—è top-k
   - 5-15% precision improvement

### Scalability

1. **Horizontal scaling:**
   - Sharding by user_id, region, –∏–ª–∏ hash
   - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ nodes –¥–ª—è –±–æ–ª—å—à–µ–≥–æ throughput

2. **Vector compression:**
   - Scalar quantization: 4x compression
   - Binary quantization: 32x compression
   - Product quantization: –¥–æ 64x

3. **Caching:**
   - Semantic caching –¥–ª—è similar queries
   - Embedding cache –¥–ª—è frequent inputs

### Monitoring

**Key metrics:**
- Query latency (p50, p95, p99)
- Recall@k
- Index size –∏ memory usage
- QPS –∏ throughput

**Tools:** Prometheus, Grafana, LangSmith, Phoenix

### Security

1. **Encryption** at rest and in transit
2. **RBAC** –¥–ª—è tenant isolation
3. **Audit logs**
4. **PII detection** –≤ embeddings pipeline

### Cost Optimization

1. **Right-size dimensions:**
   - 768d vs 1536d –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
   - Matryoshka embeddings –¥–ª—è flexibility

2. **Quantization:**
   - Scalar: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è recall
   - Binary: 32x memory reduction

3. **Tiered storage:**
   - Hot data –≤ RAM
   - Cold data –Ω–∞ SSD (DiskANN)

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—Ä–∏–∏

### –ò—Å—Ç–æ—Ä–∏—è 1: –°—Ç–∞—Ä—Ç–∞–ø —Å—Ç—Ä–æ–∏—Ç AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞

**–°–∏—Ç—É–∞—Ü–∏—è:** 4 —á–µ–ª–æ–≤–µ–∫–∞ –¥–µ–ª–∞—é—Ç SaaS –¥–ª—è —é—Ä–∏—Å—Ç–æ–≤. –ù—É–∂–µ–Ω —á–∞—Ç-–±–æ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

**–†–µ—à–µ–Ω–∏–µ:** Chroma –¥–ª—è PoC -> Pinecone –¥–ª—è production.

**–ü–æ—á–µ–º—É:** Zero ops, –≤—Ä–µ–º—è –≤–∞–∂–Ω–µ–µ –¥–µ–Ω–µ–≥ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ. $100/–º–µ—Å—è—Ü -- –Ω–∏—á—Ç–æ vs –≤—Ä–µ–º—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞.

### –ò—Å—Ç–æ—Ä–∏—è 2: E-commerce —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ —Ç–æ–≤–∞—Ä–æ–≤

**–°–∏—Ç—É–∞—Ü–∏—è:** 10M —Ç–æ–≤–∞—Ä–æ–≤, –ø–æ–∏—Å–∫ –∑–∞ 50ms, —Å–ª–æ–∂–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, self-hosting.

**–†–µ—à–µ–Ω–∏–µ:** Qdrant self-hosted.

**–ü–æ—á–µ–º—É:** Pre-filtering –≤–æ –≤—Ä–µ–º—è vector search, Rust –¥–∞—ë—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é latency, open source.

```python
results = client.search(
    collection_name="products",
    query_vector=get_embedding("–∫—Ä–∞—Å–Ω–æ–µ –ø–ª–∞—Ç—å–µ –Ω–∞ –≤—ã–ø—É—Å–∫–Ω–æ–π"),
    query_filter=Filter(
        must=[
            FieldCondition(key="category", match=MatchValue(value="dresses")),
            FieldCondition(key="price", range=Range(lte=10000)),
            FieldCondition(key="in_stock", match=MatchValue(value=True))
        ]
    ),
    limit=20
)
```

### –ò—Å—Ç–æ—Ä–∏—è 3: Enterprise Knowledge Base

**–°–∏—Ç—É–∞—Ü–∏—è:** 50,000 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Ä–∞–∑–±—Ä–æ—Å–∞–Ω–∞, –Ω—É–∂–µ–Ω —Ç–æ—á–Ω—ã–π –ò —Å–º—ã—Å–ª–æ–≤–æ–π –ø–æ–∏—Å–∫.

**–†–µ—à–µ–Ω–∏–µ:** Weaviate —Å hybrid search.

**–ü–æ—á–µ–º—É:** –õ—É—á—à–∏–π hybrid search, BM25 –¥–ª—è "–¥–æ–∫—É–º–µ–Ω—Ç 12345", vector –¥–ª—è "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å VPN".

### –ò—Å—Ç–æ—Ä–∏—è 4: PostgreSQL —É–∂–µ –µ—Å—Ç—å

**–°–∏—Ç—É–∞—Ü–∏—è:** –°—Ä–µ–¥–Ω–∏–π SaaS, PostgreSQL —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω—É–∂–µ–Ω —É–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ 500K —Å—Ç–∞—Ç–µ–π.

**–†–µ—à–µ–Ω–∏–µ:** pgvector.

**–ü–æ—á–µ–º—É:** –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å —Å JOIN, —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞, ACID –≥–∞—Ä–∞–Ω—Ç–∏–∏.

```sql
SELECT a.title, a.content, u.name as author,
       1 - (a.embedding <=> $1) as similarity
FROM articles a
JOIN users u ON a.author_id = u.id
WHERE a.published = true
ORDER BY a.embedding <=> $1
LIMIT 10;
```

---

## –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö

### Decision Tree

#### –®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª–∏ –º–∞—Å—à—Ç–∞–±

**< 1M –≤–µ–∫—Ç–æ—Ä–æ–≤:**
- PostgreSQL + pgvector (–µ—Å–ª–∏ —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ)
- Chroma (–ø—Ä–æ—Ç–æ—Ç–∏–ø)
- Qdrant Cloud / Pinecone (managed)

**1-100M –≤–µ–∫—Ç–æ—Ä–æ–≤:**
- Qdrant –∏–ª–∏ Weaviate
- Pinecone (–µ—Å–ª–∏ –±—é–¥–∂–µ—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç)

**> 100M –≤–µ–∫—Ç–æ—Ä–æ–≤:**
- Milvus (—Å data engineering —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π)
- Pinecone Enterprise
- DiskANN-based solutions

#### –®–∞–≥ 2: –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

**–ù–µ—Ç DevOps:**
- Pinecone (–¥–æ—Ä–æ–≥–æ, zero ops)
- Qdrant Cloud (–¥–µ—à–µ–≤–ª–µ)

**–ï—Å—Ç—å DevOps, –≥–æ—Ç–æ–≤—ã self-host:**
- Qdrant (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å)
- Weaviate (GraphQL, hybrid search)
- Milvus (–æ–≥—Ä–æ–º–Ω—ã–π –º–∞—Å—à—Ç–∞–±)

#### –®–∞–≥ 3: –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

| –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|------------|--------------|
| –£–∂–µ –Ω–∞ PostgreSQL | pgvector |
| –õ—É—á—à–∏–π hybrid search | Weaviate |
| –ú–æ—â–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è | Qdrant |
| GPU acceleration | Milvus |
| Edge/IoT | Qdrant (–º–∞–ª–µ–Ω—å–∫–∏–π binary) |
| Billions on SSD | DiskANN / pgvectorscale |

### –°–≤–æ–¥–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

| –°—Ü–µ–Ω–∞—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è | –ü—Ä–∏—á–∏–Ω–∞ |
|----------|--------------|---------|
| –°—Ç–∞—Ä—Ç–∞–ø, MVP | Chroma -> Pinecone | –ë—ã—Å—Ç—Ä–æ –Ω–∞—á–∞—Ç—å |
| Enterprise SaaS | Pinecone | SLA, compliance |
| Self-hosted < 50M | Qdrant | Performance/cost |
| Self-hosted > 1B | Milvus | –ü—Ä–æ–≤–µ—Ä–µ–Ω –Ω–∞ –º–∞—Å—à—Ç–∞–±–µ |
| –£–∂–µ –Ω–∞ PostgreSQL | pgvector | –ú–∏–Ω–∏–º—É–º –∏–∑–º–µ–Ω–µ–Ω–∏–π |
| Hybrid search –≤–∞–∂–µ–Ω | Weaviate | –õ—É—á—à–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è |
| Budget + Billions | DiskANN | SSD-based |

---

## –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å 2024-2025

| –¢—Ä–µ–Ω–¥ | –°—Ç–∞—Ç—É—Å | –ß—Ç–æ –≤–∞–∂–Ω–æ –∑–Ω–∞—Ç—å |
|-------|--------|-----------------|
| **pgvector 0.8.0** | ‚úÖ Production | Iterative scans, 5.7x improvement, 28x lower p95 vs Pinecone |
| **Qdrant Hybrid Cloud** | üî• Industry first | SOC 2 Type II, deploy anywhere (on-prem, air-gapped) |
| **Pinecone Serverless** | ‚úÖ GA | Multi-tenant compute, pay-as-you-go, –º–∏–ª–ª–∏–æ–Ω—ã namespaces |
| **Weaviate v1.30** | ‚úÖ New | Native generative module, HIPAA compliance (AWS) |
| **Milvus GPU (CAGRA)** | üÜï Performance | –î–æ 50x –±—ã—Å—Ç—Ä–µ–µ CPU HNSW, 21x speedup index building |
| **Binary Quantization** | ‚úÖ Mainstream | 32x memory reduction, 96% recall with rescoring |
| **DiskANN/pgvectorscale** | üî• Cost-effective | Billions –Ω–∞ SSD, 75% cheaper vs managed services |

### Market & GitHub (2025)

| Database | GitHub Stars | Docker Pulls/mo | Key Strength |
|----------|--------------|-----------------|--------------|
| **Milvus** | ~35k | ~700k | Billion-scale, GPU |
| **Qdrant** | ~9k | ‚Äî | Performance, Rust, filtering |
| **Weaviate** | ~8k | >1M | Best hybrid search |
| **Chroma** | ~6k | ‚Äî | Simplest prototyping |
| **pgvector** | ~4k | ‚Äî | PostgreSQL ecosystem |

### Community Sentiment (Reddit, HN)

**–ß—Ç–æ —Ö–≤–∞–ª—è—Ç:**
- pgvector: "75% –¥–µ—à–µ–≤–ª–µ Pinecone, benchmarks –≥–æ–≤–æ—Ä—è—Ç —Å–∞–º–∏"
- Qdrant: "Rust performance + pre-filtering ‚Äî killer combo"
- Reddit –≤—ã–±—Ä–∞–ª–∞ Milvus –¥–ª—è 1B+ scale –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤ Qdrant –∏ Milvus

**–ß—Ç–æ –∫—Ä–∏—Ç–∏–∫—É—é—Ç:**
- Pinecone: "–æ—Ç–ª–∏—á–Ω—ã–π, –Ω–æ –¥–æ—Ä–æ–≥–æ–π –¥–ª—è scale"
- Weaviate: "requires more memory at very large scale"
- Chroma: "single-node limits, not for billions"

**–û–±—â–∏–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å:**
> "Most RAG failures are self-inflicted, not database-inflicted" ‚Äî focus –Ω–∞ chunking –∏ embedding quality

### Pricing Comparison 2025

| Database | Free Tier | 1M vectors estimate |
|----------|-----------|---------------------|
| **Pinecone** | 2GB, 2M writes/mo | ~$41-89/mo |
| **Qdrant Cloud** | 1GB forever | ~$102/mo (AWS) |
| **Zilliz (Milvus)** | 5GB | ~$89-114/mo |
| **pgvector** | Self-hosted | Infrastructure only |
| **Weaviate** | 2 weeks trial | Paid after |

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

Vector databases -- —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI. –ö–ª—é—á–µ–≤—ã–µ takeaways:

1. **HNSW** -- —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤
2. **Hybrid search** (vector + BM25) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ
3. **Quantization** (scalar, binary, PQ) —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
4. **DiskANN** –¥–µ–ª–∞–µ—Ç billions –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –Ω–∞ SSD
5. **pgvector** —á–∞—Å—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö PostgreSQL deployments
6. **Chunking** –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è RAG performance

–ù–∞—á–Ω–∏ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ (Chroma –∏–ª–∏ pgvector), –ø–æ—Å—Ç—Ä–æ–π –ø—Ä–æ—Ç–æ—Ç–∏–ø, –ø–æ–π–º–∏ —Å–≤–æ–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è -- –∏ —Ç–æ–ª—å–∫–æ –ø–æ—Ç–æ–º –¥—É–º–∞–π –æ "–Ω–∞—Å—Ç–æ—è—â–µ–π" vector database.

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

- [[embeddings-complete-guide]] -- –ö–∞–∫ —Å–æ–∑–¥–∞–≤–∞—Ç—å embeddings
- [[rag-advanced-techniques]] -- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ RAG —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
- [[llm-fundamentals]] -- –û—Å–Ω–æ–≤—ã LLM

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

| # | –ò—Å—Ç–æ—á–Ω–∏–∫ | –¢–∏–ø | –í–∫–ª–∞–¥ |
|---|----------|-----|-------|
| 1 | [Pinecone: HNSW Explained](https://www.pinecone.io/learn/series/faiss/hnsw/) | Guide | HNSW –∞–ª–≥–æ—Ä–∏—Ç–º, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã |
| 2 | [OpenSearch: HNSW Hyperparameters](https://opensearch.org/blog/a-practical-guide-to-selecting-hnsw-hyperparameters/) | Guide | M, ef –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ |
| 3 | [Microsoft DiskANN](https://www.microsoft.com/en-us/research/project/project-akupara-approximate-nearest-neighbor-search-for-large-scale-semantic-search/) | Research | Billion-scale –Ω–∞ SSD |
| 4 | [Qdrant Benchmarks 2024](https://qdrant.tech/benchmarks/) | Benchmark | QPS, latency —Å—Ä–∞–≤–Ω–µ–Ω–∏—è |
| 5 | [Qdrant vs Pinecone](https://qdrant.tech/blog/comparing-qdrant-vs-pinecone-vector-databases/) | Comparison | Pricing, features |
| 6 | [Weaviate Hybrid Search](https://weaviate.io/blog/hybrid-search-explained) | Guide | RRF, alpha parameter |
| 7 | [pgvector 0.8.0](https://www.postgresql.org/about/news/pgvector-080-released-2952/) | Release | Iterative scans |
| 8 | [pgvector vs Pinecone - Timescale](https://medium.com/timescale/postgresql-and-pgvector-now-faster-than-pinecone-75-cheaper-and-100-open-source-0b0c2cca00c0) | Benchmark | 28x lower p95 |
| 9 | [Binary Quantization - HuggingFace](https://huggingface.co/blog/embedding-quantization) | Guide | 32x compression |
| 10 | [Milvus GPU Acceleration](https://thenewstack.io/how-nvidia-gpu-acceleration-supercharged-milvus-vector-database/) | Blog | CAGRA, 50x speedup |
| 11 | [Reddit Vector DB Case Study](https://milvus.io/blog/choosing-a-vector-database-for-ann-search-at-reddit.md) | Case Study | Billion-scale –≤—ã–±–æ—Ä |
| 12 | [Best Vector Databases 2025](https://www.firecrawl.dev/blog/best-vector-databases-2025) | Comparison | Market overview |
| 13 | [HNSW vs IVF Tradeoffs](https://medium.com/@ThinkingLoop/vector-db-index-internals-ivf-hnsw-tradeoffs-a67e22f1956f) | Guide | Index selection |
| 14 | [Pinecone Multi-Tenancy](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/vector-database-multi-tenancy/) | Guide | Namespace isolation |
| 15 | [Chunking Strategies - Pinecone](https://www.pinecone.io/learn/chunking-strategies/) | Guide | RAG chunking |

---

*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2024-12-28*

---

[[ai-engineering-moc|‚Üê AI Engineering MOC]] | [[embeddings-complete-guide|‚Üê Embeddings]] | [[rag-advanced-techniques|RAG Advanced ‚Üí]]

---

## –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏

**[[embeddings-complete-guide]]** ‚Äî –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω–æ —Å–≤—è–∑–∞–Ω—ã: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã, –∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ë–î –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫. –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞, —á—Ç–æ –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä –∏–Ω–¥–µ–∫—Å–∞ (HNSW, IVF) –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î.

**[[rag-advanced-techniques]]** ‚Äî –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è—é—Ç—Å—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —Å–ª–æ–µ–º –¥–ª—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–æ–≤. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ RAG-—Ç–µ—Ö–Ω–∏–∫–∏ (hybrid search, multi-index routing, hierarchical retrieval) —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –æ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ sparse+dense –ø–æ–∏—Å–∫–∞, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º, namespace-–∏–∑–æ–ª—è—Ü–∏—è. –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å retrieval-—ç—Ç–∞–ø RAG.

**[[aiml-databases-complete]]** ‚Äî –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI/ML —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç–µ–º—É –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—Å–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö. –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ë–î —á–∞—Å—Ç–æ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ —Å–≤—è–∑–∫–µ —Å —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–º–∏ (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∞–Ω–∞–ª–∏—Ç–∏–∫–∞) –∏ –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–º–∏ (—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤) –±–∞–∑–∞–º–∏. –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö ‚Äî –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ (pgvector) vs —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–∞—Å—à—Ç–∞–±–∞ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞.

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —á—Ç–µ–Ω–∏–µ

- **Bishop C.M. (2006). Pattern Recognition and Machine Learning. Springer.** ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–Ω–æ–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î
- **Goodfellow I., Bengio Y., Courville A. (2016). Deep Learning. MIT Press.** ‚Äî –≥–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ representation learning –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —á—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑–∞—Ö
- **Huyen C. (2022). Designing Machine Learning Systems. O'Reilly.** ‚Äî –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ ML-—Å–∏—Å—Ç–µ–º–∞—Ö, –≤–∫–ª—é—á–∞—è –≤—ã–±–æ—Ä —Ö—Ä–∞–Ω–∏–ª–∏—â, –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø—Ä–∏–º–µ–Ω–∏–º—ã–µ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω-–¥–µ–ø–ª–æ–π–º–µ–Ω—Ç—É –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î

---

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ü–æ—á–µ–º—É HNSW —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è ANN-–ø–æ–∏—Å–∫–∞ –≤ vector databases –≤–º–µ—Å—Ç–æ IVF –∏–ª–∏ brute-force?
> HNSW (Hierarchical Navigable Small World) –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—É–±–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ: –≤–µ—Ä—Ö–Ω–∏–µ —É—Ä–æ–≤–Ω–∏ –¥–ª—è –≥—Ä—É–±–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –Ω–∏–∂–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. Recall 95%+ –ø—Ä–∏ latency <50ms –Ω–∞ –º–∏–ª–ª–∏–æ–Ω–∞—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. IVF –±—ã—Å—Ç—Ä–µ–µ —Å—Ç—Ä–æ–∏—Ç—Å—è, –Ω–æ —Ö—É–∂–µ –Ω–∞ recall. Brute-force O(n) –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º –¥–ª—è production.

> [!question]- –£ –≤–∞—Å 50M –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, 1024-–º–µ—Ä–Ω—ã–µ embeddings, –±—é–¥–∂–µ—Ç $500/–º–µ—Å—è—Ü. –ö–∞–∫—É—é vector database –≤—ã–±–µ—Ä–µ—Ç–µ -- Pinecone, Qdrant –∏–ª–∏ pgvector?
> Qdrant self-hosted: open-source, –Ω–µ—Ç –ª–∏—Ü–µ–Ω–∑–∏–æ–Ω–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç, –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, scalar quantization —Å–Ω–∏–∂–∞–µ—Ç RAM –≤ 4x. Pinecone managed –ø—Ä–æ—â–µ, –Ω–æ –¥–æ—Ä–æ–∂–µ –Ω–∞ —Ç–∞–∫–æ–º –æ–±—ä–µ–º–µ. pgvector –ø–æ–¥–æ–π–¥–µ—Ç –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å PostgreSQL –∏ –æ–±—ä–µ–º <10M, –Ω–æ –Ω–∞ 50M –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π.

> [!question]- –ö–æ–≥–¥–∞ Hybrid Search (vector + BM25) –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º, –∞ –∫–æ–≥–¥–∞ vector search –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω?
> Hybrid –∫—Ä–∏—Ç–∏—á–µ–Ω: –¥–æ–º–µ–Ω–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∞–≤–æ), exact match –≤–∞–∂–µ–Ω (–Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ–¥—ã), –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã. Vector search –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω: –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ, –∫–æ–≥–¥–∞ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏ –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, RAG –¥–ª—è general knowledge.

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ß—Ç–æ —Ç–∞–∫–æ–µ vector database –∏ –∑–∞—á–µ–º –Ω—É–∂–Ω–∞?
?
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ë–î –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è embeddings (—á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–º—ã—Å–ª–∞) –∏ –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: "–∞–≤—Ç–æ–º–æ–±–∏–ª—å" –Ω–∞—Ö–æ–¥–∏—Ç "–º–∞—à–∏–Ω–∞", "–∞–≤—Ç–æ". Latency <50ms –Ω–∞ –º–∏–ª–ª–∏–æ–Ω–∞—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ ANN –∞–ª–≥–æ—Ä–∏—Ç–º—ã.

HNSW vs IVF vs DiskANN?
?
HNSW: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ, –ª—É—á—à–∏–π recall (95%+), —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è in-memory. IVF: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, –±—ã—Å—Ç—Ä–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ, —Ö—É–∂–µ recall. DiskANN: —Ä–∞–±–æ—Ç–∞–µ—Ç —Å SSD, –¥–ª—è –º–∞—Å—à—Ç–∞–±–æ–≤ >100M –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π RAM. PQ (Product Quantization): —Å–∂–∞—Ç–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.

–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ vector databases –≤ 2025?
?
Pinecone: managed, –ø—Ä–æ—Å—Ç–æ–π —Å—Ç–∞—Ä—Ç. Qdrant: open-source, –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. Weaviate: –º–æ–¥—É–ª—å–Ω—ã–π, GraphQL. Chroma: embedded, –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤. pgvector: PostgreSQL extension. Milvus: enterprise-scale. –í—ã–±–æ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–∞—Å—à—Ç–∞–±–∞, –±—é–¥–∂–µ—Ç–∞, managed vs self-hosted.

–ß—Ç–æ —Ç–∞–∫–æ–µ metadata filtering –∏ –∑–∞—á–µ–º?
?
–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ vector search –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º (–¥–∞—Ç–∞, –∞–≤—Ç–æ—Ä, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —è–∑—ã–∫). –ü—Ä–∏–º–µ—Ä: "–Ω–∞–π–¥–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –Ω–æ —Ç–æ–ª—å–∫–æ –∑–∞ 2024 –≥–æ–¥ –æ—Ç –æ—Ç–¥–µ–ª–∞ HR". –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è multi-tenant –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ fine-grained control –Ω–∞–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.

–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–µ–∂–¥—É managed –∏ self-hosted vector DB?
?
Managed (Pinecone, Weaviate Cloud): –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç, auto-scaling, –Ω–µ—Ç DevOps. Self-hosted (Qdrant, Milvus): –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å, data privacy, —ç–∫–æ–Ω–æ–º–∏—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–µ. –ü—Ä–∞–≤–∏–ª–æ: –ø—Ä–æ—Ç–æ—Ç–∏–ø –Ω–∞ managed, production >10M vectors -- —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å self-hosted.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[rag-advanced-techniques]] | –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vector DB –≤ RAG pipeline -- –æ—Ç chunking –¥–æ evaluation |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[aiml-databases-complete]] | Vector DB –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—Å–µ–π —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[databases-fundamentals-complete]] | –§—É–Ω–¥–∞–º–µ–Ω—Ç—ã –ë–î: –∏–Ω–¥–µ–∫—Å—ã, B-tree, hash -- –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å ANN |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ —Ä–∞–∑–¥–µ–ª–∞ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
