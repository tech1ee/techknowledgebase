---
title: "LLM API Integration - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ 2025"
tags:
  - topic/ai-ml
  - api
  - openai
  - anthropic
  - google
  - litellm
  - streaming
  - production
  - sdk
  - type/concept
  - level/intermediate
category: ai-ml
level: intermediate
created: 2025-12-28
updated: 2025-12-28
sources:
  - platform.openai.com
  - docs.anthropic.com
  - ai.google.dev
  - docs.litellm.ai
  - github.com/BerriAI/litellm
status: published
reading_time: 41
difficulty: 5
study_status: not_started
mastery: 0
last_reviewed:
next_review:
related:
  - "[[structured-outputs-tools]]"
  - "[[agent-cost-optimization]]"
  - "[[api-design]]"
---

# LLM API Integration: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ 2025

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **Python** | –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∞ Python, async/await | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **REST API** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–æ–≤, headers | [[api-design]] |
| **JSON** | –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö LLM API | –ë–∞–∑–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ |
| **Async –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ** | Streaming, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã | Python asyncio |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏** | ‚ùå –ù–µ—Ç | –°–Ω–∞—á–∞–ª–∞ Python + REST API |
| **–ë—ç–∫–µ–Ω–¥ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫** | ‚úÖ –î–∞ | –ò–¥–µ–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ |
| **Frontend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫** | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ | –§–æ–∫—É—Å –Ω–∞ SDK —Ä–∞–∑–¥–µ–ª–∞—Ö |
| **DevOps/SRE** | ‚úÖ –î–∞ | –§–æ–∫—É—Å –Ω–∞ LiteLLM Proxy, production checklist |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **LLM API** = –≤–µ–±-—Å–µ—Ä–≤–∏—Å, —á–µ—Ä–µ–∑ –∫–æ—Ç–æ—Ä—ã–π –≤–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –æ–±—â–∞–µ—Ç—Å—è —Å AI-–º–æ–¥–µ–ª—å—é

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **SDK** | Software Development Kit ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API | **–ì–æ—Ç–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç** ‚Äî –Ω–µ –ø–∏—à–µ—à—å HTTP-–∑–∞–ø—Ä–æ—Å—ã –≤—Ä—É—á–Ω—É—é |
| **Streaming** | –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ | **–ü—Ä—è–º–æ–π —ç—Ñ–∏—Ä** ‚Äî –≤–∏–¥–∏—à—å —Ç–µ–∫—Å—Ç –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ |
| **Rate Limit** | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ | **–û—á–µ—Ä–µ–¥—å –≤ –º–∞–≥–∞–∑–∏–Ω–µ** ‚Äî –Ω–µ–ª—å–∑—è –≤—Å—ë —Å—Ä–∞–∑—É |
| **Token** | –ï–¥–∏–Ω–∏—Ü–∞ —Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ç–µ–∫—Å—Ç–∞ | **–°–ª–æ–≥** ‚Äî –ø–ª–∞—Ç–∏—à—å –∑–∞ "–∫—É—Å–æ—á–∫–∏" —Ç–µ–∫—Å—Ç–∞ |
| **Exponential Backoff** | –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ | **–û—Ç—Å—Ç—É–ø–∏—Ç—å –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞** ‚Äî –∂–¥–∞—Ç—å –≤—Å—ë –¥–æ–ª—å—à–µ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ |
| **Circuit Breaker** | –ü–∞—Ç—Ç–µ—Ä–Ω –∑–∞—â–∏—Ç—ã –æ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã—Ö —Å–±–æ–µ–≤ | **–ê–≤—Ç–æ–º–∞—Ç –≤ —ç–ª–µ–∫—Ç—Ä–æ—Å–µ—Ç–∏** ‚Äî –≤—ã–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–µ |
| **Prompt Caching** | –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ | **–®–∞–±–ª–æ–Ω –ø–∏—Å—å–º–∞** ‚Äî –Ω–µ –ø–∏—à–µ—à—å –∫–∞–∂–¥—ã–π —Ä–∞–∑ —Å –Ω—É–ª—è |
| **Structured Output** | –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON-—Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ | **–§–æ—Ä–º–∞ —Å –ø–æ–ª—è–º–∏** ‚Äî –º–æ–¥–µ–ª—å –∑–∞–ø–æ–ª–Ω—è–µ—Ç, –Ω–µ –∏–º–ø—Ä–æ–≤–∏–∑–∏—Ä—É–µ—Ç |
| **Tool Use / Function Calling** | LLM –≤—ã–∑—ã–≤–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ | **–ü–æ–º–æ—â–Ω–∏–∫ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º** ‚Äî –º–æ–∂–µ—Ç –ø–æ–∑–≤–æ–Ω–∏—Ç—å —É–∑–Ω–∞—Ç—å –ø–æ–≥–æ–¥—É |
| **Fallback** | –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ | **–ü–ª–∞–Ω –ë** ‚Äî –µ—Å–ª–∏ OpenAI –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏–¥—ë–º –≤ Anthropic |

---

## TL;DR

> **LLM API Integration** - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ AI (OpenAI, Anthropic, Google) —á–µ—Ä–µ–∑ –∏—Ö SDK –∏ API.
>
> **–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã 2025:**
> - **Streaming** –¥–ª—è UX (SSE events, chunk handling)
> - **Structured Outputs** (Pydantic/Zod —Å—Ö–µ–º—ã, 100% schema compliance)
> - **Tool Use** (function calling, MCP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
> - **Error Handling** (exponential backoff, circuit breaker, fallback)
> - **Cost Optimization** (prompt caching –¥–æ 90% —Å–∫–∏–¥–∫–∞, model routing, batch API)
>
> **Unified API:** LiteLLM ‚Äî –æ–¥–∏–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è 100+ LLM, OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç.

---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ

```
+----------------------------------------------------------------------------+
|                    –ü—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å LLM API                        |
+----------------------------------------------------------------------------+
|                                                                            |
|  –ë–ï–ó –ü–†–ê–í–ò–õ–¨–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í:              –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –ü–ê–¢–¢–ï–†–ù–ê–ú–ò:         |
|                                                                            |
|  - Rate limit errors –∫—Ä–∞—à–∞—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ  + Exponential backoff + fallback  |
|  - –ù–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –æ—Ç–≤–µ—Ç—ã               + Structured outputs              |
|  - –í—ã—Å–æ–∫–∏–µ costs –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏    + Caching (90% —ç–∫–æ–Ω–æ–º–∏—è)          |
|  - –ú–µ–¥–ª–µ–Ω–Ω—ã–π UX (–æ–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞)       + Streaming –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ UX    |
|  - Vendor lock-in                       + LiteLLM ‚Äî unified API           |
|  - –†–∞–∑–Ω—ã–µ SDK –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞    + OpenAI-compatible interface     |
|                                                                            |
|  –≠–ö–û–ù–û–ú–ò–ö–ê:                                                                |
|  - Prompt caching: 90% —Å–∫–∏–¥–∫–∞ –Ω–∞ cached tokens                            |
|  - Batch API: 50% —Å–∫–∏–¥–∫–∞ –∑–∞ async –æ–±—Ä–∞–±–æ—Ç–∫—É                               |
|  - Model routing: 63% —ç–∫–æ–Ω–æ–º–∏—è —á–µ—Ä–µ–∑ cascading                            |
|  - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: -30-80% costs –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞                 |
|                                                                            |
+----------------------------------------------------------------------------+
```

---

## 1. OpenAI SDK (Python)

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –±–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```python
# pip install openai>=1.0.0

from openai import OpenAI
import os

# Best practice: API key –∏–∑ environment
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    timeout=60.0,  # —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
    max_retries=3  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—Ç—Ä–∞–∏
)

# –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain REST API in 2 sentences."}
    ],
    temperature=0.7,
    max_tokens=150
)

print(response.choices[0].message.content)
```

### Streaming

```python
# Streaming –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ UI feedback
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a haiku about coding"}],
    stream=True
)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ chunks
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

# Async streaming
import asyncio
from openai import AsyncOpenAI

async def stream_response():
    client = AsyncOpenAI()

    async for chunk in await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Tell me a story"}],
        stream=True
    ):
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    async for text in stream_response():
        print(text, end="", flush=True)

asyncio.run(main())
```

### Structured Outputs (Pydantic)

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º schema —á–µ—Ä–µ–∑ Pydantic
class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

# OpenAI –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ schema (100% compliance)
response = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",  # –¢—Ä–µ–±—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π structured outputs
    messages=[
        {"role": "system", "content": "Extract event information."},
        {"role": "user", "content": "Meeting with John and Sarah tomorrow at 3pm to discuss Q4 planning"}
    ],
    response_format=CalendarEvent
)

event = response.choices[0].message.parsed
print(f"Event: {event.name}")
print(f"Date: {event.date}")
print(f"Participants: {event.participants}")

# Streaming —Å Structured Outputs
from openai import OpenAI
from pydantic import BaseModel

class Story(BaseModel):
    title: str
    characters: list[str]
    plot: str

client = OpenAI()

# Stream JSON fields –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
with client.beta.chat.completions.stream(
    model="gpt-4o-2024-08-06",
    messages=[{"role": "user", "content": "Create a short story"}],
    response_format=Story
) as stream:
    for event in stream:
        if event.type == "content.delta":
            print(event.snapshot)  # Partial JSON
```

### Tool Use (Function Calling)

```python
from openai import OpenAI
import json

client = OpenAI()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º tools
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name, e.g., 'London'"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "default": "celsius"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the weather in Tokyo?"}],
    tools=tools,
    tool_choice="auto"  # –∏–ª–∏ "required" –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞
)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ tool call
message = response.choices[0].message
if message.tool_calls:
    for tool_call in message.tool_calls:
        args = json.loads(tool_call.function.arguments)

        # –í—ã–∑—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        result = {"temperature": 22, "condition": "Sunny"}  # –ó–∞–≥–ª—É—à–∫–∞

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ
        final_response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": "What's the weather in Tokyo?"},
                message,  # –í–∫–ª—é—á–∞–µ–º assistant message —Å tool_calls
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result)
                }
            ]
        )
        print(final_response.choices[0].message.content)
```

---

## 2. Anthropic Claude SDK (Python)

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –±–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```python
# pip install anthropic>=0.25.0

import anthropic
import os

client = anthropic.Anthropic(
    api_key=os.environ.get("ANTHROPIC_API_KEY")
)

# –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
message = client.messages.create(
    model="claude-sonnet-4-20250514",  # –ü–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Explain microservices in 3 bullet points."}
    ]
)

print(message.content[0].text)
```

### Streaming

```python
import anthropic

client = anthropic.Anthropic()

# Streaming —Å helper –º–µ—Ç–æ–¥–æ–º
with client.messages.stream(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Write a poem about AI"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)

# Event-based streaming
with client.messages.stream(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Explain quantum computing"}]
) as stream:
    for event in stream:
        if event.type == "content_block_delta":
            if event.delta.type == "text_delta":
                print(event.delta.text, end="")
        elif event.type == "message_stop":
            print("\n--- Done ---")

# Async streaming
from anthropic import AsyncAnthropic

async def stream_claude():
    client = AsyncAnthropic()

    async with client.messages.stream(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Tell me about Mars"}]
    ) as stream:
        async for text in stream.text_stream:
            print(text, end="", flush=True)
```

### Tool Use

```python
import anthropic
import json

client = anthropic.Anthropic()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º tools
tools = [
    {
        "name": "get_stock_price",
        "description": "Get the current stock price for a given ticker symbol",
        "input_schema": {
            "type": "object",
            "properties": {
                "ticker": {
                    "type": "string",
                    "description": "Stock ticker symbol (e.g., AAPL, GOOGL)"
                }
            },
            "required": ["ticker"]
        }
    }
]

# Tool use loop
messages = [{"role": "user", "content": "What's Apple's stock price?"}]

while True:
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        tools=tools,
        messages=messages
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º stop reason
    if response.stop_reason == "end_turn":
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        for block in response.content:
            if hasattr(block, 'text'):
                print(block.text)
        break

    elif response.stop_reason == "tool_use":
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º tool calls
        for block in response.content:
            if block.type == "tool_use":
                # –í—ã–∑—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
                result = {"price": 178.50, "change": "+1.2%"}  # –ó–∞–≥–ª—É—à–∫–∞

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                messages.append({"role": "assistant", "content": response.content})
                messages.append({
                    "role": "user",
                    "content": [{
                        "type": "tool_result",
                        "tool_use_id": block.id,
                        "content": json.dumps(result)
                    }]
                })
```

### Fine-grained Tool Streaming (Beta)

```python
import anthropic

client = anthropic.Anthropic()

# Beta: streaming tool parameters
with client.messages.stream(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    tools=[{
        "name": "generate_story",
        "description": "Generate a story with given parameters",
        "input_schema": {
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "plot": {"type": "string"},
                "characters": {"type": "array", "items": {"type": "string"}}
            }
        }
    }],
    messages=[{"role": "user", "content": "Generate a sci-fi story"}],
    betas=["fine-grained-tool-streaming-2025-05-14"]  # Beta feature
) as stream:
    for event in stream:
        if event.type == "content_block_delta":
            if hasattr(event.delta, 'partial_json'):
                # Streaming tool parameters
                print(f"Partial: {event.delta.partial_json}")
```

---

## 3. Google Gemini SDK (Python)

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –±–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```python
# pip install google-genai>=1.0.0

from google import genai
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞
client = genai.Client(api_key=os.environ.get("GOOGLE_API_KEY"))

# –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="Explain machine learning in simple terms."
)

print(response.text)
```

### Streaming

```python
from google import genai

client = genai.Client()

# Streaming
for chunk in client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents="Write a detailed explanation of neural networks."
):
    print(chunk.text, end="", flush=True)
```

### Grounding with Google Search

```python
from google import genai
from google.genai import types

client = genai.Client()

# Grounding ‚Äî –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ø–æ–∏—Å–∫–∞
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What are the latest AI developments in December 2025?",
    config=types.GenerateContentConfig(
        tools=[types.Tool(google_search=types.GoogleSearch())]
    )
)

print(response.text)

# Grounding metadata
if response.candidates[0].grounding_metadata:
    for source in response.candidates[0].grounding_metadata.grounding_chunks:
        print(f"Source: {source.web.title} - {source.web.uri}")
```

### Code Execution

```python
from google import genai
from google.genai import types

client = genai.Client()

# Code execution ‚Äî –º–æ–¥–µ–ª—å –ø–∏—à–µ—Ç –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç Python –∫–æ–¥
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="Calculate the fibonacci sequence up to 100 and plot it",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.CodeExecution())]
    )
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∫–ª—é—á–∞–µ—Ç –∫–æ–¥ –∏ –≤—ã–≤–æ–¥
for part in response.candidates[0].content.parts:
    if part.executable_code:
        print(f"Code:\n{part.executable_code.code}")
    if part.code_execution_result:
        print(f"Output:\n{part.code_execution_result.output}")
```

---

## 4. LiteLLM ‚Äî Unified API

### –ó–∞—á–µ–º LiteLLM?

```
+----------------------------------------------------------------------------+
|                    LiteLLM: One Interface, 100+ LLMs                        |
+----------------------------------------------------------------------------+
|                                                                            |
|  –ü–†–û–ë–õ–ï–ú–ê:                              –†–ï–®–ï–ù–ò–ï LiteLLM:                   |
|  - OpenAI SDK ‚â† Anthropic SDK          + –ï–¥–∏–Ω—ã–π OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API    |
|  - –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤              + –ï–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤—Å–µ—Ö          |
|  - Vendor lock-in                      + –õ–µ–≥–∫–æ –º–µ–Ω—è—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞          |
|  - –†–∞–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã costs tracking       + –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π cost tracking         |
|                                                                            |
|  –ü–û–î–î–ï–†–ñ–ö–ê:                                                                |
|  OpenAI, Anthropic, Google Vertex AI, Azure OpenAI, AWS Bedrock,          |
|  Cohere, Replicate, Hugging Face, vLLM, Ollama, –∏ –µ—â—ë 90+ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤    |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
# pip install litellm

from litellm import completion

# OpenAI
response = completion(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Anthropic ‚Äî —Ç–æ—Ç –∂–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
response = completion(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Google Gemini
response = completion(
    model="gemini/gemini-2.0-flash",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π)
response = completion(
    model="ollama/qwen3:14b",
    messages=[{"role": "user", "content": "Hello!"}],
    api_base="http://localhost:11434"
)

# –í—Å–µ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç!
print(response.choices[0].message.content)
```

### Streaming

```python
from litellm import completion

# Streaming —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
response = completion(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "Write a poem"}],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")

# Async streaming
from litellm import acompletion

async def stream_any_llm(model: str, prompt: str):
    response = await acompletion(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    async for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```

### LiteLLM Proxy (Production Gateway)

```yaml
# config.yaml –¥–ª—è production
model_list:
  # OpenAI
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY

  # Anthropic
  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

  # Fallback chain
  - model_name: main-model
    litellm_params:
      model: openai/gpt-4o
    model_info:
      priority: 1  # Primary

  - model_name: main-model  # Same name = fallback
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
    model_info:
      priority: 2  # Fallback –µ—Å–ª–∏ primary –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

router_settings:
  routing_strategy: simple-shuffle
  allowed_fails: 3
  cooldown_time: 60

litellm_settings:
  success_callback: ["langfuse"]  # Observability
  cache: True
  cache_params:
    type: redis
    host: localhost
    port: 6379

general_settings:
  master_key: sk-your-master-key
  database_url: postgresql://user:pass@host:5432/litellm
```

```bash
# –ó–∞–ø—É—Å–∫ proxy
docker run -d \
  -p 4000:4000 \
  -v $(pwd)/config.yaml:/app/config.yaml \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \
  docker.litellm.ai/berriai/litellm:main-stable \
  --config /app/config.yaml

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (OpenAI-compatible)
curl http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-your-master-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "main-model",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

```python
# Python client —á–µ—Ä–µ–∑ OpenAI SDK
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:4000/v1",
    api_key="sk-your-master-key"
)

response = client.chat.completions.create(
    model="main-model",  # –†–æ—É—Ç–∏—Ç—Å—è —Å–æ–≥–ª–∞—Å–Ω–æ config
    messages=[{"role": "user", "content": "Hello!"}]
)
```

---

## 5. Error Handling & Retry Patterns

### Exponential Backoff —Å Tenacity

```python
import tenacity
from openai import OpenAI, RateLimitError, APIError

client = OpenAI()

@tenacity.retry(
    wait=tenacity.wait_random_exponential(min=1, max=60),
    stop=tenacity.stop_after_attempt(5),
    retry=tenacity.retry_if_exception_type((RateLimitError, APIError)),
    before_sleep=lambda retry_state: print(f"Retry {retry_state.attempt_number}...")
)
def call_openai(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
try:
    result = call_openai("Hello!")
except tenacity.RetryError:
    print("Failed after all retries")
```

### Circuit Breaker Pattern

```python
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Optional
import time

@dataclass
class CircuitBreaker:
    failure_threshold: int = 5
    reset_timeout: int = 60
    failures: int = 0
    last_failure_time: Optional[datetime] = None
    state: str = "closed"  # closed, open, half-open

    def record_failure(self):
        self.failures += 1
        self.last_failure_time = datetime.now()

        if self.failures >= self.failure_threshold:
            self.state = "open"
            print(f"Circuit breaker OPEN")

    def record_success(self):
        self.failures = 0
        self.state = "closed"

    def can_execute(self) -> bool:
        if self.state == "closed":
            return True

        if self.state == "open":
            if datetime.now() - self.last_failure_time > timedelta(seconds=self.reset_timeout):
                self.state = "half-open"
                return True
            return False

        # half-open: allow one request
        return True

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
class ResilientLLMClient:
    def __init__(self):
        self.providers = {
            "openai": CircuitBreaker(),
            "anthropic": CircuitBreaker()
        }

    def call(self, prompt: str) -> str:
        for provider, breaker in self.providers.items():
            if breaker.can_execute():
                try:
                    result = self._call_provider(provider, prompt)
                    breaker.record_success()
                    return result
                except Exception as e:
                    breaker.record_failure()
                    print(f"{provider} failed: {e}")
                    continue

        raise Exception("All providers failed")

    def _call_provider(self, provider: str, prompt: str) -> str:
        if provider == "openai":
            # OpenAI call
            pass
        elif provider == "anthropic":
            # Anthropic call
            pass
```

### Production Error Handler

```python
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Callable
import httpx

class ErrorCategory(Enum):
    RETRYABLE = "retryable"      # 429, 500, 502, 503, 504
    NON_RETRYABLE = "non_retryable"  # 400, 401, 403, 404
    UNKNOWN = "unknown"

@dataclass
class LLMError:
    category: ErrorCategory
    message: str
    status_code: Optional[int] = None
    retry_after: Optional[int] = None

def classify_error(error: Exception) -> LLMError:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

    if isinstance(error, httpx.HTTPStatusError):
        status = error.response.status_code
        retry_after = error.response.headers.get("Retry-After")

        if status == 429:
            return LLMError(
                category=ErrorCategory.RETRYABLE,
                message="Rate limited",
                status_code=status,
                retry_after=int(retry_after) if retry_after else 60
            )
        elif status in (500, 502, 503, 504):
            return LLMError(
                category=ErrorCategory.RETRYABLE,
                message=f"Server error: {status}",
                status_code=status
            )
        elif status in (400, 401, 403, 404):
            return LLMError(
                category=ErrorCategory.NON_RETRYABLE,
                message=f"Client error: {status}",
                status_code=status
            )

    return LLMError(
        category=ErrorCategory.UNKNOWN,
        message=str(error)
    )
```

---

## 6. Cost Optimization

### Prompt Caching

```python
# OpenAI: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π caching –¥–ª—è prompts >1024 tokens
# –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π static content –≤ –Ω–∞—á–∞–ª–µ prompt

from openai import OpenAI

client = OpenAI()

# Static system prompt + documents –≤ –Ω–∞—á–∞–ª–µ
STATIC_CONTEXT = """
You are a legal document analyzer...
[–ë–æ–ª—å—à–æ–π —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç > 1024 tokens]
"""

# Cached: static context –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": STATIC_CONTEXT},  # Cached
        {"role": "user", "content": "Analyze this contract: ..."}  # Unique
    ]
)

# Anthropic: —è–≤–Ω—ã–π cache control
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "You are a code reviewer with extensive knowledge...",
            "cache_control": {"type": "ephemeral"}  # 5-min cache
        }
    ],
    messages=[{"role": "user", "content": "Review this code: ..."}]
)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º cache hit
print(f"Cache read tokens: {response.usage.cache_read_input_tokens}")
print(f"Cache creation tokens: {response.usage.cache_creation_input_tokens}")
```

### Semantic Caching

```python
# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º—ã—Å–ª–∞, –∞ –Ω–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è

import hashlib
from typing import Optional
import numpy as np
from openai import OpenAI

class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.95):
        self.client = OpenAI()
        self.cache: dict[str, tuple[list[float], str]] = {}
        self.threshold = similarity_threshold

    def _get_embedding(self, text: str) -> list[float]:
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def _cosine_similarity(self, a: list[float], b: list[float]) -> float:
        a_np, b_np = np.array(a), np.array(b)
        return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))

    def get(self, query: str) -> Optional[str]:
        query_embedding = self._get_embedding(query)

        for cached_query, (embedding, response) in self.cache.items():
            similarity = self._cosine_similarity(query_embedding, embedding)
            if similarity >= self.threshold:
                print(f"Cache hit! Similarity: {similarity:.3f}")
                return response

        return None

    def set(self, query: str, response: str):
        embedding = self._get_embedding(query)
        self.cache[query] = (embedding, response)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
cache = SemanticCache(similarity_threshold=0.92)

# –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å
query = "What is machine learning?"
cached = cache.get(query)
if not cached:
    response = call_llm(query)
    cache.set(query, response)

# –ü–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å ‚Äî cache hit
query2 = "Explain what machine learning is"
cached = cache.get(query2)  # Hit! –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–µ
```

### Model Routing / Cascading

```python
from dataclasses import dataclass
from typing import Callable

@dataclass
class ModelConfig:
    name: str
    cost_per_1k_tokens: float
    max_complexity: str  # "low", "medium", "high"

MODELS = {
    "cheap": ModelConfig("gpt-4o-mini", 0.15, "low"),
    "standard": ModelConfig("gpt-4o", 2.50, "medium"),
    "premium": ModelConfig("claude-opus-4", 15.00, "high")
}

def estimate_complexity(prompt: str) -> str:
    """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞"""
    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    word_count = len(prompt.split())

    if "explain" in prompt.lower() or "summarize" in prompt.lower():
        return "low"
    elif "analyze" in prompt.lower() or "compare" in prompt.lower():
        return "medium"
    elif "reason" in prompt.lower() or "prove" in prompt.lower():
        return "high"

    return "medium" if word_count > 100 else "low"

def route_to_model(prompt: str) -> str:
    """–†–æ—É—Ç–∏–Ω–≥ –∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
    complexity = estimate_complexity(prompt)

    if complexity == "low":
        return MODELS["cheap"].name
    elif complexity == "medium":
        return MODELS["standard"].name
    else:
        return MODELS["premium"].name

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = route_to_model("Summarize this article...")  # -> gpt-4o-mini
model = route_to_model("Analyze the philosophical implications...")  # -> gpt-4o
```

### Batch API

```python
from openai import OpenAI
import json
import time

client = OpenAI()

# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ batch file
requests = [
    {
        "custom_id": f"request-{i}",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "gpt-4o",
            "messages": [{"role": "user", "content": f"Summarize article {i}"}],
            "max_tokens": 100
        }
    }
    for i in range(100)
]

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL
with open("batch_input.jsonl", "w") as f:
    for req in requests:
        f.write(json.dumps(req) + "\n")

# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
batch_file = client.files.create(
    file=open("batch_input.jsonl", "rb"),
    purpose="batch"
)

# 3. –°–æ–∑–¥–∞—ë–º batch job
batch_job = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h"  # –î–æ 24 —á–∞—Å–æ–≤, 50% —Å–∫–∏–¥–∫–∞
)

# 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
while True:
    status = client.batches.retrieve(batch_job.id)
    print(f"Status: {status.status}")

    if status.status == "completed":
        # –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_file = client.files.content(status.output_file_id)
        results = [json.loads(line) for line in output_file.text.split("\n") if line]
        break
    elif status.status == "failed":
        print(f"Failed: {status.errors}")
        break

    time.sleep(60)
```

---

## 7. Production Checklist

```
+----------------------------------------------------------------------------+
|                    Production LLM Integration Checklist                     |
+----------------------------------------------------------------------------+
|                                                                            |
|  –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨:                                                             |
|  [ ] API keys –≤ environment variables –∏–ª–∏ secrets manager                 |
|  [ ] –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å API keys                                       |
|  [ ] Rate limiting –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è                                   |
|  [ ] Input validation (–¥–ª–∏–Ω–∞, —Ñ–æ—Ä–º–∞—Ç)                                     |
|  [ ] Output sanitization (XSS, injection)                                 |
|                                                                            |
|  –ù–ê–î–ï–ñ–ù–û–°–¢–¨:                                                               |
|  [ ] Exponential backoff —Å jitter                                         |
|  [ ] Retry-After header handling                                          |
|  [ ] Circuit breaker –¥–ª—è fallback                                         |
|  [ ] Timeout –Ω–∞ –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã                                               |
|  [ ] Graceful degradation (fallback responses)                            |
|                                                                            |
|  OBSERVABILITY:                                                            |
|  [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ request/response (–±–µ–∑ sensitive data)                    |
|  [ ] –ú–µ—Ç—Ä–∏–∫–∏: latency, tokens, cost, errors                               |
|  [ ] Tracing (LangSmith, Langfuse, etc.)                                  |
|  [ ] Alerting –Ω–∞ error rate spikes                                        |
|                                                                            |
|  COST CONTROL:                                                             |
|  [ ] Budget limits per user/team                                          |
|  [ ] Prompt caching enabled                                               |
|  [ ] Model routing –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏                                           |
|  [ ] Token limits –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã                                              |
|  [ ] Cost tracking –∏ dashboards                                           |
|                                                                            |
|  UX:                                                                       |
|  [ ] Streaming –¥–ª—è –≤—Å–µ—Ö chat interfaces                                   |
|  [ ] Loading states                                                       |
|  [ ] Error messages –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π                                     |
|  [ ] Cancellation support                                                  |
|                                                                            |
+----------------------------------------------------------------------------+
```
---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ü–æ—á–µ–º—É LiteLLM –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ –ø—Ä—è–º—ã—Ö SDK –¥–ª—è production-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏?
> LiteLLM –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è 100+ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –ø—Ä–∏ —Å–±–æ—è—Ö, –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π cost tracking, load balancing –∏ Redis-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ. –û–¥–∏–Ω –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º, —á—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç vendor lock-in –∏ —É–ø—Ä–æ—â–∞–µ—Ç –º–∏–≥—Ä–∞—Ü–∏—é.

> [!question]- –í–∞—à LLM-—Å–µ—Ä–≤–∏—Å –ø–æ–ª—É—á–∞–µ—Ç 429 (Rate Limit) –æ—à–∏–±–∫–∏ –æ—Ç OpenAI. –ö–∞–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é retry —Ä–µ–∞–ª–∏–∑—É–µ—Ç–µ –∏ –ø–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ –µ—ë?
> Exponential backoff —Å jitter: –Ω–∞—á–∞—Ç—å —Å 1 —Å–µ–∫—É–Ω–¥—ã, —É–¥–≤–∞–∏–≤–∞—Ç—å –¥–æ max 60 —Å–µ–∫—É–Ω–¥, –¥–æ–±–∞–≤–∏—Ç—å random jitter –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è thundering herd. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å Retry-After header. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ -- circuit breaker pattern —Å fallback –Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (Anthropic).

> [!question]- –ö–∞–∫ prompt caching –º–æ–∂–µ—Ç —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –¥–æ 90% –∑–∞—Ç—Ä–∞—Ç –Ω–∞ API?
> –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (system prompt, –¥–æ–∫—É–º–µ–Ω—Ç—ã) –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞. OpenAI –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã > 1024 —Ç–æ–∫–µ–Ω–æ–≤, Anthropic -- —á–µ—Ä–µ–∑ cache_control. –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö cached tokens —Å—Ç–æ—è—Ç –Ω–∞ 50-90% –¥–µ—à–µ–≤–ª–µ. –î–ª—è RAG —Å –æ–¥–Ω–∏–º system prompt –∏ —Ä–∞–∑–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ -- –æ–≥—Ä–æ–º–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è.

> [!question]- –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Structured Outputs OpenAI –∏ Tool Use Anthropic –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è JSON?
> OpenAI Structured Outputs —á–µ—Ä–µ–∑ Pydantic-–º–æ–¥–µ–ª–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç 100% compliance —Å JSON schema -- –º–æ–¥–µ–ª—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –Ω–µ –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON. Tool Use —É Anthropic –∏—Å–ø–æ–ª—å–∑—É–µ—Ç input_schema –∏ tool_use/tool_result —Ü–∏–∫–ª, –≥–¥–µ –º–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç –∫–∞–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤—ã–∑–≤–∞—Ç—å. Structured Outputs -- –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞, Tool Use -- –¥–ª—è –≤—ã–∑–æ–≤–∞ –≤–Ω–µ—à–Ω–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π.

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ö–∞–∫–∏–µ —Ç—Ä–∏ SDK –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤?
?
OpenAI: `openai` (pip install openai>=1.0.0). Anthropic: `anthropic` (pip install anthropic>=0.25.0). Google Gemini: `google-genai` (pip install google-genai>=1.0.0). –í—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç sync –∏ async, streaming, tool use.

–ß—Ç–æ —Ç–∞–∫–æ–µ Circuit Breaker pattern –¥–ª—è LLM?
?
–ü–∞—Ç—Ç–µ—Ä–Ω –∑–∞—â–∏—Ç—ã –æ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã—Ö —Å–±–æ–µ–≤. –¢—Ä–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: closed (–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞), open (–∑–∞–ø—Ä–æ—Å—ã –±–ª–æ–∫–∏—Ä—É—é—Ç—Å—è –ø–æ—Å–ª–µ N –æ—à–∏–±–æ–∫), half-open (–ø—Ä–æ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ timeout). –ü–æ–∑–≤–æ–ª—è–µ—Ç fallback –Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ø—Ä–∏ —Å–±–æ–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ.

–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Batch API –∏ –∫–∞–∫—É—é —ç–∫–æ–Ω–æ–º–∏—é –¥–∞–µ—Ç?
?
Batch API –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–æ —Å–æ—Ç–µ–Ω –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ JSONL —Ñ–∞–π–ª–µ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞ 24 —á–∞—Å–∞. –≠–∫–æ–Ω–æ–º–∏—è 50% –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ü–µ–Ω—ã. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è background processing, –∫–æ–≥–¥–∞ –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–µ –Ω—É–∂–µ–Ω.

–ß—Ç–æ —Ç–∞–∫–æ–µ Semantic Caching?
?
–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º—ã—Å–ª–∞ –∑–∞–ø—Ä–æ—Å–∞, –∞ –Ω–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ç—Ä–æ–∫. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è embeddings –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è cosine similarity –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏. –ü—Ä–∏ similarity > threshold (–æ–±—ã—á–Ω–æ 0.92-0.95) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç. –°–Ω–∏–∂–µ–Ω–∏–µ costs –¥–æ 86%, latency –¥–æ 88%.

–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç model routing –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞—Ç—Ä–∞—Ç?
?
–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏. –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ -> GPT-4o mini ($0.15/1M), —Å—Ä–µ–¥–Ω–∏–µ -> GPT-4o ($2.50/1M), —Å–ª–æ–∂–Ω—ã–µ -> Claude Opus ($15/1M). –≠–∫–æ–Ω–æ–º–∏—è –¥–æ 63% —á–µ—Ä–µ–∑ cascading.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[structured-outputs-tools]] | –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ JSON mode, function calling –∏ tool use |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[ai-cost-optimization]] | –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ LLM API |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[api-design]] | REST API –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –ø—Ä–∏–º–µ–Ω–∏–º—ã–µ –∫ LLM-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ —Ä–∞–∑–¥–µ–ª–∞ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
