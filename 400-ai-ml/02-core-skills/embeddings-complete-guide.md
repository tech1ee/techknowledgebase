---
title: "Embeddings: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
type: guide
status: published
tags:
  - topic/ai-ml
  - type/guide
  - level/intermediate
reading_time: 47
difficulty: 5
study_status: not_started
mastery: 0
last_reviewed:
next_review:
related:
  - "[[vector-databases-guide]]"
  - "[[rag-advanced-techniques]]"
  - "[[aiml-databases-complete]]"
---

# Embeddings: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

> –û—Ç Word2Vec –¥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–∞–∫ –º–∞—à–∏–Ω—ã –Ω–∞—É—á–∏–ª–∏—Å—å –ø–æ–Ω–∏–º–∞—Ç—å —Å–º—ã—Å–ª —Å–ª–æ–≤

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** –î–µ–∫–∞–±—Ä—å 2024

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ ML** | Embeddings ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –ª—é–±—ã—Ö ML-–∑–∞–¥–∞—á —Å —Ç–µ–∫—Å—Ç–æ–º | [[ml-fundamentals]] |
| **–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–µ–∫—Ç–æ—Ä** | Embedding ‚Äî —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä —á–∏—Å–µ–ª; –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —á—Ç–æ —ç—Ç–æ | –®–∫–æ–ª—å–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ |
| **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (–±–∞–∑–æ–≤–æ)** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ –º–æ–¥–µ–ª–∏ "–æ–±—É—á–∞—é—Ç—Å—è" –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º | [[deep-learning-basics]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ß—Ç–æ –ø–æ–ª—É—á–∏—Ç–µ |
|---------|--------------|
| **–ù–æ–≤–∏—á–æ–∫** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —á—Ç–æ —Ç–∞–∫–æ–µ embeddings, –∑–∞—á–µ–º –Ω—É–∂–Ω—ã, –∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å |
| **–ü—Ä–∞–∫—Ç–∏–∫** | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ chunking, quantization, fine-tuning –¥–ª—è production |
| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä** | Trade-offs –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏, benchmarks, —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è |

---

## –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è

> üí° **–ì–ª–∞–≤–Ω–∞—è –∞–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è embeddings:**
>
> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ **–±–∏–±–ª–∏–æ—Ç–µ–∫—É**, –≥–¥–µ –∫–Ω–∏–≥–∏ —Ä–∞—Å—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–µ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É, –∞ **–ø–æ —Å–º—ã—Å–ª—É**. –ö–Ω–∏–≥–∏ –æ –ª—é–±–≤–∏ —Å—Ç–æ—è—Ç —Ä—è–¥–æ–º, –¥–µ—Ç–µ–∫—Ç–∏–≤—ã ‚Äî –≤ –¥—Ä—É–≥–æ–º —É–≥–ª—É, –Ω–∞—É—á–Ω–∞—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞ ‚Äî –Ω–∞ —Ç—Ä–µ—Ç—å–µ–π –ø–æ–ª–∫–µ. **Embedding** ‚Äî —ç—Ç–æ "–∞–¥—Ä–µ—Å" –∫–Ω–∏–≥–∏ –≤ —Ç–∞–∫–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ: –Ω–∞–±–æ—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –≥–æ–≤–æ—Ä–∏—Ç –≥–¥–µ –∏–º–µ–Ω–Ω–æ –æ–Ω–∞ —Å—Ç–æ–∏—Ç –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–º—ã—Å–ª–æ–≤.

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Embedding** | –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ (—Å–ª–æ–≤–∞, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä–∞ —á–∏—Å–µ–ª | **GPS-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–º—ã—Å–ª–∞** ‚Äî –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–º–µ–µ—Ç "–∞–¥—Ä–µ—Å" –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∑–Ω–∞—á–µ–Ω–∏–π |
| **–í–µ–∫—Ç–æ—Ä (Vector)** | –£–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —á–∏—Å–µ–ª [0.2, -0.5, 0.8, ...] | **–†–µ—Ü–µ–ø—Ç –∫–æ–∫—Ç–µ–π–ª—è** ‚Äî —Å–ø–∏—Å–æ–∫ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤ –∏ –∏—Ö –ø—Ä–æ–ø–æ—Ä—Ü–∏–π |
| **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (Dimensions)** | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–µ–ª –≤ –≤–µ–∫—Ç–æ—Ä–µ (256, 512, 1024, 3072) | **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫** ‚Äî –∫–∞–∫ –æ–ø–∏—Å–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–∞: —Ä–æ—Å—Ç, –≤–µ—Å, –≤–æ–∑—Ä–∞—Å—Ç = 3 –∏–∑–º–µ—Ä–µ–Ω–∏—è |
| **Cosine Similarity** | –ú–µ—Ä–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (–æ—Ç -1 –¥–æ 1) | **–£–≥–æ–ª –º–µ–∂–¥—É —Å—Ç—Ä–µ–ª–∫–∞–º–∏ –∫–æ–º–ø–∞—Å–∞** ‚Äî —Å–º–æ—Ç—Ä—è—Ç –≤ –æ–¥–Ω—É —Å—Ç–æ—Ä–æ–Ω—É = –ø–æ—Ö–æ–∂–∏ (1.0), –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ = –∞–Ω—Ç–æ–Ω–∏–º—ã (-1.0) |
| **Dot Product** | –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ ‚Äî —Å—É–º–º–∞ –ø–æ–ø–∞—Ä–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π | **–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∫—É—Å–æ–≤** ‚Äî –µ—Å–ª–∏ –æ–±–∞ –ª—é–±—è—Ç (√ó) –æ–¥–Ω–∏ –∂–∞–Ω—Ä—ã, —Å—É–º–º–∞ –≤—ã—Å–æ–∫–∞—è |
| **Semantic Search** | –ü–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º | **–£–º–Ω—ã–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å** ‚Äî –ø–æ–Ω–∏–º–∞–µ—Ç —á—Ç–æ "–∞–≤—Ç–æ" –∏ "–º–∞—à–∏–Ω–∞" ‚Äî –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ |
| **MTEB** | Massive Text Embedding Benchmark ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π | **–ï–ì–≠ –¥–ª—è embedding-–º–æ–¥–µ–ª–µ–π** ‚Äî –µ–¥–∏–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è |
| **–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ embeddings** | –†–∞–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö | **–°–ª–æ–≤–æ "–∫–ª—é—á"** ‚Äî –¥–≤–µ—Ä–Ω–æ–π vs –º—É–∑—ã–∫–∞–ª—å–Ω—ã–π vs –∫ —É—Å–ø–µ—Ö—É ‚Äî —Ç—Ä–∏ —Ä–∞–∑–Ω—ã—Ö embedding'–∞ |
| **Quantization** | –°–∂–∞—Ç–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ (float32 ‚Üí int8 ‚Üí binary) | **JPEG –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤** ‚Äî —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞ |
| **Matryoshka (MRL)** | –û–±—É—á–µ–Ω–∏–µ, –≥–¥–µ –º–æ–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å –≤–µ–∫—Ç–æ—Ä –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ | **–¢–µ–ª–µ—Å–∫–æ–ø–∏—á–µ—Å–∫–∞—è —É–¥–æ—á–∫–∞** ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –≤ –ø–æ–ª–Ω–æ–º, –∏ –≤ —Å–ª–æ–∂–µ–Ω–Ω–æ–º –≤–∏–¥–µ |
| **Chunking** | –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è embedding'–∞ | **–ù–∞—Ä–µ–∑–∫–∞ –∫–Ω–∏–≥–∏ –Ω–∞ –≥–ª–∞–≤—ã** ‚Äî embedding –¥–ª—è –∫–∞–∂–¥–æ–π –≥–ª–∞–≤—ã –æ—Ç–¥–µ–ª—å–Ω–æ |
| **Late Chunking** | –°–Ω–∞—á–∞–ª–∞ embedding –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–æ—Ç–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ | **–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–Ω–∏–≥—É —Ü–µ–ª–∏–∫–æ–º**, –ø–æ—Ç–æ–º –≤—ã–¥–µ–ª–∏—Ç—å –≥–ª–∞–≤—ã ‚Äî –∫–∞–∂–¥–∞—è –≥–ª–∞–≤–∞ "–ø–æ–º–Ω–∏—Ç" –∫–æ–Ω—Ç–µ–∫—Å—Ç |
| **Hybrid Search** | –ö–æ–º–±–∏–Ω–∞—Ü–∏—è vector search + keyword search | **–î–≤–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—è** ‚Äî –æ–¥–∏–Ω –∏—â–µ—Ç –ø–æ —Å–º—ã—Å–ª—É, –≤—Ç–æ—Ä–æ–π –ø–æ —Ç–æ—á–Ω—ã–º —Å–ª–æ–≤–∞–º, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç |
| **Fine-tuning** | –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö | **–†–µ–ø–µ—Ç–∏—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏** ‚Äî —É—á–∏—Ç –ø–æ–Ω–∏–º–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∞–≤–æ) |
| **Reranking** | –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–æ–ª–µ–µ —É–º–Ω–æ–π –º–æ–¥–µ–ª—å—é | **–í—Ç–æ—Ä–æ–µ –º–Ω–µ–Ω–∏–µ** ‚Äî —Å–Ω–∞—á–∞–ª–∞ –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ 1000 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –ø–æ—Ç–æ–º —ç–∫—Å–ø–µ—Ä—Ç –æ—Ç–±–∏—Ä–∞–µ—Ç top-10 |

### –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º

> üéØ **–î–ª—è –Ω–æ–≤–∏—á–∫–∞:** –ù–µ –±–æ–π—Ç–µ—Å—å –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏! –í–æ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–Ω—è—Ç–∏—è:

**1. –í–µ–∫—Ç–æ—Ä = —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª**
```
–í–µ–∫—Ç–æ—Ä "–∫–æ—à–∫–∞" = [0.8, 0.2, -0.5, 0.9, ...]
                  ‚Üë     ‚Üë     ‚Üë     ‚Üë
               "–ø—É—à–∏—Å—Ç–æ—Å—Ç—å" "—Ä–∞–∑–º–µ—Ä" "–æ–ø–∞—Å–Ω–æ—Å—Ç—å" "–¥–æ–º–∞—à–Ω–æ—Å—Ç—å"
               (—É—Å–ª–æ–≤–Ω–æ ‚Äî –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ)
```

**2. Cosine Similarity = –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä—ã —Å–º–æ—Ç—Ä—è—Ç –≤ –æ–¥–Ω—É —Å—Ç–æ—Ä–æ–Ω—É**
```
           "—Å–æ–±–∞–∫–∞"
              ‚Üó
             /
            /  45¬∞  ‚Üí similarity = 0.71 (–ø–æ—Ö–æ–∂–∏!)
           /
    ------‚óè-----‚Üí "–∫–æ—à–∫–∞"
           \
            \
             \
              ‚Üò "–∞–≤—Ç–æ–º–æ–±–∏–ª—å"
                 90¬∞ ‚Üí similarity = 0.0 (–Ω–µ —Å–≤—è–∑–∞–Ω—ã)
```

**3. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å = –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è**
```
–û–ø–∏—Å–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–∞:
- 2 –∏–∑–º–µ—Ä–µ–Ω–∏—è: [—Ä–æ—Å—Ç, –≤–µ—Å] ‚Äî –≥—Ä—É–±–æ
- 10 –∏–∑–º–µ—Ä–µ–Ω–∏–π: + –≤–æ–∑—Ä–∞—Å—Ç, —Ü–≤–µ—Ç –≥–ª–∞–∑, ... ‚Äî —Ç–æ—á–Ω–µ–µ
- 1024 –∏–∑–º–µ—Ä–µ–Ω–∏—è: –≤—Å–µ –Ω—é–∞–Ω—Å—ã –ª–∏—á–Ω–æ—Å—Ç–∏ ‚Äî –æ—á–µ–Ω—å —Ç–æ—á–Ω–æ

–ë–æ–ª—å—à–µ –∏–∑–º–µ—Ä–µ–Ω–∏–π = —Ç–æ—á–Ω–µ–µ, –Ω–æ –¥–æ—Ä–æ–∂–µ —Ö—Ä–∞–Ω–∏—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
```

---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ

### –ü—Ä–æ–±–ª–µ–º–∞: –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª

| –°–∏–º–ø—Ç–æ–º | –ü—Ä–∏—á–∏–Ω–∞ | –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è |
|---------|---------|-------------|
| –ü–æ–∏—Å–∫ "–∞–≤—Ç–æ–º–æ–±–∏–ª—å" –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç "–º–∞—à–∏–Ω–∞", "–∞–≤—Ç–æ", "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç" | Keyword matching –Ω–µ –∑–Ω–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç –Ω—É–∂–Ω–æ–µ |
| RAG –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã | BM25 —Å—á–∏—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ TF-IDF | LLM –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É–µ—Ç –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ |
| –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª | –ù–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ | –ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è |
| –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –ø–æ explicit —Å–∏–≥–Ω–∞–ª–∞–º | –ù–µ—Ç implicit similarity | –•–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç, –Ω–∏–∑–∫–∏–π engagement |

### –ö–∞–∫ embeddings —Ä–µ—à–∞—é—Ç —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã

| –ó–∞–¥–∞—á–∞ | –ë–µ–∑ embeddings | –° embeddings |
|--------|----------------|--------------|
| **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** | Exact match, —Å–∏–Ω–æ–Ω–∏–º—ã –≤—Ä—É—á–Ω—É—é | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–º—ã—Å–ª–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ |
| **RAG retrieval** | BM25, keyword overlap | –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É |
| **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** | Rule-based, features –≤—Ä—É—á–Ω—É—é | Automatic feature extraction |
| **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** | Collaborative filtering only | Content-based + semantic similarity |
| **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è** | Topic modeling (LDA) | Dense representations, KMeans |
| **Duplicate detection** | Exact/fuzzy string matching | Semantic near-duplicates |

---

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ß—Ç–æ —Ç–∞–∫–æ–µ embeddings?](#—á—Ç–æ-—Ç–∞–∫–æ–µ-embeddings)
2. [–ò—Å—Ç–æ—Ä–∏—è: –æ—Ç Word2Vec –¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤](#–∏—Å—Ç–æ—Ä–∏—è-–æ—Ç-word2vec-–¥–æ-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤)
3. [–ö–∞–∫ embeddings –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —Å–º—ã—Å–ª](#–∫–∞–∫-embeddings-–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç-—Å–º—ã—Å–ª)
4. [–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ embeddings](#—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ-–º–æ–¥–µ–ª–∏-embeddings)
5. [–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã](#—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ-–∞—Å–ø–µ–∫—Ç—ã)
6. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è RAG](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏-–¥–ª—è-rag)
7. [–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π](#—Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è-—Ç–∞–±–ª–∏—Ü–∞-–º–æ–¥–µ–ª–µ–π)
8. [–ò—Å—Ç–æ—á–Ω–∏–∫–∏](#–∏—Å—Ç–æ—á–Ω–∏–∫–∏)

---

## –ß—Ç–æ —Ç–∞–∫–æ–µ embeddings?

**Embeddings** (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è) - —ç—Ç–æ —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ (—Å–ª–æ–≤, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π) –≤ –≤–∏–¥–µ —Ç–æ—á–µ–∫ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –≥–¥–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —ç—Ç–∏—Ö —Ç–æ—á–µ–∫ –∏–º–µ–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

### –ü—Ä–æ—Å—Ç–∞—è –∞–Ω–∞–ª–æ–≥–∏—è: GPS-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è —Å–º—ã—Å–ª–∞

–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–º–µ–µ—Ç —Å–≤–æ–∏ "GPS-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã" –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∑–Ω–∞—á–µ–Ω–∏–π:

- –°–ª–æ–≤–∞ "—Å–æ–±–∞–∫–∞" –∏ "–∫–æ—à–∫–∞" –Ω–∞—Ö–æ–¥—è—Ç—Å—è –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É (–æ–±–∞ - –¥–æ–º–∞—à–Ω–∏–µ –∂–∏–≤–æ—Ç–Ω—ã–µ)
- –°–ª–æ–≤–æ "–∞–≤—Ç–æ–º–æ–±–∏–ª—å" –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –¥–∞–ª–µ–∫–æ –æ—Ç –Ω–∏—Ö (–¥—Ä—É–≥–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è)
- "–©–µ–Ω–æ–∫" –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–µ–∂–¥—É "—Å–æ–±–∞–∫–æ–π" –∏ "—Ä–µ–±–µ–Ω–æ–∫" (–º–æ–ª–æ–¥–æ–µ —Å—É—â–µ—Å—Ç–≤–æ)

\`\`\`
–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω–æ):

        –∂–∏–≤–æ–µ
           |
    –∫–æ—à–∫–∞  |  —Å–æ–±–∞–∫–∞
           |
    -------|----------- –¥–æ–º–∞—à–Ω–µ–µ
           |
           |  –∞–≤—Ç–æ–º–æ–±–∏–ª—å
        –Ω–µ–∂–∏–≤–æ–µ
\`\`\`

### –ó–∞—á–µ–º –Ω—É–∂–Ω—ã embeddings?

1. **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** - –Ω–∞—Ö–æ–¥–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
2. **RAG (Retrieval-Augmented Generation)** - –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å LLM —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
3. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã
4. **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è** - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
5. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã** - –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç

---

## –ò—Å—Ç–æ—Ä–∏—è: –æ—Ç Word2Vec –¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤

### 2013: Word2Vec - —Ä–µ–≤–æ–ª—é—Ü–∏—è –Ω–∞—á–∞–ª–∞—Å—å

**Word2Vec**, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¢–æ–º–∞—à–µ–º –ú–∏–∫–æ–ª–æ–≤—ã–º –∏ –∫–æ–º–∞–Ω–¥–æ–π –≤ Google, —Å—Ç–∞–ª –ø–µ—Ä–≤—ã–º –ø–æ–¥—Ö–æ–¥–æ–º, –ø—Ä–æ–∏–∑–≤–æ–¥—è—â–∏–º –ø–æ—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ word embeddings. –û–Ω —Å–¥–µ–ª–∞–ª –≤–æ–∑–º–æ–∂–Ω—ã–º –≤–µ—Å—å –ø–æ—Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–±–ª–∞—Å—Ç–∏ NLP, –≤–∫–ª—é—á–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã.

#### –î–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Word2Vec

**CBOW (Continuous Bag of Words)**
- –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–æ–≤–æ –ø–æ –µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- –ë—ã—Å—Ç—Ä–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–æ –ø–∞–º—è—Ç–∏
- –ü—Ä–∏–º–µ—Ä: "The cat ___ on the mat" -> "sat"

**Skip-Gram**
- –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Å–ª–æ–≤—É
- –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–µ–¥–∫–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
- –ü—Ä–∏–º–µ—Ä: "sat" -> ["The", "cat", "on", "the", "mat"]

\`\`\`python
# –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä Skip-Gram
# –í—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ "–∫–æ—Ç" -> –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ–∫—Ä—É–∂–∞—é—â–∏–µ —Å–ª–æ–≤–∞
input_word = "–∫–æ—Ç"
context_words = ["–ø—É—à–∏—Å—Ç—ã–π", "–º—É—Ä–ª—ã—á–µ—Ç", "—Å–ø–∏—Ç", "–Ω–∞", "–¥–∏–≤–∞–Ω–µ"]
\`\`\`

#### –ì–ª–∞–≤–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Word2Vec

Word2Vec –¥–∞–µ—Ç **–æ–¥–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞**, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –°–ª–æ–≤–æ "bank" (–±–∞–Ω–∫ —Ä–µ–∫–∏ vs. –±–∞–Ω–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π) –ø–æ–ª—É—á–∞–µ—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, –Ω–µ –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ –Ω–∏ –æ–¥–∏–Ω –∏–∑ —Å–º—ã—Å–ª–æ–≤ —Ç–æ—á–Ω–æ.

### 2014: GloVe - –≥–ª–æ–±–∞–ª—å–Ω—ã–π –≤–∑–≥–ª—è–¥

Stanford –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª **GloVe (Global Vectors)**, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∏–ª Word2Vec, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤ –≤–æ –≤—Å–µ–º –∫–æ—Ä–ø—É—Å–µ, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

### 2016: FastText - –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ

**FastText** –æ—Ç Facebook —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –∫–∞–∫ –Ω–∞–±–æ—Ä—ã —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö n-–≥—Ä–∞–º–º. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç:
- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞
- –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å embeddings –¥–ª—è —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã–º–∏ —è–∑—ã–∫–∞–º–∏ (–≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π)

### 2018: ELMo - –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ—à–∞–µ—Ç –≤—Å–µ

**ELMo (Embeddings from Language Models)** - –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å —Å **–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–º–∏ embeddings**. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π LSTM –∏ –¥–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

### 2017-2018: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ BERT

**–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã** ("Attention is All You Need", 2017) –∏ **BERT** (2018) –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–º–µ–Ω–∏–ª–∏ –ø–æ–¥—Ö–æ–¥:

- **Self-attention** –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É "—Å–º–æ—Ç—Ä–µ—Ç—å" –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
- **–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç** - BERT –≤–∏–¥–∏—Ç —Å–ª–æ–≤–∞ —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- **Pre-training + Fine-tuning** - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º—ã–µ –ø–æ–¥ –∑–∞–¥–∞—á—É

\`\`\`
–≠–≤–æ–ª—é—Ü–∏—è: —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ -> –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ embeddings

Word2Vec (2013): "bank" = [0.2, 0.5, ...] (–≤—Å–µ–≥–¥–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π)
     |
     v
BERT (2018): "river bank" = [0.1, 0.7, ...]
             "money bank" = [0.8, 0.2, ...]
             (—Ä–∞–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è!)
\`\`\`

### 2022-2025: –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embedding-–º–æ–¥–µ–ª–∏

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∑–∞–¥–∞—á retrieval —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º contrastive learning –∏ instruction-tuning:
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–¥–æ 32K —Ç–æ–∫–µ–Ω–æ–≤)
- Matryoshka Representation Learning –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
- –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)

---

## –ö–∞–∫ embeddings –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —Å–º—ã—Å–ª

### –ó–Ω–∞–º–µ–Ω–∏—Ç—ã–π –ø—Ä–∏–º–µ—Ä: King - Man + Woman = Queen

–≠—Ç–æ —Å–∞–º—ã–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏ —Å word embeddings:

\`\`\`
vector("king") - vector("man") + vector("woman") ‚âà vector("queen")
\`\`\`

#### –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?

Word2Vec –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –æ–±—ä–µ–º—ã —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö co-occurrences —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è - –≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ 200-300 —á–∏—Å–µ–ª.

–í–µ–∫—Ç–æ—Ä \`king - man\` –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é "–∫–æ—Ä–æ–ª–µ–≤—Å–∫–æ–π –æ—Å–æ–±—ã" –±–µ–∑ –≥–µ–Ω–¥–µ—Ä–Ω–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π. –î–æ–±–∞–≤–ª—è—è \`woman\`, –º—ã –ø–æ–ª—É—á–∞–µ–º "–∫–æ—Ä–æ–ª–µ–≤—Å–∫—É—é –æ—Å–æ–±—É –∂–µ–Ω—Å–∫–æ–≥–æ –ø–æ–ª–∞" = queen.

\`\`\`
–î—Ä—É–≥–∏–µ –ø—Ä–∏–º–µ—Ä—ã:
- paris - france + poland ‚âà warsaw (–∫–æ–Ω—Ü–µ–ø—Ü–∏—è "—Å—Ç–æ–ª–∏—Ü–∞")
- walking - walk + swim ‚âà swimming (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–∞)
\`\`\`

#### –í–∞–∂–Ω–∞—è –æ–≥–æ–≤–æ—Ä–∫–∞

–í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ –≤—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞ (king, man, woman) –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞. –ë–µ–∑ —ç—Ç–æ–≥–æ –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á–∞—Å—Ç–æ –æ—Å—Ç–∞–µ—Ç—Å—è "king". –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–º "—Ç—Ä—é–∫–∞–º –∑–∞ –∫—É–ª–∏—Å–∞–º–∏".

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ç–∞–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏–∏ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è –≥–µ–Ω–¥–µ—Ä–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.

### –ö–∞–∫ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è —Å–µ–º–∞–Ω—Ç–∏–∫–µ

1. **–ì–∏–ø–æ—Ç–µ–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è** - —Å–ª–æ–≤–∞ –≤ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –∏–º–µ—é—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
2. **Co-occurrence statistics** - –ø–æ–¥—Å—á–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤
3. **Neural network training** - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
4. **Dimensionality reduction** - —Å–∂–∞—Ç–∏–µ –≤ –ø–ª–æ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ

---

## –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ embeddings

### OpenAI text-embedding-3 (–Ø–Ω–≤–∞—Ä—å 2024)

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | MTEB | MIRACL | –¶–µ–Ω–∞ |
|--------|-------------|------|--------|------|
| text-embedding-3-small | 1536 | ~62% | ~44% | \$0.02/1M —Ç–æ–∫–µ–Ω–æ–≤ |
| text-embedding-3-large | 3072 | 64.6% | 54.9% | \$0.13/1M —Ç–æ–∫–µ–Ω–æ–≤ |

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- **Matryoshka Representation Learning** - –º–æ–∂–Ω–æ —É—Ä–µ–∑–∞—Ç—å –¥–æ 256/512/1024 dimensions
- 256-–º–µ—Ä–Ω–∞—è –≤–µ—Ä—Å–∏—è text-embedding-3-large –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç 1536-–º–µ—Ä–Ω—É—é ada-002
- 8,191 —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
- –£–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏ –Ω–∞ 75% (—Å 31.4% –¥–æ 54.9% –Ω–∞ MIRACL)

### Voyage AI (2024-2025)

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | vs OpenAI | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –¶–µ–Ω–∞ |
|--------|-------------|-----------|----------|------|
| voyage-3 | 1024 | +7.55% | 32K | \$0.06/1M —Ç–æ–∫–µ–Ω–æ–≤ |
| voyage-3-lite | 512 | +3.82% | 32K | \$0.02/1M —Ç–æ–∫–µ–Ω–æ–≤ |
| voyage-3-large | 2048 | +9.74% | 32K | ~\$0.12/1M —Ç–æ–∫–µ–Ω–æ–≤ |
| voyage-code-3 | 1024 | +13.8% (–∫–æ–¥) | 32K | \$0.22/1M —Ç–æ–∫–µ–Ω–æ–≤ |

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –õ–∏–¥–µ—Ä –ø–æ –∫–∞—á–µ—Å—Ç–≤—É retrieval –≤ —è–Ω–≤–∞—Ä–µ 2025
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Matryoshka + quantization (int8, binary)
- **voyage-multimodal-3** - —Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + PDF screenshots
- **voyage-context-3** - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ chunk embeddings
- –ë–µ—Å–ø–ª–∞—Ç–Ω–æ: –ø–µ—Ä–≤—ã–µ 200M —Ç–æ–∫–µ–Ω–æ–≤

### Cohere Embed v3

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –Ø–∑—ã–∫–∏ | –¶–µ–Ω–∞ |
|--------|-------------|-------|------|
| embed-english-v3.0 | 1024 | English | \$0.12/1M —Ç–æ–∫–µ–Ω–æ–≤ |
| embed-multilingual-v3.0 | 1024 | 100+ | \$0.12/1M —Ç–æ–∫–µ–Ω–æ–≤ |

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- SOTA –Ω–∞ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö benchmarks (MIRACL)
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ multimodal (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è) —Å —è–Ω–≤–∞—Ä—è 2025
- –ë—ã—Å—Ç—Ä–µ–µ OpenAI –Ω–∞ 50-60%
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: 512 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—Ö–æ–¥

### Google Gemini Embedding (2024-2025)

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –¶–µ–Ω–∞ |
|--------|-------------|------|
| gemini-embedding-001 | 768 | \$0.15/1M —Ç–æ–∫–µ–Ω–æ–≤ |

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –¢–æ–ø-–ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ MTEB Multilingual leaderboard
- –î–æ 250 –≤—Ö–æ–¥–æ–≤ –∑–∞ –∑–∞–ø—Ä–æ—Å
- 2,048 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—Ö–æ–¥ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ)
- –ë–µ—Å–ø–ª–∞—Ç–Ω–æ —á–µ—Ä–µ–∑ Google AI Studio

### Open-source –º–æ–¥–µ–ª–∏

#### BGE-M3 (BAAI, 2024)

\`\`\`
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 1024
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 568M
- –ö–æ–Ω—Ç–µ–∫—Å—Ç: 8,192 —Ç–æ–∫–µ–Ω–∞
- –Ø–∑—ã–∫–∏: 170+
\`\`\`

**–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –¢—Ä–∏ —Ä–µ–∂–∏–º–∞ retrieval –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ: dense, sparse, multi-vector
- SOTA –Ω–∞ MIRACL (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π) –∏ MKQA (–∫—Ä–æ—Å—Å-—è–∑—ã—á–Ω—ã–π)
- –°—Ç–∞–±–∏–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞–∂–µ –Ω–∞ low-resource —è–∑—ã–∫–∞—Ö (–∞—Ä–∞–±—Å–∫–∏–π, –∫—Ö–º–µ—Ä—Å–∫–∏–π, –∏–≤—Ä–∏—Ç)

#### NVIDIA NV-Embed (2024-2025)

- –î–æ—Å—Ç–∏–≥ 69.32 –Ω–∞ MTEB (56 –∑–∞–¥–∞—á) - —Ä–µ–∫–æ—Ä–¥
- –ù–æ–≤—ã–π latent attention layer
- –î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: contrastive learning + hard negative mining

#### Stella (2024)

- –¢–æ–ø –Ω–∞ MTEB retrieval —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π
- –í–∞—Ä–∏–∞–Ω—Ç—ã 400M –∏ 1.5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

---

## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã

### –í—ã–±–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (dimensions)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏:**

| –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö | –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ | –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å |
|---------------|------------------|---------------------------|
| < 100K –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π | –ü—Ä–æ—Å—Ç–∞—è (keyword matching) | 50-100 |
| 100K - 1M | –°—Ä–µ–¥–Ω—è—è | 256-512 |
| > 1M | –°–ª–æ–∂–Ω–∞—è (semantic search) | 768-1024 |

**Sweet spot –¥–ª—è text-embedding-3-large: 1024 dimensions**
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–∞ –∂–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ 3072
- 3x –º–µ–Ω—å—à–µ storage (4KB vs 12KB –Ω–∞ –≤–µ–∫—Ç–æ—Ä)

**–ü—Ä–∞–≤–∏–ª–æ:** –±–æ–ª—å—à–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –±–æ–ª—å—à–µ –Ω—é–∞–Ω—Å–æ–≤, –Ω–æ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –î–ª—è 1M –≤–µ–∫—Ç–æ—Ä–æ–≤: 1024d = 4GB, 256d = 1GB.

### Cosine Similarity vs Dot Product

\`\`\`python
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def dot_product(a, b):
    return np.dot(a, b)
\`\`\`

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cosine Similarity:**
- –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–æ —Ç–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –Ω–µ –º–∞–≥–Ω–∏—Ç—É–¥–∞
- –î–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –¥–æ–ª–∂–Ω—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ
- Semantic search, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Dot Product:**
- –ö–æ–≥–¥–∞ –º–∞–≥–Ω–∏—Ç—É–¥–∞ –Ω–µ—Å–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (popularity, –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
- –ö–æ–≥–¥–∞ embeddings —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (dot product = cosine similarity)

**–í–∞–∂–Ω–æ:** –î–ª—è L2-–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π) —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã. OpenAI –∏ Vertex AI –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ embeddings.

### Quantization (–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)

–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä embeddings —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞:

| –¢–∏–ø | –†–∞–∑–º–µ—Ä | –£—Å–∫–æ—Ä–µ–Ω–∏–µ | –ü–æ—Ç–µ—Ä—è –∫–∞—á–µ—Å—Ç–≤–∞ |
|-----|--------|-----------|-----------------|
| Float32 | 100% | 1x | 0% |
| Scalar (int8) | 25% | ~4x | ~1-3% |
| Binary (1-bit) | 3.1% | ~24x | ~5-10% |

**–ü—Ä–∞–∫—Ç–∏–∫–∞ –≤ production:**
1. Binary quantization –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–±—ã—Å—Ç—Ä–æ, –≤ –ø–∞–º—è—Ç–∏)
2. Scalar (int8) –¥–ª—è rescoring (—Å –¥–∏—Å–∫–∞)
3. Float32 —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è

\`\`\`python
# –ü—Ä–∏–º–µ—Ä rescoring pipeline
candidates = binary_search(query, top_k=1000)  # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫
reranked = scalar_rescore(query, candidates, top_k=100)  # –£—Ç–æ—á–Ω–µ–Ω–∏–µ
final = full_precision_rank(query, reranked, top_k=10)  # –§–∏–Ω–∞–ª
\`\`\`

**MongoDB Atlas –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:**
- Float32: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
- Int8: 3.75x –º–µ–Ω—å—à–µ RAM
- Binary: 24x –º–µ–Ω—å—à–µ RAM

### Matryoshka Representation Learning (MRL)

MRL –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–∞–∫, —á—Ç–æ–±—ã embeddings –º–æ–∂–Ω–æ –±—ã–ª–æ —É–∫–æ—Ä–∞—á–∏–≤–∞—Ç—å –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:

\`\`\`python
# –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 1024 dimensions
full_embedding = model.encode("text")  # [1024 dimensions]

# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π –ø—Ä–µ—Ñ–∏–∫—Å!
short_embedding = full_embedding[:256]  # [256 dimensions] - –≤—Å–µ –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!
\`\`\`

**–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ loss –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ —Ä–∞–∑–Ω—ã–º —Å—Ä–µ–∑–∞–º: 768, 512, 256, 128, 64, 32
- –ú–æ–¥–µ–ª—å "front-loads" –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø–µ—Ä–≤—ã–µ dimensions
- –î–æ 14x –º–µ–Ω—å—à–µ storage –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ –∫–∞—á–µ—Å—Ç–≤–∞

**–ú–æ–¥–µ–ª–∏ —Å MRL:**
- OpenAI text-embedding-3-large
- Nomic nomic-embed-text-v1.5
- Voyage AI voyage-3-large
- Alibaba gte-multilingual-base

### –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ embeddings

**–ö–ª—é—á–µ–≤—ã–µ –º–æ–¥–µ–ª–∏:**

| –ú–æ–¥–µ–ª—å | –Ø–∑—ã–∫–∏ | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ |
|--------|-------|-------------|
| BGE-M3 | 170+ | Dense + sparse + multi-vector |
| LaBSE (Google) | 109 | Language-agnostic, —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–∂–µ –Ω–∞ —è–∑—ã–∫–∞—Ö –±–µ–∑ training data |
| Cohere embed-multilingual-v3 | 100+ | Cross-lingual search |
| mE5 | 100+ | –ù–∞ –±–∞–∑–µ XLM-RoBERTa |

**Cross-lingual retrieval:**
- –ü–æ–∏—Å–∫ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º –ø–æ —Ñ–∏–Ω—Å–∫–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
- –í–∞–∂–Ω–æ –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
- –ü—Ä–æ–±–ª–µ–º–∞: "monolingual overfitting" - –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö –¥–∞–∂–µ —Å multilingual backbone

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è RAG

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ embeddings

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞:**

1. **–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω—É** - –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–Ω–∏–º–∞—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
2. **Benchmark –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** - —Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ Retrieval task –≤ MTEB
3. **Context window** - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–ª—è –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤?
4. **Latency –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å** - –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –¥–ª—è production?
5. **–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å** - –Ω—É–∂–Ω–∞ –ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤?

\`\`\`python
# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ use cases
use_cases = {
    "general_english": "voyage-3 –∏–ª–∏ text-embedding-3-large",
    "budget_conscious": "text-embedding-3-small –∏–ª–∏ voyage-3-lite",
    "multilingual": "BGE-M3 –∏–ª–∏ Cohere embed-multilingual-v3",
    "code": "voyage-code-3",
    "legal": "voyage-law-2",
    "finance": "voyage-finance-2",
    "self_hosted": "BGE-M3 –∏–ª–∏ Stella"
}
\`\`\`

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ chunking

**–¢–∏–ø—ã chunking:**

| –°—Ç—Ä–∞—Ç–µ–≥–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-----------|----------|-------------------|
| Fixed-size | –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ | –ü—Ä–æ—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–∫–æ—Ä–æ—Å—Ç—å |
| Recursive | –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (\\n\\n, \\n, " ") | –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç |
| Semantic | –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –≥—Ä–∞–Ω–∏—Ü–∞–º | –ö–æ–≥–¥–∞ –∫–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ |

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- **–†–∞–∑–º–µ—Ä chunk:** 400-512 —Ç–æ–∫–µ–Ω–æ–≤
- **Overlap:** 10-20% (50-100 —Ç–æ–∫–µ–Ω–æ–≤)
- **–°—Ç–∞—Ä—Ç–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:** RecursiveCharacterTextSplitter

\`\`\`python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\\n\\n", "\\n", ". ", " ", ""]
)
\`\`\`

**Late Chunking (Jina AI, 2024):**

–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
1. –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ transformer
2. –ü–æ–ª—É—á–∏—Ç—å token-level embeddings —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
3. –ü—Ä–∏–º–µ–Ω–∏—Ç—å mean pooling –∫ –∫–∞–∂–¥–æ–º—É chunk

\`\`\`python
# API-–≤—ã–∑–æ–≤ —Å late chunking
response = client.embed(
    model="jina-embeddings-v3",
    input=["chunk1", "chunk2", "chunk3"],
    late_chunking=True  # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É chunks
)
\`\`\`

### Hybrid Search

–ö–æ–º–±–∏–Ω–∞—Ü–∏—è vector search –∏ keyword search (BM25):

\`\`\`python
# –ü—Ä–∏–º–µ—Ä hybrid search —Å Anthropic's Contextual Retrieval
def hybrid_search(query, documents):
    # 1. Dense retrieval (semantic)
    dense_results = vector_search(query, documents, top_k=20)
    
    # 2. Sparse retrieval (keyword, BM25)
    sparse_results = bm25_search(query, documents, top_k=20)
    
    # 3. Fusion (RRF - Reciprocal Rank Fusion)
    combined = reciprocal_rank_fusion(dense_results, sparse_results)
    
    # 4. Reranking –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    final = rerank(query, combined, top_k=5)
    
    return final
\`\`\`

**Anthropic Contextual Retrieval:**
- –£–º–µ–Ω—å—à–∞–µ—Ç failed retrievals –Ω–∞ 49%
- –° reranking - –Ω–∞ 67%

### Semantic Caching

–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤:

\`\`\`python
class SemanticCache:
    def __init__(self, similarity_threshold=0.95):
        self.cache = {}
        self.embeddings = {}
        self.threshold = similarity_threshold
    
    def get(self, query):
        query_embedding = embed(query)
        
        for cached_query, cached_response in self.cache.items():
            similarity = cosine_similarity(
                query_embedding, 
                self.embeddings[cached_query]
            )
            if similarity > self.threshold:
                return cached_response
        
        return None
    
    def set(self, query, response):
        self.cache[query] = response
        self.embeddings[query] = embed(query)
\`\`\`

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ production:**
- –°–Ω–∏–∂–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ LLM inference –¥–æ 86%
- –£–ª—É—á—à–µ–Ω–∏–µ latency –¥–æ 88% (—Å 2.7s –¥–æ 0.3s)

**–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è semantic caching:** \`all-mpnet-base-v2\` (–±–∞–ª–∞–Ω—Å precision, recall, latency)

### Fine-tuning embeddings

**–ö–æ–≥–¥–∞ fine-tuning –Ω—É–∂–µ–Ω:**
- –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –¥–æ–º–µ–Ω–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∞–≤–æ, —Ñ–∏–Ω–∞–Ω—Å—ã)
- Off-the-shelf –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- –ï—Å—Ç—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (query-document –ø–∞—Ä—ã)

**–ö–æ–≥–¥–∞ –ù–ï –Ω—É–∂–µ–Ω:**
- –ü—Ä–æ–±–ª–µ–º–∞ –≤ chunking, –∞ –Ω–µ –≤ –º–æ–¥–µ–ª–∏
- –ù—É–∂–µ–Ω exact keyword matching (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ hybrid search)
- –ú–∞–ª–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è —Å pre-training –∫–æ—Ä–ø—É—Å–æ–º –º–æ–¥–µ–ª–∏

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
- ~7% —É–ª—É—á—à–µ–Ω–∏–µ —Å 6.3K samples
- 3 –º–∏–Ω—É—Ç—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞ consumer GPU
- –° MRL: 99% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ 6x –º–µ–Ω—å—à–µ–º storage

\`\`\`python
from sentence_transformers import SentenceTransformer
from sentence_transformers.losses import MultipleNegativesRankingLoss

# Fine-tuning —Å contrastive loss
model = SentenceTransformer('BAAI/bge-base-en-v1.5')
train_loss = MultipleNegativesRankingLoss(model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=1,
    warmup_steps=100
)
\`\`\`

---

## –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π

### –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ API (–î–µ–∫–∞–±—Ä—å 2024)

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –¶–µ–Ω–∞ (\$/1M —Ç–æ–∫–µ–Ω–æ–≤) | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è |
|--------|-------------|----------|---------------------|------------|
| **OpenAI text-embedding-3-small** | 1536 | 8K | \$0.02 | –õ—É—á—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ |
| **OpenAI text-embedding-3-large** | 3072* | 8K | \$0.13 | MRL (–º–æ–∂–Ω–æ –¥–æ 256d) |
| **Voyage voyage-3** | 1024 | 32K | \$0.06 | +7.55% vs OpenAI large |
| **Voyage voyage-3-large** | 2048* | 32K | ~\$0.12 | SOTA —è–Ω–≤–∞—Ä—å 2025 |
| **Voyage voyage-3-lite** | 512 | 32K | \$0.02 | –≠–∫–æ–Ω–æ–º–∏—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç |
| **Cohere embed-v3** | 1024 | 512 | \$0.12 | 100+ —è–∑—ã–∫–æ–≤, multimodal |
| **Google gemini-embedding-001** | 768 | 2K | \$0.15 | –¢–æ–ø multilingual |

*–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç MRL (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞)

### Open-source –º–æ–¥–µ–ª–∏

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ |
|--------|-------------|----------|-----------|-------------|
| **BGE-M3** | 1024 | 8K | 568M | Dense+sparse+multi-vector, 170 —è–∑—ã–∫–æ–≤ |
| **Stella-1.5B** | 1024 | 8K | 1.5B | –¢–æ–ø MTEB retrieval, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –ª–∏—Ü–µ–Ω–∑–∏—è |
| **NV-Embed** | 4096 | 32K | 8B | MTEB —Ä–µ–∫–æ—Ä–¥ 69.32 |
| **gte-Qwen2-7B** | 4096 | 131K | 7B | –û—Ç–ª–∏—á–Ω—ã–π multilingual |
| **jina-embeddings-v3** | 1024 | 8K | 570M | Late chunking, MRL |

---

## –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å 2024-2025

| –¢—Ä–µ–Ω–¥ | –°—Ç–∞—Ç—É—Å | –ß—Ç–æ –≤–∞–∂–Ω–æ –∑–Ω–∞—Ç—å |
|-------|--------|-----------------|
| **Voyage-3-large** | üî• SOTA (—è–Ω–≤–∞—Ä—å 2025) | +9.74% vs OpenAI v3-large, 32K –∫–æ–Ω—Ç–µ–∫—Å—Ç, multimodal support |
| **Gemini-embedding-001** | ü•á MTEB #1 | –¢–æ–ø multilingual, 768 dims, –±–µ—Å–ø–ª–∞—Ç–Ω–æ –≤ Google AI Studio |
| **Matryoshka (MRL)** | ‚úÖ Production-ready | –î–æ 14x –º–µ–Ω—å—à–µ storage –ø—Ä–∏ 96-99% quality, front-loading info |
| **Late Chunking** | üÜï –ù–∞–±–∏—Ä–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å | Jina: 82-84% vs 70-75% similarity, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ |
| **Binary Quantization** | ‚úÖ Production | 32x memory reduction, 96% performance —Å rescoring strategy |
| **Scalar (int8) Quantization** | ‚úÖ Production | 4x memory reduction, 99% performance retention |
| **Sentence Transformers 3.0** | ‚úÖ –†–µ–ª–∏–∑ | MNR Loss, Matryoshka Loss, –ø—Ä–æ—Å—Ç–æ–π fine-tuning API |
| **Synthetic Contrastive Data** | üîÑ Best practice | LLM-generated hard negatives –¥–ª—è fine-tuning |

### Community Sentiment (Reddit, HN, Stack Overflow)

**–ß—Ç–æ —Ö–≤–∞–ª—è—Ç:**
- Voyage-3: "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ OpenAI –¥–ª—è retrieval" (r/LocalLLaMA)
- BGE-M3: "–ª—É—á—à–∏–π open-source –¥–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á"
- MRL: "game-changer –¥–ª—è production costs"

**–ß—Ç–æ –∫—Ä–∏—Ç–∏–∫—É—é—Ç:**
- OpenAI v3: "overhyped, Voyage –ª—É—á—à–µ –∑–∞ —Ç–µ –∂–µ –¥–µ–Ω—å–≥–∏"
- Cohere: "512 —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
- Fine-tuning: "—á–∞—Å—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤ chunking, –∞ –Ω–µ –≤ –º–æ–¥–µ–ª–∏"

### Benchmarks 2025

| –ú–æ–¥–µ–ª—å | MTEB Score | MIRACL (multilingual) | Retrieval |
|--------|------------|----------------------|-----------|
| Gemini-embedding-001 | 68.4 | ‚Äî | ‚Äî |
| Qwen3-Embedding | 68.2 | ‚Äî | ‚Äî |
| Voyage-3-large | ~67 | 59.2% | 64.9% |
| NV-Embed-v2 | 69.32 | ‚Äî | ‚Äî |
| OpenAI v3-large | 64.6 | 54.9% | ‚Äî |

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

| # | –ò—Å—Ç–æ—á–Ω–∏–∫ | –¢–∏–ø | –í–∫–ª–∞–¥ |
|---|----------|-----|-------|
| 1 | [OpenAI Embeddings Docs](https://platform.openai.com/docs/guides/embeddings) | Docs | API, MRL, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ |
| 2 | [Voyage AI Docs](https://docs.voyageai.com/docs/embeddings) | Docs | voyage-3 API, quantization |
| 3 | [Voyage-3-Large Announcement](https://blog.voyageai.com/2025/01/07/voyage-3-large/) | Blog | SOTA –º–æ–¥–µ–ª—å 2025, benchmarks |
| 4 | [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) | Benchmark | –ê–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π |
| 5 | [BGE-M3 Paper](https://arxiv.org/abs/2402.03216) | Paper | Multi-vector, 170 —è–∑—ã–∫–æ–≤ |
| 6 | [Binary Quantization - HuggingFace](https://huggingface.co/blog/embedding-quantization) | Guide | 32x memory reduction, rescoring |
| 7 | [Matryoshka RL - HuggingFace](https://huggingface.co/blog/matryoshka) | Guide | MRL training, –¥–æ 14x compression |
| 8 | [Late Chunking - Jina AI](https://jina.ai/news/late-chunking-in-long-context-embedding-models/) | Blog | Context preservation, 82-84% similarity |
| 9 | [Contextual Retrieval - Anthropic](https://www.anthropic.com/news/contextual-retrieval) | Blog | Hybrid search, -49% failed retrieval |
| 10 | [Sentence Transformers 3.0](https://huggingface.co/blog/train-sentence-transformers) | Guide | Fine-tuning API, losses |
| 11 | [How to Choose Embedding Model - Weaviate](https://weaviate.io/blog/how-to-choose-an-embedding-model) | Guide | Selection criteria, benchmarks |
| 12 | [Fine-Tune Embedding Model - Weaviate](https://weaviate.io/blog/fine-tune-embedding-model) | Guide | When to fine-tune, results |
| 13 | [Cohere Embed v3](https://docs.cohere.com/docs/cohere-embed) | Docs | 100+ —è–∑—ã–∫–æ–≤, multimodal |
| 14 | [Vector Similarity - Pinecone](https://www.pinecone.io/learn/vector-similarity/) | Guide | Cosine vs dot product |
| 15 | [Semantic Caching - Redis](https://redis.io/blog/what-is-semantic-caching/) | Blog | 86% cost reduction, latency |

---

## Quick Reference Card

\`\`\`
–í–´–ë–û–† –ú–û–î–ï–õ–ò:
- –û–±—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (EN): voyage-3 –∏–ª–∏ text-embedding-3-large
- –ë—é–¥–∂–µ—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: text-embedding-3-small (~\$0.02/1M)
- –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π: BGE-M3 –∏–ª–∏ Cohere multilingual
- –ö–æ–¥: voyage-code-3
- Self-hosted: BGE-M3 (568M params)

–†–ê–ó–ú–ï–†–ù–û–°–¢–¨:
- Production sweet spot: 512-1024
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ MRL –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
- 1024d embedding = 4KB storage

CHUNKING:
- –°—Ç–∞—Ä—Ç–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: 400-512 —Ç–æ–∫–µ–Ω–æ–≤
- Overlap: 10-20%
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ recursive splitter
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ late chunking –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

SIMILARITY:
- –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ vectors: dot product = cosine (–æ–¥–∏–Ω–∞–∫–æ–≤–æ)
- OpenAI/Voyage –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ embeddings
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ metric –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏

QUANTIZATION –í PRODUCTION:
- Binary –¥–ª—è –ø–æ–∏—Å–∫–∞ (32x —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
- Int8 –¥–ª—è rescoring (4x —ç–∫–æ–Ω–æ–º–∏—è)
- Float32 —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
\`\`\`

---

*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2024-12-28*

---

## –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏

### [[vector-databases-guide]]

Embeddings –∏ vector databases ‚Äî –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º–∞—è –ø–∞—Ä–∞: embeddings —Å–æ–∑–¥–∞—é—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∞ vector databases —Ö—Ä–∞–Ω—è—Ç –∏ –∏—â—É—Ç –ø–æ –Ω–∏–º. –í—ã–±–æ—Ä embedding –º–æ–¥–µ–ª–∏ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞, quantization, —Å–∫–æ—Ä–æ—Å—Ç—å inference) –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ vector database: —Ä–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞, latency –ø–æ–∏—Å–∫–∞, —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è. –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ similarity search (HNSW, IVF, PQ) –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é embedding –º–æ–¥–µ–ª—å ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, binary quantization —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ –Ω–µ —Å–æ –≤—Å–µ–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏.

### [[rag-advanced-techniques]]

Embeddings ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç retrieval-—ç—Ç–∞–ø–∞ –≤ RAG pipeline. –ö–∞—á–µ—Å—Ç–≤–æ embedding –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–π–¥—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –¢–µ—Ö–Ω–∏–∫–∏ –≤—Ä–æ–¥–µ HyDE (Hypothetical Document Embedding), semantic chunking –∏ query expansion –æ–ø–∏—Ä–∞—é—Ç—Å—è –Ω–∞ —Å–≤–æ–π—Å—Ç–≤–∞ embedding –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. Fine-tuning embedding –º–æ–¥–µ–ª–∏ –Ω–∞ domain-specific –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –¥–∞—Ç—å –¥–æ +7% improvement –≤ retrieval accuracy, —á—Ç–æ –∫–∞—Å–∫–∞–¥–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤—Å–µ–≥–æ RAG pipeline.

### [[aiml-databases-complete]]

Embeddings —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ö—Ä–∞–Ω–µ–Ω–∏—é –∏ –ø–æ–∏—Å–∫—É –¥–∞–Ω–Ω—ã—Ö. –ü–æ–º–∏–º–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö vector databases, –º–Ω–æ–≥–∏–µ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –°–£–ë–î (PostgreSQL —Å pgvector, SQLite —Å sqlite-vss) –¥–æ–±–∞–≤–ª—è—é—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É vector search, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å embeddings –≤–º–µ—Å—Ç–µ —Å metadata –≤ –æ–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ. –ü–æ–Ω–∏–º–∞–Ω–∏–µ trade-offs –º–µ–∂–¥—É native vector databases –∏ vector-—Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏ SQL –±–∞–∑ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã data layer –≤ AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.

---

[[ai-engineering-moc|‚Üê AI Engineering MOC]] | [[vector-databases-guide|Vector Databases ‚Üí]]

---

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ü–æ—á–µ–º—É Matryoshka Representation Learning (MRL) —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø—Ä–æ—Ä—ã–≤–æ–º –¥–ª—è production-—Å–∏—Å—Ç–µ–º?
> MRL –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–µ–∑–∞—Ç—å embedding –≤–µ–∫—Ç–æ—Ä –¥–æ –ª—é–±–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1024 -> 256 dimensions) –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–æ–¥–µ–ª—å "front-loads" –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø–µ—Ä–≤—ã–µ dimensions. –†–µ–∑—É–ª—å—Ç–∞—Ç: –¥–æ 14x —ç–∫–æ–Ω–æ–º–∏—è storage –ø—Ä–∏ 96-99% —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞, –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π trade-off —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ –≤ runtime.

> [!question]- –£ –≤–∞—Å RAG-—Å–∏—Å—Ç–µ–º–∞ —Å 10M –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, latency –ø–æ–∏—Å–∫–∞ 500ms. –ö–∞–∫ —Å–Ω–∏–∑–∏—Ç—å –¥–æ <50ms —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞?
> –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π pipeline: 1) Binary quantization –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ in-memory (32x —ç–∫–æ–Ω–æ–º–∏—è RAM, top-1000), 2) Scalar int8 rescoring (4x —ç–∫–æ–Ω–æ–º–∏—è, top-100), 3) Float32 reranking (top-10). –° rescoring binary –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 96%+ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ latency <50ms.

> [!question]- –ö–æ–≥–¥–∞ fine-tuning embedding –º–æ–¥–µ–ª–∏ –æ–ø—Ä–∞–≤–¥–∞–Ω, –∞ –∫–æ–≥–¥–∞ —ç—Ç–æ –ø—É—Å—Ç–∞—è —Ç—Ä–∞—Ç–∞ —Ä–µ—Å—É—Ä—Å–æ–≤?
> –û–ø—Ä–∞–≤–¥–∞–Ω: —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –¥–æ–º–µ–Ω–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∞–≤–æ), off-the-shelf –º–æ–¥–µ–ª–∏ <70% accuracy, –µ—Å—Ç—å 5000+ query-document –ø–∞—Ä. –ù–µ –æ–ø—Ä–∞–≤–¥–∞–Ω: –ø—Ä–æ–±–ª–µ–º–∞ –≤ chunking –∞ –Ω–µ –≤ –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–Ω–∞—á–∞–ª–∞!), –Ω—É–∂–µ–Ω exact matching (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ hybrid search), –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (<1000 –ø–∞—Ä), general-purpose –∑–∞–¥–∞—á–∏.

> [!question]- –ü–æ—á–µ–º—É Late Chunking –¥–∞–µ—Ç 82-84% similarity –≤–º–µ—Å—Ç–æ 70-75% —É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ chunking?
> –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π chunking —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç, –∑–∞—Ç–µ–º embed –∫–∞–∂–¥—ã–π chunk –æ—Ç–¥–µ–ª—å–Ω–æ -- chunk —Ç–µ—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç. Late Chunking —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ transformer (–ø–æ–ª—É—á–∞—è token-level embeddings —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º), –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç mean pooling –∫ –∫–∞–∂–¥–æ–º—É chunk. –ö–∞–∂–¥—ã–π chunk "–ø–æ–º–Ω–∏—Ç" –æ —á–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç.

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ß—Ç–æ —Ç–∞–∫–æ–µ embedding –∏ –∑–∞—á–µ–º –Ω—É–∂–µ–Ω?
?
–ß–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ (—Å–ª–æ–≤–∞, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –≤ –≤–∏–¥–µ –≤–µ–∫—Ç–æ—Ä–∞. –ü–æ—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É —Ç–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, RAG retrieval, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

Cosine Similarity vs Dot Product -- –∫–æ–≥–¥–∞ —á—Ç–æ?
?
Cosine Similarity: –∫–æ–≥–¥–∞ –≤–∞–∂–Ω–æ —Ç–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, —Ä–∞–∑–Ω—ã–µ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤). Dot Product: –∫–æ–≥–¥–∞ –º–∞–≥–Ω–∏—Ç—É–¥–∞ –Ω–µ—Å–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, popularity). –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (OpenAI, Voyage) —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã.

–ö–∞–∫—É—é –º–æ–¥–µ–ª—å embeddings –≤—ã–±—Ä–∞—Ç—å –≤ 2025?
?
–û–±—â–µ–µ (EN): Voyage-3 –∏–ª–∏ text-embedding-3-large. –ë—é–¥–∂–µ—Ç: text-embedding-3-small ($0.02/1M). –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π: BGE-M3 –∏–ª–∏ Cohere multilingual. –ö–æ–¥: voyage-code-3. Self-hosted: BGE-M3 (568M params). SOTA: Voyage-3-large (+9.74% vs OpenAI).

–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunking –¥–ª—è RAG?
?
–†–∞–∑–º–µ—Ä chunk: 400-512 —Ç–æ–∫–µ–Ω–æ–≤. Overlap: 10-20% (50-100 —Ç–æ–∫–µ–Ω–æ–≤). –°—Ç–∞—Ä—Ç–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: RecursiveCharacterTextSplitter. –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å Late Chunking (Jina AI) –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ß—Ç–æ —Ç–∞–∫–æ–µ Hybrid Search –∏ –∑–∞—á–µ–º –Ω—É–∂–µ–Ω?
?
–ö–æ–º–±–∏–Ω–∞—Ü–∏—è vector search (semantic similarity) –∏ keyword search (BM25). Vector search –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª, BM25 –ª–æ–≤–∏—Ç —Ç–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Reciprocal Rank Fusion + reranking. Anthropic Contextual Retrieval: -49% failed retrievals, —Å reranking -67%.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[vector-databases-guide]] | –ì–¥–µ –∏ –∫–∞–∫ —Ö—Ä–∞–Ω–∏—Ç—å embeddings –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[rag-advanced-techniques]] | –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ RAG, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ embeddings |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[database-design-optimization]] | –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ë–î, –ø—Ä–∏–º–µ–Ω–∏–º—ã–µ –∫ vector storage |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ —Ä–∞–∑–¥–µ–ª–∞ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
