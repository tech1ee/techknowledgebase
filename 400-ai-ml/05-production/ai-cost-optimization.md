---
title: "AI Cost Optimization - –ü–æ–ª–Ω–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
tags:
  - topic/ai-ml
  - cost
  - performance
  - pricing
  - tokens
  - caching
  - batch
  - rag
  - routing
  - type/concept
  - level/advanced
category: ai-ml
level: advanced
created: 2025-01-15
updated: 2026-02-13
reading_time: 86
difficulty: 6
study_status: not_started
mastery: 0
last_reviewed:
next_review:
related:
  - [llm-fundamentals]]
  - "[[ai-observability-monitoring]]"
  - "[[rag-systems]"
sources:
  - openai.com
  - anthropic.com
  - cloud.google.com
  - deepseek.com
  - microsoft.com
status: published
---

# AI Cost Optimization: –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ 60-95%

> –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ LLM API: –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–æ production-grade –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã.

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ LLM** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, inference | [[llm-fundamentals]] |
| **LLM API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** | –†–∞–±–æ—Ç–∞ —Å OpenAI, Anthropic, Google API | [[ai-api-integration]] |
| **Python** | –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞, —Å–∫—Ä–∏–ø—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥/–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ** | Tracking –∑–∞—Ç—Ä–∞—Ç | [[ai-observability-monitoring]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ AI** | ‚ö†Ô∏è –†–∞–Ω–æ | –°–Ω–∞—á–∞–ª–∞ [[llm-fundamentals]] –∏ [[ai-api-integration]] |
| **–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ —Å AI –æ–ø—ã—Ç–æ–º** | ‚úÖ –î–∞ | –§–æ–∫—É—Å –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ö–Ω–∏–∫–∞—Ö |
| **Tech Lead / Architect** | ‚úÖ –î–∞ | –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è, ROI |
| **DevOps/FinOps** | ‚úÖ –î–∞ | –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –±—é–¥–∂–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Cost Optimization** = –ø–æ–ª—É—á–∏—Ç—å —Ç–æ—Ç –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞ –º–µ–Ω—å—à–∏–µ –¥–µ–Ω—å–≥–∏ (–∏–ª–∏ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞ —Ç–µ –∂–µ –¥–µ–Ω—å–≥–∏)

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Input/Output Tokens** | –í—Ö–æ–¥–Ω—ã–µ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ "–∫—É—Å–æ—á–∫–∏" —Ç–µ–∫—Å—Ç–∞ | **–¢–∞–∫—Å–∏** ‚Äî –ø–ª–∞—Ç–∏—à—å –∑–∞ –∫–∏–ª–æ–º–µ—Ç—Ä—ã —Ç—É–¥–∞ (input) –∏ –æ–±—Ä–∞—Ç–Ω–æ (output) |
| **Prompt Caching** | –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —á–∞—Å—Ç–µ–π –∑–∞–ø—Ä–æ—Å–∞ | **–®–∞–±–ª–æ–Ω –ø–∏—Å—å–º–∞** ‚Äî –Ω–µ –ø–µ—á–∞—Ç–∞–µ—à—å —à–∞–ø–∫—É –∫–∞–∂–¥—ã–π —Ä–∞–∑ |
| **Batch API** | –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ —Å–∫–∏–¥–∫–æ–π | **–û–ø—Ç–æ–≤–∞—è –ø–æ–∫—É–ø–∫–∞** ‚Äî –¥–µ—à–µ–≤–ª–µ –∑–∞ –æ–±—ä—ë–º, –Ω–æ –∂–¥–∞—Ç—å –¥–æ–ª—å—à–µ |
| **Model Routing** | –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ | **–¢–∞–∫—Å–∏ vs –º–µ—Ç—Ä–æ** ‚Äî –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π –ø–æ–µ–∑–¥–∫–∏ –Ω–µ –Ω—É–∂–µ–Ω –ª–∏–º—É–∑–∏–Ω |
| **Semantic Caching** | –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–æ —Å–º—ã—Å–ª—É –∑–∞–ø—Ä–æ—Å–æ–≤ | **"–ö–∞–∫ –¥–µ–ª–∞?" = "–ö–∞–∫ –ø–æ–∂–∏–≤–∞–µ—à—å?"** ‚Äî –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç –Ω–∞ –æ–±–∞ |
| **Token Compression** | –°–∂–∞—Ç–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–º—ã—Å–ª–∞ | **Telegram –≤–º–µ—Å—Ç–æ email** ‚Äî –∫–æ—Ä–æ—á–µ, –Ω–æ —Å—É—Ç—å —Ç–∞ –∂–µ |
| **Self-hosting** | –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–≤–æ–∏—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö | **–°–≤–æ—è –º–∞—à–∏–Ω–∞ vs —Ç–∞–∫—Å–∏** ‚Äî –¥–æ—Ä–æ–≥–æ –∫—É–ø–∏—Ç—å, –¥—ë—à–µ–≤–æ –µ–∑–¥–∏—Ç—å |

---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ

**–ü—Ä–æ–±–ª–µ–º–∞:** LLM API —Å—Ç–æ—è—Ç –¥–æ—Ä–æ–≥–æ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏. –ö–æ–º–ø–∞–Ω–∏—è —Å 10K –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å –Ω–∞ GPT-4o –ø–ª–∞—Ç–∏—Ç ~$3,000/–º–µ—Å—è—Ü —Ç–æ–ª—å–∫–æ –∑–∞ input tokens.

**–†–µ—à–µ–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ 60-95% –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞:
- **Prompt Caching** ‚Äî 50-90% —ç–∫–æ–Ω–æ–º–∏—è –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–º–ø—Ç–∞—Ö
- **Model Routing** ‚Äî –¥–µ—à—ë–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á (70% —Ç—Ä–∞—Ñ–∏–∫–∞)
- **Batch API** ‚Äî 50% —Å–∫–∏–¥–∫–∞ –Ω–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
- **RAG** ‚Äî 70-90% —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

**–ö–æ–º—É –ø–æ–¥–æ–π–¥—ë—Ç:**
- –°—Ç–∞—Ä—Ç–∞–ø–∞–º, –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–º AI-–ø—Ä–æ–¥—É–∫—Ç—ã
- Enterprise –∫–æ–º–∞–Ω–¥–∞–º —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä—ë–º–∞–º–∏ API –≤—ã–∑–æ–≤–æ–≤
- Data scientists, —Å—Ç—Ä–æ—è—â–∏–º production ML –ø–∞–π–ø–ª–∞–π–Ω—ã

**–ß—Ç–æ –≤—ã —É–∑–Ω–∞–µ—Ç–µ:**
1. –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–¥–µ–∫–∞–±—Ä—å 2025)
2. 10 —Ç–µ—Ö–Ω–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –∫–æ–¥–æ–º
3. ROI –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
4. Production –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞—Ç—Ä–∞—Ç

---

## TL;DR

> **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞—Ç—Ä–∞—Ç –Ω–∞ LLM API** –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏. –ö–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ 2025:
> - **Prompt Caching**: 50-90% —ç–∫–æ–Ω–æ–º–∏—è –Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö (OpenAI 50%, Anthropic –¥–æ 90%)
> - **Batch API**: 50% —Å–∫–∏–¥–∫–∞ —É –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
> - **Model Routing**: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 70% –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –¥–µ—à—ë–≤—ã–µ –º–æ–¥–µ–ª–∏ (GPT-4o-mini, Gemini Flash)
> - **Semantic Caching**: 2-10x —É—Å–∫–æ—Ä–µ–Ω–∏–µ + —Å–Ω–∏–∂–µ–Ω–∏–µ API-–≤—ã–∑–æ–≤–æ–≤ —á–µ—Ä–µ–∑ GPTCache
> - **Token Compression**: LLMLingua –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–æ 20x —Å–∂–∞—Ç–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
> - **Self-hosting**: –æ–∫—É–ø–∞–µ—Ç—Å—è –ø—Ä–∏ 2M+ —Ç–æ–∫–µ–Ω–æ–≤/–¥–µ–Ω—å
>
> **–¢–∏–ø–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: 60-95% —Å–Ω–∏–∂–µ–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç. –ö–æ–º–ø–∞–Ω–∏–∏ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —Å–µ–∫—Ç–æ—Ä–µ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —ç–∫–æ–Ω–æ–º–∏–∏ –¥–æ 99.7%.
>
> **–¶–µ–Ω—ã –¥–µ–∫–∞–±—Ä—å 2025**: DeepSeek V3 $0.28/1M input, Gemini 2.0 Flash $0.10/1M, GPT-4o $2.50/1M, Claude Sonnet 4 $3/1M, Claude Opus 4.1 $20/1M.

---

## –ì–ª–æ—Å—Å–∞—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤

| –¢–µ—Ä–º–∏–Ω | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ |
|--------|-------------|
| **Tokens** | –ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (~4 —Å–∏–º–≤–æ–ª–∞ = 1 token) |
| **Input Tokens** | –¢–æ–∫–µ–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ (–ø—Ä–æ–º–ø—Ç + –∫–æ–Ω—Ç–µ–∫—Å—Ç) |
| **Output Tokens** | –¢–æ–∫–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è), –æ–±—ã—á–Ω–æ –≤ 2-5x –¥–æ—Ä–æ–∂–µ input |
| **Thinking Tokens** | –¢–æ–∫–µ–Ω—ã "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" –º–æ–¥–µ–ª–∏ (Claude 4.1, o1/o3) |
| **Prompt Caching** | –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ |
| **Semantic Caching** | –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–æ —Å–º—ã—Å–ª—É –∑–∞–ø—Ä–æ—Å–æ–≤ (GPTCache) |
| **Batch API** | –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ —Å–∫–∏–¥–∫–æ–π 50% (–¥–æ 24—á) |
| **Model Routing** | –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ |
| **RAG** | Retrieval-Augmented Generation - —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ |
| **Token Compression** | –°–∂–∞—Ç–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ (LLMLingua - –¥–æ 20x) |
| **Fine-tuning** | –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∑–∞–¥–∞—á—É |
| **PEFT/LoRA** | Parameter-Efficient Fine-Tuning - –¥–µ—à—ë–≤–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ |

---

## 1. Pricing Landscape (–î–µ–∫–∞–±—Ä—å 2025)

### –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ü–µ–Ω

```
+===========================================================================+
|                    LLM API Pricing (per 1M tokens)                        |
|                         –î–µ–∫–∞–±—Ä—å 2025                                      |
+===========================================================================+
|                                                                           |
|  Model                   Input      Output     Context    Batch    Notes  |
|  -----------------------------------------------------------------------  |
|                                                                           |
|  ULTRA-BUDGET TIER (< $0.50/1M):                                          |
|  Gemini 2.0 Flash        $0.10      $0.40      1M         50%     Best!   |
|  GPT-4o-mini             $0.15      $0.60      128K       50%     Fast    |
|  DeepSeek V3.2           $0.28      $0.42      128K       -       MIT     |
|  Gemini 2.5 Flash        $0.30      $2.50      1M         50%     Reason  |
|  Gemini 3 Flash          $0.50      $3.00      1M         50%     NEW!    |
|                                                                           |
|  BUDGET TIER ($0.50-$2.00):                                               |
|  Claude 3.5 Haiku        $0.80      $4.00      200K       50%     Fast    |
|  Gemini 2.5 Pro          $1.25      $10.00     1M         50%     Reason  |
|                                                                           |
|  MID TIER ($2.00-$5.00):                                                  |
|  GPT-4o                  $2.50      $10.00     128K       50%     Std     |
|  Claude Sonnet 4         $3.00      $15.00     200K       50%     Best    |
|  Claude Sonnet 4.5       $3.00      $15.00     200K       50%     Latest  |
|  Claude 4.1 Sonnet       $5.00      $25.00     200K       50%     +Think  |
|                                                                           |
|  PREMIUM TIER ($15+):                                                     |
|  Claude 4.1 Opus         $20.00     $80.00     200K       50%     Best    |
|  o1                      $15.00     $60.00     200K       50%     Reason  |
|  o3-mini                 $1.10      $4.40      128K       50%     Reason  |
|  o3                      $10.00     $40.00     128K       50%     NEW!    |
|                                                                           |
|  -----------------------------------------------------------------------  |
|  Price Ratio: Premium vs Ultra-Budget = 50x-200x difference!              |
|                                                                           |
|  KEY INSIGHT: Gemini 2.0 Flash at $0.10/1M is 200x cheaper than           |
|  Claude 4.1 Opus at $20/1M - choose wisely!                               |
|                                                                           |
+===========================================================================+
```

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã (Thinking Tokens)

```python
# Claude 4.1 –≤–≤—ë–ª –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é: Thinking Tokens
# –≠—Ç–æ —Ç–æ–∫–µ–Ω—ã "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–ª–∞—á–∏–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ

# Claude 4.1 Sonnet thinking pricing:
# Input: $5.00/1M
# Output: $25.00/1M
# Thinking: $10.00/1M  <-- –ù–æ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è!

# –ü—Ä–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö thinking tokens –º–æ–≥—É—Ç —Å–æ—Å—Ç–∞–≤–ª—è—Ç—å 30-50% –æ—Ç –æ–±—â–µ–≥–æ usage
# –≠—Ç–æ –≤–∞–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–∏ –±—é–¥–∂–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
```

### ROI –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä

```python
def calculate_monthly_cost(
    requests_per_day: int,
    avg_input_tokens: int,
    avg_output_tokens: int,
    input_price: float,  # per 1M tokens
    output_price: float,  # per 1M tokens
    cache_hit_rate: float = 0,  # 0-1
    cache_discount: float = 0.5,  # OpenAI 50%, Anthropic 90%
    batch_eligible_pct: float = 0  # % –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Batch API
) -> dict:
    """
    –†–∞—Å—á—ë—Ç –º–µ—Å—è—á–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ LLM API —Å —É—á—ë—Ç–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π

    Returns:
        dict —Å base_cost, optimized_cost, savings
    """
    monthly_requests = requests_per_day * 30

    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å (–±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π)
    base_input_cost = (monthly_requests * avg_input_tokens / 1_000_000) * input_price
    base_output_cost = (monthly_requests * avg_output_tokens / 1_000_000) * output_price
    base_cost = base_input_cost + base_output_cost

    # –° —É—á—ë—Ç–æ–º prompt caching
    cached_input_cost = base_input_cost * (1 - cache_hit_rate * cache_discount)

    # –° —É—á—ë—Ç–æ–º Batch API (50% —Å–∫–∏–¥–∫–∞ –Ω–∞ eligible –∑–∞–ø—Ä–æ—Å—ã)
    batch_savings = (cached_input_cost + base_output_cost) * batch_eligible_pct * 0.5

    optimized_cost = cached_input_cost + base_output_cost - batch_savings

    return {
        "base_cost": base_cost,
        "optimized_cost": optimized_cost,
        "monthly_savings": base_cost - optimized_cost,
        "savings_percent": (1 - optimized_cost / base_cost) * 100 if base_cost > 0 else 0
    }

# –ü–†–ò–ú–ï–† 1: 10K –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å, 2000 input + 500 output tokens

# –í–∞—Ä–∏–∞–Ω—Ç A: GPT-4o –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
gpt4_base = calculate_monthly_cost(10000, 2000, 500, 2.50, 10.00)
# base_cost = $3,000/–º–µ—Å—è—Ü

# –í–∞—Ä–∏–∞–Ω—Ç B: GPT-4o —Å caching (60% hit rate) + 30% batch
gpt4_optimized = calculate_monthly_cost(
    10000, 2000, 500, 2.50, 10.00,
    cache_hit_rate=0.6, cache_discount=0.5, batch_eligible_pct=0.3
)
# optimized_cost = ~$1,575/–º–µ—Å—è—Ü (47% —ç–∫–æ–Ω–æ–º–∏—è)

# –í–∞—Ä–∏–∞–Ω—Ç C: GPT-4o-mini –¥–ª—è 70% –∑–∞–ø—Ä–æ—Å–æ–≤ + GPT-4o –¥–ª—è 30%
mixed_cost = (
    calculate_monthly_cost(7000, 2000, 500, 0.15, 0.60)["base_cost"] +  # mini
    calculate_monthly_cost(3000, 2000, 500, 2.50, 10.00)["base_cost"]   # full
)
# = $126 + $900 = $1,026/–º–µ—Å—è—Ü (66% —ç–∫–æ–Ω–æ–º–∏—è)

# –í–∞—Ä–∏–∞–Ω—Ç D: Gemini 2.0 Flash –¥–ª—è –≤—Å–µ–≥–æ (–µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
gemini_cost = calculate_monthly_cost(10000, 2000, 500, 0.10, 0.40)
# = $180/–º–µ—Å—è—Ü (94% —ç–∫–æ–Ω–æ–º–∏—è!)

print(f"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (10K req/day):
---------------------------------
GPT-4o –±–∞–∑–æ–≤—ã–π:        ${gpt4_base['base_cost']:,.0f}/–º–µ—Å
GPT-4o –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: ${gpt4_optimized['optimized_cost']:,.0f}/–º–µ—Å (-47%)
Mixed routing:          ${mixed_cost:,.0f}/–º–µ—Å (-66%)
Gemini 2.0 Flash:       ${gemini_cost['base_cost']:,.0f}/–º–µ—Å (-94%)
""")
```

---

## 2. Prompt Caching

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

```
+===========================================================================+
|                       Prompt Caching Comparison                           |
+===========================================================================+
|                                                                           |
|  Provider    | Discount | Min Tokens | TTL      | Implementation          |
|  -----------------------------------------------------------------------  |
|  OpenAI      | 50%      | 1,024      | 5-60 min | Automatic               |
|  Anthropic   | 90%      | 1,024-2048 | 5 min    | Explicit cache_control  |
|  Google      | -        | 32,000     | -        | Context caching API     |
|                                                                           |
|  –í–ê–ñ–ù–û: Anthropic –≤–∑–∏–º–∞–µ—Ç +25% –∑–∞ Cache Write, –Ω–æ -90% –∑–∞ Cache Read     |
|  Break-even: 3-5 –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–¥–Ω–∏–º prefix                                  |
|                                                                           |
+===========================================================================+
```

### OpenAI Prompt Caching (–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π)

```python
from openai import OpenAI

client = OpenAI()

# Prompt caching –≤–∫–ª—é—á—ë–Ω –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –¥–ª—è:
# - GPT-4o, GPT-4o-mini, o1, o1-mini, o3, o3-mini
# - –ü—Ä–æ–º–ø—Ç–æ–≤ > 1024 —Ç–æ–∫–µ–Ω–æ–≤
# - Cache hits –≤ increments –ø–æ 128 —Ç–æ–∫–µ–Ω–æ–≤

# –ö–õ–Æ–ß –ö –£–°–ü–ï–•–£: —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –ù–ê–ß–ê–õ–ï –ø—Ä–æ–º–ø—Ç–∞

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ cache hit:
STATIC_SYSTEM_PROMPT = """
You are a customer support assistant for TechCorp.
[–î–µ—Ç–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ - 2000+ —Ç–æ–∫–µ–Ω–æ–≤]
[–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ - 3000+ —Ç–æ–∫–µ–Ω–æ–≤]
[–ü–æ–ª–∏—Ç–∏–∫–∏ –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã - 1000+ —Ç–æ–∫–µ–Ω–æ–≤]
"""  # 6000+ —Ç–æ–∫–µ–Ω–æ–≤ - –±—É–¥–µ—Ç –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω

def support_query(user_question: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": STATIC_SYSTEM_PROMPT},
            {"role": "user", "content": user_question}  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å
        ]
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º cache hit –≤ usage
    usage = response.usage
    if hasattr(usage, 'prompt_tokens_details'):
        cached = usage.prompt_tokens_details.cached_tokens
        total = usage.prompt_tokens
        print(f"Cache hit rate: {cached/total*100:.1f}% ({cached}/{total} tokens)")

    return response.choices[0].message.content

# –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: cache miss, –ø–æ–ª–Ω–∞—è —Ü–µ–Ω–∞
# –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã: ~6000 tokens cached = 50% —Å–∫–∏–¥–∫–∞ –Ω–∞ –Ω–∏—Ö
# –ü—Ä–∏ 6000 cached –∏–∑ 6100 total = 49% —ç–∫–æ–Ω–æ–º–∏—è –Ω–∞ input tokens
```

### Anthropic Prompt Caching (–Ø–≤–Ω—ã–π)

```python
import anthropic

client = anthropic.Anthropic()

# Anthropic —Ç—Ä–µ–±—É–µ—Ç –Ø–í–ù–´–• cache_control breakpoints
# Pricing: Cache Write +25%, Cache Read -90%

# –ü—Ä–∏–º–µ—Ä: –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ (–¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
def analyze_contract(contract_text: str, question: str) -> str:
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        system=[
            {
                "type": "text",
                "text": """You are a legal assistant specialized in contract law.
                [–î–µ—Ç–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ 1500+ —Ç–æ–∫–µ–Ω–æ–≤]""",
                "cache_control": {"type": "ephemeral"}  # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å system prompt
            }
        ],
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": contract_text,  # –ë–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
                        "cache_control": {"type": "ephemeral"}  # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç
                    },
                    {
                        "type": "text",
                        "text": f"Question: {question}"  # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å
                    }
                ]
            }
        ]
    )

    return response.content[0].text

# –†–ê–°–ß–Å–¢ –≠–ö–û–ù–û–ú–ò–ò (Claude Sonnet 4: $3/1M input):
# –ö–æ–Ω—Ç—Ä–∞–∫—Ç: 5000 tokens, System: 1500 tokens, Question: 100 tokens
#
# –ë–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (10 –≤–æ–ø—Ä–æ—Å–æ–≤):
# 10 √ó 6600 tokens √ó $3/1M = $0.198
#
# –° –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º (10 –≤–æ–ø—Ä–æ—Å–æ–≤):
# Cache write: 6500 √ó $3.75/1M = $0.024 (+25%)
# Cache reads: 9 √ó 6500 √ó $0.30/1M = $0.018 (-90%)
# Uncached: 10 √ó 100 √ó $3/1M = $0.003
# Total: $0.045
#
# –≠–ö–û–ù–û–ú–ò–Ø: 77% ($0.198 ‚Üí $0.045)
```

### Best Practices –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

```python
# ========================================
# –ü–†–ê–í–ò–õ–ê –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ò–Ø –ü–†–û–ú–ü–¢–û–í
# ========================================

# DO: –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –Ω–∞—á–∞–ª–µ
messages = [
    {"role": "system", "content": long_static_instructions},  # –ö—ç—à–∏—Ä—É–µ—Ç—Å—è
    {"role": "user", "content": static_few_shot_examples},    # –ö—ç—à–∏—Ä—É–µ—Ç—Å—è
    {"role": "user", "content": variable_user_query}          # –ù–ï –∫—ç—à–∏—Ä—É–µ—Ç—Å—è
]

# DON'T: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –Ω–∞—á–∞–ª–µ (–ª–æ–º–∞–µ—Ç –∫—ç—à)
messages = [
    {"role": "system", "content": f"Current time: {datetime.now()}"},  # BAD!
    {"role": "user", "content": long_instructions}  # –ù–µ –∑–∞–∫—ç—à–∏—Ä—É–µ—Ç—Å—è
]

# DON'T: –†–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä—è–¥–∫–∞ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤
# –ö–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ = –Ω–æ–≤—ã–π cache key

# ========================================
# –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ï –†–ê–ó–ú–ï–†–´ –î–õ–Ø –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø
# ========================================

CACHE_MINIMUMS = {
    "openai": 1024,      # tokens, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    "anthropic_haiku": 1024,
    "anthropic_sonnet": 2048,
    "anthropic_opus": 2048,
    "google": 32768,     # –¢—Ä–µ–±—É–µ—Ç Context Caching API
}

# ========================================
# –ú–û–ù–ò–¢–û–†–ò–ù–ì CACHE HIT RATE
# ========================================

class CacheMonitor:
    def __init__(self):
        self.hits = 0
        self.total_cached_tokens = 0
        self.total_input_tokens = 0

    def record(self, response):
        usage = response.usage
        if hasattr(usage, 'prompt_tokens_details'):
            cached = usage.prompt_tokens_details.cached_tokens
            self.total_cached_tokens += cached
            self.total_input_tokens += usage.prompt_tokens
            if cached > 0:
                self.hits += 1

    @property
    def hit_rate(self):
        if self.total_input_tokens == 0:
            return 0
        return self.total_cached_tokens / self.total_input_tokens

# Target: >60% cache hit rate –¥–ª—è production workloads
```

---

## 3. Semantic Caching (GPTCache)

### –ö–æ–Ω—Ü–µ–ø—Ü–∏—è

```
+===========================================================================+
|                      Semantic Caching vs Exact Caching                    |
+===========================================================================+
|                                                                           |
|  EXACT CACHING (Prompt Caching):                                          |
|  Query: "What is the capital of France?"                                  |
|  Cache Key: exact string hash                                             |
|  Hit: —Ç–æ–ª—å–∫–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å                                            |
|                                                                           |
|  SEMANTIC CACHING (GPTCache):                                             |
|  Query: "What is the capital of France?"                                  |
|  Similar: "Tell me France's capital city"                                 |
|  Similar: "Which city is the capital of France?"                          |
|  ‚Üí –í—Å–µ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç cached response!                                        |
|                                                                           |
|  –†–ï–ó–£–õ–¨–¢–ê–¢–´:                                                              |
|  - 2-10x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ cache hits                                          |
|  - –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç: 31% –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏        |
|    –ø–æ—Ö–æ–∂–∏ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã                                           |
|                                                                           |
+===========================================================================+
```

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è GPTCache

```python
# pip install gptcache

from gptcache import Cache
from gptcache.adapter import openai
from gptcache.embedding import Onnx
from gptcache.manager import CacheBase, VectorBase, get_data_manager
from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è semantic cache
def init_gptcache():
    # Embedding model –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    onnx = Onnx()

    # Storage backends
    cache_base = CacheBase("sqlite")  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    vector_base = VectorBase("faiss", dimension=onnx.dimension)  # –í–µ–∫—Ç–æ—Ä—ã

    data_manager = get_data_manager(cache_base, vector_base)

    cache = Cache()
    cache.init(
        embedding_func=onnx.to_embeddings,
        data_manager=data_manager,
        similarity_evaluation=SearchDistanceEvaluation(),
    )

    return cache

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å OpenAI
cache = init_gptcache()

# –ó–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä GPTCache
response = openai.ChatCompletion.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is the capital of France?"}],
    cache_obj=cache
)

# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å - cache hit!
response2 = openai.ChatCompletion.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Tell me the capital city of France"}],
    cache_obj=cache
)
# response2 –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∏–∑ –∫—ç—à–∞ –±–µ–∑ API –≤—ã–∑–æ–≤–∞
```

### GPTCache vs MeanCache (2025)

```python
# GPTCache - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:
# - 233 false hits –Ω–∞ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ (23% –æ—à–∏–±–æ–∫)
# - –ù–µ—Ç generative caching

# MeanCache (IPDPS 2025) - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è:
# - –¢–æ–ª—å–∫–æ 89 false hits –Ω–∞ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ (9% –æ—à–∏–±–æ–∫)
# - Federated learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è similarity model
# - Privacy-preserving (–ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ)

# GenerativeCache (–º–∞—Ä—Ç 2025) - —Å–∞–º–æ–µ –±—ã—Å—Ç—Ä–æ–µ:
# - 9x –±—ã—Å—Ç—Ä–µ–µ GPTCache
# - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ generative caching
# - –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π similarity algorithm

# –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:
# - –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö use cases: GPTCache (–∑—Ä–µ–ª–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangChain)
# - –î–ª—è production —Å –≤—ã—Å–æ–∫–∏–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏: MeanCache –∏–ª–∏ GenerativeCache
```

### –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

```python
class SemanticCacheMetrics:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ semantic cache"""

    def __init__(self):
        self.cache_hits = 0
        self.cache_misses = 0
        self.false_positives = 0  # –ù–µ–≤–µ—Ä–Ω—ã–µ cache hits
        self.latency_with_cache = []
        self.latency_without_cache = []

    def record_hit(self, latency_ms: float, was_correct: bool):
        self.cache_hits += 1
        self.latency_with_cache.append(latency_ms)
        if not was_correct:
            self.false_positives += 1

    def record_miss(self, latency_ms: float):
        self.cache_misses += 1
        self.latency_without_cache.append(latency_ms)

    @property
    def hit_rate(self):
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0

    @property
    def false_positive_rate(self):
        return self.false_positives / self.cache_hits if self.cache_hits > 0 else 0

    @property
    def avg_speedup(self):
        if not self.latency_with_cache or not self.latency_without_cache:
            return 1.0
        avg_hit = sum(self.latency_with_cache) / len(self.latency_with_cache)
        avg_miss = sum(self.latency_without_cache) / len(self.latency_without_cache)
        return avg_miss / avg_hit if avg_hit > 0 else 1.0

# –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
# - Hit rate: 20-40% (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç domain)
# - False positive rate: <5%
# - Speedup: 5-10x –Ω–∞ cache hits
```

---

## 4. Batch API

### 50% —Å–∫–∏–¥–∫–∞ –Ω–∞ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã

```
+===========================================================================+
|                         Batch API Comparison                              |
+===========================================================================+
|                                                                           |
|  Provider   | Discount | Max Wait | Max Requests | Status                 |
|  -----------------------------------------------------------------------  |
|  OpenAI     | 50%      | 24h      | 50,000       | Production             |
|  Anthropic  | 50%      | 24h      | Unlimited    | Production             |
|  Google     | 50%      | -        | -            | Vertex AI              |
|                                                                           |
|  USE CASES:                                                               |
|  - Content moderation at scale                                            |
|  - Document summarization                                                 |
|  - Data enrichment / classification                                       |
|  - Report generation                                                      |
|  - Embeddings generation                                                  |
|  - Bulk translations                                                      |
|                                                                           |
|  –ù–ï –ü–û–î–•–û–î–ò–¢ –î–õ–Ø:                                                         |
|  - Real-time chat                                                         |
|  - Interactive applications                                               |
|  - Latency-sensitive operations                                           |
|                                                                           |
+===========================================================================+
```

### OpenAI Batch API Implementation

```python
from openai import OpenAI
import json
import time
from pathlib import Path

client = OpenAI()

class BatchProcessor:
    """Production-ready batch processor –¥–ª—è OpenAI"""

    def __init__(self, model: str = "gpt-4o"):
        self.model = model
        self.client = OpenAI()

    def create_batch_file(self, requests: list[dict], filepath: str) -> str:
        """–°–æ–∑–¥–∞—ë—Ç JSONL —Ñ–∞–π–ª –¥–ª—è batch processing"""

        batch_requests = []
        for i, req in enumerate(requests):
            batch_requests.append({
                "custom_id": req.get("id", f"request-{i}"),
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": self.model,
                    "messages": req["messages"],
                    "max_tokens": req.get("max_tokens", 1000)
                }
            })

        with open(filepath, "w") as f:
            for req in batch_requests:
                f.write(json.dumps(req) + "\n")

        return filepath

    def submit_batch(self, filepath: str) -> str:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –∏ —Å–æ–∑–¥–∞—ë—Ç batch job"""

        # Upload file
        with open(filepath, "rb") as f:
            batch_file = self.client.files.create(file=f, purpose="batch")

        # Create batch job
        batch_job = self.client.batches.create(
            input_file_id=batch_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )

        print(f"Batch job created: {batch_job.id}")
        return batch_job.id

    def wait_for_completion(self, batch_id: str, poll_interval: int = 60) -> dict:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è batch job"""

        while True:
            status = self.client.batches.retrieve(batch_id)
            print(f"Status: {status.status} | "
                  f"Completed: {status.request_counts.completed}/"
                  f"{status.request_counts.total}")

            if status.status == "completed":
                return self._get_results(status.output_file_id)
            elif status.status in ["failed", "expired", "cancelled"]:
                raise Exception(f"Batch failed: {status.status}")

            time.sleep(poll_interval)

    def _get_results(self, output_file_id: str) -> dict:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ output file"""

        content = self.client.files.content(output_file_id)
        results = {}

        for line in content.text.strip().split("\n"):
            result = json.loads(line)
            custom_id = result["custom_id"]
            if result["response"]["status_code"] == 200:
                results[custom_id] = result["response"]["body"]["choices"][0]["message"]["content"]
            else:
                results[custom_id] = {"error": result["error"]}

        return results


# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
processor = BatchProcessor(model="gpt-4o")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
requests = [
    {"id": f"doc-{i}", "messages": [
        {"role": "user", "content": f"Summarize document {i}: {doc_content}"}
    ]}
    for i, doc_content in enumerate(documents)
]

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ batch
filepath = processor.create_batch_file(requests, "batch_requests.jsonl")
batch_id = processor.submit_batch(filepath)

# –û–∂–∏–¥–∞–Ω–∏–µ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
results = processor.wait_for_completion(batch_id)

# –≠–ö–û–ù–û–ú–ò–Ø –ø—Ä–∏ 1000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, 2000 tokens –∫–∞–∂–¥—ã–π:
# Real-time: 1000 √ó 2000 √ó $2.50/1M = $5.00
# Batch:     1000 √ó 2000 √ó $1.25/1M = $2.50 (50% —ç–∫–æ–Ω–æ–º–∏—è)
```

### Anthropic Batch API

```python
import anthropic

client = anthropic.Anthropic()

# Batch –∑–∞–ø—Ä–æ—Å
batch = client.messages.batches.create(
    requests=[
        {
            "custom_id": f"doc-{i}",
            "params": {
                "model": "claude-sonnet-4-20250514",
                "max_tokens": 1024,
                "messages": [
                    {"role": "user", "content": f"Analyze document {i}: {content}"}
                ]
            }
        }
        for i, content in enumerate(documents)
    ]
)

print(f"Batch ID: {batch.id}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
while True:
    status = client.messages.batches.retrieve(batch.id)
    print(f"Status: {status.processing_status}")

    if status.processing_status == "ended":
        break
    time.sleep(60)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
results = {}
for result in client.messages.batches.results(batch.id):
    if result.result.type == "succeeded":
        results[result.custom_id] = result.result.message.content[0].text
    else:
        results[result.custom_id] = {"error": result.result.error}
```

---

## 5. Model Routing

### Tiered Architecture

```
+===========================================================================+
|                     Intelligent Model Routing                             |
+===========================================================================+
|                                                                           |
|  User Query                                                               |
|       |                                                                   |
|       v                                                                   |
|  +--------------------------------------------+                           |
|  |     ROUTER (GPT-4o-mini or rule-based)     |                           |
|  |  Classifies: simple | medium | complex     |                           |
|  +--------------------------------------------+                           |
|            |              |              |                                |
|            v              v              v                                |
|       +---------+    +---------+    +---------+                           |
|       |  NANO   |    |  MINI   |    |  FULL   |                           |
|       | $0.10/M |    | $0.15/M |    | $3.00/M |                           |
|       |  65%    |    |  25%    |    |  10%    |                           |
|       +---------+    +---------+    +---------+                           |
|       Gemini        GPT-4o-mini     Claude                                |
|       Flash                         Sonnet 4                              |
|                                                                           |
|  -----------------------------------------------------------------------  |
|  Query Distribution ‚Üí Model:                                              |
|  - FAQ, simple lookups ‚Üí NANO (65%)                                       |
|  - Summarization, classification ‚Üí MINI (25%)                             |
|  - Complex reasoning, code ‚Üí FULL (10%)                                   |
|                                                                           |
|  COST CALCULATION:                                                        |
|  Without routing: 100% √ó $3.00 = $3.00 avg                                |
|  With routing: 65%√ó$0.10 + 25%√ó$0.15 + 10%√ó$3.00 = $0.40 avg             |
|  SAVINGS: 87%!                                                            |
|                                                                           |
+===========================================================================+
```

### Production Router Implementation

```python
from openai import OpenAI
from enum import Enum
from dataclasses import dataclass
from typing import Optional
import anthropic

class ModelTier(Enum):
    NANO = "nano"
    MINI = "mini"
    STANDARD = "standard"
    PREMIUM = "premium"

@dataclass
class ModelConfig:
    provider: str
    model: str
    input_price: float  # per 1M tokens
    output_price: float
    max_tokens: int

MODEL_CONFIGS = {
    ModelTier.NANO: ModelConfig(
        provider="google",
        model="gemini-2.0-flash",
        input_price=0.10,
        output_price=0.40,
        max_tokens=8192
    ),
    ModelTier.MINI: ModelConfig(
        provider="openai",
        model="gpt-4o-mini",
        input_price=0.15,
        output_price=0.60,
        max_tokens=16384
    ),
    ModelTier.STANDARD: ModelConfig(
        provider="anthropic",
        model="claude-sonnet-4-20250514",
        input_price=3.00,
        output_price=15.00,
        max_tokens=8192
    ),
    ModelTier.PREMIUM: ModelConfig(
        provider="anthropic",
        model="claude-4-1-opus-20251024",
        input_price=20.00,
        output_price=80.00,
        max_tokens=8192
    ),
}

class IntelligentRouter:
    """Production router —Å multiple strategies"""

    def __init__(self):
        self.openai = OpenAI()
        self.anthropic = anthropic.Anthropic()

        # Keyword-based quick routing (no LLM call needed)
        self.simple_patterns = [
            "what is", "who is", "when was", "where is",
            "define", "meaning of", "translate",
        ]
        self.complex_patterns = [
            "analyze", "compare", "design", "architect",
            "write code", "debug", "review", "explain in detail",
        ]

    def classify_query(self, query: str) -> ModelTier:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""

        query_lower = query.lower()

        # 1. Rule-based quick classification (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
        if any(p in query_lower for p in self.simple_patterns):
            if len(query) < 100:  # –ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
                return ModelTier.NANO

        if any(p in query_lower for p in self.complex_patterns):
            return ModelTier.STANDARD

        # 2. LLM-based classification –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        return self._llm_classify(query)

    def _llm_classify(self, query: str) -> ModelTier:
        """LLM classification (–¥—ë—à–µ–≤–æ —á–µ—Ä–µ–∑ mini)"""

        response = self.openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Classify query complexity. Respond with ONE word:
                    SIMPLE: factual questions, greetings, lookups, definitions
                    MEDIUM: summarization, basic analysis, standard tasks
                    COMPLEX: multi-step reasoning, code, architecture, creative
                    EXPERT: research, complex analysis requiring top models"""
                },
                {"role": "user", "content": query[:500]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            ],
            max_tokens=10,
            temperature=0
        )

        classification = response.choices[0].message.content.strip().upper()

        tier_map = {
            "SIMPLE": ModelTier.NANO,
            "MEDIUM": ModelTier.MINI,
            "COMPLEX": ModelTier.STANDARD,
            "EXPERT": ModelTier.PREMIUM,
        }

        return tier_map.get(classification, ModelTier.MINI)

    def route_and_execute(self, query: str, system_prompt: Optional[str] = None) -> dict:
        """–ü–æ–ª–Ω—ã–π pipeline: classify ‚Üí route ‚Üí execute"""

        tier = self.classify_query(query)
        config = MODEL_CONFIGS[tier]

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": query})

        # Execute on appropriate provider
        if config.provider == "openai":
            response = self.openai.chat.completions.create(
                model=config.model,
                messages=messages,
                max_tokens=config.max_tokens
            )
            content = response.choices[0].message.content
            usage = {"input": response.usage.prompt_tokens,
                     "output": response.usage.completion_tokens}

        elif config.provider == "anthropic":
            response = self.anthropic.messages.create(
                model=config.model,
                messages=messages[1:] if system_prompt else messages,
                system=system_prompt if system_prompt else None,
                max_tokens=config.max_tokens
            )
            content = response.content[0].text
            usage = {"input": response.usage.input_tokens,
                     "output": response.usage.output_tokens}

        # Calculate cost
        cost = (
            usage["input"] / 1_000_000 * config.input_price +
            usage["output"] / 1_000_000 * config.output_price
        )

        return {
            "tier": tier.value,
            "model": config.model,
            "content": content,
            "usage": usage,
            "cost": cost
        }


# –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï
router = IntelligentRouter()

# –ü—Ä–∏–º–µ—Ä—ã routing
queries = [
    "What is Python?",                    # ‚Üí NANO
    "Summarize this article: ...",        # ‚Üí MINI
    "Design a microservices architecture for e-commerce", # ‚Üí STANDARD
    "Analyze the implications of...",     # ‚Üí STANDARD/PREMIUM
]

for query in queries:
    result = router.route_and_execute(query)
    print(f"Query: {query[:50]}...")
    print(f"  Routed to: {result['tier']} ({result['model']})")
    print(f"  Cost: ${result['cost']:.6f}")
```

---

## 6. Token Compression (LLMLingua)

### –î–æ 20x —Å–∂–∞—Ç–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤

```
+===========================================================================+
|                         LLMLingua Compression                             |
+===========================================================================+
|                                                                           |
|  BEFORE COMPRESSION (800 tokens):                                         |
|  "I would like you to help me with summarizing a document. The document   |
|   is about artificial intelligence and machine learning. I would like     |
|   you to provide a concise summary that captures the main points. Please  |
|   make sure to include the key findings and conclusions. The summary      |
|   should be clear and easy to understand. Can you help me with this?      |
|   Here is the document: [content]"                                        |
|                                                                           |
|  AFTER COMPRESSION (80 tokens):                                           |
|  "Summarize document. AI/ML topic. Concise, main points, key findings,    |
|   conclusions. Clear, understandable. Document: [content]"                |
|                                                                           |
|  COMPRESSION RATIO: 10x                                                   |
|  QUALITY RETENTION: 95%+                                                  |
|                                                                           |
|  -----------------------------------------------------------------------  |
|                                                                           |
|  COST IMPACT (RAG pipeline, 10K queries/day, GPT-4o):                     |
|  Before: 10K √ó 5000 tokens √ó $2.50/1M = $125/day                          |
|  After:  10K √ó 500 tokens √ó $2.50/1M = $12.50/day                         |
|  SAVINGS: $112.50/day = $3,375/month (90%)                                |
|                                                                           |
+===========================================================================+
```

### Implementation

```python
# pip install llmlingua

from llmlingua import PromptCompressor

class CostOptimizedLLM:
    """LLM client —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–∂–∞—Ç–∏–µ–º –ø—Ä–æ–º–ø—Ç–æ–≤"""

    def __init__(self, compression_rate: float = 0.5):
        self.compressor = PromptCompressor(
            model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
            use_llmlingua2=True,  # –ë–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è
        )
        self.compression_rate = compression_rate
        self.openai = OpenAI()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.original_tokens = 0
        self.compressed_tokens = 0

    def compress_prompt(self, prompt: str, force_tokens: list[str] = None) -> str:
        """–°–∂–∏–º–∞–µ—Ç –ø—Ä–æ–º–ø—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"""

        result = self.compressor.compress_prompt(
            prompt,
            rate=self.compression_rate,
            force_tokens=force_tokens or [],  # –¢–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–ª—å–∑—è —É–¥–∞–ª—è—Ç—å
            drop_consecutive=True,  # –£–¥–∞–ª—è—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        )

        self.original_tokens += result["origin_tokens"]
        self.compressed_tokens += result["compressed_tokens"]

        return result["compressed_prompt"]

    def chat(self,
             user_message: str,
             system_prompt: str = None,
             compress_user: bool = True,
             compress_system: bool = False) -> str:
        """Chat —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π"""

        messages = []

        if system_prompt:
            if compress_system and len(system_prompt) > 500:
                system_prompt = self.compress_prompt(system_prompt)
            messages.append({"role": "system", "content": system_prompt})

        if compress_user and len(user_message) > 500:
            user_message = self.compress_prompt(user_message)

        messages.append({"role": "user", "content": user_message})

        response = self.openai.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )

        return response.choices[0].message.content

    @property
    def compression_stats(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∂–∞—Ç–∏—è"""
        if self.original_tokens == 0:
            return {"ratio": 1.0, "savings_pct": 0}

        ratio = self.original_tokens / self.compressed_tokens
        savings = 1 - (self.compressed_tokens / self.original_tokens)

        return {
            "original_tokens": self.original_tokens,
            "compressed_tokens": self.compressed_tokens,
            "compression_ratio": ratio,
            "savings_percent": savings * 100
        }


# –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï
llm = CostOptimizedLLM(compression_rate=0.3)  # –°–∂–∞—Ç—å –¥–æ 30%

# RAG context compression
long_context = """
Based on the retrieved documents, here is the relevant information:

Document 1: Introduction to Machine Learning
Machine learning is a subset of artificial intelligence that provides systems
the ability to automatically learn and improve from experience without being
explicitly programmed. Machine learning focuses on the development of computer
programs that can access data and use it to learn for themselves.
[... –µ—â—ë 4000 —Ç–æ–∫–µ–Ω–æ–≤ ...]
"""

response = llm.chat(
    user_message="What is the definition of machine learning?",
    system_prompt=long_context,
    compress_system=True  # –°–∂–∏–º–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
)

print(f"Compression stats: {llm.compression_stats}")
# –¢–∏–ø–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 5-10x —Å–∂–∞—Ç–∏–µ
```

### LongLLMLingua –¥–ª—è RAG

```python
from llmlingua import PromptCompressor

# LongLLMLingua —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è RAG
# –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "lost in the middle" –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

compressor = PromptCompressor(
    model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
    use_llmlingua2=True,
)

def compress_rag_context(
    question: str,
    retrieved_docs: list[str],
    target_ratio: float = 0.25  # –°–∂–∞—Ç—å –¥–æ 25%
) -> str:
    """–°–∂–∏–º–∞–µ—Ç RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á—ë—Ç–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –≤–æ–ø—Ä–æ—Å—É"""

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    context = "\n\n".join([
        f"Document {i+1}:\n{doc}"
        for i, doc in enumerate(retrieved_docs)
    ])

    # LongLLMLingua —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫ –≤–æ–ø—Ä–æ—Å—É —á–∞—Å—Ç–∏
    result = compressor.compress_prompt(
        context,
        question=question,  # –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø—Ä–∏ —Å–∂–∞—Ç–∏–∏
        rate=target_ratio,
        condition_in_question="after_condition",
        reorder_context="sort",  # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    )

    return result["compressed_prompt"]

# –ü—Ä–∏–º–µ—Ä
docs = [
    "Large document about Python basics...",
    "Document about machine learning algorithms...",
    "Document about web development...",
]

compressed = compress_rag_context(
    question="What machine learning algorithms are mentioned?",
    retrieved_docs=docs,
    target_ratio=0.25
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç: 75% —Ç–æ–∫–µ–Ω–æ–≤ —É–¥–∞–ª–µ–Ω–æ, –Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ ML —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
```

---

## 7. RAG –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### RAG Cost Analysis

```
+===========================================================================+
|                      RAG vs Full Context Cost                             |
+===========================================================================+
|                                                                           |
|  SCENARIO: Customer support with 100-page knowledge base                  |
|                                                                           |
|  WITHOUT RAG (Full Context):                                              |
|  +--------------------------------------------------------+               |
|  | [Entire Documentation: 50,000 tokens] [Query: 50 tok]  |               |
|  | Cost per query: 50,050 √ó $2.50/1M = $0.125             |               |
|  +--------------------------------------------------------+               |
|  Monthly (10K queries): $1,250                                            |
|                                                                           |
|  WITH RAG (Top-5 Chunks):                                                 |
|  +--------------------------------------------------------+               |
|  | [5 Relevant Chunks: 2,500 tokens] [Query: 50 tokens]   |               |
|  | Cost per query: 2,550 √ó $2.50/1M = $0.006              |               |
|  +--------------------------------------------------------+               |
|  Monthly (10K queries): $63.75                                            |
|                                                                           |
|  SAVINGS: 95% ($1,186.25/month)                                           |
|                                                                           |
|  -----------------------------------------------------------------------  |
|                                                                           |
|  RAG INFRASTRUCTURE COSTS (monthly):                                      |
|  - Embedding generation (one-time): ~$5-20                                |
|  - Vector DB (Pinecone): $70-700                                          |
|  - Retrieval queries: negligible                                          |
|                                                                           |
|  NET SAVINGS: >90% even with infrastructure costs                         |
|                                                                           |
+===========================================================================+
```

### Cost-Optimized RAG Implementation

```python
from openai import OpenAI
import chromadb
from dataclasses import dataclass

@dataclass
class RAGConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è cost-optimized RAG"""

    # Retrieval settings
    top_k: int = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks
    chunk_size: int = 500  # –†–∞–∑–º–µ—Ä chunk –≤ —Ç–æ–∫–µ–Ω–∞—Ö

    # Model settings
    embedding_model: str = "text-embedding-3-small"  # $0.02/1M tokens
    llm_model: str = "gpt-4o-mini"  # –î–µ—à—ë–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞
    llm_fallback: str = "gpt-4o"  # Fallback –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    # Optimization
    use_reranking: bool = True
    max_context_tokens: int = 3000
    compress_context: bool = True


class CostOptimizedRAG:
    """Production RAG —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∑–∞—Ç—Ä–∞—Ç"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.openai = OpenAI()
        self.chroma = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.chroma.get_or_create_collection("docs")

        # –î–ª—è LLMLingua compression
        if config.compress_context:
            from llmlingua import PromptCompressor
            self.compressor = PromptCompressor(use_llmlingua2=True)

    def embed_documents(self, documents: list[str], ids: list[str]):
        """Embeddings —á–µ—Ä–µ–∑ –¥–µ—à—ë–≤—É—é –º–æ–¥–µ–ª—å"""

        # text-embedding-3-small: $0.02/1M tokens (vs $0.13 for large)
        response = self.openai.embeddings.create(
            model=self.config.embedding_model,
            input=documents
        )

        embeddings = [e.embedding for e in response.data]

        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            ids=ids
        )

    def retrieve(self, query: str) -> list[str]:
        """Retrieve top-k relevant chunks"""

        # Query embedding
        query_embedding = self.openai.embeddings.create(
            model=self.config.embedding_model,
            input=[query]
        ).data[0].embedding

        # Vector search
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=self.config.top_k * 2  # Retrieve more for reranking
        )

        documents = results['documents'][0]

        # Optional: Reranking with Cohere or cross-encoder
        if self.config.use_reranking and len(documents) > self.config.top_k:
            documents = self._rerank(query, documents)[:self.config.top_k]

        return documents[:self.config.top_k]

    def _rerank(self, query: str, documents: list[str]) -> list[str]:
        """Simple LLM-based reranking (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ Cohere)"""
        # –î–ª—è production —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Cohere rerank API
        # –ó–¥–µ—Å—å —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
        return documents

    def query(self, user_query: str) -> dict:
        """Full RAG pipeline"""

        # 1. Retrieve relevant chunks
        chunks = self.retrieve(user_query)
        context = "\n\n".join(chunks)

        # 2. Compress context if needed
        if self.config.compress_context and len(context) > 2000:
            result = self.compressor.compress_prompt(
                context,
                question=user_query,
                rate=0.5
            )
            context = result["compressed_prompt"]

        # 3. Truncate if still too long
        # (Approximate: 1 token ‚âà 4 chars)
        max_chars = self.config.max_context_tokens * 4
        if len(context) > max_chars:
            context = context[:max_chars]

        # 4. Generate response
        messages = [
            {
                "role": "system",
                "content": f"""Answer based only on the provided context.
                If the context doesn't contain the answer, say "I don't have this information."

                Context:
                {context}"""
            },
            {"role": "user", "content": user_query}
        ]

        response = self.openai.chat.completions.create(
            model=self.config.llm_model,
            messages=messages,
            max_tokens=500
        )

        # Calculate cost
        usage = response.usage
        model_prices = {
            "gpt-4o-mini": (0.15, 0.60),
            "gpt-4o": (2.50, 10.00)
        }
        input_price, output_price = model_prices.get(
            self.config.llm_model, (0.15, 0.60)
        )
        cost = (
            usage.prompt_tokens / 1_000_000 * input_price +
            usage.completion_tokens / 1_000_000 * output_price
        )

        return {
            "answer": response.choices[0].message.content,
            "chunks_used": len(chunks),
            "context_tokens": usage.prompt_tokens,
            "output_tokens": usage.completion_tokens,
            "cost": cost
        }


# –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï
config = RAGConfig(
    top_k=5,
    llm_model="gpt-4o-mini",
    compress_context=True
)

rag = CostOptimizedRAG(config)

# Index documents (one-time)
# rag.embed_documents(documents, ids)

# Query
result = rag.query("What is the return policy?")
print(f"Answer: {result['answer']}")
print(f"Cost: ${result['cost']:.6f}")  # Typically $0.0001-0.001
```

---

## 8. Self-Hosting vs API

### When to Self-Host

```
+===========================================================================+
|                    Self-Hosting Decision Matrix                           |
+===========================================================================+
|                                                                           |
|  SELF-HOST IF:                          | USE API IF:                     |
|  ---------------------------------------|-------------------------------- |
|  - >2M tokens/day consistently          | - Variable/low volume           |
|  - Strict compliance (HIPAA, PCI)       | - Standard security OK          |
|  - Need custom fine-tuned models        | - General-purpose tasks         |
|  - No rate limits required              | - Need latest models            |
|  - Have ML infrastructure team          | - Small team, no ML expertise   |
|  - Predictable workload                 | - Spiky/unpredictable load      |
|                                                                           |
|  -----------------------------------------------------------------------  |
|                                                                           |
|  COST COMPARISON (2M tokens/day):                                         |
|                                                                           |
|  GPT-4o API:                                                              |
|  - 2M tokens √ó 30 days √ó $2.50/1M = $150/month (input only)               |
|  - Total with output: ~$500-800/month                                     |
|                                                                           |
|  Self-hosted Llama 3.1 70B (A100 GPU):                                    |
|  - AWS p4d.24xlarge: ~$25,000/month                                       |
|  - But: unlimited tokens, no per-token cost                               |
|  - Break-even: ~50M tokens/day                                            |
|                                                                           |
|  Self-hosted Llama 3.1 8B (cheaper):                                      |
|  - AWS g5.2xlarge: ~$850/month                                            |
|  - Supports ~1-2M tokens/day                                              |
|  - Break-even: ~10M tokens/day                                            |
|                                                                           |
+===========================================================================+
```

### Cost Analysis

```python
from dataclasses import dataclass
from enum import Enum

class DeploymentType(Enum):
    API = "api"
    SELF_HOSTED = "self_hosted"
    HYBRID = "hybrid"

@dataclass
class CostAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ API vs Self-hosting"""

    # Daily usage
    tokens_per_day: int

    # API costs (per 1M tokens)
    api_input_price: float = 2.50  # GPT-4o
    api_output_price: float = 10.00
    input_output_ratio: float = 0.7  # 70% input, 30% output

    # Self-hosting costs
    gpu_monthly_cost: float = 850  # g5.2xlarge
    gpu_tokens_per_day: int = 2_000_000  # Capacity
    setup_cost: float = 5000  # One-time
    maintenance_hours_per_month: int = 10
    hourly_rate: float = 100  # DevOps/ML engineer

    def calculate_api_cost(self) -> float:
        """Monthly API cost"""
        monthly_tokens = self.tokens_per_day * 30
        input_tokens = monthly_tokens * self.input_output_ratio
        output_tokens = monthly_tokens * (1 - self.input_output_ratio)

        return (
            input_tokens / 1_000_000 * self.api_input_price +
            output_tokens / 1_000_000 * self.api_output_price
        )

    def calculate_self_hosted_cost(self) -> float:
        """Monthly self-hosted cost"""
        num_gpus = max(1, self.tokens_per_day / self.gpu_tokens_per_day)

        gpu_cost = num_gpus * self.gpu_monthly_cost
        maintenance = self.maintenance_hours_per_month * self.hourly_rate

        return gpu_cost + maintenance

    def calculate_tco(self, months: int = 12) -> dict:
        """Total Cost of Ownership over period"""

        api_total = self.calculate_api_cost() * months

        self_hosted_total = (
            self.setup_cost +
            self.calculate_self_hosted_cost() * months
        )

        break_even_months = None
        if self.calculate_self_hosted_cost() < self.calculate_api_cost():
            # Self-hosted cheaper per month
            monthly_savings = self.calculate_api_cost() - self.calculate_self_hosted_cost()
            break_even_months = self.setup_cost / monthly_savings if monthly_savings > 0 else None

        recommendation = DeploymentType.API
        if self.tokens_per_day > 10_000_000:
            recommendation = DeploymentType.SELF_HOSTED
        elif self.tokens_per_day > 2_000_000:
            recommendation = DeploymentType.HYBRID

        return {
            "api_monthly": self.calculate_api_cost(),
            "self_hosted_monthly": self.calculate_self_hosted_cost(),
            "api_total_tco": api_total,
            "self_hosted_total_tco": self_hosted_total,
            "break_even_months": break_even_months,
            "recommendation": recommendation.value,
            "savings_if_self_hosted": api_total - self_hosted_total
        }


# –ü–†–ò–ú–ï–†–´ –ê–ù–ê–õ–ò–ó–ê

# Scenario 1: Low volume startup
low_volume = CostAnalysis(tokens_per_day=500_000)
print("Low volume (500K tokens/day):")
print(f"  API: ${low_volume.calculate_api_cost():,.0f}/month")
print(f"  Self-hosted: ${low_volume.calculate_self_hosted_cost():,.0f}/month")
print(f"  Recommendation: API")

# Scenario 2: High volume enterprise
high_volume = CostAnalysis(tokens_per_day=20_000_000)
print("\nHigh volume (20M tokens/day):")
print(f"  API: ${high_volume.calculate_api_cost():,.0f}/month")
print(f"  Self-hosted: ${high_volume.calculate_self_hosted_cost():,.0f}/month")
tco = high_volume.calculate_tco(12)
print(f"  12-month savings: ${tco['savings_if_self_hosted']:,.0f}")
print(f"  Recommendation: {tco['recommendation']}")
```

---

## 9. Fine-Tuning vs Prompting

### Decision Framework

```
+===========================================================================+
|                  Fine-Tuning vs Prompting Decision                        |
+===========================================================================+
|                                                                           |
|  USE PROMPTING WHEN:                    | USE FINE-TUNING WHEN:           |
|  ---------------------------------------|-------------------------------- |
|  - Still defining product/UX            | - Stable, well-defined task     |
|  - Need flexibility to iterate          | - Need consistent outputs       |
|  - Low-medium volume                    | - High volume (cost savings)    |
|  - General-purpose tasks                | - Domain-specific knowledge     |
|  - RAG can provide context              | - Knowledge must be embedded    |
|  - Budget for premium models            | - Need to use smaller model     |
|                                                                           |
|  -----------------------------------------------------------------------  |
|                                                                           |
|  COST COMPARISON (1M requests/month):                                     |
|                                                                           |
|  PROMPTING (GPT-4o with 2000 token system prompt):                        |
|  - Per request: 2000 tokens √ó $2.50/1M = $0.005                           |
|  - Monthly: 1M √ó $0.005 = $5,000 just for system prompt                   |
|                                                                           |
|  FINE-TUNED (GPT-4o-mini, no system prompt needed):                       |
|  - Fine-tuning cost: ~$500-2000 (one-time)                                |
|  - Per request: 100 tokens √ó $0.30/1M = $0.00003                          |
|  - Monthly: 1M √ó $0.00003 = $30                                           |
|  - SAVINGS: 99.4%                                                         |
|                                                                           |
|  OPEN-SOURCE FINE-TUNED (Llama 3.1 8B):                                   |
|  - Fine-tuning: $300-700 (LoRA)                                           |
|  - Inference: ~$0.001 per 1K tokens (self-hosted)                         |
|  - Monthly: ~$100-200                                                     |
|  - Total control, no API limits                                           |
|                                                                           |
+===========================================================================+
```

### Fine-Tuning Cost Estimates (2025)

```python
# –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ fine-tuning —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

FINE_TUNING_COSTS = {
    # Open-source —Å LoRA (Parameter-Efficient)
    "phi-2_2.7B_lora": {
        "training_cost": "$300-700",
        "training_time": "2-4 hours",
        "gpu_required": "1x A100",
        "inference_cost": "~$0.001/1K tokens"
    },

    "mistral_7B_lora": {
        "training_cost": "$1,000-3,000",
        "training_time": "4-8 hours",
        "gpu_required": "1x A100",
        "inference_cost": "~$0.002/1K tokens"
    },

    "llama3.1_8B_lora": {
        "training_cost": "$1,000-3,000",
        "training_time": "4-8 hours",
        "gpu_required": "1x A100",
        "inference_cost": "~$0.002/1K tokens"
    },

    "llama3.1_70B_lora": {
        "training_cost": "$5,000-15,000",
        "training_time": "12-24 hours",
        "gpu_required": "4-8x A100",
        "inference_cost": "~$0.01/1K tokens"
    },

    # Proprietary (OpenAI)
    "gpt-4o-mini_finetuning": {
        "training_cost": "$0.30/1M tokens trained",
        "inference_cost": "$0.30 input / $1.20 output per 1M",
        "min_examples": 10,
        "recommended_examples": "50-100"
    },

    "gpt-4o_finetuning": {
        "training_cost": "$25/1M tokens trained",
        "inference_cost": "$3.75 input / $15 output per 1M",
        "min_examples": 10,
        "recommended_examples": "50-100"
    }
}

# ROI –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è fine-tuning
def calculate_finetuning_roi(
    current_cost_per_request: float,
    finetuned_cost_per_request: float,
    finetuning_cost: float,
    requests_per_month: int
) -> dict:
    """–†–∞—Å—á—ë—Ç ROI –æ—Ç fine-tuning"""

    monthly_savings = (current_cost_per_request - finetuned_cost_per_request) * requests_per_month
    break_even_months = finetuning_cost / monthly_savings if monthly_savings > 0 else float('inf')
    yearly_savings = monthly_savings * 12 - finetuning_cost

    return {
        "monthly_savings": monthly_savings,
        "break_even_months": break_even_months,
        "yearly_roi": yearly_savings / finetuning_cost * 100 if finetuning_cost > 0 else 0,
        "yearly_net_savings": yearly_savings
    }

# –ü—Ä–∏–º–µ—Ä: –ø–µ—Ä–µ—Ö–æ–¥ —Å GPT-4o + –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ fine-tuned GPT-4o-mini
roi = calculate_finetuning_roi(
    current_cost_per_request=0.005,      # GPT-4o —Å 2000 token prompt
    finetuned_cost_per_request=0.0003,   # Fine-tuned GPT-4o-mini, –∫–æ—Ä–æ—Ç–∫–∏–π prompt
    finetuning_cost=1000,                # –°—Ç–æ–∏–º–æ—Å—Ç—å fine-tuning
    requests_per_month=100_000
)

print(f"""
Fine-Tuning ROI Analysis:
-------------------------
Monthly savings: ${roi['monthly_savings']:,.0f}
Break-even: {roi['break_even_months']:.1f} months
Yearly ROI: {roi['yearly_roi']:.0f}%
Yearly net savings: ${roi['yearly_net_savings']:,.0f}
""")
```

---

## 10. Cost Monitoring & Alerts

### Production Monitoring System

```python
import time
from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime, timedelta
import json

@dataclass
class UsageRecord:
    timestamp: float
    model: str
    provider: str
    input_tokens: int
    output_tokens: int
    cached_tokens: int
    cost: float
    request_id: str
    tier: str = "standard"

@dataclass
class Budget:
    daily_limit: float
    monthly_limit: float
    alert_threshold: float = 0.8  # Alert at 80%
    critical_threshold: float = 0.95  # Critical at 95%

class LLMCostTracker:
    """Production-grade cost tracking system"""

    # Updated prices December 2025
    PRICES = {
        # OpenAI
        "gpt-4o": {"input": 2.50, "output": 10.00, "cached": 1.25},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60, "cached": 0.075},
        "o1": {"input": 15.00, "output": 60.00, "cached": 7.50},
        "o3-mini": {"input": 1.10, "output": 4.40, "cached": 0.55},

        # Anthropic
        "claude-sonnet-4": {"input": 3.00, "output": 15.00, "cached": 0.30},
        "claude-4-1-sonnet": {"input": 5.00, "output": 25.00, "cached": 0.50},
        "claude-4-1-opus": {"input": 20.00, "output": 80.00, "cached": 2.00},
        "claude-3-5-haiku": {"input": 0.80, "output": 4.00, "cached": 0.08},

        # Google
        "gemini-2.0-flash": {"input": 0.10, "output": 0.40, "cached": 0.05},
        "gemini-2.5-pro": {"input": 1.25, "output": 10.00, "cached": 0.625},
        "gemini-3-flash": {"input": 0.50, "output": 3.00, "cached": 0.25},

        # DeepSeek
        "deepseek-v3": {"input": 0.28, "output": 0.42, "cached": 0.028},
    }

    def __init__(self, budget: Budget):
        self.budget = budget
        self.records: list[UsageRecord] = []
        self.alerts_sent: set = set()

    def track(self, response, model: str, provider: str = "openai") -> UsageRecord:
        """Track usage from API response"""

        usage = response.usage
        prices = self.PRICES.get(model, {"input": 0, "output": 0, "cached": 0})

        # Extract tokens
        input_tokens = getattr(usage, 'prompt_tokens', 0) or getattr(usage, 'input_tokens', 0)
        output_tokens = getattr(usage, 'completion_tokens', 0) or getattr(usage, 'output_tokens', 0)

        # Cached tokens (provider-specific)
        cached_tokens = 0
        if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:
            cached_tokens = getattr(usage.prompt_tokens_details, 'cached_tokens', 0)

        # Calculate cost
        uncached_input = input_tokens - cached_tokens
        input_cost = (uncached_input / 1_000_000) * prices["input"]
        cached_cost = (cached_tokens / 1_000_000) * prices["cached"]
        output_cost = (output_tokens / 1_000_000) * prices["output"]
        total_cost = input_cost + cached_cost + output_cost

        record = UsageRecord(
            timestamp=time.time(),
            model=model,
            provider=provider,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cached_tokens=cached_tokens,
            cost=total_cost,
            request_id=getattr(response, 'id', 'unknown')
        )

        self.records.append(record)
        self._check_alerts()

        return record

    def _check_alerts(self):
        """Check budget thresholds and send alerts"""

        daily = self.get_daily_cost()
        monthly = self.get_monthly_cost()

        # Daily alerts
        daily_pct = daily / self.budget.daily_limit
        if daily_pct >= self.budget.critical_threshold:
            self._send_alert("CRITICAL", f"Daily spend at {daily_pct*100:.1f}%: ${daily:.2f}")
        elif daily_pct >= self.budget.alert_threshold:
            self._send_alert("WARNING", f"Daily spend at {daily_pct*100:.1f}%: ${daily:.2f}")

        # Monthly alerts
        monthly_pct = monthly / self.budget.monthly_limit
        if monthly_pct >= self.budget.critical_threshold:
            self._send_alert("CRITICAL", f"Monthly spend at {monthly_pct*100:.1f}%: ${monthly:.2f}")
        elif monthly_pct >= self.budget.alert_threshold:
            self._send_alert("WARNING", f"Monthly spend at {monthly_pct*100:.1f}%: ${monthly:.2f}")

    def _send_alert(self, level: str, message: str):
        """Send alert (integrate with Slack, PagerDuty, etc.)"""
        alert_key = f"{level}:{datetime.now().strftime('%Y-%m-%d-%H')}"
        if alert_key not in self.alerts_sent:
            print(f"[{level}] {message}")
            self.alerts_sent.add(alert_key)
            # TODO: Integrate with alerting system

    def get_daily_cost(self) -> float:
        """Today's total cost"""
        today_start = time.time() - 86400
        return sum(r.cost for r in self.records if r.timestamp > today_start)

    def get_monthly_cost(self) -> float:
        """Current month's total cost"""
        month_start = time.time() - (30 * 86400)
        return sum(r.cost for r in self.records if r.timestamp > month_start)

    def get_summary(self) -> dict:
        """Comprehensive usage summary"""

        today_start = time.time() - 86400
        daily_records = [r for r in self.records if r.timestamp > today_start]

        by_model = {}
        for r in daily_records:
            if r.model not in by_model:
                by_model[r.model] = {"cost": 0, "requests": 0, "tokens": 0}
            by_model[r.model]["cost"] += r.cost
            by_model[r.model]["requests"] += 1
            by_model[r.model]["tokens"] += r.input_tokens + r.output_tokens

        total_input = sum(r.input_tokens for r in daily_records)
        total_cached = sum(r.cached_tokens for r in daily_records)
        cache_hit_rate = total_cached / total_input if total_input > 0 else 0

        return {
            "daily_cost": self.get_daily_cost(),
            "monthly_cost": self.get_monthly_cost(),
            "daily_limit": self.budget.daily_limit,
            "monthly_limit": self.budget.monthly_limit,
            "daily_usage_pct": self.get_daily_cost() / self.budget.daily_limit * 100,
            "monthly_usage_pct": self.get_monthly_cost() / self.budget.monthly_limit * 100,
            "requests_today": len(daily_records),
            "cache_hit_rate": cache_hit_rate * 100,
            "by_model": by_model,
            "avg_cost_per_request": self.get_daily_cost() / len(daily_records) if daily_records else 0
        }


# –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï
budget = Budget(daily_limit=100.0, monthly_limit=2000.0)
tracker = LLMCostTracker(budget)

# –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ API –≤—ã–∑–æ–≤–∞
response = openai.chat.completions.create(...)
record = tracker.track(response, "gpt-4o")

# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
summary = tracker.get_summary()
print(f"""
Daily Summary:
--------------
Spent: ${summary['daily_cost']:.2f} / ${summary['daily_limit']:.2f} ({summary['daily_usage_pct']:.1f}%)
Requests: {summary['requests_today']}
Cache hit rate: {summary['cache_hit_rate']:.1f}%
Avg cost/request: ${summary['avg_cost_per_request']:.6f}
""")
```

---

## 11. Optimization Checklist

```
+===========================================================================+
|                     COST OPTIMIZATION CHECKLIST                           |
+===========================================================================+
|                                                                           |
|  QUICK WINS (implement first, 1-2 days):                     Est. Savings |
|  -----------------------------------------------------------------------  |
|  [ ] Use cheaper models for 60-70% of requests               30-50%       |
|  [ ] Enable prompt caching (structure prompts correctly)     15-30%       |
|  [ ] Set reasonable max_tokens limits                        10-20%       |
|  [ ] Move batch workloads to Batch API                       50%          |
|  [ ] Remove redundant tokens from prompts                    10-20%       |
|                                                                           |
|  MEDIUM EFFORT (1-2 weeks):                                               |
|  -----------------------------------------------------------------------  |
|  [ ] Implement model routing based on complexity             30-50%       |
|  [ ] Add RAG to reduce context size                          70-90%       |
|  [ ] Set up semantic caching (GPTCache)                      20-40%       |
|  [ ] Implement cost tracking & alerts                        5-10%        |
|  [ ] Use structured outputs for predictable length           15-25%       |
|                                                                           |
|  ADVANCED (1-2 months):                                                   |
|  -----------------------------------------------------------------------  |
|  [ ] LLMLingua prompt compression                            60-80%       |
|  [ ] Fine-tune smaller model for specific tasks              40-80%       |
|  [ ] Self-host for high-volume workloads                     50-90%       |
|  [ ] Implement distillation pipeline                         50-85%       |
|  [ ] Build evaluation framework for quality monitoring       -            |
|                                                                           |
|  -----------------------------------------------------------------------  |
|                                                                           |
|  EXPECTED CUMULATIVE RESULTS:                                             |
|  - Quick wins only: 40-60% reduction                                      |
|  - Quick + Medium: 60-80% reduction                                       |
|  - Full implementation: 80-95% reduction                                  |
|                                                                           |
|  CASE STUDIES:                                                            |
|  - Fintech company: 99.7% reduction ($937,500 -> $3,000/month)           |
|  - Customer support: 50%+ reduction in servicing workload                 |
|  - Financial services: 40% reduction in back-office costs                 |
|                                                                           |
+===========================================================================+
```

---

## 12. –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

1. **Q: –ö–∞–∫—É—é —ç–∫–æ–Ω–æ–º–∏—é –¥–∞—ë—Ç prompt caching —É —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤?**
   A: OpenAI: 50% –Ω–∞ cached tokens (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–ª—è >1024 tokens). Anthropic: –¥–æ 90% –Ω–∞ cache reads, –Ω–æ +25% –Ω–∞ cache write. Break-even: 3-5 –∑–∞–ø—Ä–æ—Å–æ–≤.

2. **Q: –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Batch API?**
   A: –î–ª—è –∑–∞–¥–∞—á –±–µ–∑ realtime —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: content moderation, data enrichment, bulk classification, report generation. –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –¥–∞—é—Ç 50% —Å–∫–∏–¥–∫—É, latency –¥–æ 24 —á–∞—Å–æ–≤.

3. **Q: –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç semantic caching –∏ –∫–∞–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**
   A: GPTCache –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –≤ embeddings –∏ –Ω–∞—Ö–æ–¥–∏—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ. 31% –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ—Ö–æ–∂–∏ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: MeanCache (–º–µ–Ω—å—à–µ false positives), GenerativeCache (9x –±—ã—Å—Ç—Ä–µ–µ).

4. **Q: –ö–æ–≥–¥–∞ –æ–∫—É–ø–∞–µ—Ç—Å—è self-hosting?**
   A: –ü—Ä–∏ >2M tokens/day –∏/–∏–ª–∏ —Å—Ç—Ä–æ–≥–∏—Ö compliance —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö. Break-even –¥–ª—è Llama 8B –Ω–∞ g5.2xlarge ($850/month) –ø—Ä–∏–º–µ—Ä–Ω–æ –ø—Ä–∏ 10M tokens/day vs GPT-4o API.

5. **Q: Fine-tuning vs Prompting - –∫–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**
   A: Prompting: —Ä–∞–Ω–Ω–∏–µ —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞, –≥–∏–±–∫–æ—Å—Ç—å, RAG –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω. Fine-tuning: stable tasks, high volume, need consistency, domain knowledge, —Ö–æ—Ç–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—à—ë–≤—É—é –º–æ–¥–µ–ª—å.

6. **Q: –ö–∞–∫—É—é —Ä–µ–∞–ª—å–Ω—É—é —ç–∫–æ–Ω–æ–º–∏—é –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∫–µ–π—Å—ã?**
   A: –§–∏–Ω—Ç–µ—Ö –∫–æ–º–ø–∞–Ω–∏–∏: 99.7% (—Å $937K –¥–æ $3K/month). Financial services: 40% back-office costs. Customer support: 50%+ —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ LLM assistants.

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

### –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ pricing pages
- [OpenAI API Pricing](https://platform.openai.com/docs/pricing)
- [Anthropic Claude Pricing](https://www.anthropic.com/pricing)
- [Google Gemini Pricing](https://ai.google.dev/gemini-api/docs/pricing)
- [DeepSeek Pricing](https://api-docs.deepseek.com/quick_start/pricing)

### Cost Optimization Guides
- [LLM Cost Optimization: Complete Guide 2025 - Koombea](https://ai.koombea.com/blog/llm-cost-optimization)
- [LLM Cost Optimization Guide - FutureAGI](https://futureagi.com/blogs/llm-cost-optimization-2025)
- [10 Ways to Slash Inference Costs - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/12/llm-cost-optimization/)
- [How to Save 90% on LLM API Costs - Prem AI](https://blog.premai.io/how-to-save-90-on-llm-api-costs-without-losing-performance/)

### Prompt Caching
- [Prompt Caching Guide 2025 - PromptBuilder](https://promptbuilder.cc/blog/prompt-caching-token-economics-2025)
- [Comparing Prompt Caching: OpenAI, Anthropic, Gemini - Medium](https://medium.com/@m_sea_bass/comparing-prompt-caching-openai-anthropic-and-gemini-0eac16541898)
- [Anthropic Prompt Caching 90% Savings - ByteIota](https://byteiota.com/anthropic-prompt-caching-cuts-ai-api-costs-90/)

### Semantic Caching
- [GPTCache GitHub](https://github.com/zilliztech/GPTCache)
- [GPTCache Documentation](https://gptcache.readthedocs.io/en/latest/)
- [Deep Dive Into GPTCache - Medium](https://medium.com/@raju.samantapudi/rethinking-llm-performance-a-deep-dive-into-gptcache-and-the-future-of-semantic-caching-6f338f1f2fd2)

### Token Compression
- [LLMLingua - Microsoft Research](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)
- [LLMLingua GitHub](https://github.com/microsoft/LLMLingua)
- [Token Compression 80% Savings - Medium](https://medium.com/@yashpaddalwar/token-compression-how-to-slash-your-llm-costs-by-80-without-sacrificing-quality-bfd79daf7c7c)

### Batch Processing
- [LLM Batch Inference Guide - ZenML](https://www.zenml.io/blog/the-ultimate-guide-to-llm-batch-inference-with-openai-and-zenml)
- [Scaling LLMs with Batch Processing - Latitude](https://latitude-blog.ghost.io/blog/scaling-llms-with-batch-processing-ultimate-guide/)
- [Continuous Batching 23x Throughput - Anyscale](https://www.anyscale.com/blog/continuous-batching-llm-inference)

### Self-Hosting Analysis
- [API vs Self-Hosting Cost Comparison - DetectX](https://www.detectx.com.au/cost-comparison-api-vs-self-hosting-for-open-weight-llms/)
- [LLM Total Cost of Ownership 2025 - Ptolemay](https://www.ptolemay.com/post/llm-total-cost-of-ownership)
- [OpenAI vs Self-Hosted LLMs - Substack](https://tinyml.substack.com/p/openai-vs-self-hosted-llms-a-cost)

### Fine-Tuning
- [Is Fine-Tuning Still Worth It 2025 - Kadoa](https://www.kadoa.com/blog/is-fine-tuning-still-worth-it)
- [Prompt Engineering vs Fine-Tuning - DextraLabs](https://dextralabs.com/blog/prompt-engineering-vs-fine-tuning/)
- [Fine-Tuning vs Prompting Cost Tradeoffs - Medium](https://medium.com/write-a-catalyst/fine-tuning-vs-prompting-when-each-wins-and-cost-tradeoffs-e205522ac10c)

### Case Studies
- [99.7% Cost Reduction in LLM Deployment - Medium](https://medium.com/@richardhightower/the-economics-of-deploying-large-language-models-costs-value-and-99-7-savings-d1cd9a84fcbe)
- [LLMs in Financial Services - ScienceSoft](https://www.scnsoft.com/finance/large-language-models)
- [Enhancing ROI with LLM Technology - A3Logics](https://www.a3logics.com/success-stories/enhancing-roi-and-cost-savings-with-llm-technology/)

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏

- [[llm-fundamentals]] - –û—Å–Ω–æ–≤—ã LLM
- [[llm-inference-optimization]] - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
- [[local-llms-self-hosting]] - Self-hosting (0 API costs)
- [[ai-observability-monitoring]] - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ AI —Å–∏—Å—Ç–µ–º
- [[rag-systems]] - RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- [[prompt-engineering]] - Prompt Engineering

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ö–∞–∫–∏–µ —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä—ã—á–∞–≥–∞ —Å–Ω–∏–∂–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ LLM-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—Ç?
> –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–º–ø—Ä–µ—Å—Å–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞), –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ (–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –Ω–∞ –¥–µ—à—ë–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á), –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (batch processing, self-hosting –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö –æ–±—ä—ë–º–æ–≤). –ö–∞–∂–¥—ã–π —Ä—ã—á–∞–≥ –¥–∞—ë—Ç 30-70% —ç–∫–æ–Ω–æ–º–∏–∏, –≤–º–µ—Å—Ç–µ --- –¥–æ 90%.

> [!question]- –ö–æ–≥–¥–∞ self-hosting LLM –≤—ã–≥–æ–¥–Ω–µ–µ API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤?
> –ü—Ä–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º –æ–±—ä—ë–º–µ >100K –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å, –∫–æ–≥–¥–∞ —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è GPU >70%. –¢–æ—á–∫–∞ –±–µ–∑—É–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏: –¥–ª—è 7B –º–æ–¥–µ–ª–µ–π --- –æ—Ç 50K –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å, –¥–ª—è 70B --- –æ—Ç 200K. –ù—É–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å GPU, DevOps-–∏–Ω–∂–µ–Ω–µ—Ä–æ–≤, —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–∞ –∏ downtime.

> [!question]- –ö–∞–∫ Batch API —Å–Ω–∏–∂–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏ –∫–æ–≥–¥–∞ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?
> Batch API (OpenAI, Anthropic) –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–æ —Å–∫–∏–¥–∫–æ–π 50%. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è non-real-time –∑–∞–¥–∞—á: –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, evaluation datasets. –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 24 —á–∞—Å–æ–≤.

> [!question]- –ö–∞–∫ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ AI?
> –¢—Ä–µ–∫–∞—Ç—å: —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ –º–æ–¥–µ–ª–∏, –ø–æ feature, –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é. –ú–µ—Ç—Ä–∏–∫–∏: cost-per-task, cost-per-token, cache hit rate. –ê–ª–µ—Ä—Ç—ã –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏ (>2x –æ—Ç baseline). Dashboard —Å breakdown –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: LangSmith, Helicone, –∏–ª–∏ custom –Ω–∞ OpenTelemetry.

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ö–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤?
?
Pay-per-token (–æ—Å–Ω–æ–≤–Ω–æ–π), batch pricing (—Å–∫–∏–¥–∫–∞ 50% –∑–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å), committed use discounts (–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∑–∞ –æ–±—ä—ë–º), –∏ cached pricing (–¥–µ—à–µ–≤–ª–µ –∑–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã). –£ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Å–≤–æ—è –º–æ–¥–µ–ª—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è.

–ß—Ç–æ —Ç–∞–∫–æ–µ prompt caching –∏ –∫–∞–∫ –æ–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?
?
–ü—Ä–æ–≤–∞–π–¥–µ—Ä —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ —Å —Ç–µ–º –∂–µ –ø—Ä–µ—Ñ–∏–∫—Å–æ–º. Anthropic: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π, —Å–∫–∏–¥–∫–∞ 90% –Ω–∞ cached tokens. OpenAI: prefix caching, —Å–∫–∏–¥–∫–∞ 50%.

–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç model routing –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏?
?
–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ –∏ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å. –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã --- GPT-4o-mini/Haiku ($0.15/M), —Å–ª–æ–∂–Ω—ã–µ --- GPT-4o/Sonnet ($3-15/M). –≠–∫–æ–Ω–æ–º–∏—è 60-80% –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞.

–ß—Ç–æ —Ç–∞–∫–æ–µ token optimization –∏ –∫–∞–∫–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç?
?
–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö/–æ—Ç–≤–µ—Ç–∞—Ö. –¢–µ—Ö–Ω–∏–∫–∏: –∫–æ–º–ø—Ä–µ—Å—Å–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ (—É–±—Ä–∞—Ç—å –ª–∏—à–Ω–µ–µ), structured outputs (JSON –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–∞), max_tokens –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤, –∏ context window management.

–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å TCO (Total Cost of Ownership) –¥–ª—è AI-—Å–∏—Å—Ç–µ–º—ã?
?
TCO = API costs + infrastructure + development + maintenance. API: tokens x price. Infrastructure: GPU/CPU, storage, networking. Development: –∏–Ω–∂–µ–Ω–µ—Ä—ã, testing, evaluation. Maintenance: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, incident response.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[agent-cost-optimization]] | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[llm-inference-optimization]] | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è inference –¥–ª—è self-hosting |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[performance-optimization]] | –û–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
