---
title: "LLM Inference Optimization - –ü–æ–ª–Ω–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
tags:
  - topic/ai-ml
  - inference
  - performance
  - vllm
  - tensorrt
  - quantization
  - type/concept
  - level/advanced
category: ai-ml
level: advanced
created: 2025-01-15
updated: 2026-02-13
reading_time: 52
difficulty: 8
study_status: not_started
mastery: 0
last_reviewed:
next_review:
sources:
  - nvidia.com
  - vllm.ai
  - github.com/sgl-project
  - arxiv.org
  - lmsys.org
related:
  - "[[mobile-ai-ml-guide]]"
  - "[[local-llms-self-hosting]]"
status: published
---

# LLM Inference Optimization: –û—Ç –¢–µ–æ—Ä–∏–∏ –∫ Production

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ LLM** | –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –º–æ–¥–µ–ª–∏, attention | [[llm-fundamentals]] |
| **Python** | –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∞ Python | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **Docker/Kubernetes** | Deployment serving engines | [[devops-overview]] |
| **GPU Basics** | VRAM, CUDA, tensor cores | –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è NVIDIA |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ AI** | ‚ùå –ù–µ—Ç | –°–Ω–∞—á–∞–ª–∞ [[llm-fundamentals]] –∏ [[local-llms-self-hosting]] |
| **AI Engineer** | ‚úÖ –î–∞ | –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é |
| **ML Platform Engineer** | ‚úÖ –î–∞ | Production-grade serving |
| **DevOps/SRE** | ‚úÖ –î–∞ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ LLM |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Inference Optimization** = –∫–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å LLM –æ—Ç–≤–µ—á–∞—Ç—å –±—ã—Å—Ç—Ä–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Inference** | –ü—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ | **–ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è** ‚Äî –º–æ–¥–µ–ª—å "–¥—É–º–∞–µ—Ç" –∏ –æ—Ç–≤–µ—á–∞–µ—Ç |
| **TTFT** | Time To First Token | **–í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ñ–∏—Ü–∏–∞–Ω—Ç–∞** ‚Äî –æ—Ç –∑–∞–∫–∞–∑–∞ –¥–æ –ø–µ—Ä–≤–æ–≥–æ –±–ª—é–¥–∞ |
| **TPOT** | Time Per Output Token | **–°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–¥–∞—á–∏** ‚Äî –∫–∞–∫ –±—ã—Å—Ç—Ä–æ –ø—Ä–∏–Ω–æ—Å—è—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ –±–ª—é–¥–∞ |
| **Throughput** | –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É | **–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å** ‚Äî —Å–∫–æ–ª—å–∫–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ –æ–±—Å–ª—É–∂–∏–≤–∞–µ—à—å |
| **Batching** | –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–º–µ—Å—Ç–µ | **–ì—Ä—É–ø–ø–æ–≤–æ–π –∑–∞–∫–∞–∑** ‚Äî –≥–æ—Ç–æ–≤–∏—Ç—å –Ω–∞ –≤—Å–µ—Ö —Å—Ä–∞–∑—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ |
| **KV Cache** | –ö—ç—à –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π | **–ü–∞–º—è—Ç—å –æ –∑–∞–∫–∞–∑–µ** ‚Äî –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ |
| **Quantization** | –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏ | **–£–ø–∞–∫–æ–≤–∫–∞** ‚Äî –º–µ–Ω—å—à–µ –º–µ—Å—Ç–∞, —á—É—Ç—å —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| **PagedAttention** | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é | **–£–º–Ω—ã–π —Å–∫–ª–∞–¥** ‚Äî —ç–∫–æ–Ω–æ–º–∏—è 60-80% –ø–∞–º—è—Ç–∏ |
| **Speculative Decoding** | –£—Å–∫–æ—Ä–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ | **–ü—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑–∞** ‚Äî –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∑–∞—Ä–∞–Ω–µ–µ |

---

## –í–≤–µ–¥–µ–Ω–∏–µ: –ü–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ

–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å –º–æ—â–Ω—ã–π —Å–ø–æ—Ä—Ç–∫–∞—Ä (LLM), –Ω–æ –≤—ã –∑–∞—Å—Ç—Ä—è–ª–∏ –≤ –ø—Ä–æ–±–∫–µ (–ø–ª–æ—Ö–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å). –°–∫–æ–ª—å–∫–æ –±—ã –ª–æ—à–∞–¥–∏–Ω—ã—Ö —Å–∏–ª –Ω–∏ –±—ã–ª–æ –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º, –≤—ã –Ω–∏–∫—É–¥–∞ –Ω–µ –µ–¥–µ—Ç–µ. –ò–º–µ–Ω–Ω–æ —Ç–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç production-deployment LLM –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –º–æ–¥–µ–ª—å –µ—Å—Ç—å, –Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∂–¥—É—Ç –ø–æ 10-20 —Å–µ–∫—É–Ω–¥ –æ—Ç–≤–µ—Ç–∞, –∞ —Å—á–µ—Ç–∞ –∑–∞ GPU —Ä–∞—Å—Ç—É—Ç –∫–∞–∫ —Å–Ω–µ–∂–Ω—ã–π –∫–æ–º.

**–ú–∞—Å—à—Ç–∞–± –ø—Ä–æ–±–ª–µ–º—ã –≤ 2025 –≥–æ–¥—É:**
- –ü–æ –¥–∞–Ω–Ω—ã–º Andreessen Horowitz, 49% –∫–æ–º–ø–∞–Ω–∏–π —Å–æ–æ–±—â–∞—é—Ç, —á—Ç–æ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –∏—Ö compute-–±—é–¥–∂–µ—Ç–∞ —É—Ö–æ–¥–∏—Ç –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (–ø—Ä–æ—Ç–∏–≤ 29% –≤ 2024)
- –†–∞—Å—Ö–æ–¥—ã –Ω–∞ API –º–æ–¥–µ–ª–µ–π –≤—ã—Ä–æ—Å–ª–∏ —Å $3.5B –¥–æ $8.4B –∑–∞ –≥–æ–¥
- McKinsey —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç —Ä–æ—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GenAI –≤ –±–∏–∑–Ω–µ—Å–µ —Å 33% –¥–æ 67%

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ "nice to have", –∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç–æ—Ä, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π –±—É–¥—É—Ç –ª–∏ –≤–∞—à–∏ LLM-—Å–∏—Å—Ç–µ–º—ã —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω—ã.

---

## TL;DR –¥–ª—è –∑–∞–Ω—è—Ç—ã—Ö

> **–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
> - **Continuous Batching** - 2-23x throughput vs static batching
> - **PagedAttention** - —ç–∫–æ–Ω–æ–º–∏—è 60-80% –ø–∞–º—è—Ç–∏ KV cache
> - **Speculative Decoding** - 2-3.5x —É—Å–∫–æ—Ä–µ–Ω–∏–µ latency –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞
> - **Quantization** - AWQ/GPTQ –¥–ª—è 4-bit, FP8 –¥–ª—è H100+
>
> **–î–≤–∏–∂–∫–∏ (–æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∫ —Å–ª–æ–∂–Ω–æ–º—É):**
> 1. **vLLM** - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç, OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å, 24x vs HuggingFace
> 2. **SGLang** - –ª—É—á—à–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å latency, RadixAttention
> 3. **TensorRT-LLM** - –º–∞–∫—Å–∏–º—É–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ NVIDIA H100/B200
>
> **–ü—Ä–∞–≤–∏–ª–æ –±–æ–ª—å—à–æ–≥–æ –ø–∞–ª—å—Ü–∞:** –ù–∞—á–Ω–∏—Ç–µ —Å vLLM + AWQ quantization, –ø–æ—Ç–æ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–¥ –≤–∞—à use case.

---

## –ì–ª–æ—Å—Å–∞—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤

| –¢–µ—Ä–º–∏–Ω | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è |
|--------|-------------|----------|
| **TTFT** | Time To First Token - –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ | –í—Ä–µ–º—è –æ—Ç –∑–∞–∫–∞–∑–∞ –¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –æ—Ñ–∏—Ü–∏–∞–Ω—Ç–∞ |
| **TPOT** | Time Per Output Token - –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ | –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–¥–∞—á–∏ –±–ª—é–¥ |
| **Throughput** | –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫—É–Ω–¥—É –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–æ–≤ | –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ |
| **Goodput** | –ó–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫ –≤ —Ä–∞–º–∫–∞—Ö SLO | –î–æ–≤–æ–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã –≤ —á–∞—Å |
| **KV Cache** | –ö—ç—à Key-Value –¥–ª—è –≤–Ω–∏–º–∞–Ω–∏—è | –ó–∞–ø–∏—Å–Ω–∞—è –∫–Ω–∏–∂–∫–∞ –æ—Ñ–∏—Ü–∏–∞–Ω—Ç–∞ |
| **Prefill** | –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (compute-bound) | –ß—Ç–µ–Ω–∏–µ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–∞–∫–∞–∑–∞ |
| **Decode** | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (memory-bound) | –ü—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ –∏ –ø–æ–¥–∞—á–∞ –µ–¥—ã |
| **PagedAttention** | Paged memory –¥–ª—è KV cache | –°–∏—Å—Ç–µ–º–∞ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–æ–≤ |

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LLM –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: –ü–æ–Ω–∏–º–∞–Ω–∏–µ —É–∑–∫–∏—Ö –º–µ—Å—Ç

–ü—Ä–µ–∂–¥–µ —á–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –Ω—É–∂–Ω–æ –ø–æ–Ω—è—Ç—å –≥–¥–µ –∏–º–µ–Ω–Ω–æ "–±—É—Ç—ã–ª–æ—á–Ω–æ–µ –≥–æ—Ä–ª—ã—à–∫–æ". LLM –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–∑:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LLM Inference Pipeline                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   Request Queue ‚îÄ‚îÄ‚îÄ‚ñ∂ SCHEDULER ‚îÄ‚îÄ‚îÄ‚ñ∂ EXECUTION ENGINE            ‚îÇ
‚îÇ                          ‚îÇ                 ‚îÇ                     ‚îÇ
‚îÇ                          ‚îÇ                 ‚ñº                     ‚îÇ
‚îÇ                          ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  Phase 1: PREFILL           ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  ‚Ä¢ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å –ø—Ä–æ–º–ø—Ç ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  ‚Ä¢ Compute-bound (GPU busy) ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞   ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚îÇ                   ‚îÇ                   ‚îÇ
‚îÇ                          ‚îÇ                   ‚ñº                   ‚îÇ
‚îÇ                          ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  Phase 2: DECODE            ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  ‚Ä¢ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ 1 —Ç–æ–∫–µ–Ω—É   ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  ‚Ä¢ Memory-bound (GPU idle)  ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ  ‚Ä¢ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞  ‚îÇ   ‚îÇ
‚îÇ                          ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                          ‚îÇ                                       ‚îÇ
‚îÇ                          ‚ñº                                       ‚îÇ
‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ               ‚îÇ              KV CACHE                         ‚îÇ  ‚îÇ
‚îÇ               ‚îÇ  –•—Ä–∞–Ω–∏—Ç K/V —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤          ‚îÇ  ‚îÇ
‚îÇ               ‚îÇ  –ú–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –¥–æ 1.7GB –Ω–∞ sequence (70B)   ‚îÇ  ‚îÇ
‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ü–æ—á–µ–º—É decode —Ñ–∞–∑–∞ —Ç–∞–∫–∞—è –º–µ–¥–ª–µ–Ω–Ω–∞—è?

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –∫–æ–Ω–≤–µ–π–µ—Ä –ø–æ —Å–±–æ—Ä–∫–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π. –í prefill-—Ñ–∞–∑–µ –≤—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –∑–∞–∫–∞–∑–∞ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ). –í decode-—Ñ–∞–∑–µ –≤—ã —Å–æ–±–∏—Ä–∞–µ—Ç–µ –º–∞—à–∏–Ω—É –ø–æ –æ–¥–Ω–æ–π –¥–µ—Ç–∞–ª–∏, –∏ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –¥–æ–ª–∂–Ω—ã —Å—Ö–æ–¥–∏—Ç—å –Ω–∞ —Å–∫–ª–∞–¥ –∑–∞ —Å–ª–µ–¥—É—é—â–µ–π –¥–µ—Ç–∞–ª—å—é (memory access).

GPU –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–≥—Ä–æ–º–Ω—É—é –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –º–æ—â–Ω–æ—Å—Ç—å, –Ω–æ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –ø—Ä–æ—Å—Ç–∞–∏–≤–∞–µ—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ:
1. –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Ç—Ä–µ–±—É–µ—Ç –ø–æ–ª–Ω–æ–≥–æ forward pass
2. –ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è: token N –∑–∞–≤–∏—Å–∏—Ç –æ—Ç token N-1

**–ö–ª—é—á–µ–≤–æ–π –∏–Ω—Å–∞–π—Ç:** –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ - —ç—Ç–æ –±–æ—Ä—å–±–∞ —Å memory bandwidth bottleneck.

---

## –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏: –ß—Ç–æ –∏–∑–º–µ—Ä—è—Ç—å

### Latency Breakdown

```
Request ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ [  Queue  ] ‚îÄ‚ñ∂ [  TTFT  ] ‚îÄ‚ñ∂ [  TPOT  ] ‚îÄ‚ñ∂ ... ‚îÄ‚ñ∂ [End]
               ‚îÇ           ‚îÇ ‚îÇ          ‚îÇ   ‚îÇ          ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 –û–∂–∏–¥–∞–Ω–∏–µ      Prefill        Decode (per token)

E2E Latency = Queue Time + TTFT + (Output Tokens √ó TPOT)

–ü—Ä–∏–º–µ—Ä –¥–ª—è Llama 70B –Ω–∞ H100:
‚Ä¢ TTFT: 100-300ms (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–∞)
‚Ä¢ TPOT: 20-50ms/token
‚Ä¢ 100 —Ç–æ–∫–µ–Ω–æ–≤ output = 2-5 —Å–µ–∫—É–Ω–¥ total
```

### –ü–æ—á–µ–º—É –≤–∞–∂–Ω—ã —Ä–∞–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö use cases

| Use Case | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | TTFT Target | TPOT Target | –ü–æ—á–µ–º—É |
|----------|-----------|-------------|-------------|--------|
| **Chatbot** | Perceived speed | <500ms | <50ms | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–∏—Ç "–ø–µ—á–∞—Ç–∞–Ω–∏–µ" |
| **Code completion** | Instant feedback | <200ms | <30ms | IDE –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±—ã—Å—Ç—Ä—ã–º |
| **Voice assistant** | Real-time | <100ms | <20ms | –ü–∞—É–∑—ã –∑–∞–º–µ—Ç–Ω—ã –Ω–∞ —Å–ª—É—Ö |
| **Batch processing** | Cost efficiency | <5s | <100ms | Throughput –≤–∞–∂–Ω–µ–µ latency |
| **RAG pipeline** | Balance | <1s | <80ms | –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ |

---

## 1. Continuous Batching: –ü–æ—á–µ–º—É —ç—Ç–æ game-changer

### –ü—Ä–æ–±–ª–µ–º–∞ Static Batching

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –∞–≤—Ç–æ–±—É—Å, –∫–æ—Ç–æ—Ä—ã–π –∂–¥–µ—Ç –ø–æ–∫–∞ –≤—Å–µ –ø–∞—Å—Å–∞–∂–∏—Ä—ã –¥–æ–µ–¥—É—Ç –¥–æ –∫–æ–Ω–µ—á–Ω–æ–π, –¥–∞–∂–µ –µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ –≤—ã—Ö–æ–¥–∏—Ç —Ä–∞–Ω—å—à–µ. –ú–µ—Å—Ç–∞ –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—Ç, –Ω–æ–≤—ã–µ –ø–∞—Å—Å–∞–∂–∏—Ä—ã –∂–¥—É—Ç —Å–ª–µ–¥—É—é—â–∏–π –∞–≤—Ç–æ–±—É—Å.

```
STATIC BATCHING (–Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ):

Batch 1: [R1‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] [R2‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] [R3‚ñà‚ñà‚ñà‚ñà]
         ‚Üë            ‚Üë                   ‚Üë
         –ì–æ—Ç–æ–≤ —Ä–∞–Ω–æ   –°–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π      –ì–æ—Ç–æ–≤ —Ä–∞–Ω–æ
         –Ω–æ –∂–¥—ë—Ç      –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Ä–µ–º—è   –Ω–æ –∂–¥—ë—Ç

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ time
         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –í–µ—Å—å batch –∂–¥—ë—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
                   GPU –ø—Ä–æ—Å—Ç–∞–∏–≤–∞–µ—Ç!

CONTINUOUS BATCHING (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ):

[R1‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]‚îÄ‚îÄ‚ñ∂ done ‚Üí —Å—Ä–∞–∑—É –¥–æ–±–∞–≤–ª—è–µ–º R4
[R2‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ done ‚Üí –¥–æ–±–∞–≤–ª—è–µ–º R5
[R3‚ñà‚ñà‚ñà‚ñà]‚îÄ‚îÄ‚ñ∂ done
      [R4‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
            [R5‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ time
         GPU –≤—Å–µ–≥–¥–∞ –∑–∞–Ω—è—Ç, –Ω–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è!
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ü–æ –¥–∞–Ω–Ω—ã–º Anyscale, continuous batching –≤ vLLM –¥–∞–µ—Ç **23x throughput** –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ p50 latency –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å naive –ø–æ–¥—Ö–æ–¥–æ–º.

### –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º

Continuous batching –æ–ø–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏—Ç–µ—Ä–∞—Ü–∏–π (iteration-level scheduling), –∞ –Ω–µ –∑–∞–ø—Ä–æ—Å–æ–≤:

```python
# –ü—Å–µ–≤–¥–æ–∫–æ–¥ continuous batching scheduler
def schedule_iteration():
    active_batch = []

    # 1. –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    for request in current_batch:
        if request.is_complete():
            yield_response(request)
        else:
            active_batch.append(request)

    # 2. –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–∫–∞ –µ—Å—Ç—å –º–µ—Å—Ç–æ
    while has_capacity() and pending_queue.not_empty():
        new_request = pending_queue.pop()
        active_batch.append(new_request)

    # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é decode –¥–ª—è –≤—Å–µ–≥–æ batch
    run_forward_pass(active_batch)
```

### –í–∫–ª—é—á–µ–Ω–∏–µ –≤ vLLM

```python
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs

engine_args = AsyncEngineArgs(
    model="meta-llama/Llama-3.1-70B-Instruct",

    # Continuous batching –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    max_num_seqs=256,            # Max sequences –≤ batch
    max_num_batched_tokens=8192,  # Max —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é

    # Scheduling
    scheduler_delay_factor=0.0,   # –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–π scheduling
    enable_chunked_prefill=True,  # Chunked prefill –¥–ª—è latency
)
```

---

## 2. PagedAttention: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–∞–º—è—Ç—å—é

### –ü—Ä–æ–±–ª–µ–º–∞: KV Cache –∑–∞–Ω–∏–º–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏

**–ê–Ω–∞–ª–æ–≥–∏—è:** –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –∫–∞–∫ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å —Ä–µ—Å—Ç–æ—Ä–∞–Ω –Ω–∞ –≤–µ—á–µ—Ä, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–∏–¥—É—Ç —Ç–æ–ª—å–∫–æ 3 —á–µ–ª–æ–≤–µ–∫–∞. PagedAttention - –∫–∞–∫ —Å–∏—Å—Ç–µ–º–∞ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–æ–≤: –∑–∞–Ω–∏–º–∞–µ—Ç–µ —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –Ω—É–∂–Ω–æ.

KV Cache –¥–ª—è Llama-13B –º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å **–¥–æ 1.7GB –Ω–∞ –æ–¥–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å**. –ü—Ä–∏ batch size 32 –∏ 4K –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —ç—Ç–æ —É–∂–µ ~20GB —Ç–æ–ª—å–∫–æ –Ω–∞ –∫—ç—à. –ê –µ—â—ë –Ω—É–∂–Ω–æ –º–µ—Å—Ç–æ –¥–ª—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏!

**–ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞:** –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤—ã–¥–µ–ª—è—é—Ç –ø–∞–º—è—Ç—å –ø–æ–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞—Ä–∞–Ω–µ–µ, –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π output –±—É–¥–µ—Ç –∫–æ—Ä–æ—á–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç: **60-80% –ø–∞–º—è—Ç–∏ —Ç—Ä–∞—Ç–∏—Ç—Å—è –≤–ø—É—Å—Ç—É—é**.

### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç PagedAttention

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é –≤ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö:

```
–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π KV Cache:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Seq 1: [K1V1 K2V2 K3V3 K4V4 K5V5 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ø–æ—Ç–µ—Ä—è–Ω–æ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ        (5 —Ç–æ–∫–µ–Ω–æ–≤)              (–≤—ã–¥–µ–ª–µ–Ω–æ –ø–æ–¥ 32 —Ç–æ–∫–µ–Ω–∞)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PagedAttention:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Block 0 ‚îÇ ‚îÇ Block 1 ‚îÇ ‚îÇ Block 2 ‚îÇ ‚îÇ Block 3 ‚îÇ  ‚Üê –§–∏–∑–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
‚îÇ [K1-K16]‚îÇ ‚îÇ [K17-32]‚îÇ ‚îÇ [free]  ‚îÇ ‚îÇ [free]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ           ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
      Page Table
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Seq 1: [0, 1]     ‚îÇ  ‚Üê –õ–æ–≥–∏—á–µ—Å–∫–∏–µ ‚Üí –§–∏–∑–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
‚îÇ Seq 2: [1, 2]     ‚îÇ  ‚Üê Block 1 SHARED –º–µ–∂–¥—É Seq 1 –∏ 2!
‚îÇ Seq 3: [0]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–†–µ–∑—É–ª—å—Ç–∞—Ç: ~4% –ø–æ—Ç–µ—Ä—å –ø–∞–º—è—Ç–∏ vs 60-80% —Ä–∞–Ω–µ–µ
```

### –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

1. **–ë–ª–æ–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞** - –Ω–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
2. **–ê–ª–ª–æ–∫–∞—Ü–∏—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é** - –ø–∞–º—è—Ç—å –≤—ã–¥–µ–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–∞
3. **Sharing** - –æ–±—â–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã (system prompt) —Ö—Ä–∞–Ω—è—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑
4. **Copy-on-write** - –∫–æ–ø–∏—Ä—É–µ–º –±–ª–æ–∫–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

vLLM —Å PagedAttention –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **–¥–æ 24x throughput** –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å HuggingFace Transformers –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Å–ª—É–∂–∏–≤–∞—Ç—å **–≤ 2-4x –±–æ–ª—å—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤** –ø—Ä–∏ —Ç–æ–º –∂–µ –æ–±—ä—ë–º–µ GPU –ø–∞–º—è—Ç–∏.

---

## 3. Prefix Caching: –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

### –ò–¥–µ—è

–ï—Å–ª–∏ —É –≤–∞—Å 100 –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º system prompt, –∑–∞—á–µ–º –≤—ã—á–∏—Å–ª—è—Ç—å –µ–≥–æ KV cache 100 —Ä–∞–∑?

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ï—Å–ª–∏ –∫–∞–∂–¥—ã–π –≥–æ—Å—Ç—å –∑–∞–∫–∞–∑—ã–≤–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Å–∞–ª–∞—Ç, —à–µ—Ñ-–ø–æ–≤–∞—Ä –≥–æ—Ç–æ–≤–∏—Ç –æ–¥–Ω—É –±–æ–ª—å—à—É—é –ø–æ—Ä—Ü–∏—é, –∞ –Ω–µ 100 –º–∞–ª–µ–Ω—å–∫–∏—Ö –æ—Ç–¥–µ–ª—å–Ω–æ.

### Automatic Prefix Caching (APC) –≤ vLLM

```python
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    enable_prefix_caching=True  # –í–∫–ª—é—á–∞–µ—Ç APC
)

# –û–±—â–∏–π system prompt
system_prompt = """You are a helpful assistant specialized in Python.
Always provide working code examples with explanations."""

# –ó–∞–ø—Ä–æ—Å 1 - –≤—ã—á–∏—Å–ª—è–µ—Ç KV cache –¥–ª—è system_prompt
response1 = llm.generate([f"{system_prompt}\n\nUser: Write a sort function"])

# –ó–∞–ø—Ä–æ—Å 2 - –ü–ï–†–ï–ò–°–ü–û–õ–¨–ó–£–ï–¢ KV cache (–±—ã—Å—Ç—Ä–µ–µ!)
response2 = llm.generate([f"{system_prompt}\n\nUser: Explain decorators"])
# TTFT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ prefill –¥–ª—è system_prompt –ø—Ä–æ–ø—É—â–µ–Ω
```

### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ

APC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö–µ—à–∏ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–ª–æ–∫–æ–≤:
- –ö–∞–∂–¥—ã–π –±–ª–æ–∫ KV cache –ø–æ–ª—É—á–∞–µ—Ç —Ö–µ—à –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω—ë–º –∏ —Ö–µ—à–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –±–ª–æ–∫–∞ (parent chaining)
- –ï—Å–ª–∏ —Ö–µ—à –±–ª–æ–∫–∞ N —Å–æ–≤–ø–∞–¥–∞–µ—Ç, —Ç–æ –±–ª–æ–∫–∏ 0..N-1 –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã (—Å–≤–æ–π—Å—Ç–≤–æ causal attention)
- LRU eviction –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é

### RadixAttention (SGLang): –£–º–Ω–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ–π prefix caching

SGLang –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Radix Tree –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è:

```
Radix Tree –¥–ª—è KV Cache:
                    [system_prompt]
                    /              \
           [few_shot_1]         [few_shot_2]
           /          \              |
    [query_A]     [query_B]     [query_C]
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å structured workloads (few-shot, multi-turn)
- –î–æ **6.4x throughput** –Ω–∞ benchmark –∑–∞–¥–∞—á–∞—Ö

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ production (Chatbot Arena):**
- 52.4% cache hit rate –¥–ª—è LLaVA-Next-34B
- 74.1% cache hit rate –¥–ª—è Vicuna-33B
- 1.7x —Å–Ω–∏–∂–µ–Ω–∏–µ TTFT –≤ —Å—Ä–µ–¥–Ω–µ–º

---

## 4. Speculative Decoding: –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–∞–º, –≥–¥–µ –µ–≥–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å

### –ü—Ä–æ–±–ª–µ–º–∞

Decode —Ñ–∞–∑–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞ –ø–æ –ø—Ä–∏—Ä–æ–¥–µ: token N –∑–∞–≤–∏—Å–∏—Ç –æ—Ç token N-1. –ö–∞–∑–∞–ª–æ—Å—å –±—ã, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω. –ù–æ –µ—Å—Ç—å —Ö–∏—Ç—Ä–æ—Å—Ç—å!

### –ö–ª—é—á–µ–≤–æ–π –∏–Ω—Å–∞–π—Ç

–ß—Ç–æ –µ—Å–ª–∏ –º–∞–ª–µ–Ω—å–∫–∞—è –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å (draft) –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤, –∞ –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å (verifier) –ø—Ä–æ–≤–µ—Ä–∏—Ç –∏—Ö –≤—Å–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ?

```
–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Decoding (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π):

Large Model: [T1] ‚Üí [T2] ‚Üí [T3] ‚Üí [T4] ‚Üí [T5]
              100ms  100ms  100ms  100ms  100ms = 500ms total

Speculative Decoding:

Draft Model:  [T1 T2 T3 T4 T5]    ‚Üê –ë—ã—Å—Ç—Ä–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 5 —Ç–æ–∫–µ–Ω–æ–≤
                   20ms

                    ‚Üì

Large Model:  [Verify ALL]        ‚Üê –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                   150ms

                    ‚Üì

Result: Accept T1,T2,T3 ‚úì  Reject T4,T5 ‚úó
        ‚Üí –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º T4 –∑–∞–Ω–æ–≤–æ

Total: 170ms –¥–ª—è 3-4 —Ç–æ–∫–µ–Ω–æ–≤ vs 300-400ms —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ = ~2x speedup
```

### –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

1. **–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ—à–µ–≤–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏** - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å N —Ç–æ–∫–µ–Ω–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–æ—á—Ç–∏ —Ç–∞–∫ –∂–µ –±—ã—Å—Ç—Ä–æ, –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å 1
2. **Draft –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ —É–≥–∞–¥—ã–≤–∞–µ—Ç** - –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è "–ø—Ä–æ—Å—Ç—ã—Ö" –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π
3. **–ì–∞—Ä–∞–Ω—Ç–∏—è –∫–∞—á–µ—Å—Ç–≤–∞** - output distribution –∏–¥–µ–Ω—Ç–∏—á–Ω–∞ –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ (mathematical guarantee)

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ vLLM

```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",  # Target (–±–æ–ª—å—à–∞—è)
    speculative_model="meta-llama/Llama-3.1-8B-Instruct",  # Draft (–º–∞–ª–µ–Ω—å–∫–∞—è)
    num_speculative_tokens=5,  # –°–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å
    speculative_draft_tensor_parallel_size=1,  # Draft –Ω–∞ 1 GPU
    tensor_parallel_size=4  # Target –Ω–∞ 4 GPU
)

# Greedy sampling –¥–ª—è –ª—É—á—à–µ–≥–æ acceptance rate
sampling_params = SamplingParams(
    temperature=0.0,
    max_tokens=512
)

outputs = llm.generate(["Explain machine learning:"], sampling_params)
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (NVIDIA H200, Llama 3.3 70B)

| Draft Model | Acceptance Rate | Speedup |
|-------------|-----------------|---------|
| Llama 3.3 8B | ~75% | 2.8x |
| Llama 3.3 3B | ~65% | 3.2x |
| Llama 3.3 1B | ~55% | 3.55x |

**Trade-off:** –ú–µ–Ω—å—à–µ draft –º–æ–¥–µ–ª—å = –±–æ–ª—å—à–µ speedup, –Ω–æ –Ω–∏–∂–µ acceptance rate.

### Eagle3 - State of the Art (2025)

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–∏–ø–∞ Eagle3 –∏—Å–ø–æ–ª—å–∑—É—é—Ç hidden states –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ—ë–≤ verifier –º–æ–¥–µ–ª–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ draft:

> "Eagle3 draft models take the hidden states from three layers of the verifier model as input, capturing the verifier's latent features."

Google –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç speculative decoding –≤ AI Overviews –¥–ª—è Search, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç production-ready —Å—Ç–∞—Ç—É—Å —Ç–µ—Ö–Ω–∏–∫–∏.

---

## 5. Quantization: –ú–µ–Ω—å—à–µ –±–∏—Ç = –ë—ã—Å—Ç—Ä–µ–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

### –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤–º–µ—Å—Ç–æ –≥—Ä—É–∑–æ–≤–∏–∫–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–æ—Ç–æ—Ü–∏–∫–ª—ã. –î–∞, –∫–∞–∂–¥—ã–π –≤–µ–∑—ë—Ç –º–µ–Ω—å—à–µ, –Ω–æ –æ–Ω–∏ –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ –∏ –∏—Ö –º–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∑–∞ —Ç–æ –∂–µ –≤—Ä–µ–º—è.

Quantization —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏:
- FP32 (32 –±–∏—Ç–∞) ‚Üí FP16 (16 –±–∏—Ç) ‚Üí INT8 (8 –±–∏—Ç) ‚Üí INT4 (4 –±–∏—Ç–∞)
- –ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö = –±—ã—Å—Ç—Ä–µ–µ –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –ø–∞–º—è—Ç–∏ = –±—ã—Å—Ç—Ä–µ–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

```
–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ 70B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

FP32:  280 GB  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
FP16:  140 GB  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
INT8:   70 GB  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
INT4:   35 GB  ‚ñà‚ñà‚ñà‚ñà

Speedup: ~2x –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ (–ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ (2025)

| –ú–µ—Ç–æ–¥ | –ë–∏—Ç—ã | Accuracy | Speed | –õ—É—á—à–µ –≤—Å–µ–≥–æ –¥–ª—è |
|-------|------|----------|-------|-----------------|
| **FP8** | 8 | ~99.5% | 2x | H100/H200/B200 (native support) |
| **AWQ** | 4 | ~98% | 3-4x | –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π GPU, –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ 4-bit |
| **GPTQ** | 4 | ~97% | 3-4x | Coding tasks (–ø–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º) |
| **GGUF** | 2-8 | varies | 2-4x | CPU/hybrid, llama.cpp |

### AWQ vs GPTQ: –ß—Ç–æ –≤—ã–±—Ä–∞—Ç—å?

**AWQ (Activation-aware Weight Quantization):**
- –ó–∞—â–∏—â–∞–µ—Ç "–≤–∞–∂–Ω—ã–µ" –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç backpropagation (–±—ã—Å—Ç—Ä–µ–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å)
- **–°—Ç–∞–±–∏–ª—å–Ω–æ –ª—É—á—à–µ GPTQ –Ω–∞ 70B+ –º–æ–¥–µ–ª—è—Ö**

**GPTQ:**
- –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É —á–µ—Ä–µ–∑ Hessian-based –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
- –õ—É—á—à–µ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö coding benchmarks
- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏

> "At the 70B scale, comparing Llama-2, Llama-3.1, and Llama-3.3, AWQ consistently surpassed GPTQ, delivering stable accuracy even at 4-bit quantization."

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ AWQ

```python
# –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model = AutoAWQForCausalLM.from_pretrained("meta-llama/Llama-3.1-70B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-70B-Instruct")

quant_config = {
    "zero_point": True,
    "q_group_size": 128,
    "w_bit": 4,
    "version": "GEMM"
}

model.quantize(tokenizer, quant_config=quant_config, calib_data="pileval")
model.save_quantized("./llama-70b-awq")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ vLLM
from vllm import LLM
llm = LLM(model="./llama-70b-awq", quantization="awq")
```

### FP8 –¥–ª—è H100/H200/B200

–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –Ω–æ–≤–µ–π—à–∏–º NVIDIA GPU, FP8 - –ª—É—á—à–∏–π –≤—ã–±–æ—Ä:
- –ù–∞—Ç–∏–≤–Ω–∞—è –∞–ø–ø–∞—Ä–∞—Ç–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è –∫–∞—á–µ—Å—Ç–≤–∞ (~99.5%)
- 2x speedup –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏

```bash
# TensorRT-LLM —Å FP8
trtllm-build --checkpoint_dir ./llama-70b-ckpt \
             --output_dir ./llama-70b-fp8 \
             --use_fp8_context_fmha \
             --max_batch_size 64
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–æ–¥–µ–ª–∏

| –ú–æ–¥–µ–ª—å | GPU Memory | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|------------|--------------|
| 7-8B | 8-16GB | FP16 –∏–ª–∏ INT8 |
| 13B | 16-24GB | INT8 –∏–ª–∏ AWQ 4-bit |
| 34B | 24-48GB | AWQ 4-bit |
| 70B | 80GB (H100) | FP8 –∏–ª–∏ AWQ 4-bit |
| 70B | 40GB | AWQ 4-bit + tensor parallel |
| 405B | 8√ó80GB | FP8 + tensor parallel |

---

## 6. FlashAttention: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –∂–µ–ª–µ–∑–∞

### –ü—Ä–æ–±–ª–µ–º–∞

–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention –∏–º–µ–µ—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ –ø–∞–º—è—Ç–∏ O(N¬≤) - –¥–ª—è sequence length 4K –Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É 4K√ó4K = 16M —ç–ª–µ–º–µ–Ω—Ç–æ–≤.

### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç FlashAttention

**–ê–Ω–∞–ª–æ–≥–∏—è:** –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –æ–≥—Ä–æ–º–Ω—ã–π —Å–∫–ª–∞–¥ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, FlashAttention —Ä–∞–±–æ—Ç–∞–µ—Ç "–ø–æ—Ç–æ–∫–æ–≤–æ" - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏, –¥–µ—Ä–∂–∞ —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –≤ –±—ã—Å—Ç—Ä–æ–π –ø–∞–º—è—Ç–∏ GPU.

–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:
1. **Tiling** - —Ä–∞–∑–±–∏–≤–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–µ—â–∞—é—Ç—Å—è –≤ SRAM
2. **Kernel fusion** - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏, —É–º–µ–Ω—å—à–∞—è memory transfers
3. **Recomputation** - –∏–Ω–æ–≥–¥–∞ –≤—ã–≥–æ–¥–Ω–µ–µ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å, —á–µ–º —Ö—Ä–∞–Ω–∏—Ç—å

### FlashAttention-3 (–¥–ª—è H100)

FlashAttention-3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **–¥–æ 75% –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ H100** (–ø—Ä–æ—Ç–∏–≤ 35% —Ä–∞–Ω–µ–µ):
- 1.5-2x –±—ã—Å—Ç—Ä–µ–µ FlashAttention-2
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ FP8 –¥–ª—è –µ—â—ë –±–æ–ª—å—à–µ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ warp-specialization

### –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç

FlashAttention –ø–æ–∑–≤–æ–ª–∏–ª —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã LLM —Å 2-4K (GPT-3) –¥–æ 128K (GPT-4) –∏ –¥–∞–∂–µ 1M (Llama 3):
- 10x —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ sequence length 2K
- 20x —ç–∫–æ–Ω–æ–º–∏—è –ø—Ä–∏ sequence length 4K
- O(N) –≤–º–µ—Å—Ç–æ O(N¬≤) –ø–æ –ø–∞–º—è—Ç–∏

---

## 7. Chunked Prefill: –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É TTFT –∏ TPOT

### –ü—Ä–æ–±–ª–µ–º–∞

Prefill –∏ decode –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- **Prefill**: compute-bound, –≤—ã—Å–æ–∫–∞—è GPU utilization
- **Decode**: memory-bound, –Ω–∏–∑–∫–∞—è GPU utilization

–ö–æ–≥–¥–∞ –¥–ª–∏–Ω–Ω—ã–π prefill –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, decode –∑–∞–ø—Ä–æ—Å—ã "–≥–æ–ª–æ–¥–∞—é—Ç", –∏ –∏—Ö latency –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞.

### –†–µ—à–µ–Ω–∏–µ: Chunked Prefill

–†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π prefill –Ω–∞ chunks, —á–µ—Ä–µ–¥—É—è —Å decode:

```
–ë–µ–∑ chunked prefill:
[Prefill R1‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] [Decode R2, R3, R4]
                                 ‚Üë
                            Decode –≥–æ–ª–æ–¥–∞–µ—Ç!

–° chunked prefill:
[Prefill chunk 1][Decode][Prefill chunk 2][Decode][Prefill chunk 3][Decode]
     –ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ - decode –Ω–µ –≥–æ–ª–æ–¥–∞–µ—Ç
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ü–æ –¥–∞–Ω–Ω—ã–º TNG Technology Consulting, chunked prefill —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç throughput –Ω–∞ **+50%** –¥–ª—è evenly sized requests.

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤ vLLM

```python
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    enable_chunked_prefill=True,
    max_num_batched_tokens=2048  # Chunk size
)
```

### Trade-offs

- **–ë–æ–ª—å—à–µ chunk size** ‚Üí –ª—É—á—à–µ TTFT, —Ö—É–∂–µ TPOT consistency
- **–ú–µ–Ω—å—à–µ chunk size** ‚Üí –ª—É—á—à–µ TPOT, –Ω–µ–º–Ω–æ–≥–æ —Ö—É–∂–µ TTFT
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: `max_num_batched_tokens > 8192` –¥–ª—è throughput

---

## 8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Inference Engines (2025)

### Benchmark —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

```
Throughput (Llama 3 70B, H100, output tokens/sec):

TensorRT-LLM  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ~3000 t/s
SGLang        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~2500 t/s
vLLM v1       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~2400 t/s
vLLM v0.6     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~2000 t/s
HF Transform  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~150 t/s

TTFT Latency (single request):

TensorRT-LLM  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~35-50ms
SGLang        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~50-80ms
vLLM          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ~80-100ms
```

### vLLM: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∏ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
- OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
- –®–∏—Ä–æ–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ HuggingFace
- Multi-cloud deployment

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- PagedAttention –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- 24x –±—ã—Å—Ç—Ä–µ–µ HuggingFace Transformers
- vLLM V1 engine: +24% throughput vs v0.7.3

```bash
# –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --tensor-parallel-size 4 \
    --max-model-len 8192 \
    --quantization awq \
    --enable-chunked-prefill \
    --enable-prefix-caching
```

### SGLang: –õ—É—á—à–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å latency

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- Multi-turn conversations
- Structured workloads (few-shot learning)
- –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞—è latency

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- RadixAttention - –¥–æ 6.4x throughput –Ω–∞ structured tasks
- TPOT —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ: 4-21ms vs vLLM 5-30ms
- ~10% boost –Ω–∞–¥ vLLM –≤ multi-turn scenarios

```python
import sglang as sgl

llm = sgl.Engine(
    model_path="meta-llama/Llama-3.1-70B-Instruct",
    tp_size=4,
    mem_fraction_static=0.85
)

@sgl.function
def chat(s, user_message):
    s += sgl.system("You are a helpful assistant.")
    s += sgl.user(user_message)
    s += sgl.assistant(sgl.gen("response", max_tokens=512))
```

### TensorRT-LLM: –ú–∞–∫—Å–∏–º—É–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- Enterprise NVIDIA deployment (H100/H200/B200)
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π throughput –∫—Ä–∏—Ç–∏—á–µ–Ω
- –ì–æ—Ç–æ–≤—ã –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫—É

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ~20% –≤—ã—à–µ throughput —á–µ–º vLLM
- –ù–∞—Ç–∏–≤–Ω–∞—è FP8 –ø–æ–¥–¥–µ—Ä–∂–∫–∞
- –õ—É—á—à–∏–π TTFT: 35-50ms vs 50-80ms

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- –¢—Ä–µ–±—É–µ—Ç –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –º–æ–¥–µ–ª–∏ (—á–∞—Å—ã-–¥–Ω–∏)
- –°–ª–æ–∂–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
- –¢–æ–ª—å–∫–æ NVIDIA GPU

```bash
# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ (offline, –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ä–µ–º—è)
trtllm-build --checkpoint_dir ./llama-70b-ckpt \
             --output_dir ./llama-70b-engine \
             --gemm_plugin float16 \
             --max_batch_size 64 \
             --max_input_len 2048 \
             --max_output_len 512
```

### vLLM vs Ollama

–î–ª—è production: vLLM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **793 TPS vs Ollama 41 TPS** –∏ **80ms p99 latency vs 673ms** –ø—Ä–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ. vLLM –¥–æ **3.23x –±—ã—Å—Ç—Ä–µ–µ** Ollama –ø—Ä–∏ 128 concurrent requests.

Ollama - –æ—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –Ω–æ –Ω–µ –¥–ª—è production.

### –¢–∞–±–ª–∏—Ü–∞ –≤—ã–±–æ—Ä–∞

| –°—Ü–µ–Ω–∞—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è | –ü—Ä–∏—á–∏–Ω–∞ |
|----------|--------------|---------|
| –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç | vLLM | –ü—Ä–æ—Å—Ç–æ—Ç–∞, OpenAI API |
| Multi-turn chat | SGLang | RadixAttention, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è latency |
| Max throughput NVIDIA | TensorRT-LLM | –ì–ª—É–±–æ–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è |
| –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ | Ollama | –ü—Ä–æ—Å—Ç–µ–π—à–∏–π setup |
| Enterprise NVIDIA | NIM | –ì–æ—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ |
| AMD GPU | vLLM + ROCm | –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ |

---

## 9. GPU Hardware: H100 vs H200 vs B200

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫

| GPU | Memory | Bandwidth | FP8 TFLOPS | –õ—É—á—à–µ –≤—Å–µ–≥–æ –¥–ª—è |
|-----|--------|-----------|------------|-----------------|
| **H100** | 80GB HBM3 | 3.35 TB/s | 1979 | Production workhorse |
| **H200** | 141GB HBM3e | 4.8 TB/s | 1979 | Large KV cache workloads |
| **B200** | 192GB HBM3e | 8 TB/s | 4500 | Max throughput, FP4 |

### –†–µ–∞–ª—å–Ω—ã–µ benchmark —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

**Llama-3.1-8B –Ω–∞ vLLM (multi-GPU):**
- H200 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **9-10% –≤—ã—à–µ throughput** —á–µ–º H100
- B200 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **—Å–∞–º—É—é –Ω–∏–∑–∫—É—é latency**: 2.40ms –Ω–∞ 8 GPU

**DeepSeek 671B:**
- B200 –∏ H100 –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö

**TensorRT-LLM, 100 concurrent requests:**
- B200: TTFT 0.234s, throughput 7,236 t/s
- –û–¥–∏–Ω B200 ‚âà 3-4 H100 –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ö–æ–≥–¥–∞ –≤—ã–±–∏—Ä–∞—Ç—å –∫–∞–∫–æ–π GPU

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|----------|--------------|
| –ë—é–¥–∂–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å | H100 (—Ü–µ–Ω—ã –ø–∞–¥–∞—é—Ç) |
| KV cache bottleneck | H200 (76% –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏) |
| Tokens/sec per watt | B200 (–ª—É—á—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å) |
| FP4 inference | B200 (–Ω–∞—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞) |

---

## 10. Production Deployment

### –ß–µ–∫–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º

```python
PRODUCTION_CHECKLIST = {
    "performance": [
        "Quantization –≤—ã–±—Ä–∞–Ω –ø–æ–¥ GPU (FP8 –¥–ª—è H100+, AWQ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö)",
        "Continuous batching –≤–∫–ª—é—á—ë–Ω",
        "Prefix caching –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–º–ø—Ç–æ–≤",
        "Tensor parallelism –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –º–æ–¥–µ–ª–∏",
        "Max sequence length –æ–≥—Ä–∞–Ω–∏—á–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–º–∏ –Ω—É–∂–¥–∞–º–∏",
        "Chunked prefill –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π latency"
    ],

    "reliability": [
        "Health checks endpoint —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "Graceful shutdown handling",
        "Request timeout –Ω–∞—Å—Ç—Ä–æ–µ–Ω (60-120s typical)",
        "Memory OOM handling (swap space –Ω–∞—Å—Ç—Ä–æ–µ–Ω)",
        "Model warmup –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–ø–µ—Ä–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –º–µ–¥–ª–µ–Ω–Ω–µ–µ)"
    ],

    "observability": [
        "Prometheus metrics exposed",
        "Latency percentiles: p50, p95, p99",
        "GPU utilization monitoring",
        "KV cache utilization",
        "Request queue depth",
        "Error rate tracking"
    ],

    "scaling": [
        "Horizontal scaling –≥–æ—Ç–æ–≤ (load balancer)",
        "Auto-scaling –ø–æ queue depth / latency",
        "Sticky sessions –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç",
        "Blue-green deployment –¥–ª—è updates"
    ]
}
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama-70b
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm-llama
  template:
    metadata:
      labels:
        app: vllm-llama
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - "--model"
          - "meta-llama/Llama-3.1-70B-Instruct"
          - "--tensor-parallel-size"
          - "4"
          - "--max-model-len"
          - "8192"
          - "--quantization"
          - "awq"
          - "--enable-chunked-prefill"
          - "--enable-prefix-caching"
        resources:
          limits:
            nvidia.com/gpu: 4
            memory: 320Gi
          requests:
            nvidia.com/gpu: 4
            memory: 256Gi
        ports:
        - containerPort: 8000
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300  # Model loading takes time!
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 5
      nodeSelector:
        gpu-type: h100
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫

```python
# Prometheus metrics to expose
CRITICAL_METRICS = {
    # Latency
    "vllm_request_ttft_seconds": "Histogram of TTFT",
    "vllm_request_tpot_seconds": "Histogram of time per output token",
    "vllm_request_e2e_seconds": "End-to-end request latency",

    # Throughput
    "vllm_tokens_generated_total": "Counter of generated tokens",
    "vllm_requests_completed_total": "Counter of completed requests",

    # Resources
    "vllm_gpu_memory_used_bytes": "GPU memory usage",
    "vllm_kv_cache_usage_percent": "KV cache utilization",
    "vllm_running_requests": "Current running requests",
    "vllm_waiting_requests": "Requests in queue",
}
```

---

## –†–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã –∫–æ–º–ø–∞–Ω–∏–π (2025)

### Convirza: 10x —Å–Ω–∏–∂–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏

- **–ó–∞–¥–∞—á–∞:** –ê–Ω–∞–ª–∏–∑ –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∑–≤–æ–Ω–∫–æ–≤ call-—Ü–µ–Ω—Ç—Ä–∞ –µ–∂–µ–º–µ—Å—è—á–Ω–æ
- **–†–µ—à–µ–Ω–∏–µ:** Llama 3B —Å LoRA adapters —á–µ—Ä–µ–∑ Predibase
- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** Sub-0.1 second inference, 10x –¥–µ—à–µ–≤–ª–µ OpenAI

### ElevenLabs: 600:1 —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

- **–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:** GKE —Å NVIDIA H100
- **Stack:** NeMo + NIM –¥–ª—è inference optimization
- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 600 —Å–µ–∫—É–Ω–¥ –∞—É–¥–∏–æ –∑–∞ 1 —Å–µ–∫—É–Ω–¥—É real-time

### Klarna: –ú–∏–ª–ª–∏–æ–Ω—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤

- **–ó–∞–¥–∞—á–∞:** AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è customer service
- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ú–∏–ª–ª–∏–æ–Ω—ã conversations/month, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤

### Prosus (15,000 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤)

- **–°–∏—Å—Ç–µ–º–∞:** "Toan" - RAG-based Q&A –Ω–∞ Amazon Bedrock
- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** Hallucination rate < 2% —á–µ—Ä–µ–∑ iterative optimization
---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

> [!question]- –ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ bottleneck'–∏ LLM inference –∏ –∫–∞–∫ –∫–∞–∂–¥—ã–π –∏–∑ –Ω–∏—Ö —Ä–µ—à–∞–µ—Ç—Å—è?
> Memory bandwidth (–º–æ–¥–µ–ª—å –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ GPU) --- –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (INT8, INT4, GPTQ, AWQ). Compute (–º–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è) --- batching, speculative decoding, tensor parallelism. KV-cache (—Ä–∞—Å—Ç—ë—Ç —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞) --- PagedAttention, multi-query attention. I/O (—Å–µ—Ç–µ–≤—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏) --- prefix caching, streaming.

> [!question]- –ß–µ–º vLLM –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç TensorRT-LLM –∏ –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π?
> vLLM: Python-native, PagedAttention, –ø—Ä–æ—Å—Ç–æ–π –¥–µ–ø–ª–æ–π, –ª—É—á—à–∏–π throughput –¥–ª—è –æ–±—â–∏—Ö —Å–ª—É—á–∞–µ–≤. TensorRT-LLM: NVIDIA-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –∫–æ–º–ø–∏–ª—è—Ü–∏—è –≥—Ä–∞—Ñ–∞, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ NVIDIA GPU, –Ω–æ —Å–ª–æ–∂–Ω–µ–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ. vLLM –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤, TensorRT-LLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ performance –Ω–∞ NVIDIA hardware.

> [!question]- –ö–∞–∫ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –∏ throughput?
> INT8 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: –ø–æ—Ç–µ—Ä—è –∫–∞—á–µ—Å—Ç–≤–∞ <1%, throughput +50-80%, memory -50%. INT4 (GPTQ/AWQ): –ø–æ—Ç–µ—Ä—è –∫–∞—á–µ—Å—Ç–≤–∞ 1-3%, throughput +100-200%, memory -75%. –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (70B+) –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ–Ω—å—à–µ. AWQ –æ–±—ã—á–Ω–æ –¥–∞—ë—Ç –ª—É—á—à–µ–µ quality/speed —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —á–µ–º GPTQ.

> [!question]- –ß—Ç–æ —Ç–∞–∫–æ–µ speculative decoding –∏ –∫–∞–∫ –æ–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç inference?
> –ú–∞–ª–µ–Ω—å–∫–∞—è "draft" –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç N —Ç–æ–∫–µ–Ω–æ–≤, –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –∑–∞ –æ–¥–∏–Ω forward pass. –ï—Å–ª–∏ draft-—Ç–æ–∫–µ–Ω—ã –≤–µ—Ä–Ω—ã–µ --- –≤—ã–∏–≥—Ä—ã—à –≤ N —Ä–∞–∑. –¢–∏–ø–∏—á–Ω—ã–π speedup 2-3x –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ–¥–±–æ—Ä–µ draft –º–æ–¥–µ–ª–∏. –†–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

---

## –ö–ª—é—á–µ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

–ß—Ç–æ —Ç–∞–∫–æ–µ PagedAttention –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?
?
–¢–µ—Ö–Ω–∏–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è KV-cache, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é OS. –í–º–µ—Å—Ç–æ pre-allocation –≤—Å–µ–≥–æ context window, KV-cache –∞–ª–ª–æ—Ü–∏—Ä—É–µ—Ç—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –°–æ–∫—Ä–∞—â–∞–µ—Ç waste –ø–∞–º—è—Ç–∏ –Ω–∞ 60-80%, –ø–æ–∑–≤–æ–ª—è—è –æ–±—Å–ª—É–∂–∏–≤–∞—Ç—å –±–æ–ª—å—à–µ concurrent –∑–∞–ø—Ä–æ—Å–æ–≤.

–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ inference-—Å–µ—Ä–≤–µ—Ä—ã –¥–ª—è LLM —Å—É—â–µ—Å—Ç–≤—É—é—Ç?
?
vLLM (open-source, PagedAttention), TensorRT-LLM (NVIDIA-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π), SGLang (—Ä–∞–¥–∏–∫–∞–ª—å–Ω—ã–π scheduling), Text Generation Inference (HuggingFace), –∏ Triton Inference Server (multi-framework). –í—ã–±–æ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç hardware –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ latency.

–ß—Ç–æ —Ç–∞–∫–æ–µ continuous batching –∏ —á–µ–º –æ–Ω–æ –ª—É—á—à–µ static batching?
?
Static batching: –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤ batch –∂–¥—É—Ç —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π. Continuous batching: –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–∫–∏–¥–∞—é—Ç batch, –Ω–æ–≤—ã–µ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è. Throughput –≤ 10-20x –≤—ã—à–µ. –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ vLLM, TGI, SGLang.

–ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ LLM —Å—É—â–µ—Å—Ç–≤—É—é—Ç?
?
Post-training: GPTQ (GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π), AWQ (activation-aware, –ª—É—á—à–µ quality), GGUF/llama.cpp (CPU-friendly). Training-aware: QLoRA, QLORA. –§–æ—Ä–º–∞—Ç—ã: INT8, INT4, FP8. AWQ –∏ GPTQ --- –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–ª—è production GPU inference.

–ß—Ç–æ —Ç–∞–∫–æ–µ tensor parallelism –∏ model parallelism?
?
Tensor parallelism: –æ–¥–∏–Ω —Å–ª–æ–π –º–æ–¥–µ–ª–∏ —Ä–∞–∑–¥–µ–ª—è–µ—Ç—Å—è –º–µ–∂–¥—É GPU (–¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –Ω–µ –ø–æ–º–µ—â–∞—é—â–∏—Ö—Å—è –≤ 1 GPU). Pipeline parallelism: —Ä–∞–∑–Ω—ã–µ —Å–ª–æ–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö GPU (sequential). Tensor parallelism –¥–∞—ë—Ç –º–µ–Ω—å—à—É—é latency, pipeline --- –ª—É—á—à–∏–π throughput.

---

## –ö—É–¥–∞ –¥–∞–ª—å—à–µ

| –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ö—É–¥–∞ | –ó–∞—á–µ–º |
|-------------|------|-------|
| –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ | [[local-llms-self-hosting]] | –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π self-hosting —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π |
| –£–≥–ª—É–±–∏—Ç—å—Å—è | [[ai-fine-tuning-guide]] | Fine-tuning –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π |
| –°–º–µ–∂–Ω–∞—è —Ç–µ–º–∞ | [[jvm-gc-tuning]] | –ê–Ω–∞–ª–æ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ runtime –≤ JVM |
| –û–±–∑–æ—Ä | [[ai-engineering-moc]] | –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –∫–∞—Ä—Ç–µ AI Engineering |

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
