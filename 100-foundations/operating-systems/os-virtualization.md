---
title: "Виртуализация и контейнеры: изоляция окружений"
created: 2025-12-02
modified: 2025-12-02
type: deep-dive
status: published
area: operating-systems
confidence: high
tags:
  - topic/os
  - virtualization
  - containers
  - topic/docker
  - kvm
  - type/deep-dive
  - level/intermediate
related:
  - "[[os-overview]]"
  - "[[os-processes-threads]]"
  - "[[os-memory-management]]"
  - "[[os-scheduling]]"
  - "[[docker-for-developers]]"
  - "[[kubernetes-basics]]"
prerequisites:
  - "[[os-processes-threads]]"
  - "[[os-memory-management]]"
  - "[[os-scheduling]]"
reading_time: 45
difficulty: 6
study_status: not_started
mastery: 0
last_reviewed:
next_review:
---

# Виртуализация и Контейнеры

Виртуализация позволяет запускать несколько изолированных окружений на одном физическом сервере. Это фундамент современной облачной инфраструктуры: AWS, Google Cloud, Azure — все построены на виртуализации. Существует два принципиально разных подхода: полная виртуализация (Virtual Machines), где каждое окружение имеет собственное ядро операционной системы, и контейнеризация, где окружения разделяют ядро хоста. Выбор между ними — это компромисс между уровнем изоляции и накладными расходами.

---

## TL;DR

> **VM:** Полная изоляция с отдельным ядром. Hypervisor управляет гостевыми ОС. Безопасно, но тяжело (startup минуты, overhead 5-15%).
>
> **Контейнеры:** Изоляция через namespaces + cgroups. Общее ядро. Быстро (startup секунды, overhead <1%), но менее изолировано.
>
> **Namespaces:** Изоляция видимости (PID, NET, MNT). Контейнер "думает", что один на сервере.
>
> **Cgroups:** Лимиты ресурсов (CPU, RAM, I/O). Защита от "шумных соседей".
>
> **Type 1 hypervisor:** На bare metal (ESXi, KVM). Минимальный overhead.
> **Type 2 hypervisor:** Внутри ОС (VirtualBox). Проще, но медленнее.
>
> **Когда VM:** Multi-tenant, разные ОС, compliance. **Когда контейнеры:** Микросервисы, CI/CD, высокая плотность.

---

## Часть 1: Интуиция без кода

### Аналогия 1: Отель vs Коворкинг (VM vs Контейнер)

**Виртуальная машина — это отдельный номер в отеле:**
- У каждого гостя свой номер с дверью, ванной, кухней
- Полная изоляция: сосед не слышит вас, вы не слышите соседа
- Много места занимает — каждому номеру нужны все удобства
- Заселение долгое — ключи, уборка, проверка

**Контейнер — это место в коворкинге:**
- Общее здание, общие стены, общий туалет и кухня
- Ваш стол отгорожен перегородкой — не видите соседей
- Вы видите только свой "офис", но здание общее
- Занимает меньше места — инфраструктура общая
- Занять место быстро — сел и работаешь

```
┌─────────────────────────────────────────────────────────────┐
│                    ОТЕЛЬ (VM)                                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │ Номер 1  │  │ Номер 2  │  │ Номер 3  │                   │
│  │ [Кухня]  │  │ [Кухня]  │  │ [Кухня]  │  ← Своя кухня     │
│  │ [Ванная] │  │ [Ванная] │  │ [Ванная] │  ← Своя ванная    │
│  │ [Кровать]│  │ [Кровать]│  │ [Кровать]│  ← Своя кровать   │
│  └──────────┘  └──────────┘  └──────────┘                   │
│             ПОЛНАЯ ИЗОЛЯЦИЯ (отдельные стены)               │
├─────────────────────────────────────────────────────────────┤
│                   КОВОРКИНГ (Контейнеры)                     │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │ Стол 1   │  │ Стол 2   │  │ Стол 3   │  ← Свой стол      │
│  │ [Лампа]  │  │ [Лампа]  │  │ [Лампа]  │                   │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘                   │
│       │             │             │                          │
│  ═════│═════════════│═════════════│═════ (перегородки)      │
│       │             │             │                          │
│  ┌────▼─────────────▼─────────────▼────┐                    │
│  │   ОБЩАЯ КУХНЯ, ТУАЛЕТ, ИНТЕРНЕТ     │  ← Shared kernel   │
│  └─────────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
```

### Аналогия 2: Namespaces = Зеркальные комнаты

Представьте, что вы в комнате с зеркальными стенами. Вы видите только своё отражение, не знаете, что за стеной — другие люди в таких же комнатах.

**PID namespace:** Каждый контейнер думает, что его процесс — PID 1 (init). На хосте это PID 54321, но контейнер не знает.

**NET namespace:** Контейнер видит свой eth0 с IP 172.17.0.2. Не знает, что на хосте это один из сотен виртуальных интерфейсов.

**MNT namespace:** Контейнер видит / и думает, что это весь диск. На самом деле это overlay поверх image слоёв.

```
Реальность хоста:                    Что видит контейнер:
┌─────────────────────┐              ┌─────────────────────┐
│ PID 1 (systemd)     │              │ PID 1 (nginx)       │
│ PID 100 (sshd)      │              │ PID 2 (nginx worker)│
│ PID 54321 (nginx)   │  ─────►      │ PID 3 (nginx worker)│
│ PID 54322 (worker)  │   namespace  │                     │
│ PID 54323 (worker)  │              │ "Я один на сервере!"|
└─────────────────────┘              └─────────────────────┘
```

### Аналогия 3: Cgroups = Счётчики электричества в квартирах

В многоквартирном доме каждая квартира имеет свой счётчик электричества:
- Максимум 5 кВт на квартиру (нельзя включить 10 обогревателей)
- Если превысил — автомат выбивает (OOM killer)
- Можно отслеживать, кто сколько потребляет

```
┌─────────────────────────────────────────────────────────────┐
│                    Электрощит (Cgroup Controller)            │
│                                                              │
│  ┌─────────────────┐  ┌─────────────────┐  ┌───────────────┐│
│  │ Квартира 1      │  │ Квартира 2      │  │ Квартира 3    ││
│  │ [⚡ max: 5kW]   │  │ [⚡ max: 3kW]   │  │ [⚡ max: 10kW]││
│  │ [💾 max: 2GB]   │  │ [💾 max: 512MB] │  │ [💾 max: 8GB] ││
│  │ [📊 max: 50%CPU]│  │ [📊 max: 25%CPU]│  │ [📊 max: 100%]││
│  └─────────────────┘  └─────────────────┘  └───────────────┘│
│                                                              │
│  Превышение лимита → автомат выбивает (OOM Killer)          │
└─────────────────────────────────────────────────────────────┘
```

Cgroups предотвращают "шумного соседа": один контейнер не может забрать все ресурсы сервера.

### Аналогия 4: OverlayFS = Прозрачные плёнки

Помните, как в школе показывали слайды на проекторе? Каждая плёнка — отдельный слой, накладываешь друг на друга — получаешь полную картинку.

```
Как строится образ контейнера:

     Плёнка 4: ваше приложение
     ┌───────────────────────┐
     │    app.js, config     │  ← COPY . /app
     └───────────────────────┘
            ↓ накладываем
     ┌───────────────────────┐
     │     node_modules      │  ← RUN npm install
     └───────────────────────┘
            ↓ накладываем
     ┌───────────────────────┐
     │   Node.js runtime     │  ← FROM node:18
     └───────────────────────┘
            ↓ накладываем
     ┌───────────────────────┐
     │   Debian base image   │  ← debian:bullseye
     └───────────────────────┘
            ↓
     ========================
     MERGED: полная картина
     (контейнер видит это)

Все слои read-only. Изменения пишутся на верхний слой (Copy-on-Write).
100 контейнеров с node:18 = 1 копия node:18 на диске!
```

### Аналогия 5: Type 1 vs Type 2 гипервизор = Администратор здания

**Type 1 (Bare-metal) — Администратор живёт в здании:**
- Администратор — единственный житель первого этажа
- Прямой доступ ко всем инженерным системам
- Минимум посредников — максимальная эффективность

**Type 2 (Hosted) — Администратор снимает офис в соседнем здании:**
- Сначала звонишь в управляющую компанию (Host OS)
- Они передают администратору (Hypervisor)
- Он решает вопрос
- Дополнительный слой = дополнительная задержка

```
Type 1 (ESXi, KVM):             Type 2 (VirtualBox):
┌───────────────────┐           ┌───────────────────┐
│    Guest VMs      │           │    Guest VMs      │
├───────────────────┤           ├───────────────────┤
│   HYPERVISOR      │           │    Hypervisor     │
│ (прямо на железе) │           │  (как программа)  │
├───────────────────┤           ├───────────────────┤
│    Hardware       │           │     Host OS       │
└───────────────────┘           │ (Windows/macOS)   │
                                ├───────────────────┤
    1 слой посредник            │    Hardware       │
                                └───────────────────┘
                                    2 слоя посредников
```

---

## Часть 2: Почему это сложно

### Ошибка 1: "Контейнеры = легковесные VM"

**СИМПТОМ:** "Docker — это просто маленькая виртуальная машина, да?"

**Почему это неправильно:**
- VM виртуализирует hardware — у каждой VM своё ядро ОС
- Контейнер виртуализирует OS-level — все контейнеры разделяют ядро хоста
- VM изолирует на уровне hypervisor, контейнер — на уровне процесса

```
VM:                                 Контейнер:
┌─────────┐ ┌─────────┐            ┌─────────┐ ┌─────────┐
│  App A  │ │  App B  │            │  App A  │ │  App B  │
├─────────┤ ├─────────┤            ├─────────┤ ├─────────┤
│Kernel A │ │Kernel B │            │  Libs   │ │  Libs   │
└────┬────┘ └────┬────┘            └────┬────┘ └────┬────┘
     │           │                      │           │
┌────▼───────────▼────┐            ┌────▼───────────▼────┐
│     Hypervisor      │            │   Shared Kernel     │
└─────────────────────┘            └─────────────────────┘
  Отдельные ядра!                    Общее ядро!
```

**РЕШЕНИЕ:** Думайте о контейнере как о "процессе с изоляцией", а не как о "VM без kernel". Контейнер — это namespaces + cgroups + overlayfs, не более.

### Ошибка 2: "Контейнер так же безопасен как VM"

**СИМПТОМ:** Запуск недоверенного кода в контейнере на production сервере.

**Почему это опасно:**
- Контейнер и хост разделяют ядро
- Уязвимость в ядре = container escape (побег из контейнера)
- Примеры: Dirty COW (CVE-2016-5195), runc escape (CVE-2019-5736)

```
Изоляция:

Процессы    Контейнеры    VMs           Physical
    │            │          │               │
    └────────────┴──────────┴───────────────┘
    ──────────────────────────────────────────►
    Слабая                              Сильная

    Контейнер ≈ "процесс в песочнице", не "отдельная машина"
```

**РЕШЕНИЕ:**
- Для multi-tenant (разные клиенты) — используйте VM
- Для изоляции своих приложений — контейнеры достаточны
- Рассмотрите Kata Containers / Firecracker для компромисса

### Ошибка 3: Запуск всего под root внутри контейнера

**СИМПТОМ:** `USER root` в Dockerfile или отсутствие директивы USER.

**Почему это опасно:**
- root в контейнере = root на хосте (если нет user namespaces)
- Exploited процесс получает root-доступ к shared kernel
- Можно примонтировать хостовую файловую систему

```dockerfile
# ПЛОХО:
FROM node:18
COPY . /app
CMD ["node", "app.js"]  # Запускается как root!

# ХОРОШО:
FROM node:18
RUN useradd -m appuser
WORKDIR /app
COPY --chown=appuser:appuser . .
USER appuser  # Явно указываем непривилегированного пользователя
CMD ["node", "app.js"]
```

**РЕШЕНИЕ:** Всегда используйте `USER` в Dockerfile. Запускайте как непривилегированный пользователь.

### Ошибка 4: Хранение данных внутри контейнера

**СИМПТОМ:** "Как бэкапить контейнер?" / "Данные пропали после перезапуска!"

**Почему это неправильно:**
- Контейнер ephemeral (временный) — удалил и создал заново
- upperdir (read-write слой) удаляется вместе с контейнером
- Нельзя "бэкапить контейнер" — бэкапят volumes

```
НЕПРАВИЛЬНО:                        ПРАВИЛЬНО:
┌─────────────────┐                 ┌─────────────────┐
│   Container     │                 │   Container     │
│ ┌─────────────┐ │                 │                 │
│ │   DB Data   │ │  ← Потеряется   │      App        │
│ └─────────────┘ │    при удалении │        │        │
└─────────────────┘                 └────────│────────┘
                                             │
                                    ┌────────▼────────┐
                                    │  Named Volume   │
                                    │ /var/lib/postgres│
                                    │ (persistent!)   │
                                    └─────────────────┘
```

**РЕШЕНИЕ:** `docker run -v postgres_data:/var/lib/postgresql/data postgres`

### Ошибка 5: "Docker на Mac/Windows работает так же как на Linux"

**СИМПТОМ:** "На моём Mac работает, на сервере — нет" или медленный I/O.

**Почему это проблема:**
- Docker использует Linux kernel features (namespaces, cgroups)
- На Mac/Windows нет Linux kernel
- Docker Desktop запускает скрытую Linux VM (HyperKit/WSL2)
- Файловый I/O через VM медленнее (особенно на Mac)

```
Linux:                    Mac/Windows:
┌─────────────────┐       ┌─────────────────┐
│   Container     │       │   Container     │
├─────────────────┤       ├─────────────────┤
│  Linux Kernel   │       │  Linux Kernel   │  ← в VM!
└─────────────────┘       ├─────────────────┤
        ↑                 │  Linux VM       │
   Напрямую!              │  (скрытая)      │
                          ├─────────────────┤
                          │  macOS/Windows  │
                          └─────────────────┘
                                  ↑
                          Дополнительный слой
                          + медленный file sync
```

**РЕШЕНИЕ:**
- Тестируйте на Linux (CI/CD) перед production
- На Mac: используйте mutagen или другие инструменты синхронизации
- Не удивляйтесь разнице в производительности

### Ошибка 6: Kubernetes для трёх сервисов

**СИМПТОМ:** Месяц настройки K8s для простого приложения из 3 микросервисов.

**Когда это overkill:**
- < 10 сервисов → Docker Compose достаточно
- Один сервер → Docker Compose
- Нет команды для поддержки K8s → не используйте K8s

```
Сложность:

             ┌─────────────────────────────────────────┐
             │ Kubernetes (50+ компонентов)            │
        100% ┼───────────────────────────────────X─────┤
             │                                   │     │
             │                         X─────────┘     │
             │                    K8s overhead         │
             │               X────────────────────────┤
             │          Docker Swarm                   │
        50%  ┼─────X───────────────────────────────────┤
             │ Docker Compose                          │
             │                                         │
          0% ┼─────────────────────────────────────────┤
             └─────┴─────┴─────┴─────┴─────┴─────┴─────┘
               1     5    10    50   100  500  1000
                        Количество сервисов
```

**РЕШЕНИЕ:**
- 1-10 сервисов, 1 сервер → Docker Compose
- 10-50 сервисов, несколько серверов → Docker Swarm или Nomad
- 50+ сервисов, требуется autoscaling → Kubernetes

---

## Часть 3: Ментальные модели

### Модель 1: Спектр изоляции

```
Изоляция:     Слабая ◄────────────────────────────► Сильная
Overhead:     Низкий ◄────────────────────────────► Высокий
Startup:      Быстрый◄────────────────────────────► Медленный

┌─────────────┬───────────────┬─────────────────┬──────────────┐
│  Процессы   │  Контейнеры   │       VMs       │  Physical    │
├─────────────┼───────────────┼─────────────────┼──────────────┤
│ Общее всё   │ Общее ядро    │ Раздельные ядра │ Раздельное   │
│             │ + namespaces  │ + hypervisor    │ железо       │
│             │ + cgroups     │                 │              │
├─────────────┼───────────────┼─────────────────┼──────────────┤
│ 0 overhead  │ <1% overhead  │ 5-15% overhead  │ 0 overhead   │
├─────────────┼───────────────┼─────────────────┼──────────────┤
│ Мгновенно   │ ~100ms        │ 10-60 sec       │ Минуты-часы  │
└─────────────┴───────────────┴─────────────────┴──────────────┘

Правило выбора:
- Свои приложения → Контейнеры
- Чужие/недоверенные → VM
- Максимальная безопасность → Physical
```

### Модель 2: Контейнер = Процесс + Изоляция

```
Контейнер — это НЕ VM. Контейнер — это:

    Обычный процесс Linux
           +
    ┌──────────────────────────────────────┐
    │ PID Namespace (своя нумерация PID)   │
    │ NET Namespace (своя сеть)            │
    │ MNT Namespace (своя файловая система)│
    │ USER Namespace (свои пользователи)   │
    │ UTS Namespace (свой hostname)        │
    │ IPC Namespace (своя shared memory)   │
    └──────────────────────────────────────┘
           +
    ┌──────────────────────────────────────┐
    │ Cgroups (лимиты CPU, RAM, I/O)       │
    └──────────────────────────────────────┘
           +
    ┌──────────────────────────────────────┐
    │ OverlayFS (layered filesystem)       │
    └──────────────────────────────────────┘
           =
    КОНТЕЙНЕР (просто хорошо изолированный процесс)
```

### Модель 3: Дерево решений VM vs Контейнеры

```
                    ┌───────────────────────────────┐
                    │ Нужна ли другая ОС (Windows)? │
                    └───────────────┬───────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                   Да                              Нет
                    │                               │
                    ▼                               ▼
                ┌───────┐              ┌───────────────────────┐
                │  VM   │              │Недоверенный код/tenant?│
                └───────┘              └───────────┬───────────┘
                                                   │
                                   ┌───────────────┴───────────────┐
                                  Да                              Нет
                                   │                               │
                                   ▼                               ▼
                               ┌───────┐              ┌───────────────────────┐
                               │  VM   │              │Compliance требует VM? │
                               │(или   │              └───────────┬───────────┘
                               │Kata)  │                          │
                               └───────┘              ┌───────────┴───────────┐
                                                     Да                      Нет
                                                      │                       │
                                                      ▼                       ▼
                                                  ┌───────┐           ┌────────────┐
                                                  │  VM   │           │ Контейнеры │
                                                  └───────┘           └────────────┘
```

### Модель 4: Эволюция изоляции (исторический контекст)

```
1979        1999         2006         2013         2014        2019
chroot  →   FreeBSD  →   cgroups  →   Docker   →   K8s     →   Firecracker
  │         Jails          │           │           │            │
  ▼           ▼            ▼           ▼           ▼            ▼
Изоляция  Изоляция     Лимиты     Простой UX   Оркестра-   VM speed +
FS        FS+NET+PID   ресурсов   для          ция         container
                       (Google)   контейнеров  контейне-   isolation
                                               ров

Каждый шаг добавлял либо изоляцию, либо удобство, либо эффективность.
```

### Модель 5: Overhead пирамида

```
                        Overhead и ресурсы

                    ┌─────────────────────┐
                    │ VM с эмуляцией      │ 30-50%
                    │ (без VT-x)          │
                ┌───┴─────────────────────┴───┐
                │ VM с аппаратной             │ 5-15%
                │ виртуализацией (KVM+virtio) │
            ┌───┴─────────────────────────────┴───┐
            │ Kata Containers / Firecracker       │ 2-5%
            │ (micro-VM)                          │
        ┌───┴─────────────────────────────────────┴───┐
        │ Контейнеры                                   │ <1%
        │ (namespaces + cgroups)                      │
    ┌───┴─────────────────────────────────────────────┴───┐
    │ Bare metal                                          │ 0%
    └─────────────────────────────────────────────────────┘

Чем выше — тем больше изоляции, но и больше overhead.
Выбирайте минимально необходимый уровень.
```

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| [[os-overview]] | Базовые концепции ОС, kernel/user mode | Предыдущий материал раздела |
| [[os-processes-threads]] | Процессы и изоляция — контейнеры это "процессы с ограничениями" | Предыдущий материал раздела |
| [[os-memory-management]] | Виртуальная память, page tables — VM используют nested paging | Предыдущий материал раздела |
| Docker базовые команды | Практический опыт с контейнерами | [Docker Getting Started](https://docs.docker.com/get-started/) |

**Время на подготовку:** ~1 неделя, включая практику с Docker

---

## Терминология для новичков

| Термин | Что это | Аналогия из жизни |
|--------|---------|-------------------|
| **Hypervisor** | Программа, управляющая виртуальными машинами | Администратор отеля: распределяет номера, следит за гостями |
| **Type 1 (Bare-metal)** | Гипервизор прямо на железе (ESXi, KVM) | Отель с одним администратором, без посредников |
| **Type 2 (Hosted)** | Гипервизор внутри ОС (VirtualBox) | Отель внутри торгового центра — дополнительный слой |
| **KVM** | Модуль ядра Linux, превращающий его в гипервизор | Linux "надевает шляпу" администратора отеля |
| **VT-x / AMD-V** | Аппаратная поддержка виртуализации в CPU | Специальная комната в отеле для VIP (гостей-ОС) |
| **VM exit** | Гость передаёт управление гипервизору | Гость звонит на ресепшн: "нужна помощь" |
| **virtio** | Драйверы, знающие что они в VM (быстрее) | Гость знает, что в отеле — просит напрямую, без притворства |
| **Namespace** | Изоляция видимости (PID, NET, MNT) | Каждый арендатор видит только свой офис в БЦ |
| **Cgroup** | Лимиты ресурсов (CPU, RAM, I/O) | Лимит на электричество для каждого офиса |
| **OverlayFS** | Слоистая ФС для контейнеров | Прозрачные плёнки с рисунками: накладываешь — получаешь картину |
| **Container** | Процесс с namespaces + cgroups | Офис в БЦ: изолирован, но общие стены и коммуникации |
| **Firecracker** | Микро-VM с overhead как у контейнера | Капсульный отель: изоляция номера, но минимум места |

---

## Зачем нужна виртуализация

### Проблема без виртуализации

Представьте, что у вас один физический сервер и три приложения: web-сервер на Java, база данных PostgreSQL, и legacy-система на Python 2.7. Проблемы:

1. **Конфликты зависимостей.** Java-приложению нужна Java 17, legacy-системе — Python 2.7, который конфликтует с системным Python 3.

2. **Изоляция сбоев.** Если web-сервер съест всю память или упадёт с segfault — он может повлиять на базу данных.

3. **Безопасность.** Уязвимость в одном приложении может дать злоумышленнику доступ ко всем остальным.

4. **Масштабирование.** Нужно больше web-серверов? Придётся покупать новые физические машины.

### Решение — виртуализация

Виртуализация создаёт иллюзию, что каждое приложение работает на собственном компьютере. Эти "виртуальные компьютеры" изолированы друг от друга, но физически работают на одном железе. Можно выделить каждому столько ресурсов, сколько нужно, и масштабировать без покупки нового hardware.

---

## Типы виртуализации: гипервизоры

Гипервизор (hypervisor) — программа, управляющая виртуальными машинами. Она создаёт и запускает VM, распределяет между ними физические ресурсы (CPU, память, диск).

### Type 1: Bare-metal гипервизоры

Type 1 гипервизор работает напрямую на железе, без промежуточной операционной системы. Он сам является минимальной ОС, единственная задача которой — управление виртуальными машинами.

```
┌─────────────────────────────────────────────────────────────┐
│                      Type 1 (Bare-metal)                     │
│                                                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │  Guest   │  │  Guest   │  │  Guest   │  ← Гостевые ОС    │
│  │   OS 1   │  │   OS 2   │  │   OS 3   │    (Windows,     │
│  │ (Linux)  │  │(Windows) │  │ (Linux)  │    Linux)        │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘                   │
│       │             │             │                          │
│  ┌────▼─────────────▼─────────────▼────┐                    │
│  │           Hypervisor                 │  ← Работает       │
│  │     (VMware ESXi, Xen, Hyper-V)      │    на bare metal  │
│  └─────────────────┬───────────────────┘                    │
│                    │                                         │
│  ┌─────────────────▼───────────────────┐                    │
│  │            Hardware                  │                    │
│  └─────────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
```

**Примеры:** VMware ESXi, Microsoft Hyper-V (в роли гипервизора), Citrix XenServer.

**Преимущества:** Минимальный overhead — между VM и железом нет промежуточного слоя ОС. Лучшая производительность и безопасность.

**Недостатки:** Сложнее в настройке, требует выделенного сервера. Нельзя использовать машину для других задач.

**Где используется:** Дата-центры, облачные провайдеры, enterprise-серверы.

### Type 2: Hosted гипервизоры

Type 2 гипервизор работает как обычное приложение внутри операционной системы. Host OS управляет железом, а гипервизор создаёт VM как процессы внутри этой ОС.

```
┌─────────────────────────────────────────────────────────────┐
│                      Type 2 (Hosted)                         │
│                                                              │
│  ┌──────────┐  ┌──────────┐                                 │
│  │  Guest   │  │  Guest   │  ← Гостевые ОС                  │
│  │   OS 1   │  │   OS 2   │                                 │
│  └────┬─────┘  └────┬─────┘                                 │
│       │             │                                        │
│  ┌────▼─────────────▼────┐    ┌──────────┐                  │
│  │      Hypervisor       │    │   Apps   │  ← Другие        │
│  │ (VirtualBox, VMware   │    │ (браузер,│    приложения    │
│  │  Workstation, QEMU)   │    │  IDE)    │                  │
│  └───────────┬───────────┘    └────┬─────┘                  │
│              │                     │                         │
│  ┌───────────▼─────────────────────▼─────┐                  │
│  │              Host OS                   │  ← Хост ОС      │
│  │         (Windows, Linux, macOS)        │    (ваша        │
│  └─────────────────┬─────────────────────┘    рабочая ОС)   │
│                    │                                         │
│  ┌─────────────────▼───────────────────┐                    │
│  │            Hardware                  │                    │
│  └─────────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
```

**Примеры:** VirtualBox, VMware Workstation/Fusion, Parallels Desktop.

**Преимущества:** Легко установить и использовать. Можно работать на той же машине, где запущены VM.

**Недостатки:** Дополнительный слой (host OS) добавляет overhead. Меньшая производительность.

**Где используется:** Разработка, тестирование, запуск приложений для другой ОС на десктопе.

### KVM: гибридный подход

KVM (Kernel-based Virtual Machine) — модуль ядра Linux, превращающий Linux в Type 1 гипервизор. Технически это Type 2 (работает внутри Linux), но благодаря глубокой интеграции с ядром и аппаратной виртуализации достигает производительности Type 1.

```
┌─────────────────────────────────────────────────────────────┐
│                        Guest VM                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │                    Guest OS                             │ │
│  │    Думает, что работает на реальном железе             │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐                │ │
│  │  │ Process │  │ Process │  │ Process │                │ │
│  │  └─────────┘  └─────────┘  └─────────┘                │ │
│  │                    │                                   │ │
│  │  ┌─────────────────▼───────────────────┐              │ │
│  │  │         Guest Kernel                │              │ │
│  │  │    (обычное Linux/Windows ядро)     │              │ │
│  │  └─────────────────────────────────────┘              │ │
│  └────────────────────────────────────────────────────────┘ │
│                         │                                    │
│                         │ VM exit (при привилегированных    │
│                         │ операциях управление передаётся   │
│                         │ гипервизору)                       │
└─────────────────────────│───────────────────────────────────┘
                          │
┌─────────────────────────▼───────────────────────────────────┐
│                      Host Linux                              │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                    QEMU Process                       │   │
│  │                                                       │   │
│  │   QEMU — программа в userspace, которая:             │   │
│  │   • Создаёт виртуальные устройства (диск, сеть)      │   │
│  │   • Эмулирует BIOS/UEFI                              │   │
│  │   • Обрабатывает I/O от гостевой ОС                  │   │
│  │                                                       │   │
│  └───────────────────────┬──────────────────────────────┘   │
│                          │ ioctl(/dev/kvm)                   │
│                          │ (интерфейс к KVM модулю)         │
│  ┌───────────────────────▼──────────────────────────────┐   │
│  │                    KVM Module                         │   │
│  │                                                       │   │
│  │   KVM — модуль ядра Linux, который:                  │   │
│  │   • Использует Intel VT-x / AMD-V для запуска VM     │   │
│  │   • Управляет памятью VM (EPT/NPT)                   │   │
│  │   • Обрабатывает VM exits                            │   │
│  │                                                       │   │
│  └───────────────────────┬──────────────────────────────┘   │
│                          │                                   │
│  ┌───────────────────────▼──────────────────────────────┐   │
│  │                 Linux Kernel                          │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

**Почему KVM так эффективен:**

1. KVM использует аппаратную виртуализацию (VT-x/AMD-V), поэтому гостевой код выполняется напрямую на CPU.
2. Ядро Linux выступает планировщиком — оно распределяет CPU между VM как между обычными процессами.
3. Для I/O используется паравиртуализация (virtio) — минимальный overhead.

---

## Аппаратная поддержка виртуализации

### Проблема программной виртуализации

До появления аппаратной поддержки виртуализация была сложной и медленной. Проблема в том, что гостевая ОС хочет работать в Ring 0 (привилегированный режим процессора), но если она действительно будет в Ring 0 — она получит контроль над всем железом, и изоляции не будет.

**Решение 1 — Binary Translation (VMware):** Гипервизор на лету анализирует код гостевой ОС и заменяет привилегированные инструкции на безопасные эквиваленты. Работает, но overhead значительный.

**Решение 2 — Паравиртуализация (Xen):** Модифицировать гостевую ОС, чтобы она не использовала привилегированные инструкции, а вызывала гипервизор через специальный API. Эффективно, но требует изменения гостевой ОС.

### Intel VT-x и AMD-V: аппаратное решение

В 2005-2006 годах Intel и AMD добавили в процессоры аппаратную поддержку виртуализации. Появился новый режим работы процессора — VMX root mode (неофициально называемый Ring -1):

```
┌─────────────────────────────────────────────┐
│              Без VT-x/AMD-V                  │
│                                              │
│  Ring 3: Приложения                          │
│  Ring 0: Гостевая ОС (ПОНИЖЕНА в Ring 1!)   │
│          Гипервизор ПЕРЕХВАТЫВАЕТ каждую    │
│          привилегированную инструкцию       │
│                                              │
│  Проблема: overhead на КАЖДУЮ               │
│  привилегированную инструкцию               │
│  (обработка page fault, context switch,     │
│   доступ к устройствам — всё медленно)      │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│               С VT-x/AMD-V                   │
│                                              │
│  Ring 3: Приложения                          │
│  Ring 0: Гостевая ОС (работает НАТИВНО!)    │
│  ─────────────────────────────────          │
│  VMX Root mode: Гипервизор                   │
│                                              │
│  Гостевая ОС работает в "виртуальном"       │
│  Ring 0. CPU сам отслеживает критические    │
│  операции и передаёт управление             │
│  гипервизору только когда нужно (VM exit)   │
└─────────────────────────────────────────────┘
```

**Как это работает:**

1. Гипервизор настраивает VMCS (Virtual Machine Control Structure) — структуру, описывающую, при каких условиях нужно передать управление гипервизору.

2. Гипервизор выполняет инструкцию VMLAUNCH/VMRESUME — CPU входит в режим гостя.

3. Гостевая ОС выполняется напрямую на CPU. Большинство инструкций — без overhead.

4. При критических операциях (доступ к hardware, обновление page table, выполнение определённых инструкций) происходит VM exit — CPU автоматически передаёт управление гипервизору.

5. Гипервизор обрабатывает событие и возвращает управление гостю через VMRESUME.

**Результат:** Гостевой код выполняется с минимальным overhead. VM exit происходит только для операций, которые действительно требуют вмешательства гипервизора.

### Паравиртуализация (virtio): оптимизация I/O

Даже с аппаратной виртуализацией остаётся проблема I/O. Если гостевая ОС думает, что работает с реальной сетевой картой Intel E1000, гипервизор должен эмулировать все регистры этой карты — каждое обращение к регистру вызывает VM exit.

**Virtio** — стандарт паравиртуализированных устройств. Гостевая ОС "знает", что работает в VM, и использует специальный драйвер virtio. Вместо эмуляции регистров используются ring buffers в shared memory:

```
┌──────────────────────────────────────────────────────────────┐
│                  Полная эмуляция (E1000)                     │
│                                                               │
│  Гостевая ОС                                                 │
│      │                                                        │
│      ▼                                                        │
│  E1000 драйвер ──▶ Записать в регистр TX                     │
│                         │                                     │
│                         ▼ VM EXIT!                            │
│                    QEMU эмулирует E1000                       │
│                         │                                     │
│                         ▼                                     │
│                    Реальная сеть                              │
│                                                               │
│  КАЖДЫЙ пакет = несколько VM exits (очень медленно)          │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│                  Паравиртуализация (virtio)                   │
│                                                               │
│  Гостевая ОС                                                 │
│      │                                                        │
│      ▼                                                        │
│  virtio-net драйвер ──▶ Положить пакет в shared ring buffer  │
│                              │                                │
│                              │ (no VM exit для данных!)       │
│                              ▼                                │
│                    QEMU/хост читает из ring buffer           │
│                              │                                │
│                              ▼                                │
│                         Реальная сеть                         │
│                                                               │
│  Батч пакетов = один сигнал, минимум VM exits                │
└──────────────────────────────────────────────────────────────┘
```

Virtio обеспечивает почти native производительность для disk и network I/O. Все современные облака и виртуализационные платформы используют virtio.

---

## Контейнеры: лёгкая виртуализация

### Идея контейнеров

Виртуальные машины изолируют на уровне hardware — каждая VM имеет собственное ядро ОС. Но часто это избыточно. Если все приложения работают на Linux, зачем каждому отдельное ядро?

Контейнеры изолируют на уровне процессов. Контейнер — это обычный процесс Linux с дополнительными ограничениями: он не видит другие процессы, имеет собственную файловую систему, сеть, и ограничен в потреблении ресурсов.

```
┌─────────────────────────────────────────────────────────────┐
│                         Host Linux                           │
│                                                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │
│  │ Container 1  │  │ Container 2  │  │ Container 3  │       │
│  │              │  │              │  │              │       │
│  │ ┌──────────┐ │  │ ┌──────────┐ │  │ ┌──────────┐ │       │
│  │ │   App    │ │  │ │   App    │ │  │ │   App    │ │       │
│  │ │  (nginx) │ │  │ │(postgres)│ │  │ │ (redis)  │ │       │
│  │ └──────────┘ │  │ └──────────┘ │  │ └──────────┘ │       │
│  │ ┌──────────┐ │  │ ┌──────────┐ │  │ ┌──────────┐ │       │
│  │ │   Libs   │ │  │ │   Libs   │ │  │ │   Libs   │ │       │
│  │ │ (Ubuntu) │ │  │ │ (Alpine) │ │  │ │ (Debian) │ │       │
│  │ └──────────┘ │  │ └──────────┘ │  │ └──────────┘ │       │
│  │              │  │              │  │              │       │
│  │  Namespaces  │  │  Namespaces  │  │  Namespaces  │       │
│  │   + cgroups  │  │   + cgroups  │  │   + cgroups  │       │
│  └──────────────┘  └──────────────┘  └──────────────┘       │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                 Shared Linux Kernel                   │   │
│  │         (одно ядро для всех контейнеров)             │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

**Ключевое отличие от VM:** Контейнеры разделяют ядро хоста. Нет отдельной ОС для каждого контейнера, нет hypervisor layer. Процесс в контейнере — это обычный процесс Linux, просто с ограничениями.

### Namespaces: изоляция видимости

Linux namespaces — механизм ядра, который создаёт иллюзию, что процесс имеет собственный экземпляр определённого ресурса. Процесс внутри namespace не видит ресурсы других namespaces.

Существует 8 типов namespaces (с kernel 5.6):

**PID Namespace — изоляция процессов**

Процессы внутри PID namespace имеют собственную нумерацию, начиная с PID 1. Процесс с PID 1 внутри контейнера на самом деле имеет другой PID с точки зрения хоста (например, PID 12345).

```bash
# На хосте
ps aux | grep nginx
# 12345 nginx: master process

# Внутри контейнера
ps aux
# PID 1 nginx: master process

# Контейнер не видит процессы хоста, хост видит все процессы
```

Процессы внутри контейнера не могут увидеть или отправить сигнал процессам снаружи. Если PID 1 в контейнере умирает — контейнер останавливается (как если бы умер init).

**NET Namespace — изоляция сети**

Каждый NET namespace имеет собственные сетевые интерфейсы, IP-адреса, routing tables, iptables rules, сокеты. При создании NET namespace в нём есть только loopback интерфейс.

```bash
# Контейнер имеет собственный eth0 и IP
# Внутри контейнера
ip addr
# eth0: 172.17.0.2/16

# На хосте это veth pair, соединённый с bridge docker0
```

Docker создаёт виртуальные пары интерфейсов (veth pair): один конец в namespace контейнера (eth0), другой на хосте (vethXXX), подключённый к bridge. Это позволяет контейнерам общаться между собой и с внешним миром.

**MNT Namespace — изоляция файловой системы**

Каждый MNT namespace имеет собственную таблицу монтирования. Контейнер видит только свою файловую систему, не видит диски и разделы хоста.

```bash
# Внутри контейнера
ls /
# bin  etc  home  lib  usr  var  ...
# (это образ контейнера, не файловая система хоста)

# Можно смонтировать директорию хоста внутрь контейнера (bind mount)
docker run -v /host/path:/container/path ...
```

**USER Namespace — изоляция пользователей**

USER namespace позволяет маппить UID/GID. Процесс может быть root внутри контейнера (UID 0), но непривилегированным пользователем снаружи.

```bash
# Внутри контейнера
id
# uid=0(root) gid=0(root)

# На хосте тот же процесс имеет другой UID
# uid=100000(container_user)
```

Это важно для безопасности: даже если злоумышленник получит root внутри контейнера, он не будет root на хосте.

**Другие namespaces:**
- **UTS** — hostname и domain name
- **IPC** — shared memory, semaphores, message queues
- **CGROUP** — видимость cgroup hierarchy
- **TIME** (с kernel 5.6) — изоляция системных часов

### Cgroups: ограничение ресурсов

Namespaces изолируют видимость, но не ограничивают потребление ресурсов. Без дополнительных ограничений один контейнер может забрать всю память или CPU, вызвав "noisy neighbor" проблему.

**Cgroups (control groups)** — механизм ядра для ограничения и учёта ресурсов:

```bash
# Создание cgroup (cgroups v2)
mkdir /sys/fs/cgroup/my_container

# Ограничение памяти: максимум 512 МБ
echo 536870912 > /sys/fs/cgroup/my_container/memory.max

# Ограничение CPU: 50% одного ядра
# Формат: quota period (в микросекундах)
# 50000/100000 = 50% CPU
echo "50000 100000" > /sys/fs/cgroup/my_container/cpu.max

# Ограничение количества процессов (защита от fork bomb)
echo 100 > /sys/fs/cgroup/my_container/pids.max

# Добавление процесса в cgroup
echo $PID > /sys/fs/cgroup/my_container/cgroup.procs
```

**Контролируемые ресурсы:**

- **memory** — лимит RAM, swap. При превышении — OOM killer убивает процессы в cgroup.
- **cpu** — доля CPU time. Можно также привязать к конкретным ядрам.
- **io** — bandwidth и IOPS для block devices.
- **pids** — максимальное количество процессов.

Docker, Kubernetes и другие container runtimes автоматически создают cgroups для каждого контейнера и настраивают лимиты согласно конфигурации (docker run --memory=512m --cpus=0.5).

### Union File Systems: слоистые образы

Контейнеры используют layered файловую систему для эффективного хранения и распространения образов.

```
┌─────────────────────────────────────────────┐
│       Container Layer (read-write)           │  ← Изменения во время
│       (изменённые файлы контейнера)         │    работы контейнера
├─────────────────────────────────────────────┤
│              App Layer                       │  ← RUN npm install
├─────────────────────────────────────────────┤
│             Node.js Layer                    │  ← FROM node:18
├─────────────────────────────────────────────┤
│              Base Image                      │  ← debian:bullseye
└─────────────────────────────────────────────┘
      (все слои read-only, кроме верхнего)
```

**Как работает OverlayFS:**

- **lowerdir** — read-only слои образа. Могут быть shared между контейнерами. Если 100 контейнеров используют образ node:18 — на диске один экземпляр.

- **upperdir** — read-write слой для изменений конкретного контейнера.

- **merged** — объединённый view, который видит контейнер.

**Copy-on-Write:** Когда контейнер изменяет файл из нижнего слоя:
1. Файл копируется в upperdir
2. Изменения применяются к копии
3. Оригинал в lowerdir не меняется

Это позволяет создавать контейнеры мгновенно — не нужно копировать весь образ, только создать пустой upperdir.

---

## VM vs Контейнеры: сравнение

### Производительность

| Метрика | Контейнеры | VM | Причина |
|---------|------------|-----|---------|
| **Startup time** | 100-500 мс | 10-60 сек | Нет загрузки ядра ОС |
| **Memory overhead** | ~5-10 МБ | ~200-500+ МБ | Нет отдельного ядра |
| **CPU overhead** | <1% | 2-5% | Нет hypervisor layer |
| **Disk I/O** | ~98% native | ~78-90% native | Нет storage virtualization |
| **Network latency** | +5-10% | +20-50% | Нет виртуализации сети |

Контейнеры достигают 95-99% производительности bare metal. VM с virtio — около 85-95%.

**Почему контейнеры быстрее стартуют:** При старте контейнера нужно только создать namespaces и cgroups — это операции ядра, занимающие микросекунды. При старте VM нужно: инициализировать виртуальное железо, загрузить BIOS/UEFI, загрузить bootloader, загрузить ядро, выполнить init — это секунды.

### Изоляция и безопасность

```
              Уровень изоляции
    ◄─────────────────────────────────────────►
    Слабее                                Сильнее

    Processes   Containers   VMs    Physical
        │           │          │         │
        │           │          │         │
    Общее ядро  Общее ядро  Раздельные  Физическое
    Общие       Namespaces  ядра        разделение
    namespaces  + cgroups   Hypervisor  серверов
```

**Контейнеры:**
- Изоляция через namespaces — абстракция ядра
- Уязвимость в ядре = потенциальный container escape
- Примеры реальных уязвимостей: Dirty COW (CVE-2016-5195), runc escape (CVE-2019-5736)

**VM:**
- Изоляция на уровне hypervisor
- Компрометация гостевой ОС не даёт доступ к хосту (обычно)
- Но hypervisor тоже может иметь уязвимости: VENOM, Spectre/Meltdown

**Правило:** Контейнеры достаточно безопасны для изоляции приложений одного владельца. Для multi-tenant (разные клиенты на одном сервере) облачные провайдеры используют VM.

## Когда VM, когда контейнеры, когда ни то ни другое

### Выбор по критериям

| Критерий | VM | Контейнер | Bare metal |
|----------|-----|-----------|------------|
| Изоляция security | Сильная (разные kernels) | Слабая (shared kernel) | N/A |
| Изоляция ресурсов | Полная | Через cgroups | N/A |
| Overhead | 5-15% CPU, GB RAM | <1% CPU, MB RAM | 0% |
| Startup | Минуты | Секунды | Минуты-часы |
| Разные ОС | Да (Windows на Linux host) | Нет (только Linux на Linux) | N/A |

### Когда VM — правильный выбор

✅ **Multi-tenant с недоверенными арендаторами** — облачные провайдеры (AWS EC2)
✅ **Запуск другой ОС** — Windows приложение на Linux сервере
✅ **Compliance требует полной изоляции** — PCI-DSS, финансы
✅ **Legacy приложения** требующие специфическую ОС

### Когда контейнеры — правильный выбор

✅ **Микросервисы** — много маленьких сервисов, быстрый деплой
✅ **CI/CD** — ephemeral окружения для тестов
✅ **Density** — максимум приложений на сервер
✅ **Разработка** — "works on my machine" → контейнер

### Когда НЕ использовать виртуализацию

❌ **High-frequency trading** — каждая микросекунда важна, overhead недопустим
❌ **GPU workloads** — passthrough усложняет, bare metal проще
❌ **Простые stateful приложения** — один PostgreSQL на bare metal проще
❌ **Когда нет экспертизы** — контейнеры/K8s требуют знаний, монолит на VM проще поддерживать

### Распространённые ошибки

| Ошибка | Почему это проблема |
|--------|---------------------|
| "Контейнеры = безопасность VM" | Нет! Shared kernel = escape возможен |
| "VM для каждого микросервиса" | Overhead, медленный startup |
| "Контейнеры для stateful" | Сложно (volumes, persistence) |
| "K8s для 3 сервисов" | Overkill, Docker Compose достаточно |

### Когда что использовать

**Контейнеры идеальны для:**

- **Микросервисы** — быстрый startup критичен для autoscaling. Kubernetes может масштабировать контейнеры за секунды.

- **CI/CD** — сотни билдов в день, каждый в изолированном окружении. Контейнер создаётся, выполняет билд, удаляется.

- **Однородные workloads** — все приложения на Linux, различия только в зависимостях.

- **Высокая плотность** — можно запустить сотни контейнеров на одном сервере.

**VM идеальны для:**

- **Multi-tenant isolation** — AWS, GCP, Azure используют VM для разделения клиентов. Каждый клиент в своей VM.

- **Разные ОС** — нужен Windows и Linux на одном сервере? Только VM.

- **Legacy приложения** — требуют специфического ядра или драйверов.

- **Compliance требования** — некоторые регуляции требуют VM-level isolation.

### Гибридный подход: лучшее из двух миров

Современные решения комбинируют преимущества обоих подходов:

**Kata Containers / Firecracker:**

```
┌─────────────────────────────────────────────────────────────┐
│                    Kata Containers                           │
│                                                              │
│  Container Runtime Interface (CRI)                           │
│           │                                                  │
│           ▼                                                  │
│  ┌─────────────────────────────────────────────────────┐    │
│  │              Lightweight VM                          │    │
│  │  ┌─────────────────────────────────────────────┐   │    │
│  │  │  Container (выглядит как обычный контейнер)  │   │    │
│  │  └─────────────────────────────────────────────┘   │    │
│  │  ┌─────────────────────────────────────────────┐   │    │
│  │  │  Minimal kernel (~5MB, boots in <100ms)     │   │    │
│  │  └─────────────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────┘    │
│                          │                                   │
│                    Hypervisor (QEMU/Firecracker)            │
└─────────────────────────────────────────────────────────────┘
```

**Firecracker** (разработан AWS для Lambda и Fargate):
- Boot time: ~125 мс (vs 10+ секунд для обычной VM)
- Memory overhead: ~5 МБ на VM (vs 200+ МБ)
- Эмулирует только необходимое: virtio-net, virtio-block, serial console

Получаем изоляцию VM с overhead, близким к контейнерам. AWS Lambda использует Firecracker для изоляции функций разных пользователей.

---

## Практические команды Linux

```bash
# Namespaces
lsns                              # Список всех namespaces
lsns -t net                       # Только network namespaces
nsenter -t $PID -n ip addr        # Войти в network namespace процесса
unshare -n bash                   # Создать процесс в новом net namespace

# Cgroups v2
cat /proc/cgroups                 # Поддерживаемые контроллеры
systemd-cgls                      # Дерево cgroups (в systemd)
cat /sys/fs/cgroup/memory.current # Текущее потребление памяти

# Информация о контейнерах
docker inspect $CONTAINER_ID      # Детальная информация
docker inspect --format '{{.State.Pid}}' $CONTAINER_ID  # PID
cat /proc/$PID/cgroup             # Cgroup контейнера
ls -la /proc/$PID/ns/             # Namespaces контейнера

# KVM/QEMU
kvm-ok                            # Проверка поддержки KVM
virsh list --all                  # Список VM (libvirt)
qemu-system-x86_64 -enable-kvm    # Запуск с KVM acceleration

# Мониторинг
perf kvm stat record              # Статистика VM exits
cat /sys/kernel/debug/kvm/*       # KVM debug info
```

---

## Связь с другими темами

**[[os-overview]]** — Обзор ОС объясняет разделение kernel/user mode и механизм привилегированных инструкций, которые составляют основу виртуализации: hypervisor перехватывает привилегированные инструкции гостевой ОС (через trap-and-emulate или VT-x/AMD-V hardware extensions) и эмулирует их поведение. Системные вызовы в контейнерах проходят через seccomp-bpf фильтр ядра хоста, что ограничивает набор доступных syscalls — понимание syscall interface критично для настройки безопасности контейнеров. Концепция interrupt handling также связана: в VM hardware interrupts перехватываются hypervisor и маршрутизируются в нужную гостевую ОС через virtual interrupt controller (vAPIC).

**[[os-processes-threads]]** — Контейнеры — это, по сути, обычные Linux-процессы с дополнительной изоляцией через namespaces (PID, NET, MNT, UTS, IPC, USER) и ограничениями через cgroups. Каждый контейнер получает собственное PID namespace, где его init-процесс имеет PID 1, хотя на уровне хоста он виден как обычный процесс с произвольным PID. VM, в отличие от контейнеров, запускают полноценную гостевую ОС с собственным ядром, планировщиком и всеми процессами — QEMU/KVM создаёт процесс-хост, внутри которого работает весь guest. Понимание процессов объясняет overhead: контейнер стартует за ~100 ms (fork + exec + namespace setup), а VM — за секунды (загрузка ядра, init system).

**[[os-memory-management]]** — Виртуализация добавляет дополнительный уровень трансляции адресов: в VM гостевая ОС управляет своими page tables (guest virtual → guest physical), а hypervisor добавляет ещё один уровень — nested/extended page tables (guest physical → host physical, EPT в Intel, NPT в AMD). Это двойное отображение увеличивает стоимость TLB miss, поскольку page walk проходит через два уровня таблиц. В контейнерах memory cgroups ограничивают потребление памяти: когда контейнер превышает лимит, OOM Killer убивает процессы внутри контейнера, а не произвольные процессы хоста. Технология KSM (Kernel Same-page Merging) позволяет hypervisor дедуплицировать одинаковые страницы памяти между VM, экономя RAM при запуске множества идентичных гостевых ОС.

**[[os-scheduling]]** — Планирование CPU в контексте виртуализации имеет два уровня: hypervisor планирует виртуальные CPU (vCPU) гостевых ОС на физические ядра, а внутри каждой гостевой ОС работает собственный планировщик (CFS), не подозревающий о виртуализации. Эта двухуровневая схема создаёт проблему «stolen time»: когда hypervisor вытесняет vCPU, гостевая ОС не знает, что её поток не выполнялся, что может нарушить тайминги и scheduling-решения внутри guest. В контейнерах CPU cgroups ограничивают процессорное время: cpu.shares определяет относительный вес, а cpu.cfs_quota_us/cpu.cfs_period_us — жёсткий лимит (throttling). Для real-time workloads в VM используется CPU pinning — привязка vCPU к физическому ядру, исключающая конкуренцию с другими VM.

**[[docker-for-developers]]** — Docker — самая популярная реализация контейнеризации, и понимание OS-механизмов объясняет его внутреннее устройство: Docker daemon использует runc (OCI runtime), который вызывает clone() с флагами CLONE_NEWPID, CLONE_NEWNS, CLONE_NEWNET для создания namespaces и настраивает cgroups через /sys/fs/cgroup. Dockerfile инструкции (FROM, RUN, COPY) создают overlay filesystem слои — каждый слой является read-only snapshot, а контейнер добавляет writable layer поверх, что объясняет, почему образы можно разделять между контейнерами. Docker на Mac/Windows запускает Linux VM (HyperKit/WSL2), потому что namespaces и cgroups — это Linux-специфичные механизмы ядра, не доступные в других ОС. Понимание этого объясняет дополнительный overhead и проблемы с производительностью файловой системы при разработке на non-Linux платформах.

**[[kubernetes-basics]]** — Kubernetes строит оркестрацию поверх контейнеров, используя все OS-механизмы виртуализации: каждый Pod — это группа контейнеров, разделяющих network namespace (общий IP) и mount namespace (shared volumes). Kubelet на каждом node управляет container runtime (containerd/CRI-O), который, в свою очередь, использует runc для создания namespaces и cgroups — это цепочка от Kubernetes API до Linux kernel. Resource requests и limits в Kubernetes напрямую транслируются в cgroups: limits.memory → memory.limit_in_bytes, limits.cpu → cpu.cfs_quota_us. Понимание OS-уровня помогает диагностировать проблемы: OOMKilled означает превышение memory cgroup limit, а CPU throttling (nr_throttled в cpu.stat) — превышение CPU quota.

**Связанные концепции:**
- [[os-io-devices]] — virtio для I/O виртуализации, device passthrough (SR-IOV)
- Cloud providers: EC2 = VM, ECS/Fargate = контейнеры, Lambda = micro-VM (Firecracker)

---

## Рекомендуемые источники

### Учебники

- Tanenbaum A., Bos H. (2014). *"Modern Operating Systems, 4th Edition."* — глава 7 (Virtualization and the Cloud) — от requirements Popek-Goldberg до Type 1/Type 2 hypervisors, paravirtualization и containers; одно из первых учебников, включивших виртуализацию как полноценную тему.
- Silberschatz A., Galvin P., Gagne G. (2018). *"Operating System Concepts, 10th Edition."* — глава 18 (Virtual Machines) — история виртуализации от IBM VM/370 до современных VT-x/AMD-V, binary translation, paravirtualization, containers, application containment.
- Arpaci-Dusseau R., Arpaci-Dusseau A. (2018). *"Operating Systems: Three Easy Pieces."* — часть Virtualization (главы 3-24) рассматривает виртуализацию CPU и памяти как центральную функцию ОС; Appendix о VMMs; бесплатно.
- Bryant R., O'Hallaron D. (2015). *"Computer Systems: A Programmer's Perspective, 3rd Edition."* — глава 9 (Virtual Memory) объясняет address translation, которая является основой nested page tables в hypervisors; глава 8 (Exceptional Control Flow) — trap mechanism, используемый в trap-and-emulate виртуализации.
- Love R. (2010). *"Linux Kernel Development, 3rd Edition."* — главы о namespaces, cgroups и process isolation дают фундамент для понимания контейнеров; KVM как модуль ядра Linux описан в контексте kernel modules (глава 17).

### Книги и курсы
- [OSTEP: Virtualization chapters](https://pages.cs.wisc.edu/~remzi/OSTEP/) — бесплатная книга
- [Docker Getting Started](https://docs.docker.com/get-started/) — официальный туториал

### Официальная документация
- [KVM Documentation](https://www.linux-kvm.org/page/Documents) — документация KVM
- [Docker Documentation](https://docs.docker.com/) — официальная документация Docker
- [Linux namespaces man page](https://man7.org/linux/man-pages/man7/namespaces.7.html) — официальная документация

### Статьи и туториалы
- [GeeksforGeeks: Virtualization](https://www.geeksforgeeks.org/virtualization-cloud-computing-and-types/) — базовое объяснение
- [Julia Evans: Containers from scratch](https://jvns.ca/blog/2016/10/10/what-even-is-a-container/) — отличное объяснение
- [AWS: Firecracker](https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/) — официальный анонс

### Глубокое погружение
- [LWN: Namespaces series](https://lwn.net/Articles/531114/) — серия статей о namespaces
- [Brendan Gregg: Container Performance](https://www.brendangregg.com/blog/2017-11-14/container-performance-analysis-at-netflix.html) — performance analysis

---

*Обновлено: 2026-01-09 — добавлены педагогические секции (5 аналогий: отель/коворкинг, зеркальные комнаты/namespaces, счётчики/cgroups, прозрачные плёнки/OverlayFS, администратор/hypervisor; 6 типичных ошибок: контейнер≠VM, безопасность, root в контейнере, данные внутри контейнера, Docker Mac/Windows≠Linux, K8s overkill; 5 ментальных моделей: спектр изоляции, контейнер=процесс+изоляция, дерево решений, эволюция, overhead пирамида)*

---

## Проверь себя

> [!question]- JVM-приложение в Docker контейнере с `-Xmx2g` получает OOMKilled, хотя хост имеет 32 GB RAM. Объясни цепочку событий на уровне ОС и как правильно настроить.
> Контейнер запущен с `--memory=2g` (cgroup memory.max = 2 GB). JVM с `-Xmx2g` пытается использовать 2 GB heap, но JVM потребляет больше: heap + metaspace + thread stacks + native memory + JIT code cache ≈ 2.5-3 GB. Когда RSS процесса превышает cgroup limit, OOM Killer ядра убивает процесс внутри cgroup. **Решения:** (1) Контейнер limit должен быть на 30-50% больше Xmx: `--memory=3g` при `-Xmx2g`; (2) Java 10+ с `-XX:+UseContainerSupport` автоматически видит cgroup limits; (3) `-XX:MaxRAMPercentage=75` — JVM берёт 75% от видимой памяти; (4) мониторить: `cat /sys/fs/cgroup/memory.current` vs `memory.max`.

> [!question]- Почему контейнер НЕ обеспечивает тот же уровень безопасности, что VM, даже с seccomp + AppArmor? Приведи реальные примеры container escape.
> Контейнеры разделяют **ядро хоста** — уязвимость в ядре = потенциальный escape. VM имеют отдельные ядра, атакующий должен пробить hypervisor (меньше attack surface). **Реальные escapes:** (1) **Dirty COW (CVE-2016-5195):** race condition в kernel copy-on-write позволял записать в read-only файлы → получить root на хосте из контейнера. (2) **runc escape (CVE-2019-5736):** перезапись runc binary на хосте через /proc/self/exe из контейнера. (3) **Процесс в контейнере может видеть /proc хоста** при неправильной настройке. seccomp фильтрует syscalls, AppArmor ограничивает файловый доступ — но они защищают от known vectors, а kernel 0-day обходит всё. Для multi-tenant: только VM или Kata Containers / Firecracker.

> [!question]- AWS Lambda обслуживает миллионы функций на shared infrastructure. Объясни, почему они используют Firecracker (micro-VM), а не Docker контейнеры.
> Lambda = multi-tenant: функции разных клиентов на одном сервере. Контейнеры с shared kernel — один клиент может exploitнуть kernel vulnerability и получить доступ к данным другого. VM обеспечивают изоляцию через отдельные ядра, но обычная VM = 10+ сек boot, 200+ MB overhead. **Firecracker:** micro-VM с минимальным device model (только virtio-net, virtio-block, serial). Boot: ~125 ms, overhead: ~5 MB. Изоляция VM + скорость контейнера. Каждая Lambda function = отдельная Firecracker VM с собственным ядром. Так же используется в AWS Fargate.

> [!question]- Контейнер с приложением работает на Ubuntu хосте, но внутри — Alpine Linux. Если контейнер вызывает read(), какое ядро обрабатывает этот syscall? Почему?
> Syscall обрабатывает **ядро хоста (Ubuntu)**. Контейнер — это процесс с namespaces и cgroups, НЕ отдельная ОС. Alpine внутри контейнера — это только userspace (musl libc, busybox, APK package manager), а ядро — всегда хоста. Когда musl libc вызывает `syscall(SYS_read, ...)`, CPU переходит в kernel mode того же ядра Ubuntu. Namespaces изолируют видимость (PID, NET, MNT), cgroups ограничивают ресурсы, но syscall interface — единый. Это отличие от VM: в VM гостевая ОС имеет своё ядро, и syscall обрабатывается ядром guest, которое затем через hypervisor обращается к хосту для I/O.

---

## Ключевые карточки

VM vs Контейнер: ключевые метрики производительности?
?
| Метрика | Контейнер | VM |
|---------|-----------|-----|
| Startup | 100-500 ms | 10-60 sec |
| Memory overhead | ~5-10 MB | ~200-500 MB |
| CPU overhead | <1% | 2-5% |
| Disk I/O | ~98% native | ~78-90% |
| Network latency | +5-10% | +20-50% |
Контейнер быстрее потому что нет загрузки отдельного ядра, нет hypervisor layer.

Что делают namespaces и что делают cgroups?
?
**Namespaces:** изоляция **видимости**. Типы: PID (свой PID 1), NET (своя сеть), MNT (свои точки монтирования), UTS (hostname), IPC (shared memory), USER (uid mapping). Процесс "думает", что один на сервере. **Cgroups:** ограничение **ресурсов**. memory.max — лимит RAM, cpu.max — лимит CPU, pids.max — лимит процессов, io.max — лимит I/O bandwidth. Вместе = контейнер.

Type 1 vs Type 2 hypervisor?
?
**Type 1 (bare-metal):** устанавливается прямо на железо, гостевые ОС поверх. Минимальный overhead. Примеры: VMware ESXi, Xen, KVM (Linux + KVM module). Используется в production (облака, data centers). **Type 2 (hosted):** работает как приложение внутри обычной ОС. Больше overhead, проще установка. Примеры: VirtualBox, VMware Workstation, Parallels. Используется для разработки, desktop.

Что такое OverlayFS и как Docker его использует?
?
OverlayFS — layered файловая система. **lowerdir** — read-only слои (base image, каждый RUN/COPY). **upperdir** — read-write слой конкретного контейнера. **merged** — объединённый view. При изменении файла из lower: Copy-on-Write → файл копируется в upper. Преимущества: (1) 100 контейнеров с image node:18 = один экземпляр на диске; (2) создание контейнера мгновенно — только пустой upperdir; (3) Dockerfile layers кэшируются и переиспользуются.

Почему Docker на Mac/Windows использует VM?
?
Docker использует **Linux kernel features**: namespaces, cgroups. Mac и Windows не имеют этих механизмов. Docker Desktop запускает легковесную Linux VM: HyperKit (Mac), WSL2/Hyper-V (Windows). Контейнеры работают внутри этой VM. Следствия: (1) дополнительный overhead; (2) file system sharing между host и VM медленное (особенно на Mac); (3) networking через VM → дополнительная latency.

Что такое Firecracker и зачем AWS его создал?
?
Firecracker — micro-VM от AWS. Boot: ~125 ms (vs 10+ sec обычная VM). Memory: ~5 MB overhead (vs 200+ MB). Эмулирует только необходимое: virtio-net, virtio-block, serial console (vs полная эмуляция железа в QEMU). Создан для AWS Lambda и Fargate: нужна **изоляция VM** (multi-tenant безопасность) + **скорость контейнеров** (быстрый startup для serverless). Каждая Lambda function = отдельная Firecracker VM.

Как Kubernetes limits транслируются в cgroups?
?
`resources.limits.memory: 512Mi` → `echo 536870912 > memory.max` (cgroup v2). `resources.limits.cpu: "0.5"` → `echo "50000 100000" > cpu.max` (50% одного ядра). `resources.requests` → scheduling hints (pod размещается на node с достаточными ресурсами). При превышении memory limit → OOMKilled (ядро убивает процесс в cgroup). При превышении CPU limit → throttling (nr_throttled в cpu.stat).

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Следующий шаг | [[docker-for-developers]] | Практическое использование: Dockerfile, compose, best practices |
| Следующий шаг | [[kubernetes-basics]] | Оркестрация контейнеров: pods, services, deployments |
| Углубиться | [[os-processes-threads]] | Контейнеры — процессы с namespaces, понимание fork/clone |
| Углубиться | [[os-memory-management]] | Nested page tables в VM, memory cgroups для контейнеров |
| Смежная тема | [[os-scheduling]] | CPU cgroups, vCPU scheduling, stolen time в VM |
| Смежная тема | [[cloud-platforms-essentials]] | EC2=VM, ECS/Fargate=контейнеры, Lambda=micro-VM |
| Обзор | [[os-overview]] | Вернуться к карте раздела Operating Systems |

---

*Последнее обновление: 2026-02-14*
*Проверено: 2026-02-14*
