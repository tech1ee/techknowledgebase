---
title: "Управление памятью: виртуальная память и paging"
created: 2025-12-02
modified: 2025-12-02
type: deep-dive
status: published
area: operating-systems
confidence: high
tags:
  - topic/os
  - memory
  - virtual-memory
  - paging
  - type/deep-dive
  - level/intermediate
related:
  - "[[os-overview]]"
  - "[[os-processes-threads]]"
  - "[[jvm-memory-model]]"
  - "[[jvm-gc-tuning]]"
prerequisites:
  - "[[os-processes-threads]]"
  - "[[data-structures-fundamentals]]"
reading_time: 50
difficulty: 7
study_status: not_started
mastery: 0
last_reviewed:
next_review:
---

# Управление памятью: виртуальная память и paging

Виртуальная память — абстракция, дающая каждому процессу иллюзию собственного непрерывного адресного пространства. В реальности физическая память разбита на страницы (pages) по 4KB, разбросанные по RAM и диску. MMU (Memory Management Unit) в процессоре на лету транслирует виртуальные адреса в физические. Это обеспечивает изоляцию процессов, эффективное использование памяти и возможность запускать программы больше, чем физическая RAM.

---

## TL;DR

> **Что:** Виртуальная память — это "иллюзия", что у каждой программы своя бесконечная память. Как если бы каждый жилец ТЦ думал, что весь этаж — только его.
>
> **Как:** Память разбита на страницы (pages) по 4KB. MMU (железо в CPU) переводит виртуальные адреса в физические через Page Table.
>
> **Ключевой кэш:** TLB — кэш трансляций в CPU. TLB hit = 1 цикл, TLB miss = 100-1000 циклов (page table walk).
>
> **Page Fault:** Minor (~1µs) = страница есть, но не mapped. Major (~10ms) = страница на диске → катастрофа для производительности.
>
> **Thrashing:** Когда RAM < working set программ → система тратит всё время на swap → коллапс.
>
> **OOM Killer:** Когда памяти совсем нет, Linux убивает процессы по oom_score. Защитите критичные сервисы!

---

## Часть 1: Интуиция без кода

### 🧩 Аналогия 1: Пазл и доска

**Виртуальная память — это пазл. Физическая память — доска для пазла.**

Представь огромный пазл на 10,000 деталей, но твоя доска вмещает только 1,000. Как собрать?
- Держи на доске только те части, над которыми работаешь (working set)
- Остальные детали храни в коробке (swap на диске)
- Когда нужна другая часть — убери неактивную в коробку и достань нужную

**Page (страница)** — одна деталь пазла (кусок виртуальной памяти, обычно 4KB)
**Frame (фрейм)** — одна ячейка на доске (слот физической памяти)
**Page Table** — записная книжка: "деталь #42 лежит в ячейке #17"

```
Виртуальные страницы:     Физические фреймы (доска):
┌───┬───┬───┬───┬───┐     ┌───┬───┬───┐
│ 0 │ 1 │ 2 │ 3 │ 4 │     │ 0 │ 1 │ 2 │  ← Только 3 слота!
└─┬─┴─┬─┴─┬─┴───┴───┘     └─▲─┴─▲─┴─▲─┘
  │   │   │                 │   │   │
  │   │   └────────────────►┤   │   │   Page 2 → Frame 0
  │   └────────────────────►───►┤   │   Page 1 → Frame 1
  └────────────────────────────►───►┤   Page 0 → Frame 2

Pages 3, 4 — в "коробке" (на диске), не на доске
```

### 🏢 Аналогия 2: Офисное здание

Представь 100-этажное офисное здание (виртуальное адресное пространство), но реальных этажей построено только 10 (физическая память):

- **Виртуальный адрес** = "Этаж 47, кабинет 3" — адрес в "идеальном" здании
- **Физический адрес** = "Реальный этаж 5, кабинет 3" — где данные РЕАЛЬНО лежат
- **Page Table** = табличка лифтёра: "Когда жмут 47 — везу на 5-й"
- **TLB** = память лифтёра о последних 10 поездках (не смотрит в табличку)

Каждый арендатор (процесс) думает, что у него всё здание:
```
Арендатор A видит:        Арендатор B видит:
┌─────────────────┐       ┌─────────────────┐
│ "Мои 100 этажей"│       │ "Мои 100 этажей"│
│ Этаж 1: мой код │       │ Этаж 1: мой код │
│ Этаж 47: данные │       │ Этаж 47: данные │
└─────────────────┘       └─────────────────┘
        ↓                         ↓
    Реально: разные физические этажи!
    A не может попасть в офис B.
```

### 📞 Аналогия 3: Телефонная книга (Page Table)

**Page Table** работает как телефонная книга:
- **Имя** (виртуальная страница) → **Номер** (физический фрейм)
- Если имени нет в книге → Page Fault (надо искать)

**TLB (Translation Lookaside Buffer)** — это память на 10-20 последних звонков:
- Звонишь маме? Помнишь номер наизусть → **TLB hit** (~1 такт)
- Звонишь в ЖЭК? Надо искать в книге → **TLB miss** (~100 тактов)
- Номер отключен? Надо искать актуальный → **Page Fault** (~10,000,000 тактов!)

```
Обращение к адресу 0x12345000:

1. CPU проверяет TLB: "Знаю страницу 0x12345?"
   ├── ДА (TLB hit): Frame = 0x789  →  Физ.адрес = 0x789000  [1 такт]
   └── НЕТ (TLB miss): Иди в Page Table
                         ├── Есть запись: Frame = 0x789      [100+ тактов]
                         └── Нет записи: PAGE FAULT          [миллионы тактов]
```

### 📉 Аналогия 4: Thrashing как пробка на мосту

Thrashing — это когда система тратит всё время на swap вместо полезной работы.

Представь остров (RAM) с одним мостом (пропускная способность диска):
- На острове 100 рабочих мест (frames)
- Приехало 500 работников (страницы 5 программ)
- Working set каждой программы = 150 страниц
- Суммарно нужно: 5 × 150 = 750 страниц
- Есть места: 100

**Результат:**
- Работник приезжает → места нет → кого-то выгоняют
- Выгнанный работник тут же нужен → едет обратно
- ВСЕ стоят в пробке на мосту, НИКТО не работает

```
Нормальная работа:         Thrashing:
┌────────────────┐         ┌────────────────┐
│ RAM: 80% занят │         │ RAM: 100%      │
│ CPU: 90% работа│         │ CPU: 5% работа │
│ Disk: 10% I/O  │         │ Disk: 95% swap!│
└────────────────┘         └────────────────┘
     ✅ Всё OK               ❌ Коллапс
```

**Признаки thrashing:**
- `vmstat` показывает si/so (swap in/out) > 0 постоянно
- Диск загружен на 100%, CPU idle высокий
- Система "еле шевелится"

### 🎯 Аналогия 5: OOM Killer как охранник клуба

Когда RAM заканчивается, Linux запускает **OOM Killer** — "охранника", который решает кого выгнать:

**Критерии выбора жертвы (oom_score):**
- Кто больше всех съел памяти? (+очки)
- Кто недавно запустился? (+очки, меньше работы потеряно)
- Кто root или системный процесс? (-очки, защищён)
- У кого низкий oom_score_adj? (-очки, VIP)

```
Клуб переполнен (OOM):

Посетители:
┌──────────────────────────────────────────────────────┐
│ systemd    [oom_score_adj=-1000]  — VIP, не трогать  │
│ postgres   [oom_score_adj=-500]   — важный клиент   │
│ java_app   [oom_score=800]        — занял полклуба! │
│ chrome     [oom_score=600]        — много вкладок   │
└──────────────────────────────────────────────────────┘

OOM Killer: "java_app, на выход!" (SIGKILL)
```

**Защита от OOM:**
```bash
# Защитить критичный процесс
echo -1000 > /proc/<pid>/oom_score_adj

# В systemd unit:
OOMScoreAdjust=-1000
```

### 🔢 Численная интуиция

| Событие | Время | Сравнение |
|---------|-------|-----------|
| TLB hit | ~1 нс | Вспомнить номер друга |
| TLB miss + Page Table walk | ~100 нс | Найти номер в телефоне |
| Minor Page Fault | ~1-10 µs | Записать новый контакт |
| **Major Page Fault (с диска!)** | **~10 ms** | **Позвонить в справочную** |

**Масштаб:**
```
Major Page Fault = 10 ms = 10,000,000 ns

Если TLB hit = 1 секунда в жизни человека,
то Major Page Fault = 115 дней!

1 Major Fault = время 10,000,000 обычных обращений
```

**Почему это критично для производительности:**
- Сервер с 1000 запросов/сек
- Каждый запрос = 1 major fault
- 1000 × 10ms = 10 секунд/сек на page faults
- Это невозможно! → Сервер "ляжет"

---

## Часть 2: Почему это сложно

### ❌ Ошибка 1: Путаница виртуальных и физических адресов

**СИМПТОМ:** "У меня массив на 8GB, значит нужно 8GB RAM!"

**ПОЧЕМУ ВОЗНИКАЕТ:** Новички думают, что виртуальная память = физическая память.

**РЕАЛЬНОСТЬ:**
```c
// Этот код НЕ требует 1GB физической памяти сразу!
char* huge = malloc(1024 * 1024 * 1024);  // 1GB виртуальной

// Linux использует "lazy allocation":
// - malloc() резервирует ВИРТУАЛЬНЫЕ адреса (мгновенно)
// - Физическая память выделяется при ПЕРВОМ ОБРАЩЕНИИ
// - Если никогда не трогать huge[999999999] — памяти не потратится

huge[0] = 'A';        // Аллоцирует 1 страницу (4KB)
huge[4096] = 'B';     // Аллоцирует ещё 1 страницу
// Потрачено: 8KB физической памяти, хотя выделено 1GB виртуальной
```

**РЕШЕНИЕ:** Различайте:
- **Virtual Memory (VIRT/VSZ)** — сколько адресов зарезервировано
- **Resident Memory (RES/RSS)** — сколько физической RAM реально занято

```bash
# В top/htop:
# VIRT = виртуальная (может быть 100GB)
# RES  = резидентная (реально в RAM)
# SHR  = shared (общие библиотеки)

PID  VIRT    RES    SHR   COMMAND
1234 4.5g   850m   120m   java      # 4.5GB виртуальной, 850MB в RAM
```

---

### ❌ Ошибка 2: Непонимание цены Major Page Fault

**СИМПТОМ:** "Добавлю swap 100GB и всё будет работать!"

**ПОЧЕМУ ВОЗНИКАЕТ:** Swap кажется "бесплатным расширением RAM".

**РЕАЛЬНОСТЬ:**
```
Скорость доступа:
┌─────────────────┬──────────────┬─────────────────────────┐
│ Уровень         │ Время        │ Операций/сек            │
├─────────────────┼──────────────┼─────────────────────────┤
│ L1 cache        │ ~1 ns        │ 1,000,000,000           │
│ L3 cache        │ ~10 ns       │ 100,000,000             │
│ RAM             │ ~100 ns      │ 10,000,000              │
│ NVMe SSD        │ ~100 µs      │ 10,000                  │
│ SATA SSD        │ ~500 µs      │ 2,000                   │
│ HDD             │ ~10 ms       │ 100                     │
└─────────────────┴──────────────┴─────────────────────────┘

RAM в 1000-100,000 раз быстрее диска!
```

**КАК ПРОЯВЛЯЕТСЯ:**
```bash
# Сервер с 16GB RAM, 64GB swap, нагрузка требует 20GB
vmstat 1
procs -----------memory---------- ---swap-- -----io----
 r  b   swpd   free   buff  cache   si   so    bi    bo
15  8 4000000  1234   5678  90000  500  800  50000     0

# si/so (swap in/out) > 0 = проблема!
# bi (block in) = 50000 = 50MB/s с диска = thrashing
```

**РЕШЕНИЕ:**
1. Добавить RAM, не swap
2. Уменьшить working set (кэши, пулы соединений)
3. Ограничить память через cgroups
4. Включить `vm.swappiness=10` (меньше swap)

---

### ❌ Ошибка 3: Игнорирование TLB miss при работе с большими структурами

**СИМПТОМ:** "Почему обход массива по столбцам в 10 раз медленнее обхода по строкам?"

**ПОЧЕМУ ВОЗНИКАЕТ:** Непонимание как TLB и cache работают со страницами.

**ПРИМЕР:**
```c
#define N 10000
int matrix[N][N];

// Обход по строкам — БЫСТРО
for (int i = 0; i < N; i++)
    for (int j = 0; j < N; j++)
        matrix[i][j] = 0;
// Последовательный доступ: matrix[0][0], [0][1], [0][2]...
// Одна страница 4KB = 1024 int'а подряд
// TLB entry используется 1024 раза подряд

// Обход по столбцам — МЕДЛЕННО (в 10-100 раз!)
for (int j = 0; j < N; j++)
    for (int i = 0; i < N; i++)
        matrix[i][j] = 0;
// Каждое обращение — новая строка = новая страница
// matrix[0][0], [1][0], [2][0]... = 10000 разных страниц!
// TLB имеет ~1024 записи → постоянные TLB misses
```

**РЕШЕНИЕ:**
- Обходите данные последовательно (cache-friendly access)
- Используйте Huge Pages для больших структур (2MB вместо 4KB)
- Применяйте блочные алгоритмы (cache blocking)

---

### ❌ Ошибка 4: Оверкоммит и неожиданный OOM Kill

**СИМПТОМ:** "Программа работала, потом внезапно убита!"

**ПОЧЕМУ ВОЗНИКАЕТ:** Linux по умолчанию разрешает overcommit — выделять больше виртуальной памяти, чем есть физической + swap.

**СЦЕНАРИЙ:**
```
Память: 16GB RAM + 4GB swap = 20GB доступно
Overcommit: включен (по умолчанию)

Process A: malloc(10GB) → OK (виртуально)
Process B: malloc(10GB) → OK (виртуально)
Process C: malloc(10GB) → OK (виртуально)
           Виртуально: 30GB выделено, физически: 20GB есть

Все процессы начинают РЕАЛЬНО использовать память...
Process A использует: 8GB
Process B использует: 8GB
Process C пытается использовать: 8GB
           Нужно: 24GB, есть: 20GB

OOM KILLER: Убивает Process C (SIGKILL, без предупреждения!)
```

**РЕШЕНИЕ:**
```bash
# 1. Отключить overcommit (строгий режим)
echo 2 > /proc/sys/vm/overcommit_memory
echo 80 > /proc/sys/vm/overcommit_ratio
# Теперь malloc() вернёт NULL если памяти нет

# 2. Защитить критичные процессы
echo -1000 > /proc/<pid>/oom_score_adj

# 3. В Java: ограничить heap явно
java -Xmx8g -Xms8g ...  # Точно знаем сколько нужно
```

---

### ❌ Ошибка 5: Забыть про Copy-on-Write после fork()

**СИМПТОМ:** "fork() мгновенен" / "fork() копирует всю память"

**РЕАЛЬНОСТЬ:** fork() использует COW — оба утверждения частично верны!

```
До fork():
┌─────────────────────────────────┐
│ Parent Process (10GB heap)      │
│ Page 1: [data] ─────────────────┼──▶ Frame 100 (physical)
│ Page 2: [data] ─────────────────┼──▶ Frame 200 (physical)
│ ...                             │
└─────────────────────────────────┘

После fork() (мгновенно!):
┌─────────────────────────────────┐     ┌─────────────────────────────────┐
│ Parent Process                  │     │ Child Process                   │
│ Page 1 ─────────────────────────┼──▶  │ ◀──────────────────── Page 1    │
│ Page 2 ─────────────────────────┼──┼──┼──▶ Frame 200 (READ-ONLY)        │
│ ...                             │  │  │                                 │
└─────────────────────────────────┘  │  └─────────────────────────────────┘
                                     │
                                  Frame 100 (READ-ONLY, shared!)

Parent пишет в Page 1:
1. Page Fault! (страница read-only)
2. Ядро копирует Frame 100 → Frame 300
3. Parent теперь указывает на Frame 300 (read-write)
4. Child всё ещё на Frame 100

Это и есть Copy-on-Write!
```

**ПРОБЛЕМА:** fork() в процессе с большим heap + GC = много COW faults

```
Redis с 20GB данных:
1. fork() для BGSAVE (snapshot)
2. GC или модификации данных
3. Каждая запись → COW → копирование страницы
4. В худшем случае: 20GB копирования!
```

**РЕШЕНИЕ:**
- Используйте Huge Pages (меньше page faults)
- Минимизируйте записи во время fork-heavy операций
- Для Redis: disable transparent huge pages

---

### ❌ Ошибка 6: Непонимание Huge Pages и когда они нужны

**СИМПТОМ:** "Huge Pages ускорят моё приложение!" (не всегда)

**КОГДА HUGE PAGES ПОМОГАЮТ:**
```
Обычные страницы (4KB):
- 64GB RAM = 16 миллионов страниц
- Page Table занимает ~128MB
- TLB (1024 записи) покрывает 4MB → много TLB misses

Huge Pages (2MB):
- 64GB RAM = 32 тысячи страниц
- Page Table занимает ~256KB
- TLB (1024 записи) покрывает 2GB → мало TLB misses
```

**КОГДА НЕ ПОМОГАЮТ:**
- Мелкие аллокации (< 2MB)
- Приложения с частым malloc/free (фрагментация)
- Когда памяти мало (2MB не разбить на мелкие)

**КАК ВКЛЮЧИТЬ:**
```bash
# Проверить поддержку
grep Huge /proc/meminfo

# Зарезервировать 100 huge pages по 2MB = 200MB
echo 100 > /proc/sys/vm/nr_hugepages

# В Java (версия 17+)
java -XX:+UseLargePages -XX:LargePageSizeInBytes=2m ...
```

---

## Часть 3: Ментальные модели

### 🧠 Модель 1: "Трёхуровневая трансляция"

```
Виртуальный адрес 48 бит:
┌───────────┬───────────┬───────────┬───────────┬────────────┐
│ PML4 (9)  │ PDPT (9)  │ PD (9)    │ PT (9)    │ Offset(12) │
└─────┬─────┴─────┬─────┴─────┬─────┴─────┬─────┴──────┬─────┘
      │           │           │           │            │
      ↓           ↓           ↓           ↓            │
   ┌─────┐    ┌─────┐    ┌─────┐    ┌─────┐           │
   │PML4 │───▶│PDPT │───▶│ PD  │───▶│ PT  │──┐        │
   │Table│    │Table│    │Table│    │Table│  │        │
   └─────┘    └─────┘    └─────┘    └─────┘  │        │
                                             ↓        │
                                       ┌──────────┐   │
                                       │ Frame #  │───┼──▶ Физ. адрес
                                       └──────────┘   │
                                                      │
                                         + Offset ◄───┘
```

**Почему 4 уровня?**
- 1 уровень: 2^9 × 4KB = 2MB → мало
- 2 уровня: 2^18 × 4KB = 1GB → всё ещё мало
- 4 уровня: 2^36 × 4KB = 256TB → достаточно для современных систем

**Когда использовать эту модель:** Когда нужно понять overhead page table walk.

---

### 🧠 Модель 2: "TLB как L0 кэш памяти"

```
Иерархия памяти с TLB:

CPU Core
┌─────────────────────────────────────────────────────────────┐
│                                                              │
│   ┌─────────┐   TLB hit: 1 cycle                            │
│   │   TLB   │────────────────────────────────┐              │
│   │(~1024)  │                                │              │
│   └────┬────┘                                │              │
│        │ TLB miss                            │              │
│        ↓                                     │              │
│   ┌─────────┐   L1 hit: ~4 cycles            │              │
│   │L1 Cache │────────────────────────────────┼──────┐       │
│   │ (64KB)  │                                │      │       │
│   └────┬────┘                                │      │       │
│        │ L1 miss                             │      │       │
│        ↓                                     ↓      ↓       │
│   ┌─────────┐   ┌─────────┐                              Data│
│   │L2 Cache │   │Page Walk│ (100+ cycles)                    │
│   │ (256KB) │   │ Unit    │                                  │
│   └────┬────┘   └────┬────┘                                  │
│        │             │                                        │
└────────┼─────────────┼────────────────────────────────────────┘
         │             │
         ↓             ↓
    ┌─────────────────────┐
    │      RAM + Page     │
    │       Tables        │
    └─────────────────────┘
```

**Ключевой инсайт:** TLB miss запускает page table walk параллельно с обычным cache lookup.

---

### 🧠 Модель 3: "Жизненный цикл страницы"

```
                          ┌───────────────┐
                          │   ALLOCATED   │
                          │ (malloc/mmap) │
                          └───────┬───────┘
                                  │ First access
                                  ↓
                          ┌───────────────┐
                     ┌────│   IN MEMORY   │◄───┐
                     │    │  (resident)   │    │
                     │    └───────┬───────┘    │
                     │            │            │
              Under  │     Memory │            │ Swap in
             pressure│    pressure│            │ (major fault)
                     │            │            │
                     │            ↓            │
                     │    ┌───────────────┐    │
                     │    │  PAGE OUT     │    │
                     │    │ (candidate)   │    │
                     │    └───────┬───────┘    │
                     │            │            │
                     │     LRU    │            │
                     │    eviction│            │
                     │            ↓            │
                     │    ┌───────────────┐    │
                     └───▶│   ON DISK     │────┘
                          │   (swapped)   │
                          └───────────────┘
```

**Когда страница выгружается?**
1. Система под давлением (low memory)
2. Страница давно не использовалась (LRU)
3. Страница "чистая" (не изменялась) — можно просто забыть

---

### 🧠 Модель 4: "Карта памяти процесса"

```
Высокие адреса (0xFFFF...)
┌─────────────────────────────────────────────────────────────┐
│                      KERNEL SPACE                           │
│            (недоступно из user mode)                        │
├─────────────────────────────────────────────────────────────┤
│                         Stack                               │
│                      (grows ↓)                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ main() locals, return addresses, argv               │   │
│  │ thread 2 stack, thread 3 stack...                   │   │
│  └─────────────────────────────────────────────────────┘   │
│                           ↓                                 │
│                      [Guard page]                           │
│                                                             │
│                     ~~ свободно ~~                          │
│                                                             │
│                           ↑                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                      Heap                            │   │
│  │                   (grows ↑)                          │   │
│  │  malloc(), new, dynamically allocated               │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                         BSS                                 │
│            (неинициализированные глобалы)                   │
├─────────────────────────────────────────────────────────────┤
│                        Data                                 │
│           (инициализированные глобалы)                      │
├─────────────────────────────────────────────────────────────┤
│                        Text                                 │
│              (код программы, read-only)                     │
└─────────────────────────────────────────────────────────────┘
Низкие адреса (0x0000...)
```

**Использование модели:**
- Stack overflow → stack встретился с heap или guard page
- Heap fragmentation → дыры в heap области
- Memory leak → heap растёт неограниченно

---

### 🧠 Модель 5: "Дерево решений при page fault"

```
Page Fault произошёл
        │
        ↓
    ┌───────────────────────────┐
    │ Страница в Page Table?    │
    └───────────┬───────────────┘
                │
        ┌───────┴───────┐
        │               │
       НЕТ             ДА
        │               │
        ↓               ↓
   ┌─────────┐    ┌─────────────────┐
   │Lazy alloc│    │ Present bit = 1? │
   │первое   │    └────────┬────────┘
   │обращение│             │
   └────┬────┘     ┌───────┴───────┐
        │          │               │
        ↓         ДА              НЕТ
   MINOR FAULT     │               │
   (аллоцировать   │               ↓
    новый frame)   │         ┌───────────┐
        │          │         │ На диске? │
        ↓          │         └─────┬─────┘
   [1-10 µs]       │               │
                   │       ┌───────┴───────┐
                   │       │               │
                   │      ДА              НЕТ
                   │       │               │
                   │       ↓               ↓
                   │  MAJOR FAULT    SEGFAULT!
                   │  (read from     (невалидный
                   │   swap)          доступ)
                   │       │               │
                   │       ↓               ↓
                   │   [~10 ms]      SIGEGV
                   │
                   ↓
            Protection fault?
            (write to read-only)
                   │
           ┌───────┴───────┐
           │               │
          COW           SIGSEGV
       (copy-on-        (текст
        write)           сегмент,
           │             guard page)
           ↓
      [1-10 µs]
```

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| [[os-overview]] | Базовые концепции ОС, kernel/user mode, syscalls | Предыдущий материал раздела |
| [[os-processes-threads]] | Что такое процесс и его адресное пространство (stack, heap, text) | Предыдущий материал раздела |
| Бинарные числа и адреса | Понимание записи 0x1234, работа с битами | [Number Systems - Khan Academy](https://www.khanacademy.org/computing/computer-science/cryptography/comp-number-theory/v/hexadecimal-number-system) |
| Основы RAM и кэширования | Что такое RAM, L1/L2/L3 кэши, почему доступ к памяти не мгновенный | [Computerphile: CPU Caches](https://www.youtube.com/watch?v=mv8BKLv1aNw) |

**Время на подготовку:** ~3-5 дней если уже знаете предыдущие темы

---

## Терминология для новичков

| Термин | Что это | Аналогия из жизни |
|--------|---------|-------------------|
| **Виртуальный адрес** | Адрес, который видит программа (не реальный!) | Номер квартиры в доме: "кв. 15", хотя физически это 3-й этаж, 2-я дверь слева |
| **Физический адрес** | Реальный адрес ячейки в RAM | GPS-координаты: точное местоположение |
| **Page (страница)** | Единица виртуальной памяти (обычно 4KB) | Страница книги: ОС работает не с буквами, а со страницами целиком |
| **Frame (фрейм)** | Единица физической памяти того же размера (4KB) | Слот в книжном шкафу: одна страница помещается в один слот |
| **Page Table** | Таблица "какая страница в каком слоте" | Каталог библиотеки: "книга X находится на полке Y" |
| **TLB** | Кэш трансляций прямо в CPU (очень быстрый!) | Записная книжка библиотекаря: "недавно выдавал книгу X с полки Y" — не надо идти в каталог |
| **Page Fault** | Обращение к странице, которой нет в RAM | Книги нет на полке — надо идти в хранилище (диск) |
| **Minor Page Fault** | Страница есть, но не записана в таблицу | Книга на столе, но не записана в каталоге — быстро добавить запись |
| **Major Page Fault** | Страница на диске — надо читать (~10ms) | Книга в подвале — курьер идёт за ней (очень долго!) |
| **Swapping** | Перемещение страниц между RAM и диском | Архив: редко читаемые книги отправляются в хранилище |
| **MMU** | Железо в CPU для трансляции адресов | Автоматический переводчик адресов, работает на лету |
| **Thrashing** | Система тратит всё время на swap | Библиотекарь только бегает в хранилище и обратно, некогда обслуживать |
| **OOM Killer** | Убийца процессов при нехватке памяти | Охранник, который выгоняет посетителей, когда библиотека переполнена |
| **Huge Pages** | Большие страницы (2MB или 1GB вместо 4KB) | Альбомы вместо обычных книг: меньше записей в каталоге |

---

## Зачем нужна виртуальная память

### Проблема без виртуальной памяти

Представьте, что программы работают с физическими адресами напрямую:

```
Физическая RAM: 4 GB

Программа A запускается:
  - Использует адреса 0x00000000 - 0x10000000

Программа B запускается:
  - Тоже хочет адреса 0x00000000 - 0x10000000
  - КОНФЛИКТ! Куда её загрузить?

Программа A читает адрес 0x08000000:
  - Может случайно прочитать данные программы B
  - Или повредить kernel
  - НЕТ ИЗОЛЯЦИИ!
```

### Решение: виртуальная память

Каждый процесс имеет собственное **виртуальное адресное пространство**:

```
Процесс A                    Физическая RAM              Процесс B
┌───────────────┐                                        ┌───────────────┐
│ Virtual Addr  │                                        │ Virtual Addr  │
│ 0x00000000    │───────┐   ┌────────────────────┐ ┌─────│ 0x00000000    │
│               │       └──▶│ Frame 42           │◀┘     │               │
│ 0x00001000    │───────────│ Frame 17           │       │ 0x00001000    │
│               │           │ Frame 103          │───────│               │
│ 0x00002000    │───────────│ Frame 256          │       │ 0x00002000    │
└───────────────┘           └────────────────────┘       └───────────────┘

Оба процесса "думают", что их код по адресу 0x00000000.
В реальности они в разных физических фреймах.
Процесс A не может обратиться к памяти процесса B.
```

### Что даёт виртуальная память

**Изоляция:** Каждый процесс в своём "песочнице". Ошибка в одном процессе не затронет другие.

**Упрощение для программ:** Программа всегда загружается по одному адресу (например, 0x400000 на Linux x86-64). Не нужно перемещать код.

**Эффективное использование памяти:** Неиспользуемые страницы можно выгрузить на диск (swap). Можно запускать программы, суммарно превышающие размер RAM.

**Shared memory:** Разные процессы могут отображать одну физическую страницу — для IPC или shared libraries.

---

## Как работает paging

### Page и Frame

Память делится на блоки фиксированного размера:
- **Page** — блок в виртуальном адресном пространстве
- **Frame** — блок в физической памяти
- Размер одинаковый, обычно **4 KB** (4096 байт)

```
Виртуальный адрес (64-bit на x86-64):
┌─────────────────────────────────────┬───────────────┐
│         Page Number (52 bits)       │ Offset (12)   │
└─────────────────────────────────────┴───────────────┘

Offset (12 bits) = 2^12 = 4096 = размер страницы
Page Number указывает на запись в Page Table
```

### Page Table

Page Table — массив, где индекс = номер виртуальной страницы, значение = номер физического фрейма + флаги.

```
Page Table процесса A:
┌────────┬─────────────────────────────────────────┐
│ Index  │ Entry                                   │
├────────┼─────────────────────────────────────────┤
│ 0      │ Frame 42  | Present=1, R/W=1, User=1   │
│ 1      │ Frame 17  | Present=1, R/W=0, User=1   │  ← read-only
│ 2      │ Disk sector 5824 | Present=0           │  ← swapped out
│ 3      │ NULL      | Present=0                   │  ← not allocated
│ ...    │ ...                                     │
└────────┴─────────────────────────────────────────┘

Флаги:
- Present: страница в RAM (1) или на диске (0)
- R/W: разрешена запись
- User: доступна из user mode
- Dirty: страница была изменена
- Accessed: страница была прочитана
```

### Трансляция адресов

```
Программа обращается к адресу 0x00001234:

1. CPU извлекает номер страницы: 0x00001234 / 4096 = 1
2. CPU ищет страницу 1 в Page Table
3. Page Table говорит: страница 1 → Frame 17
4. CPU вычисляет физический адрес: Frame 17 * 4096 + offset
   = 17 * 4096 + 0x234 = 0x11234

┌─────────────────┐          ┌─────────────────┐
│ Virtual: 0x1234 │   MMU    │ Physical:       │
│ Page=1, Off=234 │─────────▶│ Frame=17, Off=234│
└─────────────────┘          └─────────────────┘
```

### Многоуровневые Page Tables

На 64-bit системе адресное пространство огромно (2^64 байт). Хранить плоскую таблицу для всех возможных страниц невозможно.

Решение — **многоуровневые таблицы**:

```
x86-64 использует 4-level paging:

Виртуальный адрес (48 значащих бит):
┌────────┬────────┬────────┬────────┬────────────┐
│ PML4   │ PDPT   │ PD     │ PT     │ Offset     │
│ (9bit) │ (9bit) │ (9bit) │ (9bit) │ (12bit)    │
└────────┴────────┴────────┴────────┴────────────┘

PML4 (Page Map Level 4) → PDPT → PD (Page Directory) → PT (Page Table)

CR3 регистр указывает на PML4

Каждый уровень: 512 записей × 8 байт = 4 KB (одна страница)
```

**Зачем многоуровневые:** Выделяются только нужные таблицы. Если процесс использует 10 MB памяти — нужно несколько таблиц, не миллионы записей.

---

## TLB: кэш трансляции

> **Подумай:** 4-уровневая таблица страниц = 4 чтения из памяти на каждое обращение. Если кэш L3 даёт ~40ns latency, то одно обращение к памяти превращается в 5 обращений (4 page table + 1 данные) = 200ns минимум. Почему тогда виртуальная память не делает систему в 5 раз медленнее?

Каждое обращение к памяти требует прохода по 4 уровням Page Table — это 4 дополнительных чтения из памяти. Если бы каждое обращение к виртуальной памяти требовало 4 дополнительных чтений, виртуальная память была бы непрактично медленной. К счастью, обращения к памяти имеют высокую временну́ю и пространственную локальность — программы обычно работают с небольшим набором страниц в каждый момент времени.

**TLB (Translation Lookaside Buffer)** — небольшой, но очень быстрый кэш в CPU, хранящий недавние трансляции виртуальных адресов в физические. TLB является частью MMU (Memory Management Unit) и работает полностью в hardware:

```
TLB (типичный размер: 64-512 записей)
┌────────────────┬──────────────┐
│ Virtual Page   │ Physical Frame│
├────────────────┼──────────────┤
│ 0x00001        │ Frame 17     │
│ 0x00002        │ Frame 42     │
│ 0x00005        │ Frame 103    │
└────────────────┴──────────────┘

TLB Hit:  ~1 cycle (практически бесплатно)
TLB Miss: ~100-1000+ cycles (page table walk)
```

**TLB Hit vs Miss:** При TLB hit трансляция происходит за 1 такт CPU — это часть instruction pipeline, не добавляющая задержки. При TLB miss CPU (или операционная система, в зависимости от архитектуры) должен выполнить page table walk — пройти все уровни таблицы страниц. На x86-64 это 4 чтения из памяти. TLB miss может быть дороже, чем cache miss для данных, потому что требуется несколько последовательных чтений памяти.

TLB обычно полностью ассоциативный (fully associative) — любая трансляция может находиться в любой ячейке, и hardware ищет параллельно во всех. Это даёт низкий miss rate, а поскольку TLB маленький, стоимость такого поиска невысока.

### TLB и context switch

При переключении процессов TLB нужно очищать (flush), потому что у нового процесса другие трансляции. Это одна из причин, почему context switch дорогой.

**PCID (Process Context ID):** Современные CPU поддерживают тегирование записей TLB идентификатором процесса. При switch не нужен полный flush — записи с другим PCID просто игнорируются.

---

## Page Fault: страница не в памяти

Когда программа обращается к странице с Present=0, возникает **page fault** — исключение, которое обрабатывает ядро.

### Типы page faults

**Minor (soft) page fault:** Страница есть в памяти, но не в Page Table процесса.
- Пример: первое обращение к выделенной, но ещё не использованной памяти
- Ядро выделяет фрейм, обновляет Page Table
- Время: ~1µs

**Major (hard) page fault:** Страницы нет в RAM, она на диске (swap).
- Ядро читает страницу с диска
- Время: ~10ms (HDD) или ~0.1ms (SSD)
- Это в 10,000-100,000 раз медленнее!

```
┌─────────────────────────────────────────────────────────────────┐
│                    PAGE FAULT HANDLER                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. CPU генерирует page fault exception                         │
│  2. Сохраняется контекст (регистры)                             │
│  3. Управление передаётся kernel page fault handler             │
│                                                                 │
│  4. Kernel проверяет причину:                                   │
│     ┌─────────────────────────────────────────────────────────┐│
│     │ Страница allocated, но не mapped?                       ││
│     │   → Minor fault: выделить frame, обновить page table    ││
│     ├─────────────────────────────────────────────────────────┤│
│     │ Страница в swap?                                        ││
│     │   → Major fault: читать с диска, может занять 10ms      ││
│     ├─────────────────────────────────────────────────────────┤│
│     │ Copy-on-Write страница?                                 ││
│     │   → Скопировать страницу, дать процессу копию           ││
│     ├─────────────────────────────────────────────────────────┤│
│     │ Адрес невалидный (segmentation fault)?                  ││
│     │   → Отправить SIGSEGV, убить процесс                    ││
│     └─────────────────────────────────────────────────────────┘│
│                                                                 │
│  5. Восстановить контекст                                       │
│  6. Повторить инструкцию, вызвавшую fault                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Demand Paging и Overcommit

### Demand Paging

Ядро не выделяет физическую память сразу при malloc/mmap. Страницы выделяются при первом обращении (demand).

```c
char* buffer = malloc(1 GB);  // Виртуальная память выделена
                               // Физическая — НЕТ (ещё)

buffer[0] = 'X';  // Page fault → выделяется 1 страница (4KB)
buffer[4096] = 'Y';  // Page fault → ещё 1 страница

// Использовали 8 KB физической памяти, хотя "выделили" 1 GB
```

### Memory Overcommit

Linux по умолчанию разрешает выделять больше памяти, чем есть физически. Это работает, потому что программы редко используют всю запрошенную память.

```
Физическая RAM: 8 GB
Swap: 2 GB

Процесс A: malloc(6 GB) ← OK
Процесс B: malloc(5 GB) ← OK (overcommit!)

Суммарно "выделено" 11 GB, есть только 10 GB (RAM + swap).

Если оба процесса реально используют всю память:
→ OOM Killer убивает процессы
```

**OOM Killer:** Когда память действительно заканчивается и выделить страницу невозможно, ядро активирует OOM Killer. Его задача — убить процесс(ы), чтобы освободить память и позволить системе продолжить работу.

**Как OOM Killer выбирает жертву:**

Для каждого процесса ядро вычисляет `oom_score` от 0 до 1000:
- Базовый score = процент используемой памяти от доступной × 10
- Процесс, использующий всю доступную память = score 1000
- Процесс, не использующий память = score 0
- Процессы root получают небольшой "бонус" (−30)
- К score добавляется `oom_score_adj` процесса (от −1000 до +1000)

Процесс с наивысшим score убивается первым.

```bash
# Проверить overcommit настройки
cat /proc/sys/vm/overcommit_memory
# 0 = heuristic (default) — ядро решает, разрешить ли overcommit
# 1 = always overcommit — всегда разрешать (опасно!)
# 2 = never overcommit — никогда не разрешать (строго)

# Посмотреть oom_score процесса
cat /proc/<pid>/oom_score

# Настроить приоритет для OOM Killer
echo -1000 > /proc/<pid>/oom_score_adj  # Полная защита (не убивать)
echo 1000 > /proc/<pid>/oom_score_adj   # Убить первым

# Для systemd сервисов — в unit файле:
# [Service]
# OOMScoreAdjust=-1000
```

**Практический совет:** Для критических сервисов (базы данных, message broker) установите `oom_score_adj = -1000`. Но помните: если все процессы защищены, а память закончилась — система зависнет.

---

## Swapping и Thrashing

### Swapping

Когда RAM заканчивается, ядро выгружает редко используемые страницы на диск (swap space):

```
RAM почти заполнена:
┌─────────────────────────────────────────┐
│ Process A pages │ Process B │ Process C │ ← RAM full
└─────────────────────────────────────────┘

Process D требует память:
1. Kernel выбирает "жертву" — страницу, к которой давно не обращались
2. Если страница dirty — записывает на swap
3. Освобождает frame
4. Выделяет frame для Process D

┌─────────────────────────────────────────┐
│ Process A (меньше) │ B │ C │ Process D  │
└─────────────────────────────────────────┘
         │
         └──▶ Swap (диск): часть страниц Process A
```

### Алгоритмы выбора жертвы (Page Replacement)

**LRU (Least Recently Used):** Вытесняем страницу, к которой дольше всего не обращались. Идеально, но требует отслеживания каждого доступа — дорого.

**Clock (Second Chance):** Приближение к LRU. Страницы в кольцевом буфере, каждая имеет "accessed" бит. Стрелка движется по кругу, сбрасывая биты. Страница без accessed бита — жертва.

**Linux:** Использует двухсписочную систему (active/inactive lists) с аппроксимацией LRU.

### Thrashing

**Thrashing** — катастрофическая ситуация, когда система тратит больше времени на swapping, чем на полезную работу:

```
Симптомы thrashing:
- CPU utilization низкий (ждём I/O)
- Disk I/O 100%
- Система еле отвечает

Причина: рабочий набор (working set) процессов > RAM

Процесс A работает → page fault (страница в swap) → ждём 10ms
Пока ждём, Process B работает → page fault → ждём
Process A получает страницу, продолжает
→ page fault (другая страница выгружена, пока ждали)
→ бесконечный цикл
```

**Решение:** Добавить RAM, уменьшить количество процессов, или убить процессы с большим working set.

---

## Huge Pages

### Проблема с 4KB страницами

При больших объёмах памяти 4KB страницы создают проблемы:

```
Приложение использует 64 GB RAM:
  64 GB / 4 KB = 16,777,216 страниц
  16M записей в Page Table

TLB типичного CPU: ~1500 записей
TLB coverage: 1500 × 4 KB = 6 MB

При обращении к памяти вне 6 MB → TLB miss → медленно
```

### Huge Pages (2MB, 1GB)

Современные CPU поддерживают большие страницы:

```
2 MB huge pages:
  64 GB / 2 MB = 32,768 страниц
  TLB coverage: 1500 × 2 MB = 3 GB

1 GB huge pages:
  64 GB / 1 GB = 64 страницы
  TLB coverage: 64 × 1 GB = 64 GB (вся память!)
```

### Использование в Linux

```bash
# Проверить поддержку
cat /proc/meminfo | grep Huge

# Transparent Huge Pages (автоматически)
cat /sys/kernel/mm/transparent_hugepage/enabled
# [always] madvise never

# Explicit huge pages (для databases)
echo 1024 > /proc/sys/vm/nr_hugepages  # Выделить 1024 × 2MB = 2GB
```

**Когда использовать:** Базы данных, JVM с большим heap, in-memory кэши. JVM поддерживает huge pages через `-XX:+UseLargePages`.

---

## Подводные камни виртуальной памяти

Виртуальная память создаёт иллюзии, которые могут обмануть разработчика.

### 1. Память "выделена", но не существует

```c
char* buf = malloc(1 GB);  // Успешно!
// buf != NULL, но физической памяти нет

memset(buf, 0, 1 GB);  // Page faults, может вызвать OOM Killer
```

**Ловушка:** `malloc` вернул указатель, но система overcommit. При реальном использовании памяти может не хватить.

**Решение:** Если критично — отключите overcommit (`vm.overcommit_memory=2`) или сразу делайте `memset`/`mlock`.

### 2. RSS vs VSZ — что реально

```bash
# top показывает:
#   PID  VIRT  RES  SHR
#   123  8.0g 2.1g 120m
```

| Метрика | Значение | Что это |
|---------|----------|---------|
| VIRT (VSZ) | 8 GB | Виртуальное — включает всё зарезервированное |
| RES (RSS) | 2.1 GB | Resident — физически в RAM сейчас |
| SHR | 120 MB | Shared — разделяется с другими процессами |

**Ловушка:** Смотришь на VIRT и паникуешь. На самом деле RSS — реальное потребление.

### 3. Shared libraries считаются многократно

```
libc.so: 2 MB × 100 процессов = 200 MB в сумме RSS?
Нет! Физически одна копия 2 MB, shared между процессами.
```

**Ловушка:** Суммирование RSS всех процессов даёт завышенную цифру.

### 4. JVM в контейнере

```yaml
# Docker: memory limit 4GB
resources:
  limits:
    memory: 4Gi
```

```
JVM: -Xmx4g
→ JVM думает, что есть 4GB для heap
→ + metaspace, thread stacks, native memory
→ Суммарно > 4GB → OOM Kill контейнера!
```

**Решение:** `-Xmx` должен быть меньше лимита контейнера. Правило: `-Xmx` ≤ 75% container memory limit.

### 5. NUMA — неравномерный доступ к памяти

На multi-socket серверах память "привязана" к CPU:
```
CPU 0 ←→ RAM банк 0 (быстро: ~80ns)
CPU 0 ←→ RAM банк 1 (медленно: ~150ns через interconnect)
```

**Ловушка:** Процесс мигрирует между CPU, но память осталась на старом банке → latency растёт.

**Решение:** `numactl --localalloc` или настройка NUMA policy для критичных приложений.

---

## Связь с JVM

### JVM Heap и Virtual Memory

JVM heap — это виртуальная память, управляемая JVM:

```
JVM запускается с -Xmx8g:

1. JVM делает mmap() для резервирования 8 GB виртуальной памяти
2. Физическая память выделяется по мере создания объектов (demand paging)
3. GC управляет объектами внутри этого региона

┌─────────────────────────────────────────────────────────────────┐
│                         JVM Process                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────┐      │
│  │ JVM Heap (8 GB virtual)                               │      │
│  │ ┌─────────────┬────────────────┬────────────────────┐│      │
│  │ │ Young Gen   │ Old Gen        │ (не выделено физ.) ││      │
│  │ │ 1 GB phys   │ 3 GB phys      │ 4 GB virtual only  ││      │
│  │ └─────────────┴────────────────┴────────────────────┘│      │
│  └──────────────────────────────────────────────────────┘      │
│                                                                 │
│  Metaspace, Thread Stacks, Native Memory...                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### GC и Page Faults

Full GC сканирует весь heap. Если часть heap в swap — катастрофа:

```
Full GC на 8 GB heap:
- Heap полностью в RAM: ~500ms
- 2 GB heap в swap: GC читает с диска → 10+ секунд!

Правило: JVM heap НЕ ДОЛЖЕН уходить в swap.
Установите -Xmx меньше доступной RAM.
```

Подробнее — в [[jvm-gc-tuning]].

---

## Практические команды Linux

```bash
# Общая информация о памяти
free -h
cat /proc/meminfo

# Память процесса
cat /proc/<pid>/status | grep -i mem
pmap -x <pid>

# Page faults процесса
cat /proc/<pid>/stat | awk '{print "minor:", $10, "major:", $12}'

# Мониторинг в реальном времени
vmstat 1          # Общая статистика
sar -B 1          # Page faults
sar -W 1          # Swapping

# Очистить page cache (для тестов)
sync; echo 3 > /proc/sys/vm/drop_caches
```

---

## Кто использует и реальные примеры

### OOM Killer в реальности

| Компания/Сценарий | Проблема | Решение |
|-------------------|----------|---------|
| **Apache + MySQL на одном сервере** | OOM Killer часто убивает Apache или MySQL первыми (большое потребление памяти) | Разнести на разные серверы или настроить oom_score_adj |
| **Kubernetes/Docker** | Контейнер превышает memory limit → OOM Kill | Установить реалистичные limits, мониторинг memory pressure |
| **JVM в контейнере** | `-Xmx4g` + metaspace + native = >4GB → OOM Kill | `-Xmx` ≤ 75% container limit, использовать `-XX:+UseContainerSupport` |
| **Redis/MongoDB** | Фоновые операции (backup, compaction) увеличивают memory → OOM | Настроить maxmemory policy, мониторинг |

### Huge Pages в продакшене

| Приложение | Настройка | Эффект |
|------------|-----------|--------|
| **Oracle Database** | Explicit huge pages (2MB) | TLB misses ↓ на порядок, latency ↓ 5-10% |
| **PostgreSQL** | `huge_pages = on` | Меньше TLB misses при работе с shared_buffers |
| **JVM (HotSpot)** | `-XX:+UseLargePages` | GC быстрее, меньше pauses на больших heap |
| **DPDK/High-freq trading** | 1GB huge pages | Критично для наносекундных latency |

### Thrashing в реальных системах

**Симптомы thrashing (что увидите в мониторинге):**
- CPU utilization низкий (20-30%), хотя система "загружена"
- Disk I/O 100% (iowait высокий)
- Load average растёт
- Приложения не отвечают или очень медленны
- `vmstat` показывает постоянный si/so (swap in/out)

**Пример из практики:**
```
# vmstat 1 на сервере с thrashing:
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 5 12 4194304  12340   1024  50000  8000 9000 80000 90000 5000 8000  5  5  0 90  0
                                    ↑ постоянный swap in/out, 90% iowait
```

### Практические числа для архитектурных решений

| Операция | Время | Сравнение |
|----------|-------|-----------|
| TLB hit | ~1 cycle (~0.3ns) | Бесплатно |
| TLB miss (page table walk) | 100-1000 cycles (~30-300ns) | 100x медленнее |
| Minor page fault | ~1µs | 3000x медленнее TLB hit |
| Major page fault (SSD) | ~0.1ms | 300,000x медленнее TLB hit |
| Major page fault (HDD) | ~10ms | 30,000,000x медленнее TLB hit |

**Вывод:** Одна major page fault = миллионы обычных операций. Избегайте swap для критичных приложений!

---

## Связь с другими темами

**[[os-overview]]** — Обзор ОС объясняет разделение kernel mode и user mode, без которого невозможно понять, почему page fault — это дорогая операция: при обращении к отсутствующей странице CPU генерирует исключение, происходит переход в kernel mode, ядро находит нужную страницу (или загружает с диска), обновляет page table и возвращает управление. Системные вызовы mmap(), brk(), madvise() — это интерфейсы kernel для управления виртуальной памятью, каждый требующий перехода через syscall boundary. Понимание архитектуры ядра также объясняет, почему ядро имеет собственное виртуальное адресное пространство (kernel space), отображённое в верхнюю часть адресного пространства каждого процесса.

**[[os-processes-threads]]** — Процессы и потоки непосредственно определяют структуру виртуальной памяти: каждый процесс получает изолированное адресное пространство с сегментами text, data, heap, stack, а каждый поток — собственный стек внутри этого пространства. Page table — это структура данных, привязанная к процессу: при context switch между процессами ядро переключает CR3 (указатель на page table), что приводит к сбросу TLB и последующим TLB miss. Механизм fork() использует Copy-on-Write — один из ключевых примеров взаимодействия управления процессами и памятью: вместо копирования страниц ядро помечает их read-only и копирует только при записи. Понимание этой связи критично для оптимизации серверных приложений, где fork() используется для создания worker processes.

**[[jvm-memory-model]]** — JVM heap целиком находится в виртуальной памяти процесса, и его поведение напрямую зависит от механизмов paging и виртуальной памяти ОС. Когда JVM запрашивает -Xmx8g, ОС выделяет виртуальные страницы, которые отображаются в физическую память по мере первого обращения (demand paging), поэтому RSS может быть значительно меньше heap size. Memory ordering и visibility гарантии JMM (volatile, happens-before) реализуются через memory barriers процессора и страничные атрибуты, что связывает Java-модель памяти с аппаратными механизмами. При memory pressure ОС может вытеснять страницы JVM heap в swap, создавая непредсказуемые latency spikes при обращении к объектам.

**[[jvm-gc-tuning]]** — Настройка сборщика мусора тесно связана с виртуальной памятью, потому что GC должен обходить весь heap для поиска мусора, и каждое обращение к swap-странице — это major page fault со стоимостью 0.1-10 ms. Full GC на 8 GB heap может занять 500 ms в RAM, но 10+ секунд если часть heap в swap, что приводит к катастрофическим stop-the-world паузам. Huge pages (2 MB вместо 4 KB) значительно улучшают производительность GC, сокращая количество TLB entries: для 8 GB heap нужно 2M записей TLB при 4 KB страницах vs 4K записей при 2 MB. Понимание paging также объясняет, почему рекомендуется отключать swap для JVM-серверов и использовать -XX:+AlwaysPreTouch для предварительного fault-in всех страниц при старте.

**Связанные концепции:**
- [[os-virtualization]] — контейнеры используют cgroups для ограничения памяти, что влияет на OOM поведение

---

## Рекомендуемые источники

### Учебники

- Tanenbaum A., Bos H. (2014). *"Modern Operating Systems, 4th Edition."* — глава 3 (Memory Management) охватывает всё от базового swapping до сегментации и paging; глава 4 (File Systems) показывает связь виртуальной памяти с файлами через memory-mapped I/O.
- Silberschatz A., Galvin P., Gagne G. (2018). *"Operating System Concepts, 10th Edition."* — главы 9 (Main Memory) и 10 (Virtual Memory) детально разбирают page replacement алгоритмы (LRU, Clock, Working Set) с формальным анализом; полезен для подготовки к интервью.
- Arpaci-Dusseau R., Arpaci-Dusseau A. (2018). *"Operating Systems: Three Easy Pieces."* — главы 13-24 (Virtual Memory) — лучшее бесплатное объяснение от address spaces до page replacement с практическими симуляторами.
- Bryant R., O'Hallaron D. (2015). *"Computer Systems: A Programmer's Perspective, 3rd Edition."* — глава 9 (Virtual Memory) даёт уникальную перспективу программиста: от VM как инструмента кэширования до malloc-реализации и memory-related bugs.
- Love R. (2010). *"Linux Kernel Development, 3rd Edition."* — главы 12 (Memory Management) и 15 (The Process Address Space) описывают реализацию mm_struct, VMA, page allocator и slab allocator в ядре Linux.

### Книги и курсы
- [OSTEP: Virtual Memory chapters](https://pages.cs.wisc.edu/~remzi/OSTEP/vm-intro.pdf) — бесплатная книга, главы 13-24 о виртуальной памяти
- [MIT 6.S081: Virtual Memory lab](https://pdos.csail.mit.edu/6.1810/2025/) — практическая работа с page tables в xv6

### Официальная документация
- [Linux Memory Management](https://www.kernel.org/doc/gorman/html/understand/understand016.html) — документация ядра Linux
- [Oracle: OOM Killer configuration](https://www.oracle.com/technical-resources/articles/it-infrastructure/dev-oom-killer.html) — практическое руководство

### Статьи и туториалы
- [GeeksforGeeks: Virtual Memory](https://www.geeksforgeeks.org/operating-systems/virtual-memory-in-operating-system/) — базовое объяснение
- [GeeksforGeeks: Paging](https://www.geeksforgeeks.org/operating-systems/paging-in-operating-system/) — детали paging
- [Baeldung: Cache Miss, TLB Miss, Page Fault](https://www.baeldung.com/cs/cache-tlb-miss-page-fault) — сравнение типов промахов
- [LWN: Huge Pages deep dive](https://lwn.net/Articles/379748/) — детальный анализ huge pages

### Практические ресурсы
- [Last9: Linux OOM Killer Guide](https://last9.io/blog/understanding-the-linux-oom-killer/) — практическое руководство по OOM
- [IBM: Solving OOM Killer puzzle](https://www.ibm.com/think/topics/out-of-memory-killer) — реальные примеры и решения

---

*Обновлено: 2026-01-09 — добавлены педагогические секции (5 аналогий: пазл/доска, офисное здание, телефонная книга/TLB, thrashing как пробка на мосту, OOM Killer как охранник; 6 типичных ошибок с СИМПТОМ/РЕШЕНИЕ: virtual vs physical, цена major fault, TLB miss, overcommit/OOM, COW после fork, Huge Pages; 5 ментальных моделей: трансляция адресов, TLB как L0, жизненный цикл страницы, карта памяти процесса, дерево решений page fault)*

---

## Проверь себя

> [!question]- Процесс использует 4 GB виртуальной памяти, но top показывает RSS 200 MB. Объясни, что происходит, и почему это нормальное поведение.
> Виртуальная память (VIRT/VSZ) — адресное пространство, включающее все маппинги: shared libraries (mmap но не accessed), memory-mapped файлы (только заголовки прочитаны), зарезервированный но не committed heap. RSS (Resident Set Size) — страницы, реально находящиеся в физической RAM. 4 GB виртуальных ≠ 4 GB физических благодаря demand paging: страница загружается в RAM только при первом обращении (page fault). Это нормально — типичный JVM-процесс с -Xmx2G может иметь VIRT 8 GB, но RSS 500 MB. Проблема начинается, когда RSS приближается к физической памяти — тогда начинается swapping.

> [!question]- Сервер с 16 GB RAM и 50 процессами начал резко тормозить, load average вырос до 200+, но CPU usage всего 5%. Диагностируй проблему и предложи 3 уровня решения.
> Это thrashing: рабочие множества процессов превышают физическую RAM, ядро тратит всё время на page fault → disk I/O → swap in/out. CPU idle потому что процессы в состоянии D (uninterruptible sleep), ожидая I/O. Диагностика: `vmstat 1` покажет высокие si/so (swap in/out), `sar -B` покажет pgpgin/pgpgout. **Решение L1:** Увеличить RAM или уменьшить количество процессов. **Решение L2:** Настроить `vm.swappiness=10` (предпочитать вытеснение page cache, не анонимных страниц), включить cgroups memory limits для изоляции. **Решение L3:** Отключить swap для критичных сервисов (JVM, Redis), использовать huge pages (2 MB) для уменьшения TLB pressure, настроить oom_score_adj для приоритизации важных процессов.

> [!question]- Почему multi-level page tables (2-4 уровня) используются вместо одноуровневой, хотя добавляют дополнительные memory accesses при трансляции?
> Одноуровневая таблица для 48-bit address space с 4 KB страницами = 2^36 записей × 8 байт = 512 GB на процесс — невозможно. Multi-level (PGD→PUD→PMD→PTE на x86-64) экономят память через sparse mapping: если процесс использует только 100 MB из 128 TB адресного пространства, большинство PGD-записей = NULL, и целые поддеревья таблиц не создаются. Стоимость дополнительных lookups (4 memory access вместо 1) компенсируется TLB: при TLB hit rate 99%+ только 1% обращений проходит полный page walk. Inverted page tables (PowerPC, IA-64) — альтернатива с O(1) по физической памяти, но требуют hash lookup и не поддерживают sharing.

> [!question]- В чём принципиальная разница между Copy-on-Write при fork() и memory-mapped файлами через mmap()? Где каждый механизм используется и почему?
> **COW при fork():** Родитель и потомок разделяют одни и те же физические страницы (PTE маркируются read-only). При первой записи — page fault → ядро копирует только изменённую страницу. Используется для: fork+exec (exec заменяет всё адресное пространство, поэтому копирование было бы бессмысленным), создание снапшотов (Redis BGSAVE). **mmap():** Файл на диске маппится в виртуальное адресное пространство. Чтение = page fault → ядро загружает блок файла в page cache → маппит страницу. Запись (MAP_SHARED) → изменения видны другим процессам и записываются на диск при msync/munmap. Используется для: работа с большими файлами без read() (SQLite, databases), shared memory между процессами (MAP_SHARED|MAP_ANONYMOUS). Ключевая разница: COW — механизм отложенного копирования анонимных страниц; mmap — механизм маппирования файлов/памяти в адресное пространство.

---

## Ключевые карточки

Что такое demand paging и почему это не оптимизация, а необходимость?
?
Demand paging загружает страницу в RAM только при первом обращении (page fault). Это необходимость, не оптимизация: процесс с 4 GB address space не может занять 4 GB RAM при старте — физической памяти не хватит на все процессы. Без demand paging потребовалось бы загрузить весь исполняемый файл, все shared libraries и весь heap в RAM до начала работы.

Чем отличается minor page fault от major page fault?
?
Minor (soft): страница уже в RAM (в page cache или другой области), нужно только обновить page table entry — стоимость ~1 µs. Major (hard): страницы нет в RAM, нужно загрузить с диска (SSD: 25-100 µs, HDD: 3-10 ms). Major faults в 100-10000x дороже. `perf stat -e page-faults,major-faults` покажет соотношение. Высокий % major faults → нехватка RAM.

Как работает TLB и почему context switch дорогой для памяти?
?
TLB (Translation Lookaside Buffer) — аппаратный кэш трансляций virtual → physical address. При TLB hit трансляция за 1 цикл (~0.5-1 ns), при miss — полный page walk через 4 уровня page tables (4 memory access ≈ 20-100 ns). При context switch TLB сбрасывается (flush) потому что у нового процесса другие page tables. PCID (Process Context ID, x86-64) позволяет хранить записи разных процессов в TLB одновременно, избегая полного flush — экономия ~100 ns на switch.

Что делает OOM Killer и по какому принципу выбирает жертву?
?
Когда физическая RAM + swap исчерпаны, ядро Linux вызывает OOM Killer. Выбор жертвы по oom_score (0-1000): учитывает RSS процесса, время работы, привилегии (root получает бонус). `oom_score_adj` (-1000..+1000) позволяет защитить процессы: `-1000` = никогда не убивать (для systemd, sshd), `+1000` = убить первым. `dmesg | grep -i oom` показывает логи. Важно: overcommit (vm.overcommit_memory) позволяет выделить больше виртуальной памяти, чем есть физической — это причина, по которой OOM случается не при malloc(), а при записи.

Назови 3 основных алгоритма page replacement и их trade-offs.
?
**LRU (Least Recently Used):** Вытесняет давно не использованную страницу. Оптимален для temporal locality, но дорог в реализации (нужно обновлять timestamp при каждом access). **Clock (Second Chance):** Приближение LRU через reference bit: при обходе списка, если R=1 → сбросить R и дать второй шанс; если R=0 → вытеснить. O(1) amortized, используется в Linux. **Working Set:** Удерживает в RAM страницы, к которым обращались за последние Δ единиц времени. Предотвращает thrashing, но сложен в настройке Δ. Linux комбинирует Clock + Working Set через active/inactive LRU lists.

В чём разница между vm.swappiness=0 и отключением swap?
?
`vm.swappiness=0`: ядро **может** использовать swap, но только в экстремальной ситуации (когда free + file-backed pages ≤ high watermark). Анонимные страницы вытесняются последними. `swapoff -a`: swap полностью отключён, ядро не может вытеснять анонимные страницы — при исчерпании RAM сразу OOM Kill. Для production серверов (JVM, Redis) рекомендуется `swappiness=1` (не 0): оставляет минимальный swap как safety net, но предпочитает вытеснять page cache.

Что такое huge pages и когда они критически важны?
?
Huge pages (2 MB или 1 GB вместо стандартных 4 KB) сокращают количество TLB-записей: для 8 GB данных нужно 2M записей при 4 KB vs 4K записей при 2 MB — TLB hit rate растёт с 95% до 99.9%+. Критичны для: баз данных (большие buffer pools), JVM (большой heap), HPC. Linux: Transparent Huge Pages (THP) — автоматически, но вызывают stalls при compaction. Explicit: `hugetlbfs`, настройка через `vm.nr_hugepages`. Redis и MongoDB рекомендуют отключать THP из-за latency spikes.

Как Copy-on-Write экономит память при fork()?
?
При `fork()` ядро НЕ копирует физические страницы — parent и child разделяют одни и те же PTE, маркированные read-only. При первой записи любым из процессов — protection fault → ядро копирует только изменённую страницу (4 KB), обновляет PTE. Экономия: `fork()` стоит ~1 ms вместо копирования всего address space (100+ ms для 1 GB). Используется в Redis BGSAVE: fork создаёт снапшот состояния, пока parent продолжает обслуживать запросы. COW-overhead = только изменённые за время snapshot страницы.

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Следующий шаг | [[os-processes-threads]] | Процессы — потребители виртуальной памяти; address space создаётся при fork() |
| Следующий шаг | [[os-scheduling]] | Планировщик учитывает memory pressure при выборе процесса |
| Углубиться | [[os-file-systems]] | mmap() связывает файловую систему с виртуальной памятью через page cache |
| Углубиться | [[os-virtualization]] | Контейнеры ограничивают память через cgroups; nested paging в VM |
| Смежная тема | [[jvm-memory-model]] | JVM heap, GC и взаимодействие со страничной памятью ОС |
| Смежная тема | [[memory-model-fundamentals]] | Аппаратная модель памяти: кэши, когерентность, memory ordering |
| Обзор | [[os-overview]] | Вернуться к карте раздела Operating Systems |

---

*Последнее обновление: 2026-02-14*
*Проверено: 2026-02-14*
