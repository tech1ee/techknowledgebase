---
title: "Ввод/Вывод и устройства: как CPU общается с внешним миром"
created: 2025-12-02
modified: 2025-12-02
type: deep-dive
status: published
area: operating-systems
confidence: high
tags:
  - topic/os
  - io
  - interrupts
  - dma
  - devices
  - type/deep-dive
  - level/intermediate
related:
  - "[[os-overview]]"
  - "[[os-processes-threads]]"
  - "[[os-file-systems]]"
  - "[[os-memory-management]]"
prerequisites:
  - "[[os-processes-threads]]"
  - "[[os-memory-management]]"
reading_time: 40
difficulty: 7
study_status: not_started
mastery: 0
last_reviewed:
next_review:
---

# Ввод/Вывод и Устройства

Подсистема ввода/вывода — это мост между процессором и внешним миром: дисками, сетью, клавиатурой, GPU. Центральная проблема I/O заключается в огромной разнице скоростей: современный CPU выполняет миллиарды операций в секунду, тогда как жёсткий диск отвечает за 10 миллисекунд, а сетевой пакет может идти 100 миллисекунд. Заставлять процессор ждать — расточительство. Поэтому ОС использует различные техники (прерывания, DMA, асинхронный I/O), чтобы CPU мог заниматься полезной работой, пока данные передаются.

---

## TL;DR

> **Проблема:** CPU в 1,000,000x быстрее диска. Ждать = тратить ресурсы.
>
> **Три способа I/O:**
> - **Polling** — CPU в цикле проверяет "готово?". Простой, но 100% CPU.
> - **Interrupts** — устройство сигналит "готово!". Эффективнее, но overhead на каждый interrupt.
> - **DMA** — устройство пишет в RAM само. CPU только инициирует и получает interrupt по завершении.
>
> **epoll vs io_uring:** epoll — "скажи когда готово" (2002). io_uring — "делай всё async" (2019). io_uring эффективнее для file I/O.
>
> **Zero-copy:** sendfile() отправляет файл по сети без копирования в userspace. До 65% экономии CPU.
>
> **Для NVMe:** I/O scheduler = none. У SSD нет seek time, планировщик только добавляет latency.

---

## Часть 1: Интуиция без кода

### Аналогия 1: Три способа ожидания курьера

Представьте, что вы ждёте курьера с посылкой:

**Polling (активное ожидание):**
Вы каждые 5 секунд подходите к двери и смотрите в глазок: "Приехал? Нет? А сейчас?" Вы ничего не можете делать — только проверять дверь. Это 100% вашего времени потрачено на ожидание.

**Interrupts (прерывание):**
Вы занимаетесь своими делами, а когда курьер приезжает — он звонит в дверной звонок. Вы прерываете работу, идёте к двери, принимаете посылку, возвращаетесь к делам. Эффективнее, но каждый звонок отвлекает от работы.

**DMA (прямая доставка):**
Курьер имеет ключ от вашего почтового ящика. Он кладёт посылку прямо в ящик и уходит, а вам приходит SMS "посылка доставлена". Вы вообще не отвлекались на процесс доставки — только на получение уведомления.

```
┌─────────────────────────────────────────────────────────────┐
│                     POLLING                                   │
│  ┌─────┐                           ┌─────┐                   │
│  │ CPU │ ← "Готово?" → "Нет" →     │ DEV │                   │
│  │     │ ← "Готово?" → "Нет" →     │     │                   │
│  │     │ ← "Готово?" → "Да!" →     │     │                   │
│  └─────┘                           └─────┘                   │
│  CPU занят 100% времени ожидания                             │
├─────────────────────────────────────────────────────────────┤
│                    INTERRUPTS                                 │
│  ┌─────┐                           ┌─────┐                   │
│  │ CPU │ ← работает →              │ DEV │                   │
│  │     │      ← IRQ! ←             │     │                   │
│  │     │ → обработка → продолжает  │     │                   │
│  └─────┘                           └─────┘                   │
│  CPU работает, но прерывается для обработки                  │
├─────────────────────────────────────────────────────────────┤
│                       DMA                                     │
│  ┌─────┐         ┌─────┐           ┌─────┐                   │
│  │ CPU │ работает│ DMA │ ← data →  │ DEV │                   │
│  │     │         │     │ → RAM     │     │                   │
│  │     │ ← IRQ!  │     │ (готово)  │     │                   │
│  └─────┘         └─────┘           └─────┘                   │
│  CPU работает, данные передаются параллельно                 │
└─────────────────────────────────────────────────────────────┘
```

### Аналогия 2: Официант и 100 столиков (epoll)

Представьте ресторан со 100 столиками и одним официантом.

**select/poll (старый подход):**
Официант обходит ВСЕ 100 столиков по очереди и спрашивает каждого: "Вам что-нибудь нужно?" Даже если 99 столиков молча сидят — он всё равно обходит всех. O(n) на каждый обход.

**epoll (современный подход):**
На каждом столике есть флажок. Когда гость хочет что-то заказать — он поднимает флажок. Официант смотрит только на столики с поднятыми флажками. O(количество готовых), не O(всех).

```
select/poll:                        epoll:
Официант → стол 1 → стол 2 →...     Официант видит: флажки на 3, 17, 42
           ↓         ↓                        ↓
        "нет"     "нет"             Идёт только к этим трём
           ↓                                  ↓
        стол 100                    O(3) вместо O(100)
           ↓
        O(100)
```

### Аналогия 3: Конвейер заказов (io_uring)

Представьте McDonald's с двумя конвейерными лентами:

**Лента заказов (Submission Queue):** Вы кладёте заказы: "Биг Мак", "Картошка", "Кола". Не ждёте каждый — просто кладёте и уходите.

**Лента выдачи (Completion Queue):** Когда заказ готов — он появляется на ленте выдачи. Забираете готовые заказы, не заходя на кухню.

```
┌─────────────────────────────────────────────────────────────┐
│                    io_uring                                   │
│                                                              │
│  Submission Queue (SQ):      Completion Queue (CQ):          │
│  ┌────┬────┬────┬────┐      ┌────┬────┬────┬────┐           │
│  │read│send│recv│    │  →   │done│done│    │    │           │
│  └────┴────┴────┴────┘      └────┴────┴────┴────┘           │
│        ↓                            ↑                        │
│     Kernel                       Kernel                      │
│     читает                       кладёт результаты           │
│     запросы                                                  │
└─────────────────────────────────────────────────────────────┘

Batching: 1 syscall на 100 операций вместо 100 syscalls
```

### Аналогия 4: Лифтовой алгоритм (I/O Scheduler для HDD)

Представьте лифт в 20-этажном здании. Люди нажимают кнопки:
- Этаж 3 (вниз)
- Этаж 15 (вниз)
- Этаж 7 (вверх)
- Этаж 19 (вниз)

**FCFS (First Come First Served):**
Лифт едет: 3 → 15 → 7 → 19. Мечется туда-сюда.

**SCAN (Elevator Algorithm):**
Лифт едет вверх до конца: 7 → 15 → 19, потом вниз: 3. Минимум перемещений.

Головка жёсткого диска — как лифт. Запросы к секторам — как вызовы. Умное переупорядочивание экономит время seek (5-10ms на каждое перемещение).

```
Запросы к секторам: 98, 183, 37, 122, 14, 124
Головка на секторе 53

FCFS: 53→98→183→37→122→14→124 = 640 секторов движения
      ───→─────→←────→───→←────→───

SCAN: 53→65→67→98→122→124→183→[край]→37→14 = 208 секторов
      ───→→→→→→→→→→→→→→→→→→→→→→→→→→→←←←←←←←←

В 3 раза меньше движений = в 3 раза быстрее!
```

### Аналогия 5: Zero-copy — курьер без посредника

Стандартная отправка файла по сети:
1. Курьер (диск) привозит посылку в офис (kernel buffer)
2. Секретарь копирует содержимое на ваш стол (user buffer)
3. Вы смотрите, несёте обратно секретарю (kernel buffer)
4. Секретарь отдаёт другому курьеру (сеть)

**3 копирования!**

Zero-copy с sendfile():
1. Курьер (диск) привозит посылку в офис (kernel buffer)
2. Офис сразу передаёт другому курьеру (сеть)

**Посылка не покидает офис — вы даже не знаете, что она была!**

```
Стандартный путь:
Диск → [copy] → Kernel → [copy] → User → [copy] → Kernel → Сеть
                                   ↑
                            3 копирования

sendfile() zero-copy:
Диск → [DMA] → Kernel Buffer → Сеть
                    ↑
              0 копирований в user space
              До 65% экономии CPU!
```

---

## Часть 2: Почему это сложно

### Ошибка 1: Путаница blocking/non-blocking vs sync/async

**СИМПТОМ:** "epoll — это асинхронный I/O, правильно?"

**Реальность:**
- **Blocking vs Non-blocking:** Ждёт ли вызов завершения операции?
- **Sync vs Async:** Кто выполняет передачу данных — вызывающий поток или ядро в фоне?

epoll — **non-blocking**, но **synchronous**: `epoll_wait()` не блокируется надолго, но когда вы вызываете `read()` на готовом fd — данные копируются синхронно, в вашем потоке.

io_uring — **non-blocking** И **asynchronous**: вы кладёте запрос, ядро выполняет его в фоне, вы получаете результат позже.

```
┌───────────────────────────────────────────────────────────┐
│              Blocking        Non-blocking                  │
├───────────────────────────────────────────────────────────┤
│ Synchronous  │ read()        │ epoll + read()             │
│              │ (ждёт данных) │ (read() всё ещё копирует)  │
├───────────────────────────────────────────────────────────┤
│ Asynchronous │ -             │ io_uring                   │
│              │               │ (ядро копирует в фоне)     │
└───────────────────────────────────────────────────────────┘
```

**РЕШЕНИЕ:** epoll говорит "данные готовы", но сам read() синхронный. Для true async — io_uring или aio.

### Ошибка 2: Использование select/poll при высокой нагрузке

**СИМПТОМ:** Сервер на 10,000 соединений загружает CPU на 100% даже без трафика.

**Почему:**
```c
// select() каждый раз сканирует ВСЕ fd
for (int i = 0; i < 10000; i++) {
    FD_SET(fds[i], &read_fds);  // Копируем 10,000 fd в ядро
}
select(max_fd + 1, &read_fds, ...);  // Ядро сканирует 10,000 fd
for (int i = 0; i < 10000; i++) {
    if (FD_ISSET(fds[i], &read_fds)) { ... }  // Сканируем 10,000 снова
}
// 30,000 операций даже если готов 1 fd!
```

**РЕШЕНИЕ:** Используйте epoll (Linux), kqueue (BSD/macOS), IOCP (Windows).
```c
// epoll регистрирует fd один раз
epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);  // Один раз при создании

// epoll_wait возвращает ТОЛЬКО готовые
int n = epoll_wait(epfd, events, 100, -1);
for (int i = 0; i < n; i++) {  // Итерируем только по готовым
    handle(events[i].data.fd);
}
```

### Ошибка 3: Edge-triggered без чтения до EAGAIN

**СИМПТОМ:** Данные "пропадают" при использовании EPOLLET.

**Почему:** Edge-triggered уведомляет при *изменении* состояния (появились новые данные). Если в буфере было 1000 байт, вы прочитали 100, следующий `epoll_wait()` НЕ вернёт этот fd — состояние не изменилось.

```
Level-triggered:           Edge-triggered:
Буфер: [######]           Буфер: [######]
read(100)                 read(100)
Буфер: [####]             Буфер: [####]
epoll_wait → ГОТОВ!       epoll_wait → НЕТ СОБЫТИЯ
          ↑                         ↑
    Есть данные —              Состояние не
    уведомляет                 изменилось —
                               молчит
```

**РЕШЕНИЕ:** При edge-triggered ВСЕГДА читайте до EAGAIN:
```c
while (1) {
    ssize_t n = read(fd, buf, sizeof(buf));
    if (n == -1) {
        if (errno == EAGAIN) break;  // Буфер пуст — выходим
        handle_error();
    }
    process_data(buf, n);
}
```

### Ошибка 4: O_DIRECT без выравнивания

**СИМПТОМ:** `write()` с O_DIRECT возвращает EINVAL.

**Почему:** O_DIRECT обходит page cache и требует:
- Буфер выровнен по границе 512 байт (или размеру сектора)
- Размер кратен 512 байт
- Offset кратен 512 байт

```c
// НЕПРАВИЛЬНО:
char buf[4096];  // Не выровнен!
int fd = open("file", O_DIRECT | O_RDWR);
write(fd, buf, 4096);  // EINVAL!

// ПРАВИЛЬНО:
char *buf;
posix_memalign((void**)&buf, 512, 4096);  // Выровнен по 512
write(fd, buf, 4096);  // OK
```

**РЕШЕНИЕ:** Используйте `posix_memalign()` или `aligned_alloc()`. Проверьте размер сектора: `ioctl(fd, BLKSSZGET, &sector_size)`.

### Ошибка 5: Предположение что write() = данные на диске

**СИМПТОМ:** После успешного `write()` происходит сбой питания — данные потеряны.

**Почему:** `write()` возвращается, когда данные скопированы в page cache ядра. На диске данные появятся позже (через 5-30 секунд или при нехватке памяти).

```
write() → [OK!] → Page Cache (RAM) → ... → [через N секунд] → Диск
                       ↑
              Данные здесь, НЕ на диске!
              Сбой питания = потеря данных
```

**РЕШЕНИЕ:** Для критичных данных — `fsync()`:
```c
write(fd, data, len);
fsync(fd);  // Ждём, пока данные на диске
// Теперь данные гарантированно сохранены
```

Или O_SYNC флаг (медленнее, но каждый write() синхронный):
```c
int fd = open("file", O_WRONLY | O_SYNC);
write(fd, data, len);  // Уже на диске после возврата
```

### Ошибка 6: Не использовать batching в io_uring

**СИМПТОМ:** io_uring работает не быстрее epoll.

**Почему:** io_uring даёт выигрыш при batching — отправке нескольких операций одним syscall. Если отправлять по одной операции — overhead почти как у обычных syscalls.

```c
// НЕПРАВИЛЬНО: один submit на операцию
io_uring_prep_read(sqe1, fd1, buf1, len, 0);
io_uring_submit(&ring);  // syscall

io_uring_prep_read(sqe2, fd2, buf2, len, 0);
io_uring_submit(&ring);  // ещё syscall

// ПРАВИЛЬНО: batch submit
io_uring_prep_read(sqe1, fd1, buf1, len, 0);
io_uring_prep_read(sqe2, fd2, buf2, len, 0);
io_uring_prep_read(sqe3, fd3, buf3, len, 0);
io_uring_submit(&ring);  // ОДИН syscall на 3 операции
```

**РЕШЕНИЕ:** Накапливайте запросы и отправляйте пачками. В polling mode — вообще без syscalls.

---

## Часть 3: Ментальные модели

### Модель 1: Матрица trade-offs

```
              CPU overhead ←───────────────────────→ Низкий
                   │                                    │
                   ▼                                    ▼
┌─────────────────────────────────────────────────────────────┐
│                 │ CPU Load │ Latency │ Throughput │ When    │
├─────────────────────────────────────────────────────────────┤
│ Polling         │   100%   │ <1μs    │  Высокий   │ NVMe,   │
│                 │          │         │            │ DPDK    │
├─────────────────────────────────────────────────────────────┤
│ Interrupts      │   Низкий │ 1-5μs   │  Средний   │ HID,    │
│                 │          │         │            │ редкие  │
├─────────────────────────────────────────────────────────────┤
│ DMA+Interrupt   │   ~0%    │ 5-10μs  │  Высокий   │ Диски,  │
│                 │          │         │            │ сеть    │
├─────────────────────────────────────────────────────────────┤
│ NAPI (hybrid)   │ Adaptive │ 1-50μs  │  Макс      │ 10GbE+  │
│                 │          │         │            │         │
└─────────────────────────────────────────────────────────────┘
```

Выбор зависит от ситуации:
- Latency критичен → Polling (predictable) или Interrupts (low load)
- Throughput критичен → DMA + batching
- CPU дорогой → Interrupts для редких событий
- Высокая нагрузка → NAPI (adaptive)

### Модель 2: Эволюция I/O (исторический прогресс)

```
1960s           1980s           2000s           2019
Polling    →    Interrupts  →   DMA        →    io_uring
  │                │              │               │
  ▼                ▼              ▼               ▼
100% CPU        CPU free        CPU free       Zero-copy +
busy wait       между IRQ       во время       batched +
                                transfer       async

Каждый шаг освобождает CPU для полезной работы
```

### Модель 3: Readiness vs Completion (два мировоззрения)

```
┌─────────────────────────────────────────────────────────────┐
│           READINESS MODEL (epoll, kqueue)                    │
│                                                              │
│  App: "Скажи когда fd готов"                                │
│  Kernel: "fd 42 готов для чтения"                           │
│  App: read(fd42, buf, len)  ← СИНХРОННО, в потоке app       │
│                                                              │
│  Проблема: read() всё ещё блокирует поток на время копирования│
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│           COMPLETION MODEL (io_uring, IOCP)                  │
│                                                              │
│  App: "Прочитай fd42 в buf когда сможешь"                   │
│  Kernel: [читает в фоне]                                    │
│  Kernel: "Готово! Результат в buf"                          │
│  App: использует buf ← данные УЖЕ там                       │
│                                                              │
│  Преимущество: копирование не блокирует поток app           │
└─────────────────────────────────────────────────────────────┘
```

### Модель 4: Дерево решений выбора I/O подхода

```
                    ┌─────────────────────┐
                    │ Сколько соединений? │
                    └──────────┬──────────┘
                               │
           ┌───────────────────┼───────────────────┐
           │                   │                   │
        < 100              100-10K              > 10K
           │                   │                   │
           ▼                   ▼                   ▼
      Threads OK           epoll/kqueue        io_uring
      (простой код)        (стандарт)         (макс perf)
           │                   │                   │
           │                   ▼                   │
           │         ┌─────────────────┐           │
           │         │ Нужен file I/O? │           │
           │         └────────┬────────┘           │
           │                  │                    │
           │         ┌────────┴────────┐           │
           │        Да               Нет           │
           │         │                │            │
           │         ▼                ▼            │
           │    io_uring          epoll OK         │
           │    (file async)                       │
           └──────────────────────────────────────┘
```

### Модель 5: Пирамида латентности (от быстрого к медленному)

```
                    ┌─────┐
                    │ L1  │ 1 ns
                    │Cache│
                ┌───┴─────┴───┐
                │  L2 Cache   │ 3-4 ns
            ┌───┴─────────────┴───┐
            │      L3 Cache       │ 10-20 ns
        ┌───┴─────────────────────┴───┐
        │           RAM               │ 60-100 ns
    ┌───┴─────────────────────────────┴───┐
    │           NVMe SSD                  │ 10-20 μs (10,000-20,000 ns)
┌───┴─────────────────────────────────────┴───┐
│               SATA SSD                      │ 50-100 μs
├─────────────────────────────────────────────┤
│                  HDD                        │ 5,000-10,000 μs (5-10 ms)
├─────────────────────────────────────────────┤
│            Network (local)                  │ 50-500 μs
├─────────────────────────────────────────────┤
│          Network (internet)                 │ 10,000-200,000 μs (10-200 ms)
└─────────────────────────────────────────────┘

Разница между L1 (1ns) и HDD (10ms) = 10,000,000x
Это как секунда vs 116 дней!
```

**Вывод:** Каждый лишний слой (syscall, copy, context switch) добавляет латентность. Zero-copy и batching убирают эти слои.

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| [[os-overview]] | Базовые концепции ОС, kernel/user mode, syscalls | Предыдущий материал раздела |
| [[os-processes-threads]] | Процессы и блокирующие операции, состояния потоков | Предыдущий материал раздела |
| Hardware interrupts интуитивно | Понимание что устройство может "прервать" CPU | [Computerphile: Interrupts](https://www.youtube.com/watch?v=M9u2Z6dI0A8) |
| Базовое понимание RAM | Что такое память, адреса | Любой курс по архитектуре |

**Время на подготовку:** ~2-3 дня если знаете предыдущие темы

---

## Терминология для новичков

| Термин | Что это | Аналогия из жизни |
|--------|---------|-------------------|
| **Polling** | CPU в цикле проверяет "готово?" (busy waiting) | Стоять у двери и постоянно дёргать ручку: "открыто?" — тратит силы |
| **Interrupt** | Сигнал от устройства к CPU: "у меня есть данные!" | Звонок телефона: не надо постоянно проверять, телефон сам позвонит |
| **IRQ** | Interrupt Request — "линия" запроса прерывания | Номер телефона устройства для связи с CPU |
| **ISR** | Interrupt Service Routine — код обработки прерывания | Инструкция "что делать когда звонит телефон" |
| **DMA** | Устройство пишет в RAM само, без CPU | Курьер доставляет посылку в почтовый ящик пока вы работаете |
| **Block device** | Произвольный доступ по блокам (диск) | Книга: можно открыть любую страницу |
| **Character device** | Последовательный поток байтов (терминал) | Аудиокнига: только вперёд, нельзя "перескочить" |
| **epoll** | Один поток следит за тысячами сокетов | Официант следит за 100 столиками: "кому принести?" |
| **io_uring** | Async I/O через shared memory (Linux 5.1+) | Конвейер заказов: кладёшь запросы, забираешь результаты — без звонков |
| **Zero-copy** | Данные не копируются в userspace | Курьер передаёт посылку напрямую, не заходя в офис |
| **NAPI** | Hybrid: interrupts при низкой нагрузке, polling при высокой | Переключение режимов: звонок когда тихо, постоянная проверка когда занято |
| **I/O Scheduler** | Переупорядочивание запросов к диску | Диспетчер такси: группирует заказы по району |

---

## Как устроена подсистема I/O

### Иерархия устройств

Между CPU и устройствами находится несколько уровней контроллеров. CPU не общается напрямую с жёстким диском — он отправляет команды контроллеру SATA, который уже управляет физическим устройством. Это разделение позволяет абстрагировать детали hardware и унифицировать интерфейс для ОС.

```
┌─────────────────────────────────────────────────────────────┐
│                         CPU                                  │
└──────────────────────────┬──────────────────────────────────┘
                           │
┌──────────────────────────▼──────────────────────────────────┐
│                    Memory Bus                                │
│                   (FSB / QPI / UPI)                         │
└──────────────────────────┬──────────────────────────────────┘
                           │
┌──────────────────────────▼──────────────────────────────────┐
│                   I/O Controller Hub                         │
│                      (Chipset)                               │
└────────┬─────────────────┬──────────────────┬───────────────┘
         │                 │                  │
    ┌────▼────┐      ┌─────▼─────┐      ┌─────▼─────┐
    │  PCIe   │      │   SATA    │      │   USB     │
    │ (GPU,   │      │  (HDD,    │      │(клавиату- │
    │  NVMe,  │      │   SSD)    │      │ра, мышь)  │
    │  NIC)   │      │           │      │           │
    └─────────┘      └───────────┘      └───────────┘
```

### Типы устройств

Устройства делятся на три категории по способу передачи данных:

**Block devices** (диски, SSD) работают с данными блоками фиксированного размера — обычно 512 байт или 4 КБ. Можно читать или писать любой блок независимо от остальных (произвольный доступ). Это позволяет файловой системе кэшировать блоки, переупорядочивать запросы для оптимизации, читать данные заранее (prefetch). Примеры: HDD, SSD, NVMe.

**Character devices** передают данные потоком байт за байтом без внутренней структуры. Нельзя "перемотать" на 100 байт назад — данные приходят последовательно. Такой режим подходит для интерактивных устройств: терминалу нельзя ждать, пока буфер заполнится — нажатие клавиши должно отображаться сразу. Примеры: клавиатура, мышь, последовательный порт.

**Network devices** — особая категория, работающая с пакетами. У них нет файлового интерфейса в привычном смысле — вместо read/write используются send/recv. Сетевые устройства буферизируют пакеты, поддерживают множественные соединения, и требуют специальной обработки (протоколы, routing).

| Тип | Характеристика | Примеры | Паттерн доступа |
|-----|---------------|---------|-----------------|
| **Block devices** | Фиксированные блоки, произвольный доступ | HDD, SSD, NVMe | read/write по секторам |
| **Character devices** | Поток байтов, последовательный доступ | Терминал, последовательный порт | getchar/putchar |
| **Network devices** | Пакетный ввод/вывод | Ethernet, WiFi | send/recv пакетов |

---

## Три способа взаимодействия с устройствами

История развития I/O — это история уменьшения нагрузки на CPU. От простейшего polling, где процессор занят на 100%, через прерывания, где CPU освобождается между операциями, до DMA, где данные передаются вообще без участия процессора.

### 1. Программный ввод/вывод (Polling)

Polling — простейший способ: CPU в цикле проверяет статус устройства, пока оно не будет готово. Это называется "busy waiting" или "активное ожидание".

```c
// Отправка данных через polling
void send_data_polling(char *data, int len) {
    for (int i = 0; i < len; i++) {
        // Ждём, пока устройство готово принять данные
        while ((inb(STATUS_PORT) & READY_BIT) == 0) {
            // Busy waiting — CPU крутится в цикле
            // Не делает ничего полезного
        }
        outb(DATA_PORT, data[i]);
    }
}
```

**Почему это плохо:** Представьте, что устройство отвечает за 1 миллисекунду. Современный CPU выполняет 3 миллиарда инструкций в секунду, то есть 3 миллиона инструкций за 1 мс. Все эти 3 миллиона инструкций CPU просто крутится в цикле, проверяя один бит. Это 100% CPU utilization при нулевой полезной работе.

**Когда polling оправдан:** Для очень быстрых устройств, где latency меньше стоимости прерывания. NVMe диски отвечают за 10-20 микросекунд, а обработка прерывания занимает 1-5 микросекунд. Если устройство отвечает быстрее, чем overhead прерывания — polling эффективнее. Также polling используется в real-time системах, где важна предсказуемость (прерывание может прийти в неудобный момент), и в высокопроизводительных сетевых приложениях (DPDK).

### 2. Прерывания (Interrupts)

Прерывание — это сигнал от устройства к процессору: "У меня есть данные" или "Я готов принять команду". Вместо постоянной проверки CPU может заниматься другими делами, а устройство само сообщит, когда будет готово.

**Как работает прерывание:**

Когда вы нажимаете клавишу на клавиатуре, происходит следующее:

1. **Устройство генерирует сигнал.** Контроллер клавиатуры замечает нажатие и поднимает электрический сигнал на линии IRQ (Interrupt Request).

2. **Контроллер прерываний (APIC) принимает сигнал.** В современных системах это Advanced Programmable Interrupt Controller. Он определяет приоритет прерывания и решает, какому CPU его отправить (в многопроцессорной системе).

3. **CPU прерывает текущую работу.** Процессор завершает текущую инструкцию (нельзя прервать посередине инструкции) и начинает обработку прерывания.

4. **Сохранение контекста.** CPU сохраняет текущее состояние: значения регистров, instruction pointer (где остановились), флаги процессора. Всё это кладётся на стек ядра.

5. **Переход в kernel mode.** Если процесс был в user mode — происходит переключение в kernel mode. Только ядро может обрабатывать прерывания.

6. **Поиск обработчика.** CPU использует номер прерывания как индекс в Interrupt Vector Table (IVT) — таблице адресов обработчиков. Каждое устройство имеет свой номер IRQ и свой обработчик.

7. **Выполнение ISR.** Interrupt Service Routine (обработчик прерывания) — код ядра, который знает, как работать с конкретным устройством. Для клавиатуры: прочитать скан-код, положить в буфер.

8. **End Of Interrupt.** Обработчик отправляет сигнал EOI контроллеру прерываний, сообщая, что обработка завершена.

9. **Восстановление контекста.** CPU восстанавливает сохранённые регистры и продолжает выполнение прерванного кода.

```c
// Пример обработчика прерывания клавиатуры
void keyboard_interrupt_handler(void) {
    // Читаем скан-код нажатой клавиши из порта устройства
    uint8_t scancode = inb(KEYBOARD_DATA_PORT);

    // Помещаем в буфер ядра для последующей обработки
    keyboard_buffer_add(scancode);

    // Сигнализируем EOI контроллеру прерываний
    outb(PIC_EOI, PIC_COMMAND_PORT);
}
```

**Стоимость прерывания:** 1-5 микросекунд прямых затрат (сохранение/восстановление контекста, переход в kernel mode). Но есть косвенные затраты: cache pollution (обработчик использует кэш, вытесняя данные прерванного процесса), branch predictor pollution.

**Проблема Interrupt Storm:** При высокой нагрузке (сеть 10+ Гбит/с) устройство может генерировать сотни тысяч прерываний в секунду. CPU тратит всё время на обработку прерываний, не успевая делать полезную работу. Это называется "livelock" — система работает, но не делает прогресс.

**Решение — NAPI (New API) в Linux:** При высокой нагрузке ядро временно отключает прерывания от сетевой карты и переходит на polling. Когда очередь пакетов обработана — прерывания снова включаются. Это adaptive подход: при низкой нагрузке используются прерывания (низкий latency), при высокой — polling (высокий throughput).

### 3. DMA (Direct Memory Access)

DMA — механизм, позволяющий устройству напрямую читать и писать в RAM без участия CPU. Процессор только инициирует передачу (указывает адреса и размер), а затем свободен для другой работы. По завершении DMA-контроллер генерирует прерывание.

**Почему это важно:** Без DMA для копирования 1 МБ данных с диска CPU должен выполнить миллион операций чтения, каждый раз читая байт из порта устройства и записывая его в память. С DMA — одна команда "скопируй 1 МБ с устройства по адресу X", и CPU свободен.

```c
// Настройка DMA-передачи
void setup_dma_transfer(void *buffer, size_t len, int direction) {
    // Получаем физический адрес буфера
    // DMA работает с физическими адресами, не виртуальными!
    dma_addr_t phys_addr = dma_map_single(dev, buffer, len, direction);

    // Программируем DMA-контроллер
    writel(phys_addr, DMA_SOURCE_ADDR);      // Откуда брать данные
    writel(device_addr, DMA_DEST_ADDR);       // Куда класть
    writel(len, DMA_LENGTH);                  // Сколько байт
    writel(DMA_START, DMA_CONTROL);           // Запуск передачи

    // CPU свободен! Можно переключиться на другой процесс
    // Прерывание придёт по завершении
}

// Обработчик завершения DMA
void dma_complete_handler(void) {
    // Важно: данные в RAM, но CPU кэш может содержать старые данные
    // Нужно инвалидировать кэш или использовать специальные области памяти
    dma_sync_single_for_cpu(dev, phys_addr, len, direction);

    // Освобождаем DMA-маппинг
    dma_unmap_single(dev, phys_addr, len, direction);

    // Уведомляем процесс, ожидающий данные
    wake_up(&wait_queue);
}
```

**Важный момент — кэш когерентность:** CPU имеет кэш (L1, L2, L3), и когда DMA записывает данные в RAM, кэш CPU может содержать старую копию этих данных. Нужно либо инвалидировать кэш после DMA-записи, либо использовать некэшируемые области памяти, либо (в современных системах) использовать cache-coherent DMA через IOMMU.

**Сравнение подходов:**

| Метод | CPU overhead | Latency | Throughput | Когда использовать |
|-------|-------------|---------|------------|-------------------|
| Polling | 100% | Минимальный | Низкий | NVMe, DPDK, real-time |
| Interrupts | 1-5 мкс на прерывание | 1-5 мкс | Средний | Клавиатура, мышь, редкие события |
| DMA | ~0.01% (только setup) | 5-10 мкс setup | Максимальный | Диски, сеть, большие передачи |

При interrupt-driven I/O без DMA передача 1 КБ генерирует 1024 прерывания (по одному на каждый байт). С DMA — одно прерывание по завершении всей передачи. Разница в CPU overhead: 12.5% vs 0.05%.

---

## Trade-offs в I/O подходах

### Polling vs Interrupts

| Аспект | Polling | Interrupts |
|--------|---------|------------|
| CPU usage | Высокий (busy waiting) | Низкий (спит пока нет события) |
| Latency | Зависит от частоты polling | Минимальная (сразу реагирует) |
| Overhead | Нет context switch | Context switch на каждый interrupt |
| Когда лучше | Высокая нагрузка, предсказуемый I/O | Редкие события, низкая нагрузка |

**Hybrid подход (современные NIC):** NAPI в Linux. Начинает с interrupts, при высокой нагрузке переключается на polling.

### Sync vs Async I/O

| Аспект | Sync | Async |
|--------|------|-------|
| Простота кода | Линейный код | Callbacks/futures/coroutines |
| Blocking | Да — поток ждёт | Нет — поток свободен |
| Throughput | Один запрос за раз на поток | Много параллельных запросов |
| Когда лучше | Простые приложения | High-concurrency, серверы |

### Buffered vs Direct I/O

| Аспект | Buffered (default) | Direct (O_DIRECT) |
|--------|-------------------|-------------------|
| Page cache | Использует | Обходит |
| CPU copy | Да (user ↔ kernel ↔ disk) | Нет (user ↔ disk) |
| Кэширование | Да (ОС кэширует) | Нет (приложение само) |
| Когда лучше | Большинство случаев | Базы данных (свой кэш) |

### DMA vs CPU copy

```
Без DMA: CPU читает из устройства → пишет в RAM (загружает CPU)
С DMA: Устройство пишет напрямую в RAM (CPU свободен)
```

**Но:** DMA требует contiguous физическую память и правильной синхронизации.

---

## Буферизация: зачем нужны промежуточные буферы

### Уровни буферизации

Данные проходят через несколько буферов на пути от устройства к приложению:

```
┌─────────────────────────────────────────────────────────────┐
│                    User Space Buffer                         │
│                   (char buf[4096] в вашем коде)             │
└──────────────────────────┬──────────────────────────────────┘
                           │ copy_to_user() / copy_from_user()
                           │ (копирование между user и kernel space)
┌──────────────────────────▼──────────────────────────────────┐
│                    Kernel Buffer                             │
│                   (Page Cache / Socket Buffer)               │
└──────────────────────────┬──────────────────────────────────┘
                           │ DMA
┌──────────────────────────▼──────────────────────────────────┐
│                   Device Buffer                              │
│                  (Аппаратный буфер контроллера)             │
└─────────────────────────────────────────────────────────────┘
```

### Зачем столько буферов?

**1. Сглаживание скорости.** CPU работает в наносекундах, диск — в миллисекундах. Это разница в миллион раз. Буфер позволяет накопить данные и передать их пакетом. Вместо 1000 операций по 1 байту — одна операция на 1000 байт.

**2. Copy semantics.** Когда приложение вызывает `write(fd, buf, 1000)`, оно ожидает, что после возврата из write() можно изменять buf — данные уже "отправлены". Но физически данные ещё не на диске. Ядро копирует данные в свой буфер, и write() возвращается. Приложение может делать что угодно с buf, а ядро запишет данные на диск позже.

**3. Переупорядочивание (I/O scheduling).** Буфер позволяет накопить несколько запросов и выполнить их в оптимальном порядке. Для HDD это критично: запросы к соседним секторам можно выполнить одним движением головки, а не двумя.

### Zero-copy I/O: избавляемся от лишних копирований

Каждое копирование — это CPU time и bandwidth памяти. При отправке файла по сети стандартный путь:

```
Диск → Kernel Buffer → User Buffer → Kernel Buffer → Сеть
         (copy 1)        (copy 2)       (copy 3)
```

Три копирования! Данные из ядра копируются в user space, чтобы приложение их прочитало, а потом обратно в ядро для отправки по сети.

С `sendfile()` (zero-copy):

```
Диск → Kernel Buffer ───────────────────────────────→ Сеть
         (данные остаются в ядре, никакого user space)
```

```c
// Стандартный способ — 3 копирования
char buf[8192];
while ((n = read(file_fd, buf, sizeof(buf))) > 0) {
    write(socket_fd, buf, n);
}

// Zero-copy с sendfile — данные не покидают kernel space
off_t offset = 0;
sendfile(socket_fd, file_fd, &offset, file_size);
```

`sendfile()` экономит до 65% CPU при отправке больших файлов. Nginx, Apache, и все современные веб-серверы используют это для статических файлов.

---

## Асинхронный I/O в Linux: эволюция

### Проблема блокирующего I/O

Традиционный `read()` блокирует поток до получения данных. Если у вас 10,000 клиентов — нужно 10,000 потоков. Каждый поток занимает 8 МБ стека, плюс context switch между ними. Это не масштабируется.

Решение — мультиплексирование: один поток обслуживает много соединений, проверяя, у какого из них есть данные.

### select/poll — устаревший подход

```c
// select: ждём события на любом из fd
fd_set read_fds;
FD_ZERO(&read_fds);
FD_SET(sock1, &read_fds);
FD_SET(sock2, &read_fds);

// Ядро сканирует ВСЕ fd при каждом вызове — O(n)!
int ready = select(max_fd + 1, &read_fds, NULL, NULL, &timeout);

// Приложение тоже сканирует все fd
for (int fd = 0; fd <= max_fd; fd++) {
    if (FD_ISSET(fd, &read_fds)) {
        handle_read(fd);
    }
}
```

**Проблема:** При 10,000 соединений каждый `select()` копирует и сканирует 10,000 дескрипторов. O(n) на каждый вызов, даже если готов только один fd. При 1000 запросов в секунду — 10 миллионов проверок в секунду.

### epoll — современный стандарт (Linux 2.6, 2002)

epoll решает проблему O(n): дескрипторы регистрируются один раз, а `epoll_wait()` возвращает только готовые:

```c
// Создаём epoll instance — структуру данных в ядре
int epfd = epoll_create1(0);

// Регистрируем интересующие fd ОДИН раз
struct epoll_event ev;
ev.events = EPOLLIN | EPOLLET;  // Читать, edge-triggered
ev.data.fd = socket_fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, socket_fd, &ev);

// Ожидание — возвращает ТОЛЬКО готовые fd!
struct epoll_event events[MAX_EVENTS];
while (1) {
    int nready = epoll_wait(epfd, events, MAX_EVENTS, -1);

    // Обрабатываем только готовые — O(количество готовых), не O(всех)
    for (int i = 0; i < nready; i++) {
        handle_event(events[i].data.fd, events[i].events);
    }
}
```

**Level-triggered vs Edge-triggered:**

- **Level-triggered** (по умолчанию): `epoll_wait()` возвращает fd, пока в нём есть данные. Если прочитали 100 байт из 1000 — следующий `epoll_wait()` снова вернёт этот fd. Безопаснее, но может приводить к лишним пробуждениям.

- **Edge-triggered** (EPOLLET): `epoll_wait()` возвращает fd только при *изменении* состояния: когда данные *появились*. Если не прочитали всё — следующий `epoll_wait()` не вернёт этот fd, пока не придут новые данные. Эффективнее, но сложнее: нужно читать до EAGAIN.

### io_uring — новый подход (Linux 5.1, 2019)

epoll решает проблему мультиплексирования, но сами операции read/write всё ещё требуют syscall. io_uring идёт дальше: submission и completion через shared memory, можно вообще без syscall.

**Как работает io_uring:**

В ядре создаются два кольцевых буфера в shared memory:
- **Submission Queue (SQ):** приложение кладёт запросы
- **Completion Queue (CQ):** ядро кладёт результаты

```
┌─────────────────────────────────────────────────────────────┐
│                     User Space                               │
│                                                              │
│  Приложение записывает       Приложение читает результаты   │
│  запросы в SQ                 из CQ                         │
│         │                            ▲                       │
│         ▼                            │                       │
│  ┌──────────────────┐      ┌──────────────────────┐         │
│  │ Submission Queue │      │  Completion Queue    │         │
│  │   [SQE][SQE]...  │      │   [CQE][CQE]...     │         │
│  └────────┬─────────┘      └──────────▲──────────┘         │
│           │                           │                     │
└───────────│───────────────────────────│─────────────────────┘
            │      Shared Memory        │
════════════│═══════════════════════════│═════════════════════
            ▼                           │
┌───────────────────────────────────────────────────────────┐
│                    Kernel Space                            │
│                                                            │
│   Ядро читает запросы       Ядро записывает результаты    │
│   из SQ, выполняет          в CQ                          │
│                                                            │
└───────────────────────────────────────────────────────────┘
```

```c
// Инициализация io_uring
struct io_uring ring;
io_uring_queue_init(256, &ring, 0);  // 256 записей в очереди

// Подготовка запроса на чтение — без syscall!
struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
io_uring_prep_read(sqe, fd, buf, len, offset);
sqe->user_data = (uint64_t)my_context;  // Идентификатор для matching

// Отправка — можно накопить много запросов и отправить одним syscall
// В polling mode — вообще без syscall
io_uring_submit(&ring);

// Получение результата
struct io_uring_cqe *cqe;
io_uring_wait_cqe(&ring, &cqe);
int result = cqe->res;
io_uring_cqe_seen(&ring, cqe);
```

**Преимущества io_uring:**

1. **Batching:** Накопить 100 запросов и отправить одним `io_uring_submit()`. Или вообще без syscall в polling mode.

2. **True async:** В отличие от epoll, сами операции read/write асинхронны. epoll говорит "данные готовы", но read() всё равно может заблокироваться.

3. **Универсальность:** Работает не только с сокетами, но и с файлами, таймерами, сигналами.

4. **Linked operations:** Можно связать операции в цепочку: "прочитай файл, потом отправь по сети".

**Производительность:** io_uring показывает ~10% прирост throughput при 1000+ соединений по сравнению с epoll. Для файлового I/O разница больше — epoll не поддерживает асинхронный file I/O.

---

## I/O Scheduler: оптимизация порядка запросов

### Зачем нужен планировщик I/O

У HDD есть механическая головка, которая физически перемещается к нужной дорожке. Seek time — 5-10 миллисекунд. Если запросы приходят к разным частям диска, головка мечется туда-сюда. Умное переупорядочивание минимизирует перемещения.

```
Запросы в порядке поступления: 98, 183, 37, 122, 14, 124, 65, 67
Головка на дорожке 53

FCFS (как пришли, так и делаем):
  53 → 98 → 183 → 37 → 122 → 14 → 124 → 65 → 67
  Суммарное перемещение: 640 дорожек

SCAN (лифтовой алгоритм — едем в одном направлении до конца):
  53 → 65 → 67 → 98 → 122 → 124 → 183 → [конец] → 37 → 14
  Суммарное перемещение: 208 дорожек

В 3 раза меньше перемещений = в 3 раза меньше время!
```

### Современные планировщики Linux

```bash
# Посмотреть текущий планировщик
cat /sys/block/sda/queue/scheduler
# [mq-deadline] kyber bfq none

# Сменить планировщик
echo "kyber" > /sys/block/sda/queue/scheduler
```

| Планировщик | Описание | Лучше для |
|------------|----------|-----------|
| **none** | Без переупорядочивания | NVMe (у SSD нет seek time) |
| **mq-deadline** | Гарантирует максимальное время ожидания | HDD, SATA SSD, общее использование |
| **kyber** | Оптимизирует latency | NVMe с требованиями к QoS |
| **bfq** | Честное распределение bandwidth | Desktop, смешанная нагрузка |

**Почему для NVMe — `none`:** У SSD нет механической головки, время доступа к любому блоку одинаковое. Любая обработка только добавляет latency. Overhead планировщика ~1-2 мкс заметен при latency NVMe в 10-20 мкс.

---

## Практические команды Linux

```bash
# Информация об устройствах
lsblk                      # Блочные устройства и разделы
lspci                      # PCI устройства (диски, сетевые карты)
lsusb                      # USB устройства
cat /proc/interrupts       # Статистика прерываний по IRQ

# Мониторинг I/O в реальном времени
iostat -x 1                # Расширенная статистика дисков
iotop                      # Какие процессы делают I/O
blktrace /dev/sda          # Детальная трассировка блочного I/O

# Настройка I/O
hdparm -Tt /dev/sda        # Тест скорости диска (кэш и без)
ionice -c 2 -n 0 command   # Запустить с высоким приоритетом I/O
blockdev --getra /dev/sda  # Размер read-ahead буфера

# Информация о DMA и прерываниях
cat /proc/dma              # DMA каналы
watch -n1 'cat /proc/interrupts | grep eth'  # Мониторинг сетевых IRQ
```

---

## Связь с другими темами

**[[os-overview]]** — Обзор ОС объясняет ключевые механизмы, на которых построена подсистема I/O: системные вызовы (read, write, ioctl) обеспечивают интерфейс между приложениями и устройствами, а переход kernel/user mode происходит при каждом I/O-запросе. Обработка прерываний (interrupts) — центральная концепция для I/O: когда устройство завершает операцию, оно генерирует hardware interrupt, CPU сохраняет состояние текущего потока и переходит в interrupt handler ядра. Без понимания interrupt-driven модели невозможно осознать, почему polling (busy-waiting) неэффективен и почему DMA (Direct Memory Access) позволяет CPU заниматься другими задачами во время передачи данных.

**[[os-processes-threads]]** — Процессы и потоки непосредственно связаны с I/O через модели ввода-вывода: blocking I/O блокирует весь поток до завершения операции, что требует отдельного потока на каждое соединение в серверных приложениях (модель thread-per-connection). Каждый процесс имеет таблицу открытых файловых дескрипторов — индексов в таблице ядра, указывающих на структуры file, которые описывают устройства, файлы или сокеты. Multiplexed I/O (select, poll, epoll) позволяет одному потоку обслуживать тысячи соединений, что революционизировало серверное программирование — nginx обслуживает 10K+ соединений на нескольких потоках благодаря epoll. Async I/O (io_uring) идёт ещё дальше: submission queue и completion queue в shared memory минимизируют syscall overhead.

**[[os-file-systems]]** — Файловые системы — это абстракция, построенная поверх подсистемы I/O: VFS (Virtual File System) предоставляет единый интерфейс для всех устройств через структуру file_operations, благодаря чему read() работает одинаково для файла на диске, сетевого сокета или /dev/random. Block I/O layer переводит файловые операции в запросы к блочным устройствам, а I/O scheduler (mq-deadline, BFQ) переупорядочивает эти запросы для оптимизации производительности диска. Page cache — ключевое связующее звено между файловыми системами и I/O: данные кэшируются в RAM, и только при cache miss происходит реальное обращение к устройству. Понимание I/O устройств объясняет, почему fsync() стоит 0.1-10 ms — данные должны физически записаться на устройство, что включает flush write cache контроллера.

**[[os-memory-management]]** — Управление памятью и I/O пересекаются в нескольких критических точках. DMA работает с физическими адресами, а приложения — с виртуальными, поэтому IOMMU (I/O Memory Management Unit) транслирует адреса для DMA, подобно MMU для CPU, обеспечивая изоляцию — устройство не может получить доступ к произвольной памяти. Page cache — связующее звено: read() сначала проверяет, есть ли данные в RAM (page cache hit), и только при промахе инициирует I/O-запрос к устройству, который заполняет страницу через DMA. mmap() связывает файлы и память напрямую: файл отображается в адресное пространство процесса, и обращение к этой памяти может вызвать page fault с последующим чтением с устройства — это zero-copy подход, исключающий промежуточное копирование данных.

**Связанные концепции:**
- [[os-virtualization]] — virtio для I/O в виртуальных машинах
- [[kotlin-coroutines|Kotlin Coroutines]] — `Dispatchers.IO` использует отдельный thread pool для блокирующего I/O, позволяя не блокировать корутины
- **Java NIO (New I/O)** — selectors (обёртка над epoll на Linux), non-blocking I/O через `Selector`, `Channel`, `ByteBuffer`; NIO2 (Java 7) добавил `AsynchronousSocketChannel`

---

## Рекомендуемые источники

### Учебники

- Tanenbaum A., Bos H. (2014). *"Modern Operating Systems, 4th Edition."* — глава 5 (Input/Output) — от принципов аппаратного I/O (interrupts, DMA) до программного (drivers, disk scheduling); одно из лучших объяснений многоуровневой архитектуры I/O-подсистемы.
- Silberschatz A., Galvin P., Gagne G. (2018). *"Operating System Concepts, 10th Edition."* — глава 12 (I/O Systems) и глава 11 (Mass-Storage Structure) — hardware I/O, application I/O interface, kernel I/O subsystem, disk scheduling алгоритмы (SCAN, C-SCAN, LOOK) с анализом производительности.
- Arpaci-Dusseau R., Arpaci-Dusseau A. (2018). *"Operating Systems: Three Easy Pieces."* — глава 36 (I/O Devices) — от polling до interrupts и DMA с чёткими диаграммами; бесплатно.
- Bryant R., O'Hallaron D. (2015). *"Computer Systems: A Programmer's Perspective, 3rd Edition."* — глава 10 (System-Level I/O) — Unix I/O модель глазами программиста: file descriptors, reading/writing files, RIO package, I/O redirection, standard I/O vs Unix I/O trade-offs.
- Love R. (2010). *"Linux Kernel Development, 3rd Edition."* — глава 14 (The Block I/O Layer) — request queues, I/O schedulers (Deadline, CFQ, Noop), bio structure; глава 7 (Interrupts and Interrupt Handlers) — top half/bottom half, softirqs, tasklets.

### Книги и курсы
- [OSTEP: I/O Devices chapters](https://pages.cs.wisc.edu/~remzi/OSTEP/file-devices.pdf) — бесплатная книга
- [Linux Device Drivers (LDD3)](https://lwn.net/Kernel/LDD3/) — бесплатная книга о драйверах

### Официальная документация
- [io_uring Documentation](https://kernel.dk/io_uring.pdf) — официальный документ от автора
- [epoll(7) man page](https://man7.org/linux/man-pages/man7/epoll.7.html) — официальная документация

### Статьи и туториалы
- [GeeksforGeeks: I/O Systems](https://www.geeksforgeeks.org/io-systems-in-operating-system/) — базовое объяснение
- [LWN: io_uring introduction](https://lwn.net/Articles/776703/) — отличное введение в io_uring
- [Jens Axboe: What is io_uring](https://kernel.dk/io_uring.pdf) — от создателя io_uring

### Практика
- [liburing examples](https://github.com/axboe/liburing) — примеры использования io_uring

---

*Обновлено: 2026-01-09 — добавлены педагогические секции (5 аналогий: курьер/polling/interrupt/DMA, официант/epoll, конвейер/io_uring, лифт/scheduler, zero-copy; 6 типичных ошибок: blocking vs async, select/poll O(n), edge-triggered, O_DIRECT alignment, write≠disk, batching; 5 ментальных моделей: матрица trade-offs, эволюция I/O, readiness vs completion, дерево решений, пирамида латентности)*

---

## Проверь себя

> [!question]- Nginx обслуживает 10,000+ соединений на 4 потоках. Объясни цепочку от HTTP запроса до ответа с точки зрения I/O подсистемы ОС: какие механизмы позволяют это?
> Nginx использует **epoll** (Linux): 4 worker потока, каждый с epoll fd, мониторящим тысячи сокетов. Цепочка: (1) клиент шлёт TCP SYN → NIC получает пакет → **DMA** копирует в kernel buffer → NIC генерирует **hardware interrupt** → kernel TCP stack обрабатывает → сокет переходит в readable state → **epoll_wait()** возвращает event. (2) Worker вызывает **read()** на сокет (non-blocking) → данные из kernel buffer в userspace. (3) Для static file: **sendfile()** (zero-copy) → данные из page cache напрямую в сокет buffer через DMA, без копирования в userspace — экономия до 65% CPU. Ключ: worker НИКОГДА не блокируется — если read() вернёт EAGAIN, поток переключается на другой сокет.

> [!question]- Почему для NVMe SSD рекомендуется I/O scheduler = none, а для HDD — mq-deadline? Объясни через физику устройств.
> **HDD:** механическая головка перемещается к нужной дорожке (seek time 5-10 ms). Запросы к разным частям диска = головка мечется. I/O scheduler (mq-deadline, BFQ) переупорядочивает запросы: группирует соседние, минимизирует перемещения (SCAN/elevator алгоритм). Выигрыш может быть 3-5x. **NVMe SSD:** нет механических частей, random access ≈ sequential (~50-100 µs, одинаково для любого блока). Переупорядочивание не даёт выигрыша, а scheduler добавляет ~1-2 µs latency — заметно при NVMe latency 10-20 µs (10% overhead!). `none` = запросы передаются устройству напрямую, SSD сам оптимизирует через FTL (Flash Translation Layer).

> [!question]- io_uring превосходит epoll для file I/O. Объясни архитектурную разницу (readiness vs completion model) и почему это важно.
> **epoll (readiness model):** epoll говорит "fd готов для чтения/записи". Но это обещание — реальный `read()` может заблокироваться (page cache miss, disk I/O). Для file I/O epoll вообще бесполезен — regular files ВСЕГДА "ready" по epoll, хотя read может ждать диск. **io_uring (completion model):** приложение помещает операцию (read, write, fsync) в submission queue (shared memory ring buffer). Ядро выполняет операцию асинхронно и помещает результат в completion queue. Нет промежуточного шага "проверь готовность" — операция либо завершена, либо нет. Для file I/O это единственный true-async подход на Linux. Плюс batching: 100 операций одним `io_uring_submit()` vs 100 отдельных syscalls.

> [!question]- sendfile() обеспечивает zero-copy передачу файла по сети. Нарисуй путь данных БЕЗ sendfile и С sendfile, и объясни где экономия.
> **Без sendfile (read+write):** Disk → DMA → Kernel buffer → CPU copy → User buffer → CPU copy → Socket buffer → DMA → NIC. **4 копирования, 2 context switches.** **С sendfile:** Disk → DMA → Kernel buffer → Socket buffer (DMA scatter-gather) → NIC. **2 копирования (оба DMA), 0 context switches** для данных (1 syscall вместо 2). Экономия: нет копирования в/из userspace (CPU не участвует в перемещении данных), нет переключения user↔kernel для каждого чанка. При отправке 1 GB файла: ~200ms CPU time без sendfile vs ~70ms с sendfile (~65% экономия). Nginx, Apache, Kafka используют sendfile для static content.

---

## Ключевые карточки

Три способа I/O: polling, interrupts, DMA — когда какой?
?
**Polling:** CPU в цикле проверяет статус устройства. Простой, без overhead на interrupt. Хорош для: очень быстрых устройств (NVMe в io_uring polling mode), embedded. Плох: 100% CPU даже при простое. **Interrupts:** Устройство сигналит CPU. Эффективнее polling для медленных устройств. Плох при high-rate events (interrupt livelock). **DMA:** Устройство пишет в RAM само, CPU только инициирует и получает interrupt по завершении. Необходим для: high-bandwidth устройств (диски, сеть, GPU). Минимизирует CPU involvement.

Чем epoll отличается от select/poll?
?
**select/poll:** O(N) — ядро проверяет ВСЕ fd при каждом вызове. Лимит 1024 fd (select). Передаёт весь набор fd каждый раз. **epoll:** O(1) — ядро отслеживает fd через callback, возвращает только готовые. Нет лимита. `epoll_create` → `epoll_ctl(ADD)` → `epoll_wait`. Разница при 10K соединений: select ~10ms на проверку, epoll ~0.01ms. Поэтому epoll — стандарт для high-concurrency серверов (Nginx, Node.js на Linux).

Что такое zero-copy и какие syscalls его реализуют?
?
Zero-copy — передача данных без копирования в userspace buffer. **sendfile(out_fd, in_fd, offset, count):** файл → сокет без user buffer. Используется Nginx, Kafka. **mmap + write:** файл маппится в адресное пространство, write отправляет через socket buffer. **splice():** передача между двумя fd через pipe без userspace. Экономия: до 65% CPU при отправке файлов по сети.

Что такое io_uring и почему он лучше epoll для файлов?
?
io_uring (Linux 5.1, 2019) — asynchronous I/O framework. Использует два ring buffers в shared memory: submission queue (SQ) и completion queue (CQ). **Vs epoll:** epoll = readiness model (говорит "готов", но read может заблокироваться). io_uring = completion model (операция реально выполнена). Для файлов epoll бесполезен (regular files всегда "ready"), io_uring — единственный true-async file I/O на Linux. Плюс: batching (100 операций одним syscall), polling mode (0 syscalls).

Почему I/O scheduler нужен для HDD, но не для NVMe?
?
**HDD:** механическая головка, seek time 5-10 ms. Scheduler переупорядочивает запросы для минимизации перемещений (elevator/SCAN). Выигрыш: 3-5x throughput. **NVMe SSD:** random access ≈ sequential (~50-100 µs). Нет seek time → переупорядочивание не помогает. Scheduler добавляет 1-2 µs latency (10% от NVMe latency). Поэтому `echo none > /sys/block/nvme0n1/queue/scheduler`.

Что такое interrupt livelock и как с ним борются?
?
При высокой нагрузке (тысячи пакетов/сек) каждый пакет генерирует interrupt. CPU тратит больше времени на обработку прерываний, чем на полезную работу — система "захлёбывается". **Решения:** (1) NAPI (Linux) — при высокой нагрузке переключается с interrupts на polling; (2) Interrupt coalescing — устройство накапливает пакеты и генерирует один interrupt; (3) io_uring polling mode — приложение само polling, без interrupts.

Блочное vs символьное устройство: разница и примеры?
?
**Блочное:** данные в блоках фиксированного размера (512B, 4KB), поддерживает seek (произвольный доступ), буферизуется через page cache. Примеры: HDD, SSD, NVMe. `/dev/sda`. **Символьное:** поток байтов, нет seek, нет буферизации на уровне ядра. Примеры: терминал (`/dev/tty`), serial port, `/dev/null`, `/dev/random`. Файловые системы монтируются только на блочные устройства.

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Следующий шаг | [[os-file-systems]] | Файловая система — абстракция поверх I/O устройств |
| Следующий шаг | [[os-memory-management]] | DMA, IOMMU, page cache — связь памяти и I/O |
| Углубиться | [[os-processes-threads]] | Blocking I/O, thread-per-connection vs event loop |
| Углубиться | [[os-virtualization]] | virtio для I/O в VM, device passthrough (SR-IOV) |
| Смежная тема | [[kotlin-coroutines]] | Dispatchers.IO — отдельный thread pool для блокирующего I/O |
| Смежная тема | [[android-networking]] | OkHttp использует non-blocking I/O для HTTP-запросов |
| Обзор | [[os-overview]] | Вернуться к карте раздела Operating Systems |

---

*Последнее обновление: 2026-02-14*
*Проверено: 2026-02-14*
