---
title: "CPU Architecture: что должен знать программист"
created: 2026-01-04
modified: 2026-02-13
type: deep-dive
reading_time: 19
difficulty: 5
study_status: not_started
mastery: 0
last_reviewed:
next_review:
status: published
tags:
  - topic/cs-foundations
  - type/deep-dive
  - level/intermediate
related:
  - "[[memory-model-fundamentals]]"
  - "[[processes-threads-fundamentals]]"
prerequisites:
  - "[[cs-foundations-overview]]"
---

# CPU Architecture: что должен знать программист

> **TL;DR:** CPU выполняет инструкции через цикл Fetch → Decode → Execute. Регистры — сверхбыстрая память внутри CPU (1 cycle access). Cache hierarchy: L1 (1-3 cycles, 32-128 KB) → L2 (~10 cycles, 256 KB-1 MB) → L3 (~40 cycles, 4-64 MB) → RAM (~100+ cycles). Cache miss дорогой: ~100x медленнее cache hit. Для производительности: data locality, cache-friendly patterns, избегать branch misprediction.

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| **Memory Model** | Понимание stack/heap | [[memory-model-fundamentals]] |

---

## Терминология

| Термин | Что это | Аналогия из жизни |
|--------|---------|-------------------|
| **CPU** | Центральный процессор | Мозг компьютера |
| **Register** | Сверхбыстрая память в CPU | Карманы рабочего |
| **Cache** | Быстрая память между CPU и RAM | Ящик рабочего стола |
| **Pipeline** | Конвейер выполнения инструкций | Конвейер на заводе |
| **ALU** | Арифметико-логическое устройство | Калькулятор |
| **Instruction** | Команда для CPU | Задача для работника |

---

## Зачем это знать

Большинство разработчиков воспринимают процессор как "чёрный ящик": подаёшь код — получаешь результат. Но эта абстракция ломается, когда ты сталкиваешься с performance-проблемами, которые невозможно объяснить на уровне алгоритмов. Два алгоритма с одинаковой O(n) сложностью, но один работает в 8 раз быстрее — почему? Сортировка массива перед обработкой ускоряет цикл в 5 раз — как? Ответы лежат на уровне CPU.

Для мобильного разработчика это особенно актуально. Смартфоны работают на ARM-процессорах с ограниченным энергобюджетом. Неэффективный код не просто медленный — он разряжает батарею. Понимание pipeline, cache hierarchy и branch prediction позволяет писать код, который работает "с процессором", а не "против него".

> **Ключевая идея:** CPU — не просто "быстрый калькулятор". Это сложная система с предсказанием, конвейером и многоуровневым кэшем. Код, который учитывает эти механизмы, может быть на порядок быстрее "наивного" кода с тем же алгоритмом.

---

## Историческая справка

Первый коммерчески успешный микропроцессор — **Intel 4004** (1971) — выполнял одну инструкцию за 10.8 микросекунд (около 740 КГц). Он не имел ни кэша, ни pipeline — каждая инструкция выполнялась последовательно от начала до конца.

В 1985 году Intel выпустил **386**, первый x86-процессор с pipeline. А в 1989 году **Intel i486** ввёл встроенный кэш (8 KB L1) — до этого кэш был отдельной микросхемой на материнской плате. Идея pipeline пришла из промышленности: Генри Форд показал, что конвейерная сборка автомобилей в разы быстрее, чем если один рабочий собирает машину целиком.

Patterson и Hennessy в своей книге "Computer Organization and Design" (1-е издание 1994) формализовали подход к объяснению архитектуры процессоров для программистов. Они ввели концепцию RISC (Reduced Instruction Set Computer), противопоставив её CISC (Complex Instruction Set Computer, x86). ARM — классический пример RISC-архитектуры. Сегодня граница размылась: современные x86-процессоры внутри разбивают CISC-инструкции на RISC-подобные микрооперации.

Революционная статья Ульриха Дреппера "What Every Programmer Should Know About Memory" (2007) показала, что для современных программ bottleneck — не вычисления, а **доступ к памяти**. Процессор может выполнять миллиарды операций в секунду, но если данные не в кэше, он простаивает, ожидая ответа от RAM. Эта статья изменила мышление поколения программистов.

---

## ПОЧЕМУ программисту знать CPU

### Проблема: код медленный, и непонятно почему

```kotlin
// Вариант A: ~100ms
for (i in 0 until rows) {
    for (j in 0 until cols) {
        sum += matrix[i][j]  // Row-major: cache-friendly
    }
}

// Вариант B: ~800ms (8x медленнее!)
for (j in 0 until cols) {
    for (i in 0 until rows) {
        sum += matrix[i][j]  // Column-major: cache-unfriendly
    }
}
```

Оба делают одно и то же, но разница в 8 раз. Понимание CPU объясняет почему.

### Что даёт понимание CPU

- Объяснение "магических" оптимизаций
- Понимание почему branches дорогие
- Выбор правильных структур данных
- Отладка performance проблем

---

## ЧТО такое CPU

### Основные компоненты

```
┌─────────────────────────────────────────────────────────────┐
│                    CPU ARCHITECTURE                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   ┌─────────────────────────────────────────────────────┐  │
│   │                    CPU CORE                          │  │
│   │  ┌───────────────┐    ┌───────────────┐            │  │
│   │  │   REGISTERS   │    │  CONTROL UNIT │            │  │
│   │  │ ───────────── │    │ ───────────── │            │  │
│   │  │ PC, SP, R0-R15│    │ Fetch/Decode  │            │  │
│   │  │ FLAGS, etc.   │    │ Control logic │            │  │
│   │  └───────────────┘    └───────────────┘            │  │
│   │                                                      │  │
│   │  ┌───────────────┐    ┌───────────────┐            │  │
│   │  │     ALU       │    │     FPU       │            │  │
│   │  │ ───────────── │    │ ───────────── │            │  │
│   │  │ +, -, *, /    │    │ Float ops     │            │  │
│   │  │ AND, OR, XOR  │    │               │            │  │
│   │  └───────────────┘    └───────────────┘            │  │
│   │                                                      │  │
│   │  ┌─────────────────────────────────────────┐       │  │
│   │  │              L1 CACHE                    │       │  │
│   │  │  I-Cache (instructions) + D-Cache (data) │       │  │
│   │  └─────────────────────────────────────────┘       │  │
│   └─────────────────────────────────────────────────────┘  │
│                          │                                  │
│                          ▼                                  │
│   ┌─────────────────────────────────────────────────────┐  │
│   │                    L2 CACHE                          │  │
│   └─────────────────────────────────────────────────────┘  │
│                          │                                  │
│                          ▼                                  │
│   ┌─────────────────────────────────────────────────────┐  │
│   │              L3 CACHE (shared)                       │  │
│   └─────────────────────────────────────────────────────┘  │
│                          │                                  │
│                          ▼                                  │
│   ┌─────────────────────────────────────────────────────┐  │
│   │                    RAM                               │  │
│   └─────────────────────────────────────────────────────┘  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Регистры: самая быстрая память

### Типы регистров

| Регистр | Назначение |
|---------|------------|
| **Program Counter (PC)** | Адрес следующей инструкции |
| **Stack Pointer (SP)** | Вершина стека |
| **Instruction Register (IR)** | Текущая инструкция |
| **General Purpose (R0-R15)** | Данные для вычислений |
| **Flags/Status** | Результаты сравнений (zero, carry) |

### Регистры x86-64

```
┌─────────────────────────────────────────────────────────────┐
│                x86-64 GENERAL PURPOSE REGISTERS             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   64-bit    32-bit    16-bit    8-bit                      │
│   ───────   ───────   ───────   ───────                    │
│   RAX       EAX       AX        AL        Accumulator      │
│   RBX       EBX       BX        BL        Base             │
│   RCX       ECX       CX        CL        Counter          │
│   RDX       EDX       DX        DL        Data             │
│   RSI       ESI       SI        SIL       Source Index     │
│   RDI       EDI       DI        DIL       Dest Index       │
│   RSP       ESP       SP        SPL       Stack Pointer    │
│   RBP       EBP       BP        BPL       Base Pointer     │
│   R8-R15    R8D-R15D  R8W-R15W  R8B-R15B  Additional       │
│                                                             │
│   Доступ: 1 cycle (< 1 ns)                                 │
│   Количество: ограничено (~16 GPR)                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Почему регистры быстрые

- Физически внутри CPU
- Нет bus latency
- Прямое подключение к ALU
- Hardware address (не memory address)

---

## Instruction Cycle: как CPU работает

### Fetch → Decode → Execute

```
┌─────────────────────────────────────────────────────────────┐
│                 INSTRUCTION CYCLE                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   1. FETCH                                                  │
│      ┌─────────┐                                           │
│      │ Memory  │──▶ Загрузить инструкцию по адресу в PC    │
│      └─────────┘    PC++                                   │
│           │                                                 │
│           ▼                                                 │
│   2. DECODE                                                 │
│      ┌─────────┐                                           │
│      │ Decoder │──▶ Определить операцию                    │
│      └─────────┘    Определить операнды                    │
│           │                                                 │
│           ▼                                                 │
│   3. EXECUTE                                                │
│      ┌─────────┐                                           │
│      │   ALU   │──▶ Выполнить операцию                     │
│      └─────────┘    (сложение, сравнение, etc.)            │
│           │                                                 │
│           ▼                                                 │
│   4. MEMORY (опционально)                                  │
│      ┌─────────┐                                           │
│      │ Memory  │──▶ Load/Store данных                      │
│      └─────────┘                                           │
│           │                                                 │
│           ▼                                                 │
│   5. WRITE BACK                                             │
│      ┌─────────┐                                           │
│      │Register │──▶ Записать результат                     │
│      └─────────┘                                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Pipeline: сборочный конвейер внутри процессора

Аналогия с заводским конвейером — лучший способ понять pipeline. Представь фабрику по сборке смартфонов. Без конвейера: один рабочий берёт корпус, вставляет плату, ставит экран, закручивает винты, кладёт в коробку. Следующий смартфон начинается только после полной сборки предыдущего. Если каждый этап занимает 1 минуту, а этапов 5 — один смартфон за 5 минут.

С конвейером: 5 рабочих, каждый делает один этап. Пока первый рабочий вставляет плату в корпус #2, второй ставит экран на корпус #1. Каждую минуту с конвейера сходит готовый смартфон (после начального "разогрева"). Вместо 1 смартфона за 5 минут — 1 смартфон каждую минуту. Ускорение в 5 раз, при тех же рабочих.

CPU pipeline работает по тому же принципу. Каждая инструкция разбивается на этапы (stages), и несколько инструкций обрабатываются одновременно — каждая на своём этапе. Современные процессоры имеют от 14 (ARM Cortex-A78) до 20+ стадий pipeline (Intel Core).

Но pipeline имеет уязвимость: если одна инструкция зависит от результата предыдущей, конвейер **останавливается** (stall). Это как если второй рабочий не может ставить экран, пока первый не вставит плату. Три типа проблем: **data hazard** (одна инструкция ждёт результат другой), **control hazard** (условный переход — CPU не знает, какую следующую инструкцию загружать) и **structural hazard** (два этапа хотят использовать один ресурс одновременно).

```
┌─────────────────────────────────────────────────────────────┐
│                    CPU PIPELINE                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Без pipeline: 5 инструкций = 25 cycles                   │
│   ┌─────┬─────┬─────┬─────┬─────┐                          │
│   │ I1  │ I2  │ I3  │ I4  │ I5  │                          │
│   │FDEXW│FDEXW│FDEXW│FDEXW│FDEXW│                          │
│   └─────┴─────┴─────┴─────┴─────┘                          │
│   Cycles: 5 + 5 + 5 + 5 + 5 = 25                           │
│                                                             │
│   С pipeline: 5 инструкций = 9 cycles                      │
│   Cycle:  1   2   3   4   5   6   7   8   9                │
│   I1:     F   D   E   X   W                                │
│   I2:         F   D   E   X   W                            │
│   I3:             F   D   E   X   W                        │
│   I4:                 F   D   E   X   W                    │
│   I5:                     F   D   E   X   W                │
│                                                             │
│   F=Fetch, D=Decode, E=Execute, X=Memory, W=WriteBack      │
│                                                             │
│   Выигрыш: почти 3x быстрее!                               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Pipeline Hazards и Branch Prediction

**Branch misprediction:** CPU предсказывает направление branch. Если ошибся — pipeline flush (~10-20 cycles penalty).

```kotlin
// Непредсказуемый branch — дорого для процессора
if (random() > 0.5) {  // 50/50 — CPU не может предсказать
    doA()
} else {
    doB()
}

// Предсказуемый branch — почти бесплатный
if (user.isAdmin) {  // Обычно false — CPU быстро учится
    showAdminPanel()
}
```

Этот код иллюстрирует важную разницу: первый if непредсказуем (50/50), и процессор будет ошибаться примерно в половине случаев. Каждая ошибка — потеря 10-20 тактов на сброс pipeline. Второй if предсказуем (почти всегда false), и после нескольких итераций процессор научится всегда предсказывать false с точностью ~99%.

---

## Branch Prediction: КАК процессор "угадывает" будущее

### Проблема: конвейер не знает, куда идти

Когда pipeline встречает условный переход (if/else, цикл), возникает дилемма: какую следующую инструкцию загружать? Результат сравнения станет известен только через несколько тактов (когда инструкция сравнения дойдёт до стадии Execute). Но pipeline не может ждать — каждый такт он должен загружать новую инструкцию.

Решение — **branch prediction** (предсказание переходов). CPU "угадывает", куда пойдёт ветвление, и начинает выполнять инструкции из предсказанной ветки **спекулятивно** (speculative execution). Если угадал — выигрыш: никакого простоя. Если ошибся — **pipeline flush**: все спекулятивно выполненные инструкции выбрасываются, и pipeline начинает заполняться заново.

### Аналогия: навигатор в автомобиле

Представь, что ты едешь по шоссе со скоростью 100 км/ч и приближаешься к развилке. Навигатор должен сказать "налево" или "направо", но связь с интернетом плохая, и ответ придёт через 5 секунд. Если ты остановишься и подождёшь — потеряешь 5 секунд. Если угадаешь правильно — ни секунды потери. Если ошибёшься — придётся развернуться (ещё дороже, чем просто подождать).

Branch predictor — это "интуиция" навигатора, основанная на паттернах. "Обычно на этой развилке поворачивают направо" — static prediction. "Последние 10 раз ты поворачивал налево" — dynamic prediction. Современные процессоры имеют точность предсказания 95-99% для типичного кода.

### Типы предсказателей

**Static prediction** — простейший подход. Правило: переходы назад (циклы) предсказываются как "taken" (потому что цикл обычно повторяется), переходы вперёд — как "not taken". Этого достаточно для простых циклов.

**Dynamic prediction (2-bit saturating counter)** — для каждого branch хранится 2-битный счётчик с четырьмя состояниями: Strongly Not Taken → Weakly Not Taken → Weakly Taken → Strongly Taken. Два ошибочных предсказания подряд нужны, чтобы "перевернуть" предсказание. Это защищает от единичных "отклонений" (например, выход из цикла после 1000 итераций ломает предсказание только один раз).

```
┌─────────────────────────────────────────────────────────────┐
│               BRANCH PREDICTION IMPACT                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Знаменитый пример со StackOverflow:                      │
│                                                             │
│   Обработка массива 0..255 (случайные числа):              │
│                                                             │
│   // БЕЗ сортировки: ~12 секунд                            │
│   for (x in data) {                                        │
│       if (x >= 128) sum += x   // Непредсказуемый branch   │
│   }                                                         │
│                                                             │
│   // С сортировкой: ~4 секунды (3x быстрее!)               │
│   data.sort()                                               │
│   for (x in data) {                                        │
│       if (x >= 128) sum += x   // Предсказуемый: NNN...YYY │
│   }                                                         │
│                                                             │
│   После сортировки branch предсказуем:                     │
│   [0,1,2,...,127,128,129,...,255]                           │
│   NNNNN...NNNNNYYYYY...YYYYY                               │
│   CPU ошибается только 1 раз (в точке перехода 127→128)    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

Этот пример демонстрирует, как предсортировка данных может ускорить обработку в разы за счёт предсказуемости ветвлений. В несортированном массиве branch `x >= 128` принимает случайные значения — процессор ошибается в ~50% случаев. В отсортированном массиве branch сначала всегда false (значения 0-127), потом всегда true (128-255) — процессор ошибается ровно один раз.

> **Практический вывод:** Если ты обрабатываешь данные в hot loop с условием — рассмотри предварительную сортировку или разделение данных на две группы. Branchless-код (замена if на арифметику) — ещё лучше, но менее читаем.

Мы разобрали, как pipeline и branch prediction влияют на скорость выполнения инструкций. Но даже идеальный pipeline бесполезен, если данные не готовы. Следующая тема — иерархия кэшей, которая определяет, как быстро CPU получает данные.

---

## Cache Hierarchy: ПОЧЕМУ многоуровневый кэш

### Проблема: processor-memory gap

С 1980-х годов скорость процессоров росла примерно на 60% в год, а скорость памяти (DRAM) — только на 7% в год. К 2000-м разрыв стал огромным: процессор может выполнить сотню операций за то время, пока один запрос к RAM вернёт данные. Этот разрыв называется **processor-memory gap**, и именно для его решения была изобретена иерархия кэшей.

Аналогия: представь программиста за столом. На столе (регистры) — карандаш и лист бумаги. Мгновенный доступ. В ящике стола (L1 cache) — несколько справочников. 2-3 секунды. На полке за спиной (L2 cache) — больше книг. 10 секунд. На стеллаже в другом конце комнаты (L3 cache) — целая библиотека. 40 секунд. В библиотеке через дорогу (RAM) — миллионы книг. 5 минут на дорогу туда и обратно.

Очевидная стратегия: держи на столе то, что используешь чаще всего. Это и делает кэш — хранит недавно использованные и "соседние" данные ближе к процессору.

### Конкретные числа латентности

| Уровень | Размер (типичный) | Латентность | Аналогия |
|---------|-------------------|-------------|----------|
| **Регистры** | ~1 KB | < 1 ns (1 cycle) | Карман |
| **L1 Cache** | 32-128 KB per core | ~1-2 ns (3-5 cycles) | Ящик стола |
| **L2 Cache** | 256 KB - 1 MB per core | ~3-5 ns (10-15 cycles) | Полка за спиной |
| **L3 Cache** | 4-64 MB shared | ~10-20 ns (30-50 cycles) | Стеллаж в комнате |
| **RAM (DRAM)** | 8-64 GB | ~50-100 ns (100-300 cycles) | Библиотека через дорогу |
| **SSD** | 256 GB - 4 TB | ~25-100 μs | Архив в другом городе |
| **HDD** | 1-10 TB | ~5-10 ms | Архив на другом континенте |

Обрати внимание на числа: разница между L1 и RAM — **примерно 100x**. Это означает, что один cache miss стоит столько же, сколько 100 арифметических операций. Вот почему data locality (расположение данных рядом) часто важнее алгоритмической сложности для массивов до нескольких тысяч элементов.

### ПОЧЕМУ несколько уровней, а не один большой кэш?

Потому что размер и скорость — противоположные свойства. Физически быстрая память (SRAM) дорогая и занимает много места на кристалле. Чтобы L1 был быстрым (1-3 цикла), он должен быть маленьким и расположенным максимально близко к ядру. L2 побольше, но чуть дальше. L3 ещё больше, но разделяется между ядрами и ещё медленнее.

Аналогия: почему в магазине у кассы стоят маленькие полки с жвачкой и шоколадками (самые ходовые товары, мгновенный доступ), а не весь ассортимент? Потому что место у кассы ограничено. Чем больше ассортимент — тем дальше от кассы (склад), тем дольше до товара.

### Уровни cache

```
┌─────────────────────────────────────────────────────────────┐
│                   MEMORY HIERARCHY                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Скорость                           Размер                 │
│   ▲                                  ▲                     │
│   │  ┌─────────────┐                │                      │
│   │  │  REGISTERS  │  1 cycle       │  ~1 KB               │
│   │  └─────────────┘                │                      │
│   │         ▼                       │                      │
│   │  ┌─────────────┐                │                      │
│   │  │  L1 CACHE   │  1-3 cycles    │  32-128 KB           │
│   │  └─────────────┘                │                      │
│   │         ▼                       │                      │
│   │  ┌─────────────┐                │                      │
│   │  │  L2 CACHE   │  ~10 cycles    │  256 KB - 1 MB       │
│   │  └─────────────┘                │                      │
│   │         ▼                       │                      │
│   │  ┌─────────────┐                │                      │
│   │  │  L3 CACHE   │  ~40 cycles    │  4-64 MB             │
│   │  └─────────────┘                │                      │
│   │         ▼                       │                      │
│   │  ┌─────────────┐                │                      │
│   │  │    RAM      │  ~100+ cycles  │  8-64 GB             │
│   │  └─────────────┘                ▼                      │
│   │                                                         │
│   │  Разница L1 vs RAM: ~100x!                             │
│   │                                                         │
└─────────────────────────────────────────────────────────────┘
```

### Cache Hit vs Miss

```
┌─────────────────────────────────────────────────────────────┐
│                CACHE HIT vs MISS                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   CACHE HIT (данные в cache):                              │
│   CPU → L1 → ✓ Found! → Return data                        │
│   Время: 1-3 cycles                                        │
│                                                             │
│   CACHE MISS (данные не в cache):                          │
│   CPU → L1 → ✗ → L2 → ✗ → L3 → ✗ → RAM → Load to cache    │
│   Время: 100+ cycles                                       │
│                                                             │
│   Разница: до 100x медленнее!                              │
│                                                             │
│   Locality of Reference:                                    │
│   - Temporal: недавно использованное нужно снова           │
│   - Spatial: соседние данные тоже нужны                    │
│                                                             │
│   Поэтому cache загружает целые cache lines (~64 bytes)    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Практика: cache-friendly код

### Row-major vs Column-major

```kotlin
val matrix = Array(1000) { IntArray(1000) }

// ✅ Cache-friendly: sequential access
for (i in 0 until 1000) {
    for (j in 0 until 1000) {
        sum += matrix[i][j]  // Соседние элементы в памяти
    }
}
// matrix[0][0], [0][1], [0][2]... в одной cache line

// ❌ Cache-unfriendly: random access pattern
for (j in 0 until 1000) {
    for (i in 0 until 1000) {
        sum += matrix[i][j]  // Прыжки через 1000 элементов
    }
}
// matrix[0][0], [1][0], [2][0]... в разных cache lines
```

### Struct of Arrays vs Array of Structs

```kotlin
// ❌ Array of Structs (AoS)
data class Particle(val x: Float, val y: Float, val z: Float, val mass: Float)
val particles = Array(10000) { Particle(0f, 0f, 0f, 1f) }

// При обработке только x: загружаем y, z, mass тоже
for (p in particles) {
    sumX += p.x  // Cache загружает все 16 bytes
}

// ✅ Struct of Arrays (SoA)
class ParticleSystem(val n: Int) {
    val x = FloatArray(n)
    val y = FloatArray(n)
    val z = FloatArray(n)
    val mass = FloatArray(n)
}

// При обработке только x: загружаем только x
for (i in 0 until n) {
    sumX += x[i]  // Cache эффективен
}
```

---

## Подводные камни

### Распространённые ошибки

| Ошибка | Последствие | Решение |
|--------|-------------|---------|
| Случайный доступ к большому массиву | Cache misses | Sequential access |
| Много branches в hot loop | Pipeline stalls | Branchless code, sorting |
| False sharing (многопоточность) | Cache invalidation | Padding между данными потоков |

### Мифы и заблуждения

**Миф:** "Современные CPU настолько быстрые, что оптимизация не нужна"
**Реальность:** Cache miss в 100x дороже cache hit. На больших данных это критично.

**Миф:** "Компилятор всё оптимизирует"
**Реальность:** Компилятор не может изменить алгоритм или layout данных. Программист должен думать о locality.

---

## Распространённые заблуждения

**Заблуждение:** "Современные CPU настолько быстрые, что оптимизация не нужна"
**Реальность:** Вычислительная мощность CPU растёт, но bottleneck давно переместился с вычислений на доступ к памяти. Cache miss в 100x дороже cache hit. На больших данных (массивы от 10K+ элементов) разница между cache-friendly и cache-unfriendly кодом может составлять 10x. Это не теория — это измеримые числа на реальном hardware.

**Заблуждение:** "Компилятор всё оптимизирует автоматически"
**Реальность:** Компилятор отлично оптимизирует инструкции (register allocation, instruction scheduling, loop unrolling). Но он **не может изменить** алгоритм, layout данных или порядок обхода массива. Решение использовать Array of Structs или Struct of Arrays — ответственность программиста. Компилятор не знает, какие поля ты будешь обрабатывать чаще.

**Заблуждение:** "Branch prediction — экзотика, о которой думают только авторы компиляторов"
**Реальность:** Знаменитый вопрос на StackOverflow "Why is processing a sorted array faster than an unsorted array?" (2012) набрал 35 000+ голосов — потому что разница в 3-5x шокирует. Branch prediction влияет на каждый if в hot loop. Предсортировка данных, branchless code, устранение непредсказуемых branches — практические техники, доступные любому программисту.

---

## Связь с другими темами

**[[memory-model-fundamentals]]** — модель памяти (stack, heap, адресация) — фундамент для понимания кэширования. Cache работает на уровне адресов памяти, и знание того, как данные раскладываются в memory, помогает понять cache behavior.

**[[processes-threads-fundamentals]]** — каждый поток использует CPU pipeline и конкурирует за кэш. Понимание CPU объясняет, почему context switch дорогой (сброс кэша) и почему false sharing (два потока модифицируют данные в одной cache line) убивает производительность.

**[[os-fundamentals-for-devs]]** — ОС управляет CPU через scheduling (какой процесс выполняется) и virtual memory (трансляция адресов). Без понимания CPU невозможно понять, ПОЧЕМУ context switch дорогой и ПОЧЕМУ TLB miss влияет на производительность.

---

## Источники и дальнейшее чтение

- Patterson, D. & Hennessy, J. (2017). *Computer Organization and Design: The Hardware/Software Interface*. — Стандартный учебник по архитектуре компьютеров. Главы о pipeline, cache hierarchy и branch prediction написаны с позиции программиста, не hardware-инженера. RISC-V edition (2020) содержит актуальные примеры.
- Drepper, U. (2007). *What Every Programmer Should Know About Memory*. — Классическая статья (114 страниц), объясняющая иерархию памяти, cache-friendly programming и NUMA. Несмотря на возраст, фундаментальные принципы не изменились. Обязательное чтение.
- Bryant, R. & O'Hallaron, D. (2015). *Computer Systems: A Programmer's Perspective*. — Главы 4 (Processor Architecture) и 6 (The Memory Hierarchy) — лучшее объяснение pipeline и кэширования с практическими примерами влияния на производительность кода.
- Hennessy, J. & Patterson, D. (2019). *Computer Architecture: A Quantitative Approach*. — Более продвинутая книга тех же авторов. Подробный количественный анализ branch prediction, out-of-order execution и cache coherence. Для тех, кто хочет понять глубже.

---

## Проверь себя

> [!question]- Почему обход двумерного массива по строкам (row-major) быстрее, чем по столбцам, и как это связано с кэш-иерархией?
> Кэш загружает данные cache line'ами (обычно 64 байта). При row-major обходе элементы расположены последовательно в памяти — одна загрузка cache line даёт доступ к ~16 int значениям. При column-major обходе каждый следующий элемент отстоит на размер строки — каждое обращение = cache miss. Разница может достигать 10-100x для больших массивов. Это data locality — ключевой принцип кэш-эффективного программирования.

> [!question]- Доступ к L1 кэшу занимает 1-3 цикла, а к RAM — 100+ циклов. Как эта разница влияет на проектирование структур данных?
> Структуры данных, дружественные к кэшу, значительно быстрее. ArrayList (contiguous memory) быстрее LinkedList (scattered nodes) для итерации, потому что элементы ArrayList в одной cache line. HashMap с open addressing быстрее HashMap с chaining (меньше pointer chasing). Компактные объекты (примитивы, value classes) лучше объектов с указателями (меньше indirection = меньше cache misses). Это причина, почему IntArray быстрее Array<Int> на JVM.

> [!question]- Что такое branch misprediction и почему отсортированный массив обрабатывается быстрее неотсортированного при if-проверке каждого элемента?
> CPU использует branch predictor — предсказывает, какая ветка if/else выполнится, и начинает исполнять инструкции заранее (speculative execution). В отсортированном массиве паттерн предсказуем: сначала все false, потом все true — предсказатель быстро "учится". В неотсортированном — случайное чередование, предсказатель ошибается ~50% времени. Каждый misprediction = flush pipeline (~15-20 потерянных циклов). На массиве из миллионов элементов разница может быть 5-6x.

---

## Ключевые карточки

Каковы уровни cache hierarchy и их латентности?
?
L1: 1-3 цикла, 32-128 KB (per core). L2: ~10 циклов, 256 KB-1 MB (per core). L3: ~40 циклов, 4-64 MB (shared). RAM: ~100+ циклов, гигабайты. Каждый уровень ~3-10x медленнее предыдущего, но ~10-100x больше по объёму. Cache miss на L1 = обращение к L2, miss на всех уровнях = обращение к RAM.

---

Что такое pipeline в CPU и как он ускоряет исполнение?
?
Pipeline — разделение исполнения инструкции на стадии (Fetch, Decode, Execute, Memory, Write-back), выполняемые параллельно для разных инструкций. Без pipeline: 5 циклов на инструкцию. С 5-stage pipeline: после заполнения — 1 инструкция за цикл (5x ускорение). Современные CPU: 14-20 стадий. Pipeline hazards (data, control, structural) снижают эффективность.

---

Чем RISC отличается от CISC?
?
RISC (ARM): простые инструкции фиксированной длины, много регистров, load/store архитектура — только load/store обращаются к памяти. CISC (x86): сложные инструкции переменной длины, операции напрямую с памятью. Современные x86 CPU внутренне decode CISC в micro-ops (RISC-подобные) — граница размыта. ARM доминирует в мобильных (iOS, Android) из-за энергоэффективности.

---

Что такое cache line и почему его размер важен?
?
Cache line — минимальная единица загрузки кэша (обычно 64 байта). При обращении к одному байту — загружаются 64 байта вокруг. Следствия: (1) Последовательный доступ к данным — бесплатные попадания в кэш. (2) False sharing: два потока модифицируют разные переменные в одной cache line — invalidation на каждую запись. (3) Alignment структур данных на cache line boundary улучшает производительность.

---

Что такое out-of-order execution?
?
CPU переупорядочивает инструкции для максимального использования pipeline. Если инструкция A ждёт данных из RAM (100 циклов), CPU выполняет независимые инструкции B, C, D пока ждёт. Результаты "упорядочиваются" обратно (retirement). Это маскирует латентность памяти. Программист видит последовательное исполнение, но hardware параллелизирует.

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Следующий шаг | [[memory-model-fundamentals]] | Как программист работает с памятью (stack, heap, виртуальная память) |
| Углубиться | [[os-fundamentals-for-devs]] | Как ОС управляет CPU, памятью и процессами |
| Смежная тема | [[compilation-pipeline]] | Как компилятор генерирует инструкции для CPU |
| Обзор | [[cs-foundations-overview]] | Вернуться к навигации по разделу |

---

*Проверено: 2026-02-13*
