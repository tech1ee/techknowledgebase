---
title: "Compilation Pipeline: путь кода от текста до исполнения"
created: 2026-01-04
modified: 2026-02-13
type: deep-dive
reading_time: 33
difficulty: 5
study_status: not_started
mastery: 0
last_reviewed:
next_review:
status: published
tags:
  - topic/cs-foundations
  - type/deep-dive
  - level/intermediate
related:
  - "[[bytecode-virtual-machines]]"
  - "[[native-compilation-llvm]]"
  - "[[interpretation-jit]]"
prerequisites:
  - "[[cpu-architecture-basics]]"
---

# Compilation Pipeline: путь кода от текста до исполнения

> **TL;DR:** Компиляция — превращение текста программы в исполняемый код через цепочку этапов: разбить на слова (токены) → построить дерево структуры (AST) → проверить смысл (типы, scope) → создать промежуточный код (IR) → оптимизировать → сгенерировать машинный код. Для KMP критично: Kotlin компилирует один код в разные форматы (JVM bytecode, LLVM IR, JavaScript) через единый frontend (K2) и разные backend'ы.

---

## Зачем это знать

Большинство разработчиков воспринимают компилятор как чёрный ящик: код вошёл — бинарник вышел. Но каждое сообщение об ошибке, которое показывает IDE, каждая оптимизация, которая ускоряет программу, каждый warning — это результат работы конкретного этапа pipeline. Понимание этих этапов превращает загадочные ошибки компиляции в понятные диагностические сообщения. Когда ты видишь "Unexpected token 'val' at position 15", ты уже знаешь: проблема на этапе лексического анализа, компилятор не смог разбить текст на слова. А "Type mismatch: expected Int, found String" — это семантический анализ, третий этап, уже после успешного построения дерева.

Для KMP-разработчика это понимание вдвойне важно: один и тот же Kotlin-код проходит через единый frontend, но затем расходится по разным backend'ам. Ошибка может возникнуть на одной платформе и не возникнуть на другой — и чтобы понять почему, нужно знать, на каком этапе pipeline она рождается.

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| **Базовое программирование** | Понимать что такое код | Любой туториал |
| **Переменные и типы** | Понять type checking | Основы языка |

---

## Терминология

| Термин | Что это | Аналогия |
|--------|---------|----------|
| **Compiler** | Программа, переводящая код в другой формат | Переводчик книги |
| **Token** | Минимальная единица кода | Слово в предложении |
| **Lexer / Tokenizer** | Разбивает код на токены | Делит текст на слова |
| **Parser** | Строит структуру из токенов | Разбирает грамматику предложения |
| **AST** (Abstract Syntax Tree) | Дерево структуры программы | Схема предложения |
| **IR** (Intermediate Representation) | Промежуточный код между исходником и машинным | Черновик перевода |
| **SSA** (Static Single Assignment) | Форма IR где переменная присваивается один раз | Каждая запись — новая строка |
| **Frontend** | Часть компилятора до IR | Понимание исходного языка |
| **Backend** | Часть компилятора после IR | Генерация целевого кода |

---

## ПОЧЕМУ появились компиляторы

### До компиляторов: эпоха машинного кода

В 1940-х программисты писали программы напрямую в машинном коде — последовательности чисел, которые процессор понимает. Это выглядело примерно так:

```
10110000 01100001  ; MOV AL, 0x61 - записать число 97 в регистр
```

Каждая инструкция — число. Адреса памяти — числа. Даже данные — числа. Программист держал в голове что значит каждое число. Ошибки были неизбежны, отладка мучительна.

Представь себе архитектора, который вместо чертежей описывает здание в виде координат каждого кирпича: "кирпич 1: x=0, y=0, z=0; кирпич 2: x=0.25, y=0, z=0..." Технически верно. Практически невозможно. Именно так чувствовали себя программисты 1940-х годов.

### 1952: Grace Hopper и первый компилятор

Grace Hopper, работая на компьютере UNIVAC, создала A-0 — первый компилятор. Идея была революционной: что если машина сама переведёт понятный человеку код в машинный?

Коллеги скептически отнеслись к идее. Компьютер — вычислительная машина, как он может "понимать" текст? Но Hopper доказала: компилятор — это просто программа, которая берёт один текст и производит другой.

```
┌─────────────────────────────────────────────────────────────────┐
│                    ИДЕЯ GRACE HOPPER                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   БЫЛО (машинный код):                                          │
│   10110000 01100001 11001101 00010000 ...                       │
│                                                                 │
│   СТАЛО (человеческий код):                                     │
│   PRINT "Hello"                                                 │
│                                                                 │
│   Компилятор: программа → машинный код                          │
│                                                                 │
│   "Я считала, что раз могу написать программу, значит           │
│    могу написать программу, которая пишет программы"            │
│                                    — Grace Hopper               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 1957: FORTRAN и практическое применение

Первый широко используемый компилятор появился с языком FORTRAN (FORmula TRANslation). Джон Бэкус и его команда в IBM потратили 3 года на создание компилятора, который переводил математические формулы в машинный код.

Скептики утверждали: автоматически сгенерированный код будет медленнее ручного. Команда FORTRAN поставила задачу: код должен быть не хуже написанного вручную. И справились — FORTRAN-программы работали с такой же скоростью.

### Зачем нужны компиляторы сегодня

Компиляторы решают три фундаментальные проблемы:

**1. Абстракция над железом.** Программист думает о логике, не о регистрах процессора. Код `val x = a + b` работает на любом процессоре — компилятор адаптирует.

**2. Проверка ошибок до запуска.** Компилятор находит опечатки, несоответствия типов, отсутствующие переменные. Лучше узнать об ошибке при компиляции, чем когда программа упадёт у пользователя.

**3. Оптимизация.** Компилятор видит паттерны, которые человек может упустить. Он убирает лишний код, упрощает вычисления, переупорядочивает инструкции для скорости.

---

## ЧТО такое Compilation Pipeline

Компиляция — не одно действие, а конвейер (pipeline) этапов. Каждый этап берёт результат предыдущего и производит вход для следующего. Аналогия из реальной жизни: сборочная линия на заводе. Сырьё входит с одного конца, на каждом этапе рабочие выполняют свою операцию (резка, сварка, покраска, проверка качества), и готовый продукт выходит с другого конца. Каждый рабочий видит только свою операцию и результат предыдущего этапа.

```
┌─────────────────────────────────────────────────────────────────┐
│                  COMPILATION PIPELINE                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Source Code                                                   │
│   "val x = 2 + 3"                                               │
│         │                                                       │
│         ▼                                                       │
│   ┌─────────────────┐                                           │
│   │ LEXICAL ANALYSIS│  → Токены: [val] [x] [=] [2] [+] [3]     │
│   └────────┬────────┘                                           │
│            ▼                                                    │
│   ┌─────────────────┐                                           │
│   │ SYNTAX ANALYSIS │  → AST: дерево структуры                  │
│   └────────┬────────┘                                           │
│            ▼                                                    │
│   ┌─────────────────┐                                           │
│   │SEMANTIC ANALYSIS│  → Проверены типы, разрешены имена       │
│   └────────┬────────┘                                           │
│            ▼                                                    │
│   ┌─────────────────┐                                           │
│   │  IR GENERATION  │  → Промежуточный код                      │
│   └────────┬────────┘                                           │
│            ▼                                                    │
│   ┌─────────────────┐                                           │
│   │  OPTIMIZATION   │  → Улучшенный IR                          │
│   └────────┬────────┘                                           │
│            ▼                                                    │
│   ┌─────────────────┐                                           │
│   │ CODE GENERATION │  → Машинный код / Bytecode                │
│   └─────────────────┘                                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Frontend и Backend

Компилятор делится на две части:

**Frontend** — понимает исходный язык:
- Lexer (разбивает на токены)
- Parser (строит AST)
- Semantic Analyzer (проверяет типы)
- IR Generator (создаёт промежуточный код)

**Backend** — генерирует целевой код:
- Optimizer (улучшает IR)
- Code Generator (создаёт машинный код)

Почему такое разделение? Представь: есть 5 языков и 5 платформ. Без разделения нужно 25 компиляторов (каждый язык для каждой платформы). С разделением — 5 frontend'ов + 5 backend'ов = 10 компонентов. Frontend генерирует общий IR, backend'ы его потребляют.

Это та же идея, что и розетка с адаптером. Вместо того чтобы делать для каждого прибора свой провод к электростанции, мы стандартизировали розетку. IR — это "розетка" компилятора: стандартный интерфейс между пониманием языка и генерацией кода.

```
┌─────────────────────────────────────────────────────────────────┐
│                FRONTEND / BACKEND РАЗДЕЛЕНИЕ                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   FRONTENDS          COMMON IR           BACKENDS               │
│   ─────────         ─────────           ────────               │
│   Kotlin ───┐                      ┌─── JVM Bytecode           │
│   Java ─────┼───▶ [ IR ] ◀────────┼─── ARM Assembly            │
│   Scala ────┘                      └─── x86 Assembly           │
│                                                                 │
│   5 языков + 5 платформ = 10 компонентов                        │
│   (вместо 25 отдельных компиляторов)                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Мы разобрали общую архитектуру pipeline. Теперь пройдём каждый этап в деталях — от первого символа исходного кода до последней машинной инструкции. Для наглядности проведём один пример через все этапы: выражение `val x = a + b * c`.

---

## Сквозной пример: `val x = a + b * c` через все этапы

Чтобы по-настоящему понять pipeline, нужно увидеть один и тот же код глазами каждого этапа. Представьте шестерых специалистов на конвейере. Каждый получает результат работы предыдущего, делает свою работу, и передаёт дальше. Каждый видит код по-своему.

Наш пример — простое выражение: `val x = a + b * c`. Оно выглядит элементарно, но проходит через все шесть этапов, и на каждом с ним происходят трансформации.

```
┌─────────────────────────────────────────────────────────────────┐
│              СКВОЗНОЙ ПРИМЕР: val x = a + b * c                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Исходный текст ──▶ "val x = a + b * c"                        │
│        │                                                        │
│  [1] Lexer    ──▶ [val][x][=][a][+][b][*][c]                   │
│        │                                                        │
│  [2] Parser   ──▶ Дерево: x = (a + (b * c))                   │
│        │                                                        │
│  [3] Semantic ──▶ x:Int = (a:Int + (b:Int * c:Int)):Int        │
│        │                                                        │
│  [4] IR Gen   ──▶ t1 = b * c; t2 = a + t1; x = t2             │
│        │                                                        │
│  [5] Optimize ──▶ (зависит от значений a, b, c)               │
│        │                                                        │
│  [6] CodeGen  ──▶ MUL R1,R2,R3; ADD R4,R0,R1; STR R4,[x]     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Разберём каждый этап подробно.

---

## КАК работает каждый этап

### 1. Lexical Analysis (Лексический анализ)

#### ЧТО делает

Первый этап: превратить поток символов в поток токенов. Токен — минимальная смысловая единица языка. Lexer (он же tokenizer, он же scanner) читает исходный код символ за символом и группирует символы в токены — точно так же, как человек при чтении группирует буквы в слова.

Аналогия: представь, что тебе дали текст без пробелов: "вальксравнояплюсбэумножитьцэ". Первое, что сделает мозг — разобьёт на слова: "валь-кс-равно-я-плюс-бэ-умножить-цэ". Lexer делает ровно это, но с кодом.

#### ЗАЧЕМ нужен

Без лексического анализа компилятор видит поток символов: `v`, `a`, `l`, ` `, `x`, ` `, `=`, ` `, `a`, ` `, `+`, ` `, `b`, ` `, `*`, ` `, `c`. Это 17 символов, и среди них — пробелы, которые ничего не значат, но занимают место. Lexer убирает шум и создаёт чистый поток смысловых единиц.

Кроме того, lexer запоминает позицию каждого токена в исходном файле — номер строки и столбец. Эта информация нужна для сообщений об ошибках: "Error at line 5, column 12" — это позиция, которую сохранил lexer.

#### Сквозной пример: `val x = a + b * c`

Lexer читает символ за символом и распознаёт:

```
┌─────────────────────────────────────────────────────────────────┐
│         ЛЕКСИЧЕСКИЙ АНАЛИЗ: val x = a + b * c                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Входная строка: "val x = a + b * c"                          │
│                    ↓↓↓ ↓   ↓ ↓   ↓ ↓ ↓                        │
│                                                                 │
│   Поток токенов:                                                │
│   ┌──────────┬──────────┬───────┬───────────────────────┐       │
│   │  Тип     │ Значение │ Строка│ Позиция               │       │
│   ├──────────┼──────────┼───────┼───────────────────────┤       │
│   │ KEYWORD  │  "val"   │   1   │  0..2                 │       │
│   │ IDENT    │  "x"     │   1   │  4                    │       │
│   │ ASSIGN   │  "="     │   1   │  6                    │       │
│   │ IDENT    │  "a"     │   1   │  8                    │       │
│   │ PLUS     │  "+"     │   1   │  10                   │       │
│   │ IDENT    │  "b"     │   1   │  12                   │       │
│   │ STAR     │  "*"     │   1   │  14                   │       │
│   │ IDENT    │  "c"     │   1   │  16                   │       │
│   │ EOF      │          │   1   │  17                   │       │
│   └──────────┴──────────┴───────┴───────────────────────┘       │
│                                                                 │
│   Что убрано: 5 пробелов (позиции 3, 5, 7, 9, 11, 13, 15)     │
│   Что добавлено: EOF (конец файла) — сигнал парсеру            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Обрати внимание: lexer не знает, что `b * c` нужно вычислять раньше `a + ...`. Для него `+` и `*` — просто токены типа "оператор". Приоритет операций — задача следующего этапа.

#### ЧТО передаёт дальше

Lexer передаёт парсеру упорядоченный список токенов. Каждый токен содержит: тип (KEYWORD, IDENT, NUMBER, OPERATOR...), значение (текстовое содержимое) и позицию в исходном файле.

#### КАКИЕ ошибки ловит

Lexer обнаруживает ошибки на уровне символов — то, что нельзя распознать как токен:

- **Неожиданный символ:** `val x = 5 § 3` — символ `§` не принадлежит ни одному типу токенов
- **Незакрытый строковый литерал:** `val s = "hello` — кавычка открыта, но не закрыта
- **Недопустимый числовой литерал:** `val x = 0x` — начало hex-числа без цифр

Если ты видишь ошибку "Unexpected character", "Illegal character", или "Unterminated string literal" — проблема обнаружена на первом этапе, в lexer.

**Как lexer различает токены?**

Lexer использует правила (обычно регулярные выражения):
- Если начинается с буквы и продолжается буквами/цифрами → идентификатор или ключевое слово
- Если состоит из цифр → число
- Если символ `+`, `-`, `*`, `/` → оператор

Порядок правил важен: `val` сначала проверяется как ключевое слово, и только если не подошло — как идентификатор.

Мы превратили поток символов в поток токенов. Но токены — это всё ещё плоский список, как слова без грамматики. Фраза "Кот ест рыбу" и "Рыбу ест кот" — одинаковые слова, но разная структура. Чтобы понять структуру, нужен следующий этап — парсер.

---

### 2. Syntax Analysis (Синтаксический анализ)

#### ЧТО делает

Второй этап: построить дерево структуры программы — AST (Abstract Syntax Tree). Если lexer делит текст на слова, то parser определяет грамматику — как эти слова связаны друг с другом. Parser отвечает на вопрос: "Какая структура у этого кода?"

Аналогия: школьный разбор предложения. "Мальчик быстро бежал по дороге" — нужно определить подлежащее (мальчик), сказуемое (бежал), обстоятельство (быстро, по дороге). Parser делает то же самое с кодом.

#### ЗАЧЕМ нужен

Токены `[a] [+] [b] [*] [c]` — плоский список. Но `a + b * c` означает `a + (b * c)`, а не `(a + b) * c`. Откуда это знание? Из грамматики языка, которая задаёт приоритет операций. Parser знает: умножение имеет более высокий приоритет, чем сложение, и строит дерево соответственно.

Без parser'а компилятор не мог бы отличить корректный код от бессмыслицы. Каждый отдельный токен в `val = 10 count +` валиден сам по себе, но вместе они не образуют осмысленной конструкции.

#### Сквозной пример: `val x = a + b * c`

Parser получает токены и строит AST. Ключевой момент — приоритет операций. Умножение `b * c` вычисляется первым, результат складывается с `a`:

```
┌─────────────────────────────────────────────────────────────────┐
│            AST ДЛЯ "val x = a + b * c"                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                    ValDeclaration                               │
│                    ┌──────────────────┐                         │
│                    │ name: "x"        │                         │
│                    │ type: (inferred) │                         │
│                    └────────┬─────────┘                         │
│                             │ initializer                       │
│                             ▼                                   │
│                      BinaryExpr(+)                              │
│                    ┌──────────────────┐                         │
│                    │ operator: ADD    │                         │
│                    └────────┬─────────┘                         │
│                      ┌──────┴──────┐                            │
│                      │             │                            │
│                      ▼             ▼                            │
│                  Ident("a")   BinaryExpr(*)                     │
│                              ┌──────────────────┐              │
│                              │ operator: MUL    │              │
│                              └────────┬─────────┘              │
│                                ┌──────┴──────┐                  │
│                                │             │                  │
│                                ▼             ▼                  │
│                           Ident("b")    Ident("c")              │
│                                                                 │
│   Дерево отражает ПРИОРИТЕТ: умножение глубже,                 │
│   значит вычисляется ПЕРВЫМ.                                   │
│                                                                 │
│   Если бы было (a + b) * c:                                    │
│   MUL был бы корневой операцией, ADD — дочерней.               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Обрати внимание: дерево не содержит скобок, ключевого слова `val` как отдельного узла, знака `=`. Это всё "синтаксический сахар" — информация, которая нужна для чтения кода человеком, но не для структуры программы. Поэтому дерево называется *абстрактным* — оно абстрагируется от конкретного синтаксиса.

#### Parse Tree vs AST

Есть два вида деревьев. Parse tree (дерево разбора) сохраняет все грамматические детали — каждую скобку, каждое ключевое слово, каждый оператор как отдельный узел. AST — упрощённая версия, только существенная структура. На практике компиляторы строят AST, потому что parse tree содержит слишком много избыточной информации.

Для `(a + b)` parse tree включает узлы для открывающей и закрывающей скобок. AST содержит только узел сложения с двумя дочерними узлами. Скобки уже "вшиты" в структуру дерева — если `+` является дочерним узлом `*`, значит сложение выполняется первым.

#### ЧТО передаёт дальше

Parser передаёт AST — дерево, в котором каждый узел представляет конструкцию языка (объявление переменной, бинарное выражение, вызов функции). У каждого узла есть дочерние узлы (операнды, аргументы). Дерево ещё не содержит информации о типах — это задача следующего этапа.

#### КАКИЕ ошибки ловит

Parser обнаруживает структурные ошибки — когда токены идут в неправильном порядке:

- **Неожиданный токен:** `val = 10 count` — после `val` ожидается имя переменной, а не `=`
- **Незакрытые скобки:** `val x = (a + b` — открывающая скобка без закрывающей
- **Пропущенный оператор:** `val x = a b` — два идентификатора подряд без оператора между ними
- **Незавершённое выражение:** `val x = a +` — оператор без правого операнда

Если ты видишь "Unexpected token", "Expected )", или "Expecting an expression" — проблема на этапе синтаксического анализа.

> **Ключевая идея:** Lexer проверяет "слова" (символьный уровень), parser проверяет "грамматику" (структурный уровень). Если lexer — это проверка орфографии, то parser — проверка синтаксиса предложения.

У нас есть дерево структуры. Но оно ещё "немое" — не знает типов, не проверяло, существуют ли переменные `a`, `b`, `c`. Можно ли складывать строку с числом? Объявлена ли функция, которую вызываем? Это задачи семантического анализа.

---

### 3. Semantic Analysis (Семантический анализ)

#### ЧТО делает

Третий этап: проверить, что программа имеет смысл. Если lexer проверяет "буквы", parser — "грамматику", то семантический анализ проверяет "смысл". Синтаксически `val x: String = 42` корректно — все токены валидны, структура правильная. Но семантически — ошибка: число 42 не является строкой.

Аналогия: предложение "Бесцветные зелёные идеи яростно спят" грамматически правильное (подлежащее + сказуемое + наречие), но семантически бессмысленное. Семантический анализ — это проверка смысла.

#### ЗАЧЕМ нужен

Этот этап превращает "голое" AST в "обогащённое": каждый узел дерева получает информацию о типе, каждое имя привязывается к конкретному объявлению. Без семантического анализа компилятор не знал бы, что `a + b` — это сложение двух чисел (которое можно оптимизировать), а не конкатенация строк (которая работает иначе).

Семантический анализ проверяет три вещи:

- **Type checking** — типы совпадают? `Int + Int` = ok, `Int + String` = ошибка
- **Scope resolution** — переменная объявлена перед использованием? В какой области видимости?
- **Overload resolution** — какую версию перегруженной функции вызвать?

#### Сквозной пример: `val x = a + b * c`

Допустим, `a`, `b` и `c` уже объявлены как `Int`. Семантический анализатор обходит AST снизу вверх и "раскрашивает" каждый узел типом:

```
┌─────────────────────────────────────────────────────────────────┐
│           СЕМАНТИЧЕСКИЙ АНАЛИЗ: val x = a + b * c              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Шаг 1: Найти "a" в Symbol Table → a: Int ✓                  │
│   Шаг 2: Найти "b" в Symbol Table → b: Int ✓                  │
│   Шаг 3: Найти "c" в Symbol Table → c: Int ✓                  │
│   Шаг 4: Проверить b * c → Int * Int = Int ✓                  │
│   Шаг 5: Проверить a + (b*c) → Int + Int = Int ✓              │
│   Шаг 6: Вывести тип x → Int (type inference)                 │
│   Шаг 7: Записать x: Int в Symbol Table                        │
│                                                                 │
│   Обогащённое AST:                                              │
│                                                                 │
│                 ValDeclaration                                   │
│                 x: Int (inferred)                                │
│                       │                                         │
│                BinaryExpr(+) : Int                               │
│                ┌──────┴──────┐                                  │
│           Ident("a")   BinaryExpr(*) : Int                      │
│           type: Int    ┌──────┴──────┐                          │
│                   Ident("b")   Ident("c")                       │
│                   type: Int    type: Int                         │
│                                                                 │
│   Если бы a была String:                                        │
│   ❌ "Type mismatch: String + Int is not defined"               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Symbol Table — память компилятора

Для семантического анализа компилятор ведёт Symbol Table — таблицу всех известных имён. Это как справочник: при объявлении переменной или функции — запись добавляется, при использовании — запись ищется.

Symbol Table имеет вложенную структуру — scope'ы (области видимости). Переменная, объявленная внутри функции, не видна снаружи. При поиске имени компилятор сначала ищет в текущем scope, потом в родительском, и так до глобального.

Представь себе систему папок: если файл не найден в текущей папке, поиск продолжается в родительской, затем в корне. Если файл не найден нигде — ошибка.

#### ЧТО передаёт дальше

Семантический анализ передаёт обогащённое AST (annotated AST) и заполненную Symbol Table. Теперь каждый узел дерева знает свой тип, каждое имя привязано к объявлению, каждый вызов — к конкретной функции. Дерево полностью "понято" — его можно переводить в промежуточный код.

#### КАКИЕ ошибки ловит

Семантические ошибки — самый богатый класс ошибок:

- **Type mismatch:** `val x: Int = "hello"` — строка не является числом
- **Unresolved reference:** `println(z)` — переменная `z` не объявлена
- **Argument count:** `fun f(a: Int)` вызвана как `f(1, 2)` — лишний аргумент
- **Access violation:** обращение к `private` полю из другого класса
- **Smart cast impossible:** компилятор не может гарантировать тип после проверки

> **Ключевая идея:** Семантический анализ — самый "интеллектуальный" этап frontend'а. Kotlin K2 особенно силён в type inference — он выводит типы автоматически, и часто ошибки типов — это подсказки компилятора, а не препятствия.

Мы полностью "поняли" код: разбили на слова, построили структуру, проверили типы. Теперь нужно перевести это понимание на язык, понятный оптимизатору и кодогенератору. Этот язык — IR, промежуточное представление.

---

### 4. IR Generation (Генерация промежуточного кода)

#### ЧТО делает

Четвёртый этап: перевести обогащённое AST в промежуточное представление (Intermediate Representation, IR). IR — это код на "внутреннем языке" компилятора. Он проще исходного языка (нет синтаксического сахара, классов, замыканий), но абстрактнее машинного кода (нет конкретных регистров процессора).

Аналогия: перевод книги через язык-посредник. Вместо перевода с японского напрямую на русский, переводчик сначала переводит на английский (lingua franca), а затем с английского — на русский. IR — это "английский язык" компиляторов.

#### ЗАЧЕМ нужен

IR решает проблему комбинаторного взрыва: без него каждая пара "язык + платформа" требует отдельного компилятора. С IR — frontend'ы генерируют общий формат, backend'ы потребляют его. Kotlin компилятор генерирует один и тот же IR для JVM, Native, JS и WASM — и каждый backend транслирует его в свой формат.

Кроме того, IR — удобная форма для оптимизации. Высокоуровневый код (с классами, лямбдами, pattern matching) сложно оптимизировать — слишком много деталей. Машинный код — слишком низкоуровневый, оптимизации платформо-зависимы. IR — "золотая середина".

#### Three-Address Code

Популярная форма IR, где каждая инструкция имеет максимум три операнда — два источника и один результат. Сложные выражения разбиваются на цепочку простых операций.

#### Сквозной пример: `val x = a + b * c`

Выражение `a + b * c` содержит две операции. IR разбивает его на две инструкции, каждая с тремя адресами:

```
┌─────────────────────────────────────────────────────────────────┐
│            IR GENERATION: val x = a + b * c                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Обход AST снизу вверх:                                        │
│                                                                 │
│   1. Посещаем BinaryExpr(*): b * c                             │
│      → Создаём: t1 = b * c                                    │
│                                                                 │
│   2. Посещаем BinaryExpr(+): a + t1                            │
│      → Создаём: t2 = a + t1                                   │
│                                                                 │
│   3. Посещаем ValDeclaration: x = t2                           │
│      → Создаём: x = t2                                        │
│                                                                 │
│   Результат (Three-Address Code):                               │
│   ┌────────────────────────────────────────┐                    │
│   │  t1 = b * c    // временная переменная │                    │
│   │  t2 = a + t1   // временная переменная │                    │
│   │  x  = t2       // запись результата    │                    │
│   └────────────────────────────────────────┘                    │
│                                                                 │
│   Каждая строка — одна операция.                                │
│   Дерево "расплющено" в линейную последовательность.            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Временные переменные `t1`, `t2` — изобретение компилятора. Они не существуют в исходном коде и будут позже отображены на регистры процессора.

#### SSA (Static Single Assignment)

Современные компиляторы (LLVM, GCC, Kotlin) используют SSA-форму IR. Правило: каждая переменная присваивается ровно один раз. Если в коде переменная изменяется, каждое присваивание создаёт новую "версию" переменной.

```
┌─────────────────────────────────────────────────────────────────┐
│                       SSA ФОРМА                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Обычный код:          SSA форма:                              │
│   ─────────────         ──────────                              │
│   x = 1                 x1 = 1                                  │
│   x = x + 2             x2 = x1 + 2                             │
│   x = x * 3             x3 = x2 * 3                             │
│                                                                 │
│   Каждое присваивание создаёт НОВУЮ версию переменной           │
│                                                                 │
│   Зачем?                                                        │
│   - Однозначность: для каждого использования легко найти,       │
│     где значение было определено (одно место!)                 │
│   - Упрощает data-flow analysis                                 │
│   - Оптимизации проще: если x2 не используется, можно удалить  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Зачем SSA? Представь библиотеку, где все книги называются "Книга". Чтобы найти нужную, надо помнить, в каком порядке они поступали. Если каждая книга имеет уникальное имя — поиск тривиален. SSA даёт каждому значению уникальное имя.

Для условных конструкций SSA использует phi-функции (phi). Когда два пути сходятся в один, phi-функция "выбирает" правильное значение:

```
if (cond)
    x = 1
else
    x = 2
use(x)
```

Становится:
```
if cond goto L1 else L2
L1: x1 = 1; goto L3
L2: x2 = 2; goto L3
L3: x3 = φ(x1, x2)  // выбирает x1 или x2 в зависимости от пути
use(x3)
```

#### ЧТО передаёт дальше

IR генератор передаёт линейную последовательность инструкций в SSA-форме. Каждая инструкция — простая операция с двумя-тремя операндами. Информация о типах сохранена. Структура программы "расплющена" из дерева в линейный поток.

#### КАКИЕ ошибки ловит

На этапе IR generation ошибки редки — к этому моменту код уже прошёл все проверки frontend'а. Однако могут возникнуть:

- **Internal compiler errors:** если AST содержит конструкцию, для которой нет правила трансляции
- **Platform-specific limitations:** конструкция языка не поддерживается на целевой платформе

Мы получили чистый, плоский IR. Теперь его можно улучшить — не меняя смысл программы, но ускоряя её исполнение.

---

### 5. Optimization (Оптимизация)

#### ЧТО делает

Пятый этап: улучшить IR, сохранив семантику программы. Оптимизатор применяет десятки "проходов" (passes), каждый из которых ищет и устраняет конкретный вид неэффективности. Название "оптимизация" — не совсем точное: компилятор не находит оптимальный код (это NP-hard задача), а лишь улучшает его эвристическими методами.

Аналогия: редактор книги. Автор написал текст (IR), редактор убирает повторы, сокращает длинные фразы, выстраивает логику — но не меняет смысл книги. Каждый проход редактора ищет конкретную проблему: один проход — пустые слова, другой — повторяющиеся абзацы, третий — слишком длинные предложения.

#### ЗАЧЕМ нужна

Программисты пишут код для людей, не для машин. Код должен быть читаемым, модульным, расширяемым. Но читаемый код — не всегда быстрый. Оптимизатор "переписывает" код для машины, сохраняя поведение, но убирая человеческие абстракции.

Кроме того, оптимизация на уровне IR — универсальна. Одна и та же оптимизация (например, constant folding) работает для всех исходных языков и всех целевых платформ. Это ещё один аргумент в пользу IR как промежуточного звена.

#### Основные техники

**Constant Folding (свёртка констант):**

Если оба операнда — константы, результат можно вычислить во время компиляции. Зачем вычислять `2 + 3` при каждом запуске, если ответ известен заранее?

```
// До:
t1 = 2 + 3
t2 = t1 * 4

// После:
t1 = 5      // 2 + 3 вычислено compile-time
t2 = 20     // 5 * 4 вычислено compile-time
```

**Constant Propagation (распространение констант):**

Если переменная имеет известное значение, все её использования можно заменить этим значением.

```
// До:
x = 5
y = x + 3

// После constant propagation:
x = 5
y = 5 + 3

// После constant folding:
x = 5
y = 8
```

Заметь: одна оптимизация открывает возможности для другой. Propagation создаёт две константы, folding их вычисляет. Поэтому компиляторы запускают passes многократно — каждый проход может создать новые возможности для других.

**Dead Code Elimination (удаление мёртвого кода):**

Код, который не влияет на результат — удаляется. Если переменная вычисляется, но нигде не используется, её вычисление не нужно.

**Function Inlining (встраивание функций):**

Подстановка тела функции вместо вызова. Главное преимущество — не в устранении call overhead (он минимален), а в том, что inlining открывает другие оптимизации.

```
┌─────────────────────────────────────────────────────────────────┐
│               INLINING ВКЛЮЧАЕТ ДРУГИЕ ОПТИМИЗАЦИИ              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   fun double(x: Int) = x * 2                                    │
│   val result = double(5)                                        │
│                                                                 │
│   После inlining:                                               │
│   val result = 5 * 2                                            │
│                                                                 │
│   После constant folding:                                       │
│   val result = 10                                               │
│                                                                 │
│   Без inlining компилятор не знал бы что x = 5                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Loop Invariant Code Motion (LICM):**

Вынос вычислений, не зависящих от итерации, за пределы цикла. Если выражение даёт одинаковый результат на каждой итерации — зачем вычислять его тысячу раз?

#### Сквозной пример: `val x = a + b * c`

Для нашего примера оптимизации зависят от контекста. Если `a`, `b`, `c` — константы (например, `a = 1, b = 2, c = 3`):

```
┌─────────────────────────────────────────────────────────────────┐
│             ОПТИМИЗАЦИЯ: val x = a + b * c                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Случай 1: a, b, c — переменные (значения неизвестны)         │
│   t1 = b * c       → без изменений                             │
│   t2 = a + t1      → без изменений                             │
│   x  = t2          → без изменений                             │
│   Оптимизатору нечего упрощать                                 │
│                                                                 │
│   Случай 2: a = 1, b = 2, c = 3 (компилятор это знает)        │
│   t1 = 2 * 3       → constant folding → t1 = 6                │
│   t2 = 1 + 6       → constant folding → t2 = 7                │
│   x  = 7           → вся арифметика исчезла!                   │
│                                                                 │
│   Случай 3: c = 0 (компилятор это знает)                       │
│   t1 = b * 0       → algebraic simplification → t1 = 0        │
│   t2 = a + 0       → algebraic simplification → t2 = a        │
│   x  = a           → умножение и сложение исчезли!             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### ЧТО передаёт дальше

Оптимизатор передаёт улучшенный IR — тот же формат, но с убранными избыточностями, свёрнутыми константами, встроенными функциями. IR готов к финальному переводу в машинный код.

Мы максимально улучшили IR. Остался последний шаг — превратить абстрактные инструкции в конкретные команды для целевого процессора.

---

### 6. Code Generation (Генерация кода)

#### ЧТО делает

Последний этап: превратить оптимизированный IR в целевой формат — машинный код для конкретного процессора, JVM bytecode, JavaScript, или WASM. Кодогенератор решает три задачи: выбрать инструкции (instruction selection), распределить регистры (register allocation), упорядочить инструкции (instruction scheduling).

#### ЗАЧЕМ нужен

IR — абстрактный: "сложить t1 и t2, результат в t3". Но процессор не знает, что такое "t1" — он работает с регистрами (R0, R1, R2...) и конкретными инструкциями (ADD, MUL, LDR, STR). Кодогенератор отображает абстракции IR на реальные ресурсы целевой платформы.

Аналогия: архитектор нарисовал чертёж здания (IR). Прораб (кодогенератор) смотрит на чертёж и решает: эту стену строить из кирпича или блоков? Где поставить леса? В каком порядке заливать этажи? Чертёж один — но реализация зависит от доступных материалов и площадки.

#### Сквозной пример: `val x = a + b * c`

Для нашего IR (`t1 = b * c; t2 = a + t1; x = t2`) кодогенератор для ARM64:

```
┌─────────────────────────────────────────────────────────────────┐
│          CODE GENERATION: val x = a + b * c (ARM64)            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   IR:                        ARM64 Assembly:                    │
│   ────                       ─────────────────                  │
│   t1 = b * c        →       LDR W1, [SP, #8]   // b в W1      │
│                              LDR W2, [SP, #12]  // c в W2      │
│                              MUL W3, W1, W2     // W3 = b * c  │
│                                                                 │
│   t2 = a + t1       →       LDR W0, [SP, #4]   // a в W0      │
│                              ADD W4, W0, W3     // W4 = a + t1 │
│                                                                 │
│   x  = t2           →       STR W4, [SP, #16]  // x = W4      │
│                                                                 │
│   Register allocation:                                          │
│   t1 → W3, t2 → W4, a → W0, b → W1, c → W2                  │
│                                                                 │
│   Для JVM bytecode:                                             │
│   ────                                                          │
│   iload_1           // a на стек                                │
│   iload_2           // b на стек                                │
│   iload_3           // c на стек                                │
│   imul              // b * c → стек                             │
│   iadd              // a + (b*c) → стек                         │
│   istore 4          // стек → x                                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Обрати внимание: один и тот же IR даёт совершенно разный выходной код для ARM64 (register-based, 6 инструкций) и JVM (stack-based, 6 инструкций). Backend адаптирует IR под целевую архитектуру.

#### Instruction Selection

Одна IR-операция может соответствовать разным машинным инструкциям. Для `t1 = a * 2` кодогенератор может выбрать:
- `MUL R0, R1, #2` — обычное умножение
- `LSL R0, R1, #1` — сдвиг влево на 1 (быстрее для степеней двойки)
- `ADD R0, R1, R1` — сложение числа с самим собой

Выбор зависит от целевого процессора и латентности инструкций.

#### Register Allocation

Процессор имеет ограниченное число регистров (ARM64: 31 general-purpose). Если IR использует 100 временных переменных, кодогенератор должен решить: какие 31 переменных держать в регистрах, остальные — в памяти (stack spilling). Это NP-hard задача, решаемая эвристиками (graph coloring algorithm).

#### ЧТО передаёт дальше

Кодогенератор производит финальный результат компиляции:

Для Kotlin это может быть:
- **JVM Bytecode** — для Android и JVM
- **LLVM IR → Native binary** — для iOS через Kotlin/Native
- **JavaScript** — для Web
- **WASM** — для WebAssembly

---

## ПОЧЕМУ знание pipeline помогает понять сообщения компилятора

Каждая ошибка компиляции рождается на конкретном этапе pipeline. Зная этапы, ты можешь по тексту ошибки определить, где именно компилятор споткнулся — и это резко сужает область поиска проблемы.

```
┌─────────────────────────────────────────────────────────────────┐
│        ЭТАПЫ PIPELINE И СООБЩЕНИЯ ОБ ОШИБКАХ                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   LEXER (этап 1):                                               │
│   "Illegal character '§'"                                       │
│   "Unterminated string literal"                                 │
│   "Invalid escape sequence"                                     │
│   → Проблема: символ, который нельзя распознать                │
│   → Решение: проверь спецсимволы, кавычки, escape-коды         │
│                                                                 │
│   PARSER (этап 2):                                              │
│   "Expecting ')'"                                               │
│   "Unexpected token 'val'"                                      │
│   "Expecting an expression"                                     │
│   → Проблема: структура кода неправильная                      │
│   → Решение: проверь скобки, порядок ключевых слов             │
│                                                                 │
│   SEMANTIC (этап 3):                                            │
│   "Type mismatch: expected Int, found String"                   │
│   "Unresolved reference: x"                                     │
│   "Too many arguments for function"                             │
│   → Проблема: код синтаксически верен, но бессмыслен           │
│   → Решение: проверь типы, объявления, аргументы               │
│                                                                 │
│   CODEGEN (этап 6):                                             │
│   "Cannot access class X: it is internal in module Y"           │
│   "Native interop limitation"                                   │
│   → Проблема: платформо-зависимое ограничение                  │
│   → Решение: проверь видимость и совместимость платформ         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

> **Практический совет:** Когда IDE показывает ошибку, определи этап. Lexer-ошибки — опечатки и символы. Parser-ошибки — структура (скобки, ключевые слова). Semantic-ошибки — типы и имена. Этот навык экономит часы отладки.

---

## Kotlin Compiler: K2 и Multiplatform

Kotlin использует архитектуру с единым frontend и несколькими backend'ами. Это прямое воплощение идеи frontend/backend разделения, которую мы обсуждали.

```
┌─────────────────────────────────────────────────────────────────┐
│                    KOTLIN COMPILER (K2)                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Source (.kt)                                                  │
│        │                                                        │
│        ▼                                                        │
│   ┌────────────┐                                                │
│   │ K2 FRONTEND│  PSI → FIR → Resolved FIR                     │
│   └─────┬──────┘                                                │
│         │                                                       │
│         ▼                                                       │
│   ┌────────────┐                                                │
│   │     IR     │  Backend IR (общий для всех платформ)          │
│   └─────┬──────┘                                                │
│         │                                                       │
│   ┌─────┼─────────────┬─────────────┬───────────┐               │
│   │     │             │             │           │               │
│   ▼     ▼             ▼             ▼           ▼               │
│ ┌────┐ ┌─────┐ ┌──────────┐ ┌────────┐ ┌──────────┐            │
│ │JVM │ │ JS  │ │  Native  │ │  WASM  │ │  ...     │            │
│ └────┘ └─────┘ └──────────┘ └────────┘ └──────────┘            │
│   │       │          │           │                              │
│   ▼       ▼          ▼           ▼                              │
│ .class   .js    executable     .wasm                           │
│                 (via LLVM)                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**K2 vs K1:**

K1 (старый frontend) использовал две структуры данных. K2 использует одну — FIR (Frontend Intermediate Representation). Результат:
- Компиляция в ~2 раза быстрее
- Улучшенный type inference
- Более точные smart casts

**Для KMP это означает:**

Один и тот же Kotlin-код компилируется в разные форматы. `commonMain` код проходит через K2 frontend один раз, затем backend'ы генерируют код для каждой платформы:

- **JVM backend:** JVM bytecode (`.class` файлы)
- **Native backend:** LLVM IR → машинный код для iOS/macOS/Linux
- **JS backend:** JavaScript код
- **WASM backend:** WebAssembly модули

---

## Распространённые заблуждения

| Миф | Реальность | Почему так думают |
|-----|------------|-------------------|
| `-O3` намного быстрее `-O2` | Исследования показывают минимальную разницу. `-O3` может навредить из-за увеличения размера кода | Люди ассоциируют большую цифру с лучшим результатом |
| `inline` гарантирует встраивание | `inline` в Kotlin/C++ влияет на linkage. Для гарантии нужен `@Suppress("NOTHING_TO_INLINE")` или `always_inline` | Название "inline" подразумевает встраивание |
| Компилятор создаёт оптимальный код | "Optimization" — неточное название. Компилятор улучшает, но не находит оптимум | Слово "оптимальный" вводит в заблуждение |
| Больше проходов оптимизации = лучше | Каждый проход имеет cost. Иногда компиляция занимает больше времени, чем экономит | Кажется логичным: больше работы = лучший результат |
| Раздельная компиляция всегда быстрее | Линковка медленная. Для небольших проектов unity build (всё в один файл) может быть быстрее | Раздельная компиляция — стандартный подход |

---

## Подводные камни

### Когда понимание pipeline особенно важно

**Отладка ошибок компиляции:**
- Lexer error: "Unexpected character" → проблема с символами
- Parser error: "Unexpected token" → неправильная структура
- Semantic error: "Type mismatch" → логическая ошибка

**Performance debugging:**
- Если функция не inline'ится — проверь, не слишком ли она большая
- Если константы не сворачиваются — возможно, они не константы compile-time
- Если код медленный на JVM, но быстрый на Native — проблема в backend'е

**KMP специфика:**
- Ошибки могут проявляться на одной платформе и не на другой (разные backend'ы)
- IR различается между backend'ами — одна оптимизация может работать на JVM, но не на Native
- `expect/actual` проверяется на уровне семантического анализа — типы должны совпадать

---

## Связь с другими темами

Compilation pipeline — фундамент для понимания всех остальных тем модуля компиляции. Каждая из связанных тем берёт один из этапов pipeline и углубляется в него.

**[[bytecode-virtual-machines]]** — продолжение этапа Code Generation. Когда backend генерирует не машинный код, а bytecode — появляется потребность в виртуальной машине для его исполнения. Понимание pipeline объясняет, откуда bytecode берётся и почему он выглядит так, а не иначе. Рекомендуется читать сразу после этой статьи.

**[[native-compilation-llvm]]** — детализация backend'а для native-компиляции. LLVM берёт на себя этапы оптимизации и кодогенерации. Если pipeline — это общая карта, то LLVM — масштабированная карта последних двух этапов. Рекомендуется после bytecode-virtual-machines.

**[[interpretation-jit]]** — альтернативный подход к исполнению: вместо компиляции всего кода заранее, JIT компилирует "по требованию". Знание pipeline помогает понять, какие этапы JIT выполняет в runtime. Рекомендуется третьим в последовательности.

**[[cpu-architecture-basics]]** — prerequisite. Кодогенератор (этап 6) работает с конкретными инструкциями и регистрами процессора. Понимание CPU-архитектуры объясняет, почему instruction selection и register allocation — сложные задачи.

---

## Источники и дальнейшее чтение

- **Aho, A., Lam, M., Sethi, R., Ullman, J. (2006). Compilers: Principles, Techniques, and Tools ("Dragon Book").** — Каноническая книга по компиляторам. Математически строгое изложение каждого этапа pipeline. Глава 2 даёт полный обзор всех этапов, главы 3-9 разбирают каждый этап в деталях. Тяжёлая, но незаменимая.

- **Appel, A. (1998). Modern Compiler Implementation in ML/Java/C.** — Более практичный подход, чем Dragon Book. Каждая глава заканчивается реализацией этапа. Отлично показывает связь теории с практикой. Версия на Java наиболее доступна.

- **Nystrom, R. (2021). Crafting Interpreters.** — Лучший ресурс для начинающих. Бесплатная онлайн-книга. Проводит читателя через создание двух интерпретаторов от нуля. Объясняет каждый этап pipeline на примере реального кода.

- **Grune, D. et al. (2012). Modern Compiler Design.** — Современный взгляд на компиляцию. Хорошо покрывает оптимизации и кодогенерацию. Полезна для тех, кто хочет понять современные компиляторы (LLVM, GCC).

- [JetBrains: K2 Compiler](https://blog.jetbrains.com/kotlin/2024/05/celebrating-kotlin-2-0-fast-smart-and-multiplatform/) — Официальный анонс K2. Объясняет архитектуру нового frontend'а Kotlin.

---

## Проверь себя

> [!question]- Ты видишь ошибку "Type mismatch: expected Int, found String" в IDE. На каком этапе compilation pipeline она возникла и почему именно на этом?
> Это ошибка семантического анализа (этап 3). Lexer (этап 1) успешно разбил код на токены, parser (этап 2) построил синтаксически корректное AST. Но семантический анализатор, обходя AST и проверяя типы через Symbol Table, обнаружил несовпадение: ожидался Int, а получен String. Этот этап проверяет "смысл" кода, а не его структуру.

> [!question]- Почему разделение компилятора на frontend и backend через общий IR сокращает количество компонентов с M*N до M+N?
> Без IR для M языков и N платформ нужно M*N отдельных компиляторов (каждая комбинация). С IR каждый из M языков генерирует общий промежуточный формат (M frontend'ов), а каждая из N платформ потребляет этот формат (N backend'ов). Итого M+N компонентов. Kotlin использует этот принцип: K2 frontend один, а backend'ы (JVM, Native, JS, WASM) — отдельные.

> [!question]- У тебя один и тот же Kotlin-код компилируется успешно для JVM, но падает с ошибкой для Native. На каком уровне pipeline может возникнуть различие и почему?
> Различие возникает на уровне backend'а (этапы 5-6). Frontend K2 общий для всех платформ — код проходит одинаковый лексический, синтаксический и семантический анализ. Но backend'ы различаются: JVM backend генерирует bytecode, Native backend генерирует LLVM IR. Ошибка может быть связана с платформо-зависимыми ограничениями: например, конструкция поддерживается в JVM backend, но ещё не реализована или ограничена в Native backend. Также `expect/actual` проверяется на уровне семантического анализа, но с учётом платформы.

> [!question]- Почему компиляторы запускают optimization passes многократно, а не один раз?
> Одна оптимизация может создавать возможности для другой. Например, constant propagation заменяет переменную на значение, что позволяет constant folding вычислить выражение, что создаёт мёртвый код, который удаляет dead code elimination. Каждый проход может "открыть двери" для других оптимизаций. Поэтому компиляторы запускают passes итеративно, пока не будет найдено значимых улучшений.

---

## Ключевые карточки

Какие 6 этапов включает compilation pipeline?
?
1) Lexical Analysis (токенизация), 2) Syntax Analysis (построение AST), 3) Semantic Analysis (проверка типов и scope), 4) IR Generation (промежуточный код), 5) Optimization (оптимизация IR), 6) Code Generation (машинный код/bytecode).

---

Что такое AST и чем оно отличается от parse tree?
?
AST (Abstract Syntax Tree) — упрощённое дерево структуры программы, содержащее только существенную информацию. Parse tree сохраняет все грамматические детали (скобки, ключевые слова). AST абстрагируется от конкретного синтаксиса — приоритет операций "вшит" в структуру дерева.

---

Что такое SSA-форма IR и зачем она нужна?
?
SSA (Static Single Assignment) — форма IR, где каждая переменная присваивается ровно один раз. При изменении переменной создаётся новая "версия" (x1, x2, x3). Это упрощает data-flow analysis, делает связь "определение-использование" однозначной и облегчает оптимизации (например, легко определить мёртвый код).

---

Что такое constant propagation и как она связана с constant folding?
?
Constant propagation заменяет использования переменной с известным значением на само значение (x=5; y=x+3 -> y=5+3). Constant folding вычисляет выражения из констант compile-time (y=5+3 -> y=8). Они работают в паре: propagation создаёт константные выражения, folding их вычисляет.

---

Какую архитектуру использует Kotlin-компилятор K2 для KMP?
?
K2 использует единый frontend (PSI -> FIR -> Resolved FIR -> Backend IR) и несколько backend'ов: JVM (-> .class), JS (-> .js), Native (-> LLVM -> executable), WASM (-> .wasm). Это реализация принципа frontend/backend разделения через общий IR.

---

Какой этап pipeline отвечает за Symbol Table и что она хранит?
?
Семантический анализ (этап 3) ведёт Symbol Table — таблицу всех объявленных имён. Она хранит имя, тип, scope (область видимости) для каждой переменной, функции, класса. При использовании имени компилятор ищет его в текущем scope, затем в родительском, до глобального. Если не найдено — ошибка "Unresolved reference".

---

## Куда дальше

| Направление | Куда | Зачем |
|-------------|------|-------|
| Следующий шаг | [[bytecode-virtual-machines]] | Понять, как backend генерирует bytecode и как VM его исполняет |
| Углубиться | [[native-compilation-llvm]] | Изучить AOT-компиляцию через LLVM для Kotlin/Native |
| Смежная тема | [[interpretation-jit]] | Понять альтернативный подход — JIT-компиляцию в runtime |
| Обзор | [[cs-foundations-overview]] | Вернуться к навигации по разделу |

---

*Проверено: 2026-02-13*
