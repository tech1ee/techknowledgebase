---
title: "Модель памяти: Stack и Heap"
created: 2026-01-04
modified: 2026-02-10
type: deep-dive
status: published
tags:
  - topic/cs-foundations
  - type/deep-dive
  - level/intermediate
related:
  - "[[garbage-collection-explained]]"
  - "[[reference-counting-arc]]"
  - "[[kmp-memory-management]]"
prerequisites:
  - "[[cpu-architecture-basics]]"
---

# Модель памяти: Stack и Heap

> **TL;DR:** Программа использует два типа памяти. Stack — быстрый, автоматический, но маленький; работает как стопка тарелок (последняя положенная — первая снятая). Heap — большой, гибкий, но медленный и требует уборки. Stack для временных данных внутри функции. Heap для данных, которые живут дольше одного вызова.

---

## Зачем это знать

Когда ты пишешь `val user = User("John")` в Kotlin, что происходит? Где хранится `user`? Где хранится сам объект `User`? Почему иногда программа падает с `StackOverflowError`, а иногда с `OutOfMemoryError`?

Без понимания модели памяти ты будешь:
- Не понимать, почему рекурсия "взрывается"
- Удивляться утечкам памяти
- Не понимать разницу между JVM и Kotlin/Native

Это фундамент. После этого материала Garbage Collection и ARC станут понятны.

---

## Терминология

| Термин | Что это | Аналогия |
|--------|---------|----------|
| **Stack** | Область памяти для локальных переменных и вызовов функций | Стопка тарелок: кладёшь сверху, берёшь сверху |
| **Heap** | Область памяти для объектов с динамическим временем жизни | Склад: ставишь куда хочешь, сам следишь за порядком |
| **Stack frame** | Блок памяти для одного вызова функции | Один лист в блокноте для одной задачи |
| **Stack pointer** | Указатель на текущую "верхушку" стека | Закладка "читать здесь" |
| **Allocation** | Выделение памяти | Бронирование места |
| **Deallocation** | Освобождение памяти | Освобождение места |
| **Fragmentation** | Дробление свободной памяти на мелкие куски | Дырки в сыре |
| **Allocator** | Подсистема, управляющая выделением и освобождением блоков heap-памяти | Администратор склада, решающий куда поставить груз |
| **Escape analysis** | Анализ компилятора, определяющий покидает ли объект scope функции | Решение менеджера: документ на стол или в архив |

---

## ПОЧЕМУ память разделена на Stack и Heap

### Проблема: разные данные живут по-разному

Представь программу как офис. Есть два типа документов:

**Тип 1: Черновики.** Написал, использовал, выбросил. Нужны только пока работаешь над задачей. Таких — большинство.

**Тип 2: Договоры.** Подписал, положил в архив. Нужны долго, к ним обращаются разные отделы.

Хранить их одинаково — неэффективно. Черновики можно на стол, быстро взял и выбросил. Договоры — в шкаф, с каталогом.

Так же и с памятью:
- **Локальные переменные** — черновики. Нужны только внутри функции.
- **Объекты** — договоры. Могут передаваться между функциями, жить долго.

### Историческая справка

**1950-е, Fortran:** Вся память выделялась при компиляции. Размер массива? Укажи заранее. Рекурсия? Невозможна. Просто, но негибко.

**1958, ALGOL 60:** Появился stack. Функции могут вызывать друг друга, каждая получает свой "блок" памяти. Рекурсия стала возможной. Революция.

**1960-е, Lisp:** Появился heap. Данные могут жить дольше функции. Появился Garbage Collection — автоматическая уборка.

**1970-е, C:** Stack + Heap, но без GC. Программист сам управляет heap через `malloc`/`free`.

Сегодня эта модель — стандарт. JVM, .NET, Go, Rust, Kotlin/Native — все используют stack и heap.

> **Ключевая идея:** Разделение на stack и heap — не произвольное техническое решение. Это следствие фундаментального наблюдения: данные в программе имеют два радикально разных паттерна времени жизни, и оптимизировать оба одновременно одним механизмом невозможно.

---

## ЧТО такое Stack

### Принцип работы: LIFO

Stack работает по принципу LIFO — Last In, First Out. Последний вошёл — первый вышел.

```
Аналогия: стопка тарелок

     ┌─────────┐
     │ Тарелка │  ← Последняя положенная
     ├─────────┤
     │ Тарелка │
     ├─────────┤
     │ Тарелка │  ← Первая положенная
     └─────────┘

Можешь взять только верхнюю.
Чтобы достать нижнюю — сними все сверху.
```

Почему именно так? Потому что функции работают вложенно: `main()` вызывает `processUser()`, та вызывает `validateEmail()`. Когда `validateEmail()` завершается, её данные больше не нужны — снимаем "тарелку". Потом `processUser()` завершается — снимаем следующую.

### Stack Frame

Каждый вызов функции создаёт **stack frame** — блок памяти с:
- Параметрами функции
- Локальными переменными
- Адресом возврата (куда вернуться после завершения)

```
STACK при вызове validateEmail() из processUser() из main()

    Высокие адреса
    ┌─────────────────────┐
    │   main()            │ ← Первый вызов
    │   - args            │
    │   - user            │
    ├─────────────────────┤
    │   processUser()     │ ← Второй вызов
    │   - userId          │
    │   - result          │
    │   - return address  │
    ├─────────────────────┤
    │   validateEmail()   │ ← Текущий (верхушка)
    │   - email           │
    │   - isValid         │
    │   - return address  │
    └─────────────────────┘ ← Stack Pointer
    Низкие адреса

    Stack растёт ВНИЗ (от высоких адресов к низким)
```

### ПОЧЕМУ Stack быстрый: глубокое объяснение

На поверхности ответ прост: "потому что одна инструкция". Но почему одной инструкции достаточно? Чтобы это понять, нужно увидеть разницу на уровне принципов, а не только деталей реализации.

**Принцип 1: Предсказуемость порядка.**

Stack работает строго по LIFO. Это значит, что следующий свободный адрес всегда известен заранее — он равен текущему значению stack pointer. Не нужен поиск, не нужен каталог свободных мест, не нужна проверка "а не занято ли тут". Выделить память на stack — это буквально вычесть число из регистра процессора. Одна арифметическая операция, один такт.

Представь это как блокнот, в котором ты пишешь сверху вниз. Следующая свободная строка — всегда та, на которой стоит карандаш. Не нужно листать, не нужно искать.

**Принцип 2: Пространственная локальность (spatial locality).**

Когда процессор обращается к адресу в памяти, он загружает в кэш не один байт, а целую строку (cache line, обычно 64 байта). Если следующее обращение попадёт в ту же строку — данные уже в кэше. Это называется cache hit, и он в 50-100 раз быстрее, чем обращение к основной памяти (RAM).

Stack идеально использует эту особенность. Все данные текущей функции лежат рядом — параметры, локальные переменные, адрес возврата. Когда процессор загрузил одну переменную, соседние уже в кэше. Функция работает с данными, которые физически расположены в последовательных ячейках памяти.

**Принцип 3: Отсутствие фрагментации.**

Фрагментация — это когда свободная память разбита на мелкие несмежные куски. Stack никогда не фрагментируется, потому что освобождение происходит в строго обратном порядке. Это как если бы ты всегда вырывал из блокнота только последний написанный лист. Дырок внутри не бывает.

**Принцип 4: Нет синхронизации.**

Каждый поток (thread) имеет собственный stack. Два потока никогда не борются за один участок stack-памяти. Это означает, что не нужны блокировки, мьютексы и другие механизмы синхронизации, которые сами по себе стоят десятки и сотни тактов процессора.

> **Ключевая идея:** Stack быстрый не потому что "так сделали". Он быстрый потому что его ограничения (строгий LIFO, фиксированный размер, одна владелец-поток) делают все операции тривиальными. Ограничения — это не недостаток, а источник производительности.

В терминах Брайанта и О'Халларона (CS:APP), stack использует "temporal locality" (недавно использованные данные нужны снова) и "spatial locality" (соседние данные нужны вместе) — два столпа, на которых построена вся иерархия памяти в современных компьютерах.

### Ограничения Stack

- **Фиксированный размер.** Обычно 1MB на поток. Превысил — `StackOverflowError`.
- **Только LIFO.** Нельзя освободить данные "из середины".
- **Локальный для потока.** Каждый поток имеет свой stack. Нельзя передать данные напрямую.

---

Мы разобрали, как работает Stack — быстрый, но ограниченный. Но что делать, когда данные не вписываются в LIFO-модель? Когда объект создаётся в одной функции, но нужен в другой? Здесь на сцену выходит Heap.

## ЧТО такое Heap

### Принцип работы: свободное размещение

Heap — это "склад". Ты просишь место, система находит свободный участок и даёт тебе адрес.

```
Аналогия: пляж

┌─────────────────────────────────────────┐
│  ПЛЯЖ (Heap)                            │
│                                         │
│  [Занято]    [Свободно]    [Занято]     │
│     ↑                         ↑         │
│   Чьё-то        Можно      Чьё-то      │
│   полотенце    занять      полотенце    │
│                                         │
│  [Свободно]  [Занято]  [Свободно]       │
│                                         │
└─────────────────────────────────────────┘

- Ставишь вещи где хочешь (где есть место)
- Забираешь когда хочешь
- Если не уберёшь — будет захламлено
```

### Как работают аллокаторы: кто решает куда поставить объект

Когда ты пишешь `new User()` или `malloc(100)`, за кулисами работает **аллокатор** (allocator) — подсистема, которая находит подходящий свободный блок в heap. Это не так просто, как кажется.

Аллокатор поддерживает **список свободных блоков** (free list) — цепочку записей о том, где в heap есть незанятые участки и какого они размера. Когда приходит запрос на N байт, аллокатор должен выбрать, какой именно свободный блок отдать. Здесь существует несколько стратегий, каждая со своими компромиссами.

**Стратегия First-Fit: "первый подходящий".**

Аллокатор идёт по списку свободных блоков с начала и берёт первый, который достаточно велик. Быстро? Да — часто не нужно просматривать весь список. Но со временем в начале списка накапливаются мелкие "огрызки" от предыдущих разделений. Каждый запрос начинает с перебора этих маленьких блоков, прежде чем найдёт подходящий.

Представь это как парковку, где ты всегда начинаешь искать место от входа. Ближайшие к входу места заняты мелкими машинами, и тебе приходится ехать дальше, даже если на дальних рядах полно свободного пространства.

**Стратегия Best-Fit: "наиболее подходящий".**

Аллокатор просматривает весь список и выбирает блок, размер которого наиболее близок к запрошенному. Кажется идеальным — минимум потерь. Но есть две проблемы. Во-первых, нужно просмотреть все свободные блоки, что дорого. Во-вторых, от каждого разделения остаются крошечные "хвосты", которые слишком малы для чего-либо полезного. Это парадокс: стремление к экономии создаёт ещё больше мусора.

**Стратегия Buddy System: "приятельская система".**

Эта элегантная стратегия, описанная Кнутом в "The Art of Computer Programming", делит память на блоки, размеры которых — степени двойки: 16, 32, 64, 128, 256 байт и так далее. Когда нужен блок, скажем, 50 байт, аллокатор выделяет блок 64 байта (ближайшая степень двойки). Если свободного блока 64 байт нет — он разделяет блок 128 байт на два "приятеля" по 64.

```
Buddy System: разделение блока 256 байт для запроса 50 байт

Шаг 0: [_______________256_______________]

Шаг 1: [______128______][______128______]
         разделили на двух "приятелей"

Шаг 2: [__64__][__64__][______128______]
         разделили левого приятеля

Шаг 3: [ЗАНЯТ][__64__][______128______]
         выделили 64 байта (ближайшая степень ≥ 50)

При освобождении:
Шаг 4: [__64__][__64__][______128______]
         приятели свободны — сливаем!

Шаг 5: [______128______][______128______]
         сливаем дальше!

Шаг 6: [_______________256_______________]
         полностью восстановлено
```

Главное преимущество Buddy System — простота слияния (coalescing). Когда блок освобождается, аллокатор проверяет его "приятеля" (buddy) — парный блок того же размера. Если приятель тоже свободен, они сливаются в блок вдвое больше. Процесс повторяется рекурсивно. Это резко снижает фрагментацию.

Недостаток — **внутренняя фрагментация**: запросил 50 байт, получил 64. 14 байт потеряно внутри блока. Но этот компромисс часто оправдан, потому что внешняя фрагментация (невозможность выделить большой непрерывный блок) — гораздо серьёзнее.

> **Для любознательных:** Ядро Linux использует Buddy System для управления физическими страницами памяти. Приложения получают heap через аллокаторы пользовательского пространства (glibc malloc, jemalloc, tcmalloc), которые запрашивают страницы у ядра и нарезают их на мелкие блоки. Это двухуровневая система.

### Проблема: фрагментация

Со временем heap становится "дырявым":

```
После многих allocation/deallocation:

┌───┬─────┬───┬───────┬───┬─────┬───┐
│ X │     │ X │       │ X │     │ X │
└───┴─────┴───┴───────┴───┴─────┴───┘
    ↑         ↑           ↑
  50 байт  100 байт    75 байт свободно

Всего свободно: 225 байт
Но запросить 200 байт НЕЛЬЗЯ — нет непрерывного куска!

Это — внешняя фрагментация.
```

Есть ещё **внутренняя фрагментация**: система выделяет чуть больше, чем просили (для выравнивания или служебных данных). Buddy System — классический пример: запросил 33 байта, получил 64.

Фрагментация — одна из фундаментальных проблем управления памятью. Таненбаум в "Modern Operating Systems" сравнивает это с проблемой пакования чемодана: вещи разного размера трудно уложить без зазоров, и чем чаще ты перекладываешь вещи, тем больше пустых промежутков образуется.

### ПОЧЕМУ Heap медленнее: системный взгляд

Теперь, когда мы понимаем как работает stack и как работают аллокаторы heap, можно глубоко разобрать, почему heap медленнее. Дело не только в "поиске свободного места" — причины системные.

**Причина 1: Вычислительная сложность аллокации.**

Stack allocation — одна арифметическая инструкция (сдвиг указателя). Heap allocation — это вызов аллокатора, который должен: пройти по списку свободных блоков, выбрать подходящий, возможно разделить его, обновить метаданные. Даже в лучшем случае это десятки инструкций. В худшем — сотни.

**Причина 2: Непредсказуемость расположения.**

Объекты на heap разбросаны по памяти. Когда программа обращается к объекту A, потом к объекту B, они могут находиться в разных cache lines, на разных страницах памяти. Процессор загружает cache line (64 байта), но если объект B далеко — это cache miss. Обращение к основной памяти (RAM) вместо кэша L1 — это разница между 1 наносекундой и 100 наносекундами. Сто раз медленнее.

На stack все данные функции рядом. На heap — разброс, и кэш процессора работает неэффективно.

**Причина 3: Синхронизация между потоками.**

Heap общий для всех потоков. Когда два потока одновременно пытаются выделить память, аллокатор должен гарантировать, что они не получат один и тот же блок. Для этого нужны блокировки (locks), а каждая блокировка — это десятки тактов процессора на захват и освобождение, плюс возможное ожидание, если блокировка уже захвачена другим потоком.

Современные аллокаторы (jemalloc, tcmalloc) используют thread-local arenas — каждый поток имеет собственную область heap для мелких аллокаций. Это снижает конкуренцию, но не устраняет её полностью.

**Причина 4: Метаданные и bookkeeping.**

Каждый выделенный блок heap хранит метаданные: размер, флаги, указатели на соседние свободные блоки. Эти метаданные тоже занимают память и требуют обновления. На stack метаданных нет — размер frame известен компилятору заранее.

> **Контекст Брайанта и О'Халларона:** В CS:APP (Computer Systems: A Programmer's Perspective) авторы показывают, что даже лучшие аллокаторы общего назначения тратят 30-60% времени на bookkeeping при интенсивных аллокациях. Это не баг — это фундаментальная цена гибкости.

### Преимущества Heap

- **Гибкий размер.** Ограничен только доступной памятью.
- **Любой порядок освобождения.** Можно удалить объект из "середины".
- **Общий для потоков.** Объекты доступны из любого потока.
- **Долгоживущие данные.** Переживают завершение функции.

---

Мы разобрали, как устроены Stack и Heap по отдельности. Но в реальной программе они работают вместе — ссылки на stack указывают на объекты в heap. Как именно это устроено?

## КАК Stack и Heap работают вместе

### Типичный сценарий в JVM/Kotlin

Рассмотрим простую функцию на Kotlin. Обрати внимание: переменная `user` на stack — это не сам объект, а лишь адрес (ссылка) на объект в heap. Это критическое различие.

```kotlin
fun createUser(name: String): User {
    val age = 25           // примитив → stack
    val user = User(name, age)  // объект → heap, ссылка → stack
    return user            // возвращаем ссылку
}
```

Этот код демонстрирует типичную картину: примитивные значения вроде `age` живут на stack, а объекты — в heap. Переменная `user` на stack занимает 8 байт (размер указателя на 64-битной системе) и хранит адрес реального объекта в heap.

```
STACK                           HEAP
┌─────────────────┐            ┌─────────────────┐
│ createUser()    │            │                 │
│   name: ref ────┼────────────┼──→ "John"       │
│   age: 25       │            │                 │
│   user: ref ────┼────────────┼──→ User {       │
│                 │            │      name: ref  │
│                 │            │      age: 25    │
└─────────────────┘            │    }            │
                               └─────────────────┘
```

**Что где хранится:**

| Данные | Где | Почему |
|--------|-----|--------|
| `age = 25` (примитив) | Stack | Маленький, короткоживущий |
| `user` (ссылка) | Stack | Это просто адрес, 8 байт |
| `User(...)` (объект) | Heap | Может жить после завершения функции |
| `"John"` (строка) | Heap | Объект, переживает функцию |

### Что происходит при возврате из функции

1. Stack frame `createUser()` удаляется
2. Переменная `user` (ссылка) исчезает
3. Но объект `User` в heap остаётся!
4. Если возвращённую ссылку кто-то сохранит — объект живёт
5. Если никто не сохранит — объект станет "мусором" (для GC)

---

## Escape Analysis: как компилятор решает куда поместить объект

### Что такое escape analysis

Мы сказали, что объекты идут в heap, а примитивы — на stack. Но это упрощение. Современные компиляторы и JIT-компиляторы (в частности HotSpot JVM) умеют анализировать, **действительно ли объект нуждается в heap**. Этот анализ называется escape analysis — анализ "убегания".

Суть проста: если объект создаётся внутри функции и никогда не покидает её — зачем ему heap? Можно разместить его на stack, и он автоматически исчезнет при возврате из функции. Быстрее, без нагрузки на сборщик мусора.

### Как компилятор рассуждает: пошаговый разбор

Представь, что компилятор — это менеджер офиса, который решает: положить документ на стол (stack) или отправить в архив (heap). Он задаёт серию вопросов.

**Вопрос 1: Возвращается ли объект из функции?**

Если функция возвращает ссылку на объект — объект "убегает" (escapes). Вызывающий код получит ссылку, и объект должен жить после завершения функции. Значит, только heap.

**Вопрос 2: Присваивается ли объект полю другого объекта?**

Если объект сохраняется в поле другого heap-объекта, он "убегает" через этого хозяина. Другой код может получить доступ к нему позже. Heap.

**Вопрос 3: Передаётся ли объект в другой поток?**

Если ссылка на объект передаётся другому потоку, объект живёт в общей памяти и не может быть на stack (stack — per-thread). Heap.

**Вопрос 4: Передаётся ли объект в метод, который компилятор не может проанализировать?**

Если объект передаётся в виртуальный метод или внешнюю библиотеку — компилятор не знает, что с ним произойдёт. На всякий случай — heap.

```
Escape Analysis: дерево решений

  Объект создан внутри функции
  │
  ├── Возвращается из функции? ──── ДА → HEAP
  │
  ├── Сохраняется в поле объекта? ─ ДА → HEAP
  │
  ├── Передаётся другому потоку? ── ДА → HEAP
  │
  ├── Передаётся в непрозрачный
  │   метод (virtual, native)? ──── ДА → HEAP
  │
  └── Ничего из вышеперечисленного?
      │
      └── STACK (или scalar replacement)
          Объект разбирается на поля,
          каждое поле — отдельная переменная на stack
```

### Scalar Replacement: объект превращается в набор переменных

Самая агрессивная оптимизация escape analysis — **scalar replacement**. Если объект не убегает и компилятор знает все его поля, он может вообще не создавать объект. Вместо этого каждое поле становится отдельной локальной переменной на stack.

Представим следующий код. Мы создаём объект `Point`, но используем его только локально для вычисления расстояния.

```kotlin
fun distance(x1: Int, y1: Int, x2: Int, y2: Int): Double {
    val p = Point(x2 - x1, y2 - y1)  // Объект Point
    return sqrt((p.x * p.x + p.y * p.y).toDouble())
}
```

JIT-компилятор видит: `Point` не убегает. После scalar replacement код превращается примерно в это (внутри JVM, не Kotlin):

```kotlin
fun distance(x1: Int, y1: Int, x2: Int, y2: Int): Double {
    val p_x = x2 - x1   // Бывшее поле p.x — теперь просто переменная
    val p_y = y2 - y1   // Бывшее поле p.y — теперь просто переменная
    return sqrt((p_x * p_x + p_y * p_y).toDouble())
}
```

Результат: никакой аллокации вообще. Ни на stack, ни на heap. Поля объекта стали обычными переменными, возможно даже живущими в регистрах процессора. Это самый быстрый путь.

### Ограничения escape analysis

Escape analysis — мощная оптимизация, но она работает не всегда:

- **Полиморфизм.** Если метод виртуальный, JIT не знает, какая реализация будет вызвана. Анализ невозможен.
- **Размер объекта.** Слишком большие объекты не размещаются на stack (ограничение реализации).
- **Сложные потоки управления.** Try-catch блоки, сложные ветвления усложняют анализ.
- **JIT warmup.** Escape analysis — оптимизация JIT-компилятора. В первые вызовы (до warmup) объект всё равно пойдёт в heap.

> **Важно:** Escape analysis — это оптимизация, а не гарантия. Программист не должен полагаться на неё. Пиши код корректно, а JIT оптимизирует где сможет. Ты не контролируешь, применится ли escape analysis к конкретному объекту.

---

## Различия в разных платформах

### JVM (Android, Backend)

```
┌─────────────────────────────────────────┐
│                JVM                       │
├──────────────┬──────────────────────────┤
│    Stack     │         Heap             │
│  (per thread)│       (shared)           │
├──────────────┼──────────────────────────┤
│ - Примитивы  │ - Все объекты            │
│ - Ссылки     │ - Массивы                │
│ - Параметры  │ - Строки                 │
│              │                          │
│ Размер: -Xss │ Размер: -Xms, -Xmx       │
│ Default: 1MB │ Default: varies          │
├──────────────┴──────────────────────────┤
│ Управление heap: Garbage Collector      │
│ (G1, ZGC, Shenandoah)                   │
└─────────────────────────────────────────┘
```

### Kotlin/Native (iOS)

```
┌─────────────────────────────────────────┐
│           Kotlin/Native                  │
├──────────────┬──────────────────────────┤
│    Stack     │         Heap             │
│  (per thread)│       (shared)           │
├──────────────┼──────────────────────────┤
│ - Примитивы  │ - Все объекты            │
│ - Struct-ы   │ - Classes                │
│ - Параметры  │                          │
├──────────────┴──────────────────────────┤
│ Управление heap:                        │
│ - Tracing GC (с Kotlin 1.7.20)          │
│ - Ранее: ARC + freeze model             │
└─────────────────────────────────────────┘
```

### C/C++ (Native)

```
┌─────────────────────────────────────────┐
│              Native C/C++                │
├──────────────┬──────────────────────────┤
│    Stack     │         Heap             │
├──────────────┼──────────────────────────┤
│ - Локальные  │ - malloc() / new         │
│   переменные │ - Ручное управление      │
│ - Автоматич. │ - free() / delete        │
│   очистка    │                          │
├──────────────┴──────────────────────────┤
│ Управление heap: ПРОГРАММИСТ            │
│ Забыл free() = утечка памяти            │
│ Двойной free() = undefined behavior     │
└─────────────────────────────────────────┘
```

---

## Распространённые заблуждения

### 1. Stack Overflow

**Причина:** Слишком глубокая рекурсия или большие локальные массивы.

Каждый рекурсивный вызов — это новый stack frame. Если рекурсия уходит на тысячи уровней в глубину, stack frame'ы накапливаются, и stack заканчивается. Это не баг языка — это фундаментальное ограничение фиксированного размера stack.

```kotlin
// Каждый рекурсивный вызов добавляет ~50-100 байт stack frame
// При stack 1MB это ≈ 10 000 - 20 000 вызовов
fun factorial(n: Long): Long {
    if (n <= 1) return 1
    return n * factorial(n - 1)  // Новый frame на каждый вызов
}
factorial(100000) // → StackOverflowError
```

Этот код демонстрирует, что даже простая рекурсия без локальных переменных может исчерпать stack. Решение — итерация вместо рекурсии, или `tailrec` в Kotlin, который компилятор превращает в цикл.

### 2. Возврат указателя на локальную переменную (C/C++)

```c
// Когда функция завершается, stack frame уничтожается
// Переменная x жила в этом frame — её адрес теперь невалиден
int* dangerous() {
    int x = 42;
    return &x;  // x исчезнет после возврата!
}

int* p = dangerous();
*p = 10;  // Undefined behavior! Память уже не наша.
```

В Kotlin/Java этой проблемы нет — объекты на heap, ссылки безопасны.

### 3. Утечки памяти (Memory Leaks)

**Heap без GC (C/C++):**
```c
void leak() {
    int* p = malloc(1000);
    // Забыли free(p)! Память занята, но недоступна
}
```

**Heap с GC (Java/Kotlin):** Утечки тоже возможны, если держишь ссылки на ненужные объекты.

```kotlin
// Статический список — никогда не очищается GC,
// потому что ссылка от companion object → список → элементы
object Cache {
    private val items = mutableListOf<Data>()
    fun add(data: Data) { items.add(data) }
}
```

Этот код показывает, что GC защищает от use-after-free, но не от логических утечек. Объекты формально "достижимы" через статическую ссылку, поэтому GC считает их живыми, хотя программа их больше не использует.

### 4. Миф: "Примитивы всегда на stack"

Это упрощение. На самом деле:

| Ситуация | Где хранится |
|----------|--------------|
| Локальная переменная `val x = 5` | Stack |
| Поле класса `class User(val age: Int)` | Heap (внутри объекта) |
| Элемент массива `intArrayOf(1,2,3)` | Heap (массив — объект) |
| Boxing `val x: Int? = 5` | Heap (обёртка Integer) |

### 5. Миф: "Heap всегда медленнее"

Современные аллокаторы очень оптимизированы:
- **Thread-local allocation buffers (TLAB)** в JVM — почти как stack
- **Bump-pointer allocation** — простое увеличение указателя
- **Escape analysis** — JIT может перенести объект на stack

TLAB — это маленький кусочек heap, зарезервированный за конкретным потоком. Аллокация внутри TLAB — это просто сдвиг указателя, без блокировок. По скорости это приближается к stack allocation. Только когда TLAB заканчивается, нужна синхронизация для получения нового.

---

Мы разобрали, ГДЕ живут данные и КАК память организована. Но кто убирает объекты из heap, когда они больше не нужны? Если stack чистит себя сам (при выходе из функции), то за heap должен кто-то следить. Этот вопрос ведёт нас к двум фундаментальным подходам: Garbage Collection и Reference Counting.

## Связь с другими темами

### [[garbage-collection-explained]] — Автоматическая уборка heap

Garbage Collection — ответ на вопрос "кто убирает heap". GC-алгоритмы (mark-and-sweep, copying, generational) используют те самые свойства heap, которые мы разобрали: непредсказуемый порядок освобождения, граф ссылок между объектами, фрагментацию. Понимание heap из этого материала — обязательная основа для понимания GC. Без знания того, как устроен free list и фрагментация, невозможно понять, зачем GC делает compaction.

### [[reference-counting-arc]] — Подсчёт ссылок как альтернатива GC

Reference Counting — другой ответ на тот же вопрос. Вместо периодического обхода графа объектов, каждый объект сам знает, сколько ссылок на него указывает. Когда счётчик обнуляется — объект удаляется немедленно. ARC (Automatic Reference Counting) в Swift автоматизирует этот процесс. Это особенно важно для KMP: на JVM работает GC, а на iOS Kotlin/Native взаимодействует с ARC-миром Swift.

### [[kmp-memory-management]] — Практика управления памятью в KMP

В Kotlin Multiplatform один и тот же Kotlin-код работает на разных платформах с разными моделями памяти. Этот материал объясняет, как различия между JVM GC и Kotlin/Native GC влияют на архитектуру shared-кода. Понимание stack/heap — предпосылка для осмысленного выбора между shared и platform-specific решениями.

---

## Ключевые выводы

1. **Stack** — быстрый, автоматический, LIFO. Для локальных переменных и вызовов функций.

2. **Heap** — гибкий, общий, требует управления. Для объектов с динамическим временем жизни.

3. **Stack быстрый** потому что его ограничения (LIFO, per-thread, фиксированный размер) делают все операции тривиальными и дружественными к кэшу.

4. **Heap медленный** из-за поиска свободных блоков, фрагментации, необходимости синхронизации и плохой пространственной локальности.

5. **Аллокаторы** (first-fit, best-fit, buddy system) — это компромиссы между скоростью аллокации, фрагментацией и использованием памяти.

6. **Escape analysis** позволяет JIT-компилятору размещать объекты на stack вместо heap, когда объект не "убегает" из функции.

7. **StackOverflowError** = переполнение stack (рекурсия, большие локальные данные).

8. **OutOfMemoryError** = нехватка heap (много объектов, утечки).

---

## Источники и дальнейшее чтение

- **Bryant, R. & O'Hallaron, D. (2015). Computer Systems: A Programmer's Perspective.** — фундаментальный учебник, главы 9-10 подробно объясняют иерархию памяти, виртуальную память и динамическую аллокацию. Идеален для понимания связи между hardware и software уровнями управления памятью.

- **Tanenbaum, A. (2014). Modern Operating Systems.** — классический учебник по ОС, глава 3 посвящена управлению памятью. Объясняет, как ОС предоставляет процессам виртуальное адресное пространство, поверх которого работают stack и heap. Даёт необходимый контекст "уровнем ниже".

- **Knuth, D. (1997). The Art of Computer Programming, Vol. 1.** — секция 2.5 содержит исчерпывающее описание алгоритмов динамической аллокации, включая Buddy System. Математически строгий, но удивительно читаемый.

- [Baeldung: Stack and Heap](https://www.baeldung.com/cs/memory-stack-vs-heap) — техническое объяснение
- [CS225 Illinois: Stack and Heap Memory](https://courses.grainger.illinois.edu/cs225/fa2022/resources/stack-heap/) — академический курс
- [Microsoft: Memory Allocation History](https://learn.microsoft.com/en-us/archive/blogs/abhinaba/back-to-basics-memory-allocation-a-walk-down-the-history) — историческая справка
- [Kotlin Docs: Native Memory Manager](https://kotlinlang.org/docs/native-memory-manager.html) — официальная документация

---

*Проверено: 2026-02-10*
