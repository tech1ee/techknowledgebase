---
title: "Интерпретация и JIT: от байткода к скорости"
created: 2026-01-04
modified: 2026-02-10
type: deep-dive
status: published
tags:
  - topic/cs-foundations
  - type/deep-dive
  - level/advanced
related:
  - "[[compilation-pipeline]]"
  - "[[bytecode-virtual-machines]]"
  - "[[native-compilation-llvm]]"
prerequisites:
  - "[[compilation-pipeline]]"
  - "[[bytecode-virtual-machines]]"
---

# Интерпретация и JIT: от байткода к скорости

> **TL;DR:** Интерпретатор выполняет код построчно — быстрый старт, медленное исполнение. Компилятор транслирует весь код заранее — медленный старт, быстрое исполнение. JIT (Just-In-Time) — гибрид: начинает с интерпретации, компилирует "горячий" код во время работы. HotSpot JVM использует tiered compilation (5 уровней), V8 — 4-tier pipeline. Inline caching ускоряет dynamic dispatch. Деоптимизация возвращает к интерпретации при нарушении предположений. Для KMP критично: понимание JIT помогает писать "дружественный" к оптимизатору код.

---

## Зачем это знать

Каждый раз, когда Android-приложение "подтормаживает" при первом запуске, а потом работает плавно — это JIT-компиляция в действии. Каждый раз, когда серверное приложение на JVM показывает посредственные бенчмарки в первые 30 секунд, а потом ускоряется в 5-10 раз — это JIT "прогрелся".

JIT — одна из самых недопонятых технологий в разработке. Разработчики знают, что она существует, но редко понимают, *как* она работает — и это мешает писать код, который JIT может эффективно оптимизировать. Код с предсказуемыми типами может работать в 10 раз быстрее, чем polymorphic-код — и единственная разница в том, насколько JIT "дружелюбен" к вашему коду.

Для KMP-разработчика это знание критично: один и тот же Kotlin-код исполняется через JIT (на JVM/Android) и через AOT (на iOS/Native). Понимание JIT объясняет, почему одна и та же логика может иметь разные характеристики производительности на разных платформах.

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| **Bytecode** | Что JIT компилирует | [[bytecode-virtual-machines]] |
| **AOT compilation** | Альтернатива JIT | [[native-compilation-llvm]] |
| **Compilation pipeline** | Фазы компиляции | [[compilation-pipeline]] |

---

## Терминология

| Термин | Что это | Аналогия |
|--------|---------|----------|
| **Interpreter** | Выполняет код построчно | Синхронный переводчик |
| **JIT** | Компиляция во время выполнения | Переводчик, записывающий частые фразы |
| **Hot code** | Часто выполняемый код | Протоптанная тропинка |
| **Warmup** | Период до peak performance | Разогрев двигателя |
| **Deoptimization** | Откат оптимизаций | Возврат к базовой версии |
| **Inline Caching** | Кэширование результатов lookup | Записная книжка номеров |
| **Tiered Compilation** | Многоуровневая компиляция | Лестница оптимизаций |
| **Profile-Guided Optimization** | Оптимизация на основе данных о работе | Навигатор, учащийся на пробках |

---

## Спектр подходов: от интерпретации до AOT

Прежде чем погружаться в JIT, важно увидеть полную картину. Между "чистой интерпретацией" и "полной AOT-компиляцией" существует целый спектр подходов. Каждый — компромисс между скоростью запуска и скоростью исполнения.

### Чистый интерпретатор

Интерпретатор читает инструкции одну за другой и выполняет каждую немедленно. Никакой предварительной обработки — как синхронный переводчик, который слышит фразу и тут же переводит её.

**Преимущества:** мгновенный старт, простая реализация, лёгкая отладка (можно остановить на любой инструкции).

**Недостатки:** медленное исполнение. Каждая инструкция требует: прочитать opcode, определить тип операции (dispatch), загрузить операнды, выполнить, сохранить результат. Для простого `a + b` интерпретатор тратит десятки машинных инструкций на overhead, тогда как native-код делает это за одну.

**Кто использует:** CPython (Python), ранние Ruby-интерпретаторы, простые скриптовые языки.

### Baseline compiler (быстрая компиляция без оптимизаций)

Компилирует bytecode в native-код, но максимально просто — почти 1:1 трансляция. Не тратит время на анализ и оптимизации. Результат лучше интерпретации, но далёк от оптимума.

**Преимущества:** быстрая компиляция (миллисекунды), код лучше интерпретатора в 5-10 раз.

**Недостатки:** код хуже оптимизированного в 3-5 раз.

**Кто использует:** V8 Sparkplug, JVM C1 (Level 1, без профилирования).

### Оптимизирующий JIT-компилятор

Компилирует bytecode в native-код с агрессивными оптимизациями: inlining, escape analysis, loop unrolling, dead code elimination. Использует данные профилирования для спекулятивных оптимизаций.

**Преимущества:** производительность, сравнимая с AOT, а иногда выше (runtime-специфичные оптимизации).

**Недостатки:** медленная компиляция (секунды), большое потребление памяти, непредсказуемые паузы.

**Кто использует:** JVM C2, V8 TurboFan, JavaScriptCore FTL.

### AOT-компилятор (Ahead-Of-Time)

Компилирует весь код заранее — до запуска программы. Результат — native binary, не требующий VM.

**Преимущества:** мгновенный старт, предсказуемая производительность, минимальное потребление памяти.

**Недостатки:** нет runtime-оптимизаций, медленная сборка, бо'льший размер бинарника.

**Кто использует:** GCC/Clang (C/C++), Kotlin/Native, Swift, GraalVM Native Image.

```
┌─────────────────────────────────────────────────────────────────┐
│              СПЕКТР: ОТ ИНТЕРПРЕТАЦИИ ДО AOT                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Скорость    Интерпретатор  Baseline   JIT Opt.    AOT        │
│   старта:     ★★★★★          ★★★★       ★★★         ★★         │
│   Скорость    ★              ★★★        ★★★★★       ★★★★       │
│   исполнения:                                                   │
│   Потребление ★★★★★          ★★★★       ★★          ★★★★       │
│   памяти:                                                       │
│                                                                 │
│   ◄────────────────────────────────────────────────────────►    │
│   Мгновенный старт                    Максимальная скорость    │
│                                                                 │
│   JIT живёт посередине — и пытается дать лучшее из обоих.      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

> **Ключевая идея:** Не существует "лучшего" подхода. CLI-утилита, работающая 0.5 секунды, не нуждается в JIT — интерпретатор или AOT лучше. Серверное приложение с uptime в месяцы — идеальный кандидат для JIT. Мобильное приложение — требует гибрида.

Мы увидели весь спектр. Теперь разберём, как работает JIT — самый сложный и интересный подход.

---

## КАК JIT "учится": профилирование → компиляция → деоптимизация

### Аналогия: повар с новым рецептом

Представь шеф-повара, который впервые готовит блюдо. Он открывает книгу рецептов и читает каждый шаг: "взять 200 г муки... просеять... добавить щепотку соли...". Это интерпретация — медленно, но надёжно.

После десятого приготовления повар запоминает рецепт наизусть. Он больше не открывает книгу — руки делают всё автоматически. Это JIT-компиляция — рецепт "скомпилирован" в мышечную память.

Но повар пошёл дальше. Он заметил, что клиенты всегда заказывают это блюдо без лука. Поэтому он перестал чистить лук заранее — спекулятивная оптимизация. Однажды клиент попросил блюдо *с* луком. Повар "деоптимизируется" — возвращается к книге, чтобы вспомнить, как готовить оригинальную версию.

Этот процесс — точная аналогия JIT: начинаем с чтения инструкций (интерпретация), запоминаем частые операции (компиляция), оптимизируем под типичные случаи (спекуляция), откатываемся при нарушении предположений (деоптимизация).

### Шаг 1: Интерпретация + профилирование

JIT начинает с интерпретации bytecode — медленной, но немедленной. Одновременно собирает статистику (profile data):

**Что именно собирает профайлер:**

- **Invocation counts** — сколько раз вызван каждый метод. Метод, вызванный 10000 раз, — кандидат на компиляцию. Метод, вызванный 3 раза, — нет.

- **Branch frequencies** — какие ветки if/else выполняются чаще. Если `if (x > 0)` срабатывает в 99% случаев, JIT может оптимизировать "быстрый путь" для true.

- **Type information** — какие реальные типы приходят в аргументы. Если параметр `Any` всегда получает `Int`, JIT может сгенерировать специализированный код для `Int`.

- **Call site targets** — какие конкретные реализации вызываются на каждом call site. Если `list.forEach { it.process() }` всегда вызывает `UserImpl.process()`, JIT может заинлайнить этот конкретный метод.

Профилирование имеет cost — оно замедляет интерпретацию на 5-15%. Но без профиля JIT не может делать спекулятивные оптимизации, которые дают основной прирост скорости.

### Шаг 2: Обнаружение "горячего" кода

JVM считает метод "горячим", когда:
- Метод вызван больше threshold раз (по умолчанию ~10000 вызовов для C2)
- Или цикл внутри метода выполнил больше threshold итераций (OSR — On-Stack Replacement)

OSR — особенно важная техника. Представь метод с циклом на миллион итераций. Без OSR JIT скомпилирует метод только после его *завершения*. С OSR — JIT может заменить интерпретируемый цикл на скомпилированный *прямо посреди выполнения*. Это как замена двигателя на ходу.

### Шаг 3: Компиляция в native-код

Когда метод признан "горячим", JIT-компилятор транслирует его bytecode в native-код. Это полноценная компиляция с оптимизациями — но с одним отличием от AOT: JIT использует данные профилирования.

**Что JIT знает, а AOT — нет:**

- Какие типы реально приходят в аргументы (не все возможные, а конкретные)
- Какие ветки кода выполняются чаще (не все, а реальные)
- Какие объекты "убегают" из метода, а какие — нет
- Какие виртуальные вызовы monomorphic (один тип), какие polymorphic

Именно эта информация делает JIT-код потенциально *быстрее* AOT-кода. AOT видит тип `Any` и генерирует универсальный код. JIT видит, что `Any` всегда `Int`, и генерирует специализированный.

### Шаг 4: Деоптимизация

Спекулятивные оптимизации могут оказаться неверными. Если JIT предположил, что `x` всегда `Int`, а пришёл `Double` — скомпилированный код неправильный.

```
┌─────────────────────────────────────────────────────────────────┐
│                    DEOPTIMIZATION FLOW                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Optimized Code                                                │
│       │                                                         │
│       ▼                                                         │
│   ┌─────────────────┐                                           │
│   │  Guard Check    │  ← "x instanceof Int?"                    │
│   └────────┬────────┘                                           │
│            │                                                    │
│       ┌────┴────┐                                               │
│       │         │                                               │
│      YES       NO                                               │
│       │         │                                               │
│       ▼         ▼                                               │
│   Fast Path   Bailout                                           │
│   (optimized) (деоптимизация)                                   │
│                 │                                               │
│                 ▼                                               │
│            Interpreter                                          │
│            (сбор нового профиля)                                │
│                 │                                               │
│                 ▼                                               │
│            Re-JIT с новым профилем                              │
│            (теперь знает про Double)                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

JIT вставляет "guard'ы" (проверки) перед каждым спекулятивным участком. Guard — это быстрая проверка: "предположение всё ещё верно?" Если да — быстрый путь. Если нет — bailout: откат к интерпретации, сбор нового профиля, перекомпиляция.

Деоптимизация — не катастрофа. Это нормальная часть работы JIT. Обычно после деоптимизации JIT перекомпилирует метод с учётом новой информации, и код снова становится быстрым — но теперь для более широкого набора типов.

**Два типа деоптимизации:**

- **Synchronous** — guard сработал в текущем потоке. Текущий метод откатывается к интерпретации.
- **Asynchronous** — другой поток изменил class hierarchy (загрузил новый класс, изменил наследование). Все скомпилированные методы, зависящие от старой иерархии, инвалидируются.

---

## ПОЧЕМУ существует "warmup" и что это значит для пользователей

### Что происходит при старте JVM-приложения

Когда вы запускаете Java/Kotlin приложение, первые секунды (иногда минуты) оно работает медленнее, чем потом. Это warmup — время, необходимое JIT для "разгона".

```
┌─────────────────────────────────────────────────────────────────┐
│              WARMUP: ЧТО ПРОИСХОДИТ ПО СЕКУНДАМ                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   0-1s:   Class loading + Bytecode verification                │
│           Все классы загружаются, проверяются, инициализируются │
│           Производительность: 10-20% от пика                   │
│                                                                 │
│   1-5s:   Интерпретация + начало профилирования                │
│           Горячие методы начинают накапливать вызовы            │
│           Производительность: 20-40% от пика                   │
│                                                                 │
│   5-15s:  C1 компиляция (быстрая, базовые оптимизации)         │
│           Первые методы получают native-код                     │
│           Производительность: 50-70% от пика                   │
│                                                                 │
│   15-60s: C2 компиляция (медленная, агрессивные оптимизации)   │
│           Горячие методы перекомпилируются с полным профилем    │
│           Производительность: 80-95% от пика                   │
│                                                                 │
│   60s+:   Стабильное состояние (steady state)                  │
│           Основные методы скомпилированы                        │
│           Производительность: ~100% от пика                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Почему warmup — проблема

Для long-running серверов warmup — мелкая неприятность. Сервер работает месяцами, 30 секунд прогрева — ничто.

Но для других сценариев warmup — катастрофа:

**Serverless функции** (AWS Lambda, Google Cloud Functions) — время жизни измеряется миллисекундами. JIT не успевает даже начать. Весь код выполняется интерпретатором. Решение: AOT (GraalVM Native Image) или pre-warming.

**CLI-утилиты** — пользователь ожидает мгновенный результат. `kotlinc --help` не должно "прогреваться" 10 секунд.

**Мобильные приложения** — пользователь открывает приложение и ожидает мгновенный отклик. Android ART решает это гибридной моделью: AOT для hot path'ов + JIT для остального.

### Решения проблемы warmup

**CRaC (Coordinated Restore at Checkpoint):**
"Замораживает" JVM в оптимизированном состоянии — со всеми JIT-скомпилированными методами, загруженными классами, warm cache'ами. При следующем запуске — восстановление из checkpoint'а за миллисекунды. Ограничение: требует Linux, не все ресурсы (файлы, сокеты) можно "заморозить".

**AOT Method Profiling (JEP 515):**
Сохраняет профили горячего кода между запусками. При следующем старте JIT не ждёт 10000 вызовов — он уже знает, какие методы горячие, и начинает компиляцию немедленно. Warmup сокращается с минуты до секунд.

**GraalVM Native Image:**
Полная AOT-компиляция: весь код компилируется до запуска. Instant startup, но теряются runtime-оптимизации JIT. Trade-off: мгновенный старт vs чуть ниже пиковая производительность.

**Tiered Compilation Tuning:**
`-XX:TieredStopAtLevel=1` — только C1 (быстрая компиляция). Warmup сокращается, но пиковая производительность ниже. Хорошо для short-lived процессов.

---

## JVM Tiered Compilation: пять уровней

### Как работает лестница оптимизаций

HotSpot JVM использует tiered compilation с 5 уровнями. Каждый метод поднимается по "лестнице" — от интерпретации к максимальной оптимизации.

| Level | Компилятор | Что происходит | Зачем |
|-------|-----------|----------------|-------|
| **0** | Interpreter | Интерпретация + полное профилирование | Начальный сбор данных |
| **1** | C1 | Быстрая компиляция, без профилирования | Для trivial-методов (getters, setters) |
| **2** | C1 | Компиляция с лёгким профилированием | Когда C2 занят (очередь полна) |
| **3** | C1 | Компиляция с полным профилированием | Основной путь: сбор данных для C2 |
| **4** | C2 | Агрессивные оптимизации | Максимальная производительность |

### Типичный путь метода

Большинство методов проходят путь `0 → 3 → 4`:

```
┌─────────────────────────────────────────────────────────────────┐
│              ТИПИЧНЫЙ ПУТЬ МЕТОДА ЧЕРЕЗ TIERED                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Level 0: Interpreter                                          │
│   "Метод processUser() интерпретируется.                       │
│    Профайлер считает: вызван 0 → 100 → 1000 раз..."           │
│         │                                                       │
│         │ [достигнут порог ~2000 вызовов]                       │
│         ▼                                                       │
│   Level 3: C1 с полным профилированием                         │
│   "Скомпилирован за 1 мс. Работает в 5x быстрее.              │
│    Продолжает собирать профиль: типы, ветки, call sites."      │
│         │                                                       │
│         │ [достигнут порог ~10000 вызовов + профиль готов]      │
│         ▼                                                       │
│   Level 4: C2 с агрессивными оптимизациями                     │
│   "Скомпилирован за 100 мс. Работает в 20x быстрее.            │
│    Inlining, escape analysis, loop unrolling, SIMD."           │
│                                                                 │
│   Маленькие методы (getters): 0 → 1 (C1, без профиля, навсегда)│
│   Когда C2 занят: 0 → 2 → 4 (C1 с лёгким профилем)           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### C1 vs C2: два компилятора внутри одной JVM

**C1 (Client Compiler):**
Быстрый компилятор с базовыми оптимизациями. Компилирует метод за 1-10 мс. Делает: inlining (маленьких методов), constant folding, dead code elimination. Не делает: escape analysis, loop unrolling, vectorization. Цель: быстро дать native-код, лучше интерпретатора.

**C2 (Server Compiler):**
Медленный компилятор с агрессивными оптимизациями. Компилирует метод за 50-500 мс. Использует sea-of-nodes IR для глубокого анализа. Делает всё, что C1, плюс: escape analysis, loop unrolling, vectorization, speculative inlining, range check elimination. Цель: максимальная производительность для горячего кода.

Два компилятора — не роскошь, а необходимость. C2 слишком медленный для всех методов: если компилировать каждый метод с агрессивными оптимизациями, компиляция займёт больше времени, чем интерпретация. C1 недостаточно быстрый для горячего кода. Tiered compilation даёт каждому методу уровень оптимизации, соответствующий его "температуре".

### Graal Compiler: альтернативный C2

Graal — оптимизирующий компилятор, написанный на Java (в отличие от C2, написанного на C++). Преимущества: проще разрабатывать и отлаживать, partial escape analysis, лучше для некоторых паттернов. Graal — основа GraalVM и может использоваться как замена C2 в HotSpot.

---

## V8 Pipeline (Chrome/Node.js)

### Четыре уровня (2023+)

V8 эволюционировал в 4-tier систему. JavaScript — динамический язык, и его оптимизация требует других подходов, чем для Java/Kotlin.

```
┌─────────────────────────────────────────────────────────────────┐
│                    V8 COMPILATION PIPELINE                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   JavaScript Source                                             │
│         ↓                                                       │
│   ┌─────────────┐                                               │
│   │   Parser    │ → AST                                         │
│   └─────────────┘                                               │
│         ↓                                                       │
│   ┌─────────────┐                                               │
│   │  Ignition   │ → Bytecode (interpreter)                      │
│   └─────────────┘                                               │
│         ↓ [warm]                                                │
│   ┌─────────────┐                                               │
│   │  Sparkplug  │ → Baseline native (быстрый, без оптимизаций) │
│   └─────────────┘                                               │
│         ↓ [hot]                                                 │
│   ┌─────────────┐                                               │
│   │   Maglev    │ → Mid-tier optimized (SSA, быстрая компиляция)│
│   └─────────────┘                                               │
│         ↓ [very hot]                                            │
│   ┌─────────────┐                                               │
│   │  TurboFan   │ → Fully optimized (sea-of-nodes, максимум)   │
│   └─────────────┘                                               │
│                                                                 │
│   Каждый уровень = баланс между скоростью компиляции           │
│   и качеством результата. Четыре ступени — плавная лестница.   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

V8 добавил Sparkplug (2021) и Maglev (2023) потому, что разрыв между Ignition (интерпретатор) и TurboFan (оптимизирующий компилятор) был слишком большим. Sparkplug — "ступенька" между интерпретацией и оптимизированным кодом: почти мгновенная компиляция, но в 5-10 раз быстрее интерпретатора. Maglev заполняет промежуток между Sparkplug и TurboFan.

---

## Tracing JIT: альтернативный подход

### Method JIT vs Tracing JIT

Существуют два фундаментально разных подхода к определению того, "что компилировать":

**Method JIT (JVM, V8):** единица компиляции — метод/функция. Компилируется весь метод целиком. Inlining — явное решение ("встроить вызываемый метод в вызывающий").

**Tracing JIT (LuaJIT, PyPy):** единица компиляции — trace (горячий путь через один или несколько циклов). VM записывает реальный путь исполнения — последовательность операций одной итерации цикла — и компилирует этот путь.

### Как работает Tracing JIT

Аналогия: представь, что ты каждый день ходишь на работу. Method JIT — выучить наизусть все улицы города (компилировать весь метод). Tracing JIT — запомнить только маршрут, которым ты реально ходишь (скомпилировать только горячий trace).

```
1. Интерпретировать код
2. Обнаружить горячий цикл (> threshold итераций)
3. Начать "запись": следить за каждой операцией
4. Записать все операции одной итерации (включая вызовы функций!)
5. Скомпилировать записанный trace в native-код
6. Исполнять native (с guard'ами на каждом ветвлении)
```

**Преимущество tracing:** автоматический inlining. Если внутри цикла вызывается функция, trace включает тело функции. Не нужно принимать решение "инлайнить или нет" — trace записывает реальный путь.

**Недостаток tracing:** performance cliffs. Если цикл содержит непредсказуемые ветвления, guard'ы срабатывают часто → деоптимизация → перезапись trace → опять деоптимизация. Результат — непредсказуемые провалы производительности.

LuaJIT — лучший пример tracing JIT. Для предсказуемых циклов он генерирует код, сравнимый с C. Для polymorphic-кода — может быть медленнее интерпретатора.

---

## Inline Caching: как JIT ускоряет динамический dispatch

### Проблема: виртуальные вызовы

В объектно-ориентированном коде вызов метода через interface или abstract class требует lookup: "какая конкретная реализация у этого объекта?" Это называется dynamic dispatch.

Аналогия: ты звонишь в call-центр (interface). Каждый раз тебя соединяют с оператором (конкретная реализация). Lookup: "какой оператор свободен?" занимает время. Если ты звонишь по одному вопросу каждый день и тебя соединяют с одним и тем же оператором — было бы быстрее звонить ему напрямую.

Inline caching — это "записная книжка" с прямыми номерами. JIT записывает: "на этом call site обычно вызывается User.process()". При следующем вызове — проверяет: "объект — User? Да → вызвать User.process() напрямую, без lookup".

### Три состояния inline cache

```
┌─────────────────────────────────────────────────────────────────┐
│                    INLINE CACHING STATES                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   UNINITIALIZED                                                 │
│   "Ещё не знаем, какой тип приходит."                          │
│       ↓ [первый вызов с типом User]                            │
│                                                                 │
│   MONOMORPHIC (один тип)                                       │
│   "Если тип = User → вызвать User.process() напрямую"          │
│   Самый быстрый: одна проверка типа + прямой вызов.            │
│       ↓ [вызов с типом Admin]                                  │
│                                                                 │
│   POLYMORPHIC (несколько типов, обычно 2-4)                    │
│   "Если User → User.process()                                  │
│    Если Admin → Admin.process()                                │
│    Иначе → slow lookup"                                        │
│   Быстрый: цепочка проверок (2-4) + прямой вызов.             │
│       ↓ [вызовы с 5+ разными типами]                           │
│                                                                 │
│   MEGAMORPHIC (много типов)                                    │
│   "Слишком много типов. Всегда slow lookup."                   │
│   Медленный: виртуальная таблица (vtable) или hashtable lookup.│
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Статистика из реальных программ

Эмпирические данные из исследований Smalltalk (Hölzle, Chambers, Ungar, 1991) и Java (Ishizaki et al., 2000):
- **~90%** call sites — monomorphic (один тип). Почти бесплатный dispatch.
- **~9%** — polymorphic (2-4 типа). Быстрый dispatch через цепочку проверок.
- **~1%** — megamorphic (5+ типов). Медленный lookup.

Вывод: подавляющее большинство вызовов в реальных программах — monomorphic. JIT оптимизирует под этот случай, и 90% вызовов становятся почти бесплатными.

### Влияние на код

Зная это, можно писать JIT-friendly код:

Код с предсказуемыми типами позволяет JIT использовать monomorphic inline cache — самый быстрый вариант dispatch. Когда в коллекции один конкретный тип объектов, каждый вызов метода — прямой, без lookup.

```kotlin
// Monomorphic: JIT видит только User
// Один тип → прямой вызов → максимальная скорость
val users: List<User> = getUsers()
users.forEach { it.process() }
```

Когда типов много, JIT не может предсказать, какой метод вызывать, и вынужден использовать медленный lookup каждый раз. Это megamorphic call site — самый медленный вариант.

```kotlin
// Megamorphic: JIT видит 10+ разных типов
// Каждый вызов — lookup через vtable → медленно
val items: List<Processable> = getMixedItems()
items.forEach { it.process() }
```

Разница в производительности может составлять 5-10x для горячих циклов.

---

## Оптимизации JIT

### Inlining — ключ к другим оптимизациям

Inlining — подстановка тела вызываемой функции вместо вызова. Это самая важная оптимизация JIT, потому что она открывает двери для всех остальных.

Без inlining'а JIT видит: "вызвать функцию sum(a, b)". Он не знает, что внутри. С inlining'ом JIT видит: "a + b" — и может применить constant folding, dead code elimination, и другие оптимизации.

JVM контролирует inlining через bytecode-размер метода: методы меньше 35 байт bytecode инлайнятся "бесплатно" (always inline), от 35 до 325 байт — по решению JIT на основе профиля. Больше 325 байт — никогда.

### Escape Analysis — устранение аллокаций

JIT анализирует, "убегает" ли объект из метода. Если нет — объект можно аллоцировать на стеке (или вообще разложить на скалярные поля). Это устраняет GC-давление.

Аналогия: если ты берёшь книгу с полки, читаешь главу и ставишь обратно — книгу не нужно выносить из библиотеки. Escape analysis определяет: объект "остаётся в библиотеке" (на стеке) или "выносится" (в heap).

### Loop Unrolling — уменьшение overhead циклов

Каждая итерация цикла имеет overhead: проверка условия, инкремент счётчика, jump. Loop unrolling "разворачивает" цикл: вместо 1000 итераций по 1 операции — 250 итераций по 4 операции. Overhead делится на 4.

### Constant Folding + Propagation

Вычисление константных выражений при компиляции и распространение известных значений. Часто работает в паре с inlining'ом: после подстановки тела функции JIT видит конкретные значения аргументов и может свернуть выражения.

---

## Подводные камни

### 1. Megamorphic call sites

Главный враг JIT-оптимизации. Когда на одном call site встречается больше 4-5 разных типов, inline cache "сдаётся" и переходит в megamorphic-режим. Все последующие вызовы — через медленный vtable lookup.

### 2. Reflection ломает оптимизации

JIT не может инлайнить вызов через reflection. `method.invoke(obj, args)` — чёрный ящик для оптимизатора. Если горячий код использует reflection, JIT не может его оптимизировать.

### 3. Непредсказуемые branches

JIT оптимизирует предсказуемые ветвления: если `if (condition)` срабатывает в 99% случаев, JIT помещает "быстрый путь" первым. Ветвление 50/50 — не поддаётся оптимизации.

### 4. Warmup в бенчмарках

Классическая ошибка: измерить производительность JVM-кода без warmup'а. Первые 1000 вызовов — интерпретация. Следующие 10000 — C1. Только после этого — C2. Правильный бенчмарк включает фазу warmup'а (JMH делает это автоматически).

### Распространённые заблуждения

| Миф | Реальность | Почему так думают |
|-----|------------|-------------------|
| JIT всегда быстрее AOT | JIT имеет warmup overhead. Для short-lived processes AOT лучше | JIT даёт высокую пиковую производительность |
| Больше оптимизаций = лучше | Каждая оптимизация имеет cost. Иногда C1 достаточно | Кажется логичным |
| JIT работает одинаково на всех JVM | HotSpot, OpenJ9, GraalVM — разные JIT с разными оптимизациями | JVM воспринимается как единая сущность |
| `final` и `sealed` ускоряют JIT | JIT и без них определяет monomorphic call sites по профилю | `final` ограничивает наследование |

---

## Связь с другими темами

**[[bytecode-virtual-machines]]** — prerequisite. JIT компилирует *bytecode* — без понимания bytecode невозможно понять, что именно JIT оптимизирует. Stack-based инструкции JVM и register-based инструкции Dalvik — это "вход" для JIT-компилятора. Рекомендуется прочитать перед этой статьёй.

**[[compilation-pipeline]]** — JIT-компилятор — это полноценный компилятор, работающий в runtime. Он проходит те же этапы pipeline: parsing IR → optimization → code generation. Разница: вместо исходного кода на входе — bytecode, а вместо файла на выходе — код в памяти. Знание pipeline помогает понять, почему JIT-компиляция требует времени и ресурсов.

**[[native-compilation-llvm]]** — альтернатива JIT. AOT через LLVM даёт мгновенный старт, но без runtime-оптимизаций. Kotlin/Native использует LLVM для iOS — понимание JIT объясняет, почему один и тот же Kotlin-код может вести себя по-разному на JVM (с JIT) и на Native (без JIT). GraalVM Native Image — попытка дать JVM-программам AOT, сохранив часть оптимизаций.

---

## Источники и дальнейшее чтение

- **Aycock, J. (2003). A Brief History of Just-In-Time.** — Фундаментальная статья, прослеживающая историю JIT от 1960-х до 2000-х. Показывает, как идея runtime-компиляции развивалась от McCarthy через Smalltalk к HotSpot. Короткая (20 страниц), но плотная — обязательное чтение для понимания контекста.

- **Wuerthinger, T. et al. (2013). One VM to Rule Them All.** — Статья от создателей GraalVM. Описывает идею meta-compilation: фреймворк Truffle позволяет написать интерпретатор языка, а Graal автоматически генерирует JIT. Показывает будущее JIT-технологий.

- **Hölzle, U., Chambers, C., Ungar, D. (1991). Optimizing Dynamically-Typed Object-Oriented Languages With Polymorphic Inline Caches.** — Классическая работа по inline caching. Показывает, что 90% call sites monomorphic — основа для всех последующих JIT-оптимизаций. Без этой статьи не было бы ни HotSpot, ни V8.

- **Nystrom, R. (2021). Crafting Interpreters.** — Практическое руководство. Часть II ("A Bytecode Virtual Machine") показывает реализацию интерпретатора. Помогает "почувствовать", что JIT ускоряет и почему интерпретация медленная.

- [Oracle: JIT Compilation in HotSpot](https://docs.oracle.com/en/java/javase/21/vm/java-hotspot-virtual-machine-performance-enhancements.html) — Официальная документация HotSpot. Описывает tiered compilation, пороги, tuning-параметры.

- [V8 Blog: Ignition, Sparkplug, Maglev, TurboFan](https://v8.dev/blog) — Блог V8. Каждый компонент pipeline описан в отдельном посте с деталями реализации.

---

*Проверено: 2026-02-10*
