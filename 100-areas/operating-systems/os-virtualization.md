---
title: "Виртуализация и контейнеры: изоляция окружений"
created: 2025-12-02
modified: 2025-12-02
type: deep-dive
status: published
area: operating-systems
confidence: high
tags:
  - topic/os
  - virtualization
  - containers
  - docker
  - kvm
  - type/deep-dive
  - level/intermediate
related:
  - "[[os-overview]]"
  - "[[os-processes-threads]]"
  - "[[os-memory-management]]"
  - "[[os-scheduling]]"
  - "[[docker-for-developers]]"
  - "[[kubernetes-basics]]"
---

# Виртуализация и Контейнеры

Виртуализация позволяет запускать несколько изолированных окружений на одном физическом сервере. Это фундамент современной облачной инфраструктуры: AWS, Google Cloud, Azure — все построены на виртуализации. Существует два принципиально разных подхода: полная виртуализация (Virtual Machines), где каждое окружение имеет собственное ядро операционной системы, и контейнеризация, где окружения разделяют ядро хоста. Выбор между ними — это компромисс между уровнем изоляции и накладными расходами.

---

## TL;DR

> **VM:** Полная изоляция с отдельным ядром. Hypervisor управляет гостевыми ОС. Безопасно, но тяжело (startup минуты, overhead 5-15%).
>
> **Контейнеры:** Изоляция через namespaces + cgroups. Общее ядро. Быстро (startup секунды, overhead <1%), но менее изолировано.
>
> **Namespaces:** Изоляция видимости (PID, NET, MNT). Контейнер "думает", что один на сервере.
>
> **Cgroups:** Лимиты ресурсов (CPU, RAM, I/O). Защита от "шумных соседей".
>
> **Type 1 hypervisor:** На bare metal (ESXi, KVM). Минимальный overhead.
> **Type 2 hypervisor:** Внутри ОС (VirtualBox). Проще, но медленнее.
>
> **Когда VM:** Multi-tenant, разные ОС, compliance. **Когда контейнеры:** Микросервисы, CI/CD, высокая плотность.

---

## Часть 1: Интуиция без кода

### Аналогия 1: Отель vs Коворкинг (VM vs Контейнер)

**Виртуальная машина — это отдельный номер в отеле:**
- У каждого гостя свой номер с дверью, ванной, кухней
- Полная изоляция: сосед не слышит вас, вы не слышите соседа
- Много места занимает — каждому номеру нужны все удобства
- Заселение долгое — ключи, уборка, проверка

**Контейнер — это место в коворкинге:**
- Общее здание, общие стены, общий туалет и кухня
- Ваш стол отгорожен перегородкой — не видите соседей
- Вы видите только свой "офис", но здание общее
- Занимает меньше места — инфраструктура общая
- Занять место быстро — сел и работаешь

```
┌─────────────────────────────────────────────────────────────┐
│                    ОТЕЛЬ (VM)                                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │ Номер 1  │  │ Номер 2  │  │ Номер 3  │                   │
│  │ [Кухня]  │  │ [Кухня]  │  │ [Кухня]  │  ← Своя кухня     │
│  │ [Ванная] │  │ [Ванная] │  │ [Ванная] │  ← Своя ванная    │
│  │ [Кровать]│  │ [Кровать]│  │ [Кровать]│  ← Своя кровать   │
│  └──────────┘  └──────────┘  └──────────┘                   │
│             ПОЛНАЯ ИЗОЛЯЦИЯ (отдельные стены)               │
├─────────────────────────────────────────────────────────────┤
│                   КОВОРКИНГ (Контейнеры)                     │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │ Стол 1   │  │ Стол 2   │  │ Стол 3   │  ← Свой стол      │
│  │ [Лампа]  │  │ [Лампа]  │  │ [Лампа]  │                   │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘                   │
│       │             │             │                          │
│  ═════│═════════════│═════════════│═════ (перегородки)      │
│       │             │             │                          │
│  ┌────▼─────────────▼─────────────▼────┐                    │
│  │   ОБЩАЯ КУХНЯ, ТУАЛЕТ, ИНТЕРНЕТ     │  ← Shared kernel   │
│  └─────────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
```

### Аналогия 2: Namespaces = Зеркальные комнаты

Представьте, что вы в комнате с зеркальными стенами. Вы видите только своё отражение, не знаете, что за стеной — другие люди в таких же комнатах.

**PID namespace:** Каждый контейнер думает, что его процесс — PID 1 (init). На хосте это PID 54321, но контейнер не знает.

**NET namespace:** Контейнер видит свой eth0 с IP 172.17.0.2. Не знает, что на хосте это один из сотен виртуальных интерфейсов.

**MNT namespace:** Контейнер видит / и думает, что это весь диск. На самом деле это overlay поверх image слоёв.

```
Реальность хоста:                    Что видит контейнер:
┌─────────────────────┐              ┌─────────────────────┐
│ PID 1 (systemd)     │              │ PID 1 (nginx)       │
│ PID 100 (sshd)      │              │ PID 2 (nginx worker)│
│ PID 54321 (nginx)   │  ─────►      │ PID 3 (nginx worker)│
│ PID 54322 (worker)  │   namespace  │                     │
│ PID 54323 (worker)  │              │ "Я один на сервере!"|
└─────────────────────┘              └─────────────────────┘
```

### Аналогия 3: Cgroups = Счётчики электричества в квартирах

В многоквартирном доме каждая квартира имеет свой счётчик электричества:
- Максимум 5 кВт на квартиру (нельзя включить 10 обогревателей)
- Если превысил — автомат выбивает (OOM killer)
- Можно отслеживать, кто сколько потребляет

```
┌─────────────────────────────────────────────────────────────┐
│                    Электрощит (Cgroup Controller)            │
│                                                              │
│  ┌─────────────────┐  ┌─────────────────┐  ┌───────────────┐│
│  │ Квартира 1      │  │ Квартира 2      │  │ Квартира 3    ││
│  │ [⚡ max: 5kW]   │  │ [⚡ max: 3kW]   │  │ [⚡ max: 10kW]││
│  │ [💾 max: 2GB]   │  │ [💾 max: 512MB] │  │ [💾 max: 8GB] ││
│  │ [📊 max: 50%CPU]│  │ [📊 max: 25%CPU]│  │ [📊 max: 100%]││
│  └─────────────────┘  └─────────────────┘  └───────────────┘│
│                                                              │
│  Превышение лимита → автомат выбивает (OOM Killer)          │
└─────────────────────────────────────────────────────────────┘
```

Cgroups предотвращают "шумного соседа": один контейнер не может забрать все ресурсы сервера.

### Аналогия 4: OverlayFS = Прозрачные плёнки

Помните, как в школе показывали слайды на проекторе? Каждая плёнка — отдельный слой, накладываешь друг на друга — получаешь полную картинку.

```
Как строится образ контейнера:

     Плёнка 4: ваше приложение
     ┌───────────────────────┐
     │    app.js, config     │  ← COPY . /app
     └───────────────────────┘
            ↓ накладываем
     ┌───────────────────────┐
     │     node_modules      │  ← RUN npm install
     └───────────────────────┘
            ↓ накладываем
     ┌───────────────────────┐
     │   Node.js runtime     │  ← FROM node:18
     └───────────────────────┘
            ↓ накладываем
     ┌───────────────────────┐
     │   Debian base image   │  ← debian:bullseye
     └───────────────────────┘
            ↓
     ========================
     MERGED: полная картина
     (контейнер видит это)

Все слои read-only. Изменения пишутся на верхний слой (Copy-on-Write).
100 контейнеров с node:18 = 1 копия node:18 на диске!
```

### Аналогия 5: Type 1 vs Type 2 гипервизор = Администратор здания

**Type 1 (Bare-metal) — Администратор живёт в здании:**
- Администратор — единственный житель первого этажа
- Прямой доступ ко всем инженерным системам
- Минимум посредников — максимальная эффективность

**Type 2 (Hosted) — Администратор снимает офис в соседнем здании:**
- Сначала звонишь в управляющую компанию (Host OS)
- Они передают администратору (Hypervisor)
- Он решает вопрос
- Дополнительный слой = дополнительная задержка

```
Type 1 (ESXi, KVM):             Type 2 (VirtualBox):
┌───────────────────┐           ┌───────────────────┐
│    Guest VMs      │           │    Guest VMs      │
├───────────────────┤           ├───────────────────┤
│   HYPERVISOR      │           │    Hypervisor     │
│ (прямо на железе) │           │  (как программа)  │
├───────────────────┤           ├───────────────────┤
│    Hardware       │           │     Host OS       │
└───────────────────┘           │ (Windows/macOS)   │
                                ├───────────────────┤
    1 слой посредник            │    Hardware       │
                                └───────────────────┘
                                    2 слоя посредников
```

---

## Часть 2: Почему это сложно

### Ошибка 1: "Контейнеры = легковесные VM"

**СИМПТОМ:** "Docker — это просто маленькая виртуальная машина, да?"

**Почему это неправильно:**
- VM виртуализирует hardware — у каждой VM своё ядро ОС
- Контейнер виртуализирует OS-level — все контейнеры разделяют ядро хоста
- VM изолирует на уровне hypervisor, контейнер — на уровне процесса

```
VM:                                 Контейнер:
┌─────────┐ ┌─────────┐            ┌─────────┐ ┌─────────┐
│  App A  │ │  App B  │            │  App A  │ │  App B  │
├─────────┤ ├─────────┤            ├─────────┤ ├─────────┤
│Kernel A │ │Kernel B │            │  Libs   │ │  Libs   │
└────┬────┘ └────┬────┘            └────┬────┘ └────┬────┘
     │           │                      │           │
┌────▼───────────▼────┐            ┌────▼───────────▼────┐
│     Hypervisor      │            │   Shared Kernel     │
└─────────────────────┘            └─────────────────────┘
  Отдельные ядра!                    Общее ядро!
```

**РЕШЕНИЕ:** Думайте о контейнере как о "процессе с изоляцией", а не как о "VM без kernel". Контейнер — это namespaces + cgroups + overlayfs, не более.

### Ошибка 2: "Контейнер так же безопасен как VM"

**СИМПТОМ:** Запуск недоверенного кода в контейнере на production сервере.

**Почему это опасно:**
- Контейнер и хост разделяют ядро
- Уязвимость в ядре = container escape (побег из контейнера)
- Примеры: Dirty COW (CVE-2016-5195), runc escape (CVE-2019-5736)

```
Изоляция:

Процессы    Контейнеры    VMs           Physical
    │            │          │               │
    └────────────┴──────────┴───────────────┘
    ──────────────────────────────────────────►
    Слабая                              Сильная

    Контейнер ≈ "процесс в песочнице", не "отдельная машина"
```

**РЕШЕНИЕ:**
- Для multi-tenant (разные клиенты) — используйте VM
- Для изоляции своих приложений — контейнеры достаточны
- Рассмотрите Kata Containers / Firecracker для компромисса

### Ошибка 3: Запуск всего под root внутри контейнера

**СИМПТОМ:** `USER root` в Dockerfile или отсутствие директивы USER.

**Почему это опасно:**
- root в контейнере = root на хосте (если нет user namespaces)
- Exploited процесс получает root-доступ к shared kernel
- Можно примонтировать хостовую файловую систему

```dockerfile
# ПЛОХО:
FROM node:18
COPY . /app
CMD ["node", "app.js"]  # Запускается как root!

# ХОРОШО:
FROM node:18
RUN useradd -m appuser
WORKDIR /app
COPY --chown=appuser:appuser . .
USER appuser  # Явно указываем непривилегированного пользователя
CMD ["node", "app.js"]
```

**РЕШЕНИЕ:** Всегда используйте `USER` в Dockerfile. Запускайте как непривилегированный пользователь.

### Ошибка 4: Хранение данных внутри контейнера

**СИМПТОМ:** "Как бэкапить контейнер?" / "Данные пропали после перезапуска!"

**Почему это неправильно:**
- Контейнер ephemeral (временный) — удалил и создал заново
- upperdir (read-write слой) удаляется вместе с контейнером
- Нельзя "бэкапить контейнер" — бэкапят volumes

```
НЕПРАВИЛЬНО:                        ПРАВИЛЬНО:
┌─────────────────┐                 ┌─────────────────┐
│   Container     │                 │   Container     │
│ ┌─────────────┐ │                 │                 │
│ │   DB Data   │ │  ← Потеряется   │      App        │
│ └─────────────┘ │    при удалении │        │        │
└─────────────────┘                 └────────│────────┘
                                             │
                                    ┌────────▼────────┐
                                    │  Named Volume   │
                                    │ /var/lib/postgres│
                                    │ (persistent!)   │
                                    └─────────────────┘
```

**РЕШЕНИЕ:** `docker run -v postgres_data:/var/lib/postgresql/data postgres`

### Ошибка 5: "Docker на Mac/Windows работает так же как на Linux"

**СИМПТОМ:** "На моём Mac работает, на сервере — нет" или медленный I/O.

**Почему это проблема:**
- Docker использует Linux kernel features (namespaces, cgroups)
- На Mac/Windows нет Linux kernel
- Docker Desktop запускает скрытую Linux VM (HyperKit/WSL2)
- Файловый I/O через VM медленнее (особенно на Mac)

```
Linux:                    Mac/Windows:
┌─────────────────┐       ┌─────────────────┐
│   Container     │       │   Container     │
├─────────────────┤       ├─────────────────┤
│  Linux Kernel   │       │  Linux Kernel   │  ← в VM!
└─────────────────┘       ├─────────────────┤
        ↑                 │  Linux VM       │
   Напрямую!              │  (скрытая)      │
                          ├─────────────────┤
                          │  macOS/Windows  │
                          └─────────────────┘
                                  ↑
                          Дополнительный слой
                          + медленный file sync
```

**РЕШЕНИЕ:**
- Тестируйте на Linux (CI/CD) перед production
- На Mac: используйте mutagen или другие инструменты синхронизации
- Не удивляйтесь разнице в производительности

### Ошибка 6: Kubernetes для трёх сервисов

**СИМПТОМ:** Месяц настройки K8s для простого приложения из 3 микросервисов.

**Когда это overkill:**
- < 10 сервисов → Docker Compose достаточно
- Один сервер → Docker Compose
- Нет команды для поддержки K8s → не используйте K8s

```
Сложность:

             ┌─────────────────────────────────────────┐
             │ Kubernetes (50+ компонентов)            │
        100% ┼───────────────────────────────────X─────┤
             │                                   │     │
             │                         X─────────┘     │
             │                    K8s overhead         │
             │               X────────────────────────┤
             │          Docker Swarm                   │
        50%  ┼─────X───────────────────────────────────┤
             │ Docker Compose                          │
             │                                         │
          0% ┼─────────────────────────────────────────┤
             └─────┴─────┴─────┴─────┴─────┴─────┴─────┘
               1     5    10    50   100  500  1000
                        Количество сервисов
```

**РЕШЕНИЕ:**
- 1-10 сервисов, 1 сервер → Docker Compose
- 10-50 сервисов, несколько серверов → Docker Swarm или Nomad
- 50+ сервисов, требуется autoscaling → Kubernetes

---

## Часть 3: Ментальные модели

### Модель 1: Спектр изоляции

```
Изоляция:     Слабая ◄────────────────────────────► Сильная
Overhead:     Низкий ◄────────────────────────────► Высокий
Startup:      Быстрый◄────────────────────────────► Медленный

┌─────────────┬───────────────┬─────────────────┬──────────────┐
│  Процессы   │  Контейнеры   │       VMs       │  Physical    │
├─────────────┼───────────────┼─────────────────┼──────────────┤
│ Общее всё   │ Общее ядро    │ Раздельные ядра │ Раздельное   │
│             │ + namespaces  │ + hypervisor    │ железо       │
│             │ + cgroups     │                 │              │
├─────────────┼───────────────┼─────────────────┼──────────────┤
│ 0 overhead  │ <1% overhead  │ 5-15% overhead  │ 0 overhead   │
├─────────────┼───────────────┼─────────────────┼──────────────┤
│ Мгновенно   │ ~100ms        │ 10-60 sec       │ Минуты-часы  │
└─────────────┴───────────────┴─────────────────┴──────────────┘

Правило выбора:
- Свои приложения → Контейнеры
- Чужие/недоверенные → VM
- Максимальная безопасность → Physical
```

### Модель 2: Контейнер = Процесс + Изоляция

```
Контейнер — это НЕ VM. Контейнер — это:

    Обычный процесс Linux
           +
    ┌──────────────────────────────────────┐
    │ PID Namespace (своя нумерация PID)   │
    │ NET Namespace (своя сеть)            │
    │ MNT Namespace (своя файловая система)│
    │ USER Namespace (свои пользователи)   │
    │ UTS Namespace (свой hostname)        │
    │ IPC Namespace (своя shared memory)   │
    └──────────────────────────────────────┘
           +
    ┌──────────────────────────────────────┐
    │ Cgroups (лимиты CPU, RAM, I/O)       │
    └──────────────────────────────────────┘
           +
    ┌──────────────────────────────────────┐
    │ OverlayFS (layered filesystem)       │
    └──────────────────────────────────────┘
           =
    КОНТЕЙНЕР (просто хорошо изолированный процесс)
```

### Модель 3: Дерево решений VM vs Контейнеры

```
                    ┌───────────────────────────────┐
                    │ Нужна ли другая ОС (Windows)? │
                    └───────────────┬───────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                   Да                              Нет
                    │                               │
                    ▼                               ▼
                ┌───────┐              ┌───────────────────────┐
                │  VM   │              │Недоверенный код/tenant?│
                └───────┘              └───────────┬───────────┘
                                                   │
                                   ┌───────────────┴───────────────┐
                                  Да                              Нет
                                   │                               │
                                   ▼                               ▼
                               ┌───────┐              ┌───────────────────────┐
                               │  VM   │              │Compliance требует VM? │
                               │(или   │              └───────────┬───────────┘
                               │Kata)  │                          │
                               └───────┘              ┌───────────┴───────────┐
                                                     Да                      Нет
                                                      │                       │
                                                      ▼                       ▼
                                                  ┌───────┐           ┌────────────┐
                                                  │  VM   │           │ Контейнеры │
                                                  └───────┘           └────────────┘
```

### Модель 4: Эволюция изоляции (исторический контекст)

```
1979        1999         2006         2013         2014        2019
chroot  →   FreeBSD  →   cgroups  →   Docker   →   K8s     →   Firecracker
  │         Jails          │           │           │            │
  ▼           ▼            ▼           ▼           ▼            ▼
Изоляция  Изоляция     Лимиты     Простой UX   Оркестра-   VM speed +
FS        FS+NET+PID   ресурсов   для          ция         container
                       (Google)   контейнеров  контейне-   isolation
                                               ров

Каждый шаг добавлял либо изоляцию, либо удобство, либо эффективность.
```

### Модель 5: Overhead пирамида

```
                        Overhead и ресурсы

                    ┌─────────────────────┐
                    │ VM с эмуляцией      │ 30-50%
                    │ (без VT-x)          │
                ┌───┴─────────────────────┴───┐
                │ VM с аппаратной             │ 5-15%
                │ виртуализацией (KVM+virtio) │
            ┌───┴─────────────────────────────┴───┐
            │ Kata Containers / Firecracker       │ 2-5%
            │ (micro-VM)                          │
        ┌───┴─────────────────────────────────────┴───┐
        │ Контейнеры                                   │ <1%
        │ (namespaces + cgroups)                      │
    ┌───┴─────────────────────────────────────────────┴───┐
    │ Bare metal                                          │ 0%
    └─────────────────────────────────────────────────────┘

Чем выше — тем больше изоляции, но и больше overhead.
Выбирайте минимально необходимый уровень.
```

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| [[os-overview]] | Базовые концепции ОС, kernel/user mode | Предыдущий материал раздела |
| [[os-processes-threads]] | Процессы и изоляция — контейнеры это "процессы с ограничениями" | Предыдущий материал раздела |
| [[os-memory-management]] | Виртуальная память, page tables — VM используют nested paging | Предыдущий материал раздела |
| Docker базовые команды | Практический опыт с контейнерами | [Docker Getting Started](https://docs.docker.com/get-started/) |

**Время на подготовку:** ~1 неделя, включая практику с Docker

---

## Терминология для новичков

| Термин | Что это | Аналогия из жизни |
|--------|---------|-------------------|
| **Hypervisor** | Программа, управляющая виртуальными машинами | Администратор отеля: распределяет номера, следит за гостями |
| **Type 1 (Bare-metal)** | Гипервизор прямо на железе (ESXi, KVM) | Отель с одним администратором, без посредников |
| **Type 2 (Hosted)** | Гипервизор внутри ОС (VirtualBox) | Отель внутри торгового центра — дополнительный слой |
| **KVM** | Модуль ядра Linux, превращающий его в гипервизор | Linux "надевает шляпу" администратора отеля |
| **VT-x / AMD-V** | Аппаратная поддержка виртуализации в CPU | Специальная комната в отеле для VIP (гостей-ОС) |
| **VM exit** | Гость передаёт управление гипервизору | Гость звонит на ресепшн: "нужна помощь" |
| **virtio** | Драйверы, знающие что они в VM (быстрее) | Гость знает, что в отеле — просит напрямую, без притворства |
| **Namespace** | Изоляция видимости (PID, NET, MNT) | Каждый арендатор видит только свой офис в БЦ |
| **Cgroup** | Лимиты ресурсов (CPU, RAM, I/O) | Лимит на электричество для каждого офиса |
| **OverlayFS** | Слоистая ФС для контейнеров | Прозрачные плёнки с рисунками: накладываешь — получаешь картину |
| **Container** | Процесс с namespaces + cgroups | Офис в БЦ: изолирован, но общие стены и коммуникации |
| **Firecracker** | Микро-VM с overhead как у контейнера | Капсульный отель: изоляция номера, но минимум места |

---

## Зачем нужна виртуализация

### Проблема без виртуализации

Представьте, что у вас один физический сервер и три приложения: web-сервер на Java, база данных PostgreSQL, и legacy-система на Python 2.7. Проблемы:

1. **Конфликты зависимостей.** Java-приложению нужна Java 17, legacy-системе — Python 2.7, который конфликтует с системным Python 3.

2. **Изоляция сбоев.** Если web-сервер съест всю память или упадёт с segfault — он может повлиять на базу данных.

3. **Безопасность.** Уязвимость в одном приложении может дать злоумышленнику доступ ко всем остальным.

4. **Масштабирование.** Нужно больше web-серверов? Придётся покупать новые физические машины.

### Решение — виртуализация

Виртуализация создаёт иллюзию, что каждое приложение работает на собственном компьютере. Эти "виртуальные компьютеры" изолированы друг от друга, но физически работают на одном железе. Можно выделить каждому столько ресурсов, сколько нужно, и масштабировать без покупки нового hardware.

---

## Типы виртуализации: гипервизоры

Гипервизор (hypervisor) — программа, управляющая виртуальными машинами. Она создаёт и запускает VM, распределяет между ними физические ресурсы (CPU, память, диск).

### Type 1: Bare-metal гипервизоры

Type 1 гипервизор работает напрямую на железе, без промежуточной операционной системы. Он сам является минимальной ОС, единственная задача которой — управление виртуальными машинами.

```
┌─────────────────────────────────────────────────────────────┐
│                      Type 1 (Bare-metal)                     │
│                                                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │  Guest   │  │  Guest   │  │  Guest   │  ← Гостевые ОС    │
│  │   OS 1   │  │   OS 2   │  │   OS 3   │    (Windows,     │
│  │ (Linux)  │  │(Windows) │  │ (Linux)  │    Linux)        │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘                   │
│       │             │             │                          │
│  ┌────▼─────────────▼─────────────▼────┐                    │
│  │           Hypervisor                 │  ← Работает       │
│  │     (VMware ESXi, Xen, Hyper-V)      │    на bare metal  │
│  └─────────────────┬───────────────────┘                    │
│                    │                                         │
│  ┌─────────────────▼───────────────────┐                    │
│  │            Hardware                  │                    │
│  └─────────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
```

**Примеры:** VMware ESXi, Microsoft Hyper-V (в роли гипервизора), Citrix XenServer.

**Преимущества:** Минимальный overhead — между VM и железом нет промежуточного слоя ОС. Лучшая производительность и безопасность.

**Недостатки:** Сложнее в настройке, требует выделенного сервера. Нельзя использовать машину для других задач.

**Где используется:** Дата-центры, облачные провайдеры, enterprise-серверы.

### Type 2: Hosted гипервизоры

Type 2 гипервизор работает как обычное приложение внутри операционной системы. Host OS управляет железом, а гипервизор создаёт VM как процессы внутри этой ОС.

```
┌─────────────────────────────────────────────────────────────┐
│                      Type 2 (Hosted)                         │
│                                                              │
│  ┌──────────┐  ┌──────────┐                                 │
│  │  Guest   │  │  Guest   │  ← Гостевые ОС                  │
│  │   OS 1   │  │   OS 2   │                                 │
│  └────┬─────┘  └────┬─────┘                                 │
│       │             │                                        │
│  ┌────▼─────────────▼────┐    ┌──────────┐                  │
│  │      Hypervisor       │    │   Apps   │  ← Другие        │
│  │ (VirtualBox, VMware   │    │ (браузер,│    приложения    │
│  │  Workstation, QEMU)   │    │  IDE)    │                  │
│  └───────────┬───────────┘    └────┬─────┘                  │
│              │                     │                         │
│  ┌───────────▼─────────────────────▼─────┐                  │
│  │              Host OS                   │  ← Хост ОС      │
│  │         (Windows, Linux, macOS)        │    (ваша        │
│  └─────────────────┬─────────────────────┘    рабочая ОС)   │
│                    │                                         │
│  ┌─────────────────▼───────────────────┐                    │
│  │            Hardware                  │                    │
│  └─────────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
```

**Примеры:** VirtualBox, VMware Workstation/Fusion, Parallels Desktop.

**Преимущества:** Легко установить и использовать. Можно работать на той же машине, где запущены VM.

**Недостатки:** Дополнительный слой (host OS) добавляет overhead. Меньшая производительность.

**Где используется:** Разработка, тестирование, запуск приложений для другой ОС на десктопе.

### KVM: гибридный подход

KVM (Kernel-based Virtual Machine) — модуль ядра Linux, превращающий Linux в Type 1 гипервизор. Технически это Type 2 (работает внутри Linux), но благодаря глубокой интеграции с ядром и аппаратной виртуализации достигает производительности Type 1.

```
┌─────────────────────────────────────────────────────────────┐
│                        Guest VM                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │                    Guest OS                             │ │
│  │    Думает, что работает на реальном железе             │ │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐                │ │
│  │  │ Process │  │ Process │  │ Process │                │ │
│  │  └─────────┘  └─────────┘  └─────────┘                │ │
│  │                    │                                   │ │
│  │  ┌─────────────────▼───────────────────┐              │ │
│  │  │         Guest Kernel                │              │ │
│  │  │    (обычное Linux/Windows ядро)     │              │ │
│  │  └─────────────────────────────────────┘              │ │
│  └────────────────────────────────────────────────────────┘ │
│                         │                                    │
│                         │ VM exit (при привилегированных    │
│                         │ операциях управление передаётся   │
│                         │ гипервизору)                       │
└─────────────────────────│───────────────────────────────────┘
                          │
┌─────────────────────────▼───────────────────────────────────┐
│                      Host Linux                              │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                    QEMU Process                       │   │
│  │                                                       │   │
│  │   QEMU — программа в userspace, которая:             │   │
│  │   • Создаёт виртуальные устройства (диск, сеть)      │   │
│  │   • Эмулирует BIOS/UEFI                              │   │
│  │   • Обрабатывает I/O от гостевой ОС                  │   │
│  │                                                       │   │
│  └───────────────────────┬──────────────────────────────┘   │
│                          │ ioctl(/dev/kvm)                   │
│                          │ (интерфейс к KVM модулю)         │
│  ┌───────────────────────▼──────────────────────────────┐   │
│  │                    KVM Module                         │   │
│  │                                                       │   │
│  │   KVM — модуль ядра Linux, который:                  │   │
│  │   • Использует Intel VT-x / AMD-V для запуска VM     │   │
│  │   • Управляет памятью VM (EPT/NPT)                   │   │
│  │   • Обрабатывает VM exits                            │   │
│  │                                                       │   │
│  └───────────────────────┬──────────────────────────────┘   │
│                          │                                   │
│  ┌───────────────────────▼──────────────────────────────┐   │
│  │                 Linux Kernel                          │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

**Почему KVM так эффективен:**

1. KVM использует аппаратную виртуализацию (VT-x/AMD-V), поэтому гостевой код выполняется напрямую на CPU.
2. Ядро Linux выступает планировщиком — оно распределяет CPU между VM как между обычными процессами.
3. Для I/O используется паравиртуализация (virtio) — минимальный overhead.

---

## Аппаратная поддержка виртуализации

### Проблема программной виртуализации

До появления аппаратной поддержки виртуализация была сложной и медленной. Проблема в том, что гостевая ОС хочет работать в Ring 0 (привилегированный режим процессора), но если она действительно будет в Ring 0 — она получит контроль над всем железом, и изоляции не будет.

**Решение 1 — Binary Translation (VMware):** Гипервизор на лету анализирует код гостевой ОС и заменяет привилегированные инструкции на безопасные эквиваленты. Работает, но overhead значительный.

**Решение 2 — Паравиртуализация (Xen):** Модифицировать гостевую ОС, чтобы она не использовала привилегированные инструкции, а вызывала гипервизор через специальный API. Эффективно, но требует изменения гостевой ОС.

### Intel VT-x и AMD-V: аппаратное решение

В 2005-2006 годах Intel и AMD добавили в процессоры аппаратную поддержку виртуализации. Появился новый режим работы процессора — VMX root mode (неофициально называемый Ring -1):

```
┌─────────────────────────────────────────────┐
│              Без VT-x/AMD-V                  │
│                                              │
│  Ring 3: Приложения                          │
│  Ring 0: Гостевая ОС (ПОНИЖЕНА в Ring 1!)   │
│          Гипервизор ПЕРЕХВАТЫВАЕТ каждую    │
│          привилегированную инструкцию       │
│                                              │
│  Проблема: overhead на КАЖДУЮ               │
│  привилегированную инструкцию               │
│  (обработка page fault, context switch,     │
│   доступ к устройствам — всё медленно)      │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│               С VT-x/AMD-V                   │
│                                              │
│  Ring 3: Приложения                          │
│  Ring 0: Гостевая ОС (работает НАТИВНО!)    │
│  ─────────────────────────────────          │
│  VMX Root mode: Гипервизор                   │
│                                              │
│  Гостевая ОС работает в "виртуальном"       │
│  Ring 0. CPU сам отслеживает критические    │
│  операции и передаёт управление             │
│  гипервизору только когда нужно (VM exit)   │
└─────────────────────────────────────────────┘
```

**Как это работает:**

1. Гипервизор настраивает VMCS (Virtual Machine Control Structure) — структуру, описывающую, при каких условиях нужно передать управление гипервизору.

2. Гипервизор выполняет инструкцию VMLAUNCH/VMRESUME — CPU входит в режим гостя.

3. Гостевая ОС выполняется напрямую на CPU. Большинство инструкций — без overhead.

4. При критических операциях (доступ к hardware, обновление page table, выполнение определённых инструкций) происходит VM exit — CPU автоматически передаёт управление гипервизору.

5. Гипервизор обрабатывает событие и возвращает управление гостю через VMRESUME.

**Результат:** Гостевой код выполняется с минимальным overhead. VM exit происходит только для операций, которые действительно требуют вмешательства гипервизора.

### Паравиртуализация (virtio): оптимизация I/O

Даже с аппаратной виртуализацией остаётся проблема I/O. Если гостевая ОС думает, что работает с реальной сетевой картой Intel E1000, гипервизор должен эмулировать все регистры этой карты — каждое обращение к регистру вызывает VM exit.

**Virtio** — стандарт паравиртуализированных устройств. Гостевая ОС "знает", что работает в VM, и использует специальный драйвер virtio. Вместо эмуляции регистров используются ring buffers в shared memory:

```
┌──────────────────────────────────────────────────────────────┐
│                  Полная эмуляция (E1000)                     │
│                                                               │
│  Гостевая ОС                                                 │
│      │                                                        │
│      ▼                                                        │
│  E1000 драйвер ──▶ Записать в регистр TX                     │
│                         │                                     │
│                         ▼ VM EXIT!                            │
│                    QEMU эмулирует E1000                       │
│                         │                                     │
│                         ▼                                     │
│                    Реальная сеть                              │
│                                                               │
│  КАЖДЫЙ пакет = несколько VM exits (очень медленно)          │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│                  Паравиртуализация (virtio)                   │
│                                                               │
│  Гостевая ОС                                                 │
│      │                                                        │
│      ▼                                                        │
│  virtio-net драйвер ──▶ Положить пакет в shared ring buffer  │
│                              │                                │
│                              │ (no VM exit для данных!)       │
│                              ▼                                │
│                    QEMU/хост читает из ring buffer           │
│                              │                                │
│                              ▼                                │
│                         Реальная сеть                         │
│                                                               │
│  Батч пакетов = один сигнал, минимум VM exits                │
└──────────────────────────────────────────────────────────────┘
```

Virtio обеспечивает почти native производительность для disk и network I/O. Все современные облака и виртуализационные платформы используют virtio.

---

## Контейнеры: лёгкая виртуализация

### Идея контейнеров

Виртуальные машины изолируют на уровне hardware — каждая VM имеет собственное ядро ОС. Но часто это избыточно. Если все приложения работают на Linux, зачем каждому отдельное ядро?

Контейнеры изолируют на уровне процессов. Контейнер — это обычный процесс Linux с дополнительными ограничениями: он не видит другие процессы, имеет собственную файловую систему, сеть, и ограничен в потреблении ресурсов.

```
┌─────────────────────────────────────────────────────────────┐
│                         Host Linux                           │
│                                                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │
│  │ Container 1  │  │ Container 2  │  │ Container 3  │       │
│  │              │  │              │  │              │       │
│  │ ┌──────────┐ │  │ ┌──────────┐ │  │ ┌──────────┐ │       │
│  │ │   App    │ │  │ │   App    │ │  │ │   App    │ │       │
│  │ │  (nginx) │ │  │ │(postgres)│ │  │ │ (redis)  │ │       │
│  │ └──────────┘ │  │ └──────────┘ │  │ └──────────┘ │       │
│  │ ┌──────────┐ │  │ ┌──────────┐ │  │ ┌──────────┐ │       │
│  │ │   Libs   │ │  │ │   Libs   │ │  │ │   Libs   │ │       │
│  │ │ (Ubuntu) │ │  │ │ (Alpine) │ │  │ │ (Debian) │ │       │
│  │ └──────────┘ │  │ └──────────┘ │  │ └──────────┘ │       │
│  │              │  │              │  │              │       │
│  │  Namespaces  │  │  Namespaces  │  │  Namespaces  │       │
│  │   + cgroups  │  │   + cgroups  │  │   + cgroups  │       │
│  └──────────────┘  └──────────────┘  └──────────────┘       │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                 Shared Linux Kernel                   │   │
│  │         (одно ядро для всех контейнеров)             │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

**Ключевое отличие от VM:** Контейнеры разделяют ядро хоста. Нет отдельной ОС для каждого контейнера, нет hypervisor layer. Процесс в контейнере — это обычный процесс Linux, просто с ограничениями.

### Namespaces: изоляция видимости

Linux namespaces — механизм ядра, который создаёт иллюзию, что процесс имеет собственный экземпляр определённого ресурса. Процесс внутри namespace не видит ресурсы других namespaces.

Существует 8 типов namespaces (с kernel 5.6):

**PID Namespace — изоляция процессов**

Процессы внутри PID namespace имеют собственную нумерацию, начиная с PID 1. Процесс с PID 1 внутри контейнера на самом деле имеет другой PID с точки зрения хоста (например, PID 12345).

```bash
# На хосте
ps aux | grep nginx
# 12345 nginx: master process

# Внутри контейнера
ps aux
# PID 1 nginx: master process

# Контейнер не видит процессы хоста, хост видит все процессы
```

Процессы внутри контейнера не могут увидеть или отправить сигнал процессам снаружи. Если PID 1 в контейнере умирает — контейнер останавливается (как если бы умер init).

**NET Namespace — изоляция сети**

Каждый NET namespace имеет собственные сетевые интерфейсы, IP-адреса, routing tables, iptables rules, сокеты. При создании NET namespace в нём есть только loopback интерфейс.

```bash
# Контейнер имеет собственный eth0 и IP
# Внутри контейнера
ip addr
# eth0: 172.17.0.2/16

# На хосте это veth pair, соединённый с bridge docker0
```

Docker создаёт виртуальные пары интерфейсов (veth pair): один конец в namespace контейнера (eth0), другой на хосте (vethXXX), подключённый к bridge. Это позволяет контейнерам общаться между собой и с внешним миром.

**MNT Namespace — изоляция файловой системы**

Каждый MNT namespace имеет собственную таблицу монтирования. Контейнер видит только свою файловую систему, не видит диски и разделы хоста.

```bash
# Внутри контейнера
ls /
# bin  etc  home  lib  usr  var  ...
# (это образ контейнера, не файловая система хоста)

# Можно смонтировать директорию хоста внутрь контейнера (bind mount)
docker run -v /host/path:/container/path ...
```

**USER Namespace — изоляция пользователей**

USER namespace позволяет маппить UID/GID. Процесс может быть root внутри контейнера (UID 0), но непривилегированным пользователем снаружи.

```bash
# Внутри контейнера
id
# uid=0(root) gid=0(root)

# На хосте тот же процесс имеет другой UID
# uid=100000(container_user)
```

Это важно для безопасности: даже если злоумышленник получит root внутри контейнера, он не будет root на хосте.

**Другие namespaces:**
- **UTS** — hostname и domain name
- **IPC** — shared memory, semaphores, message queues
- **CGROUP** — видимость cgroup hierarchy
- **TIME** (с kernel 5.6) — изоляция системных часов

### Cgroups: ограничение ресурсов

Namespaces изолируют видимость, но не ограничивают потребление ресурсов. Без дополнительных ограничений один контейнер может забрать всю память или CPU, вызвав "noisy neighbor" проблему.

**Cgroups (control groups)** — механизм ядра для ограничения и учёта ресурсов:

```bash
# Создание cgroup (cgroups v2)
mkdir /sys/fs/cgroup/my_container

# Ограничение памяти: максимум 512 МБ
echo 536870912 > /sys/fs/cgroup/my_container/memory.max

# Ограничение CPU: 50% одного ядра
# Формат: quota period (в микросекундах)
# 50000/100000 = 50% CPU
echo "50000 100000" > /sys/fs/cgroup/my_container/cpu.max

# Ограничение количества процессов (защита от fork bomb)
echo 100 > /sys/fs/cgroup/my_container/pids.max

# Добавление процесса в cgroup
echo $PID > /sys/fs/cgroup/my_container/cgroup.procs
```

**Контролируемые ресурсы:**

- **memory** — лимит RAM, swap. При превышении — OOM killer убивает процессы в cgroup.
- **cpu** — доля CPU time. Можно также привязать к конкретным ядрам.
- **io** — bandwidth и IOPS для block devices.
- **pids** — максимальное количество процессов.

Docker, Kubernetes и другие container runtimes автоматически создают cgroups для каждого контейнера и настраивают лимиты согласно конфигурации (docker run --memory=512m --cpus=0.5).

### Union File Systems: слоистые образы

Контейнеры используют layered файловую систему для эффективного хранения и распространения образов.

```
┌─────────────────────────────────────────────┐
│       Container Layer (read-write)           │  ← Изменения во время
│       (изменённые файлы контейнера)         │    работы контейнера
├─────────────────────────────────────────────┤
│              App Layer                       │  ← RUN npm install
├─────────────────────────────────────────────┤
│             Node.js Layer                    │  ← FROM node:18
├─────────────────────────────────────────────┤
│              Base Image                      │  ← debian:bullseye
└─────────────────────────────────────────────┘
      (все слои read-only, кроме верхнего)
```

**Как работает OverlayFS:**

- **lowerdir** — read-only слои образа. Могут быть shared между контейнерами. Если 100 контейнеров используют образ node:18 — на диске один экземпляр.

- **upperdir** — read-write слой для изменений конкретного контейнера.

- **merged** — объединённый view, который видит контейнер.

**Copy-on-Write:** Когда контейнер изменяет файл из нижнего слоя:
1. Файл копируется в upperdir
2. Изменения применяются к копии
3. Оригинал в lowerdir не меняется

Это позволяет создавать контейнеры мгновенно — не нужно копировать весь образ, только создать пустой upperdir.

---

## VM vs Контейнеры: сравнение

### Производительность

| Метрика | Контейнеры | VM | Причина |
|---------|------------|-----|---------|
| **Startup time** | 100-500 мс | 10-60 сек | Нет загрузки ядра ОС |
| **Memory overhead** | ~5-10 МБ | ~200-500+ МБ | Нет отдельного ядра |
| **CPU overhead** | <1% | 2-5% | Нет hypervisor layer |
| **Disk I/O** | ~98% native | ~78-90% native | Нет storage virtualization |
| **Network latency** | +5-10% | +20-50% | Нет виртуализации сети |

Контейнеры достигают 95-99% производительности bare metal. VM с virtio — около 85-95%.

**Почему контейнеры быстрее стартуют:** При старте контейнера нужно только создать namespaces и cgroups — это операции ядра, занимающие микросекунды. При старте VM нужно: инициализировать виртуальное железо, загрузить BIOS/UEFI, загрузить bootloader, загрузить ядро, выполнить init — это секунды.

### Изоляция и безопасность

```
              Уровень изоляции
    ◄─────────────────────────────────────────►
    Слабее                                Сильнее

    Processes   Containers   VMs    Physical
        │           │          │         │
        │           │          │         │
    Общее ядро  Общее ядро  Раздельные  Физическое
    Общие       Namespaces  ядра        разделение
    namespaces  + cgroups   Hypervisor  серверов
```

**Контейнеры:**
- Изоляция через namespaces — абстракция ядра
- Уязвимость в ядре = потенциальный container escape
- Примеры реальных уязвимостей: Dirty COW (CVE-2016-5195), runc escape (CVE-2019-5736)

**VM:**
- Изоляция на уровне hypervisor
- Компрометация гостевой ОС не даёт доступ к хосту (обычно)
- Но hypervisor тоже может иметь уязвимости: VENOM, Spectre/Meltdown

**Правило:** Контейнеры достаточно безопасны для изоляции приложений одного владельца. Для multi-tenant (разные клиенты на одном сервере) облачные провайдеры используют VM.

## Когда VM, когда контейнеры, когда ни то ни другое

### Выбор по критериям

| Критерий | VM | Контейнер | Bare metal |
|----------|-----|-----------|------------|
| Изоляция security | Сильная (разные kernels) | Слабая (shared kernel) | N/A |
| Изоляция ресурсов | Полная | Через cgroups | N/A |
| Overhead | 5-15% CPU, GB RAM | <1% CPU, MB RAM | 0% |
| Startup | Минуты | Секунды | Минуты-часы |
| Разные ОС | Да (Windows на Linux host) | Нет (только Linux на Linux) | N/A |

### Когда VM — правильный выбор

✅ **Multi-tenant с недоверенными арендаторами** — облачные провайдеры (AWS EC2)
✅ **Запуск другой ОС** — Windows приложение на Linux сервере
✅ **Compliance требует полной изоляции** — PCI-DSS, финансы
✅ **Legacy приложения** требующие специфическую ОС

### Когда контейнеры — правильный выбор

✅ **Микросервисы** — много маленьких сервисов, быстрый деплой
✅ **CI/CD** — ephemeral окружения для тестов
✅ **Density** — максимум приложений на сервер
✅ **Разработка** — "works on my machine" → контейнер

### Когда НЕ использовать виртуализацию

❌ **High-frequency trading** — каждая микросекунда важна, overhead недопустим
❌ **GPU workloads** — passthrough усложняет, bare metal проще
❌ **Простые stateful приложения** — один PostgreSQL на bare metal проще
❌ **Когда нет экспертизы** — контейнеры/K8s требуют знаний, монолит на VM проще поддерживать

### Распространённые ошибки

| Ошибка | Почему это проблема |
|--------|---------------------|
| "Контейнеры = безопасность VM" | Нет! Shared kernel = escape возможен |
| "VM для каждого микросервиса" | Overhead, медленный startup |
| "Контейнеры для stateful" | Сложно (volumes, persistence) |
| "K8s для 3 сервисов" | Overkill, Docker Compose достаточно |

### Когда что использовать

**Контейнеры идеальны для:**

- **Микросервисы** — быстрый startup критичен для autoscaling. Kubernetes может масштабировать контейнеры за секунды.

- **CI/CD** — сотни билдов в день, каждый в изолированном окружении. Контейнер создаётся, выполняет билд, удаляется.

- **Однородные workloads** — все приложения на Linux, различия только в зависимостях.

- **Высокая плотность** — можно запустить сотни контейнеров на одном сервере.

**VM идеальны для:**

- **Multi-tenant isolation** — AWS, GCP, Azure используют VM для разделения клиентов. Каждый клиент в своей VM.

- **Разные ОС** — нужен Windows и Linux на одном сервере? Только VM.

- **Legacy приложения** — требуют специфического ядра или драйверов.

- **Compliance требования** — некоторые регуляции требуют VM-level isolation.

### Гибридный подход: лучшее из двух миров

Современные решения комбинируют преимущества обоих подходов:

**Kata Containers / Firecracker:**

```
┌─────────────────────────────────────────────────────────────┐
│                    Kata Containers                           │
│                                                              │
│  Container Runtime Interface (CRI)                           │
│           │                                                  │
│           ▼                                                  │
│  ┌─────────────────────────────────────────────────────┐    │
│  │              Lightweight VM                          │    │
│  │  ┌─────────────────────────────────────────────┐   │    │
│  │  │  Container (выглядит как обычный контейнер)  │   │    │
│  │  └─────────────────────────────────────────────┘   │    │
│  │  ┌─────────────────────────────────────────────┐   │    │
│  │  │  Minimal kernel (~5MB, boots in <100ms)     │   │    │
│  │  └─────────────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────┘    │
│                          │                                   │
│                    Hypervisor (QEMU/Firecracker)            │
└─────────────────────────────────────────────────────────────┘
```

**Firecracker** (разработан AWS для Lambda и Fargate):
- Boot time: ~125 мс (vs 10+ секунд для обычной VM)
- Memory overhead: ~5 МБ на VM (vs 200+ МБ)
- Эмулирует только необходимое: virtio-net, virtio-block, serial console

Получаем изоляцию VM с overhead, близким к контейнерам. AWS Lambda использует Firecracker для изоляции функций разных пользователей.

---

## Практические команды Linux

```bash
# Namespaces
lsns                              # Список всех namespaces
lsns -t net                       # Только network namespaces
nsenter -t $PID -n ip addr        # Войти в network namespace процесса
unshare -n bash                   # Создать процесс в новом net namespace

# Cgroups v2
cat /proc/cgroups                 # Поддерживаемые контроллеры
systemd-cgls                      # Дерево cgroups (в systemd)
cat /sys/fs/cgroup/memory.current # Текущее потребление памяти

# Информация о контейнерах
docker inspect $CONTAINER_ID      # Детальная информация
docker inspect --format '{{.State.Pid}}' $CONTAINER_ID  # PID
cat /proc/$PID/cgroup             # Cgroup контейнера
ls -la /proc/$PID/ns/             # Namespaces контейнера

# KVM/QEMU
kvm-ok                            # Проверка поддержки KVM
virsh list --all                  # Список VM (libvirt)
qemu-system-x86_64 -enable-kvm    # Запуск с KVM acceleration

# Мониторинг
perf kvm stat record              # Статистика VM exits
cat /sys/kernel/debug/kvm/*       # KVM debug info
```

---

## Проверь себя

<details>
<summary>1. Почему контейнеры быстрее стартуют чем VM?</summary>

**Ответ:** VM должна загрузить и инициализировать целую ОС — kernel, init system, services. Это минуты. Контейнер использует уже запущенный kernel хоста, запускает только приложение в изолированном namespace. Page cache хоста уже прогрет. Startup — секунды или меньше.

</details>

<details>
<summary>2. Что такое namespaces и cgroups в Linux?</summary>

**Ответ:**
- **Namespaces:** Изоляция видимости. Процесс видит свой PID=1, свою сеть, свои mounts. Типы: pid, net, mnt, uts, ipc, user.
- **Cgroups:** Лимиты ресурсов. Сколько CPU, RAM, I/O может использовать группа процессов.

Вместе дают изоляцию контейнера без overhead VM.

</details>

<details>
<summary>3. В чём разница между Type 1 и Type 2 hypervisor?</summary>

**Ответ:**
- **Type 1 (bare-metal):** Hypervisor прямо на железе, гости сверху. Примеры: VMware ESXi, Xen, Hyper-V. Минимальный overhead.
- **Type 2 (hosted):** Hypervisor внутри обычной ОС как приложение. Примеры: VirtualBox, VMware Workstation. Больше overhead, проще для desktop.

</details>

<details>
<summary>4. Почему Docker на Mac/Windows использует VM под капотом?</summary>

**Ответ:** Docker использует Linux kernel features (namespaces, cgroups). На Mac/Windows нет Linux kernel. Поэтому Docker Desktop запускает легковесную Linux VM (с помощью HyperKit на Mac, WSL2/Hyper-V на Windows), и контейнеры работают внутри этой VM. Отсюда дополнительный overhead на non-Linux системах.

</details>

---

## Связи

**Фундамент:**
- [[os-overview]] — базовые концепции (kernel/user mode, syscalls), которые виртуализация эмулирует или перехватывает
- [[os-processes-threads]] — контейнеры = процессы с isolation, VM = отдельные процессы-guests
- [[os-memory-management]] — nested page tables для VM, memory cgroups для контейнеров

**Углубление:**
- [[os-io-devices]] — virtio для I/O виртуализации, device passthrough
- [[os-scheduling]] — CPU scheduling гостей hypervisor'ом

**Применение:**
- Kubernetes использует контейнеры + дополнительную оркестрацию
- Cloud providers: EC2 = VM, ECS/Fargate = контейнеры, Lambda = micro-VM (Firecracker)

---

## Рекомендуемые источники

### Книги и курсы
- [OSTEP: Virtualization chapters](https://pages.cs.wisc.edu/~remzi/OSTEP/) — бесплатная книга
- [Docker Getting Started](https://docs.docker.com/get-started/) — официальный туториал

### Официальная документация
- [KVM Documentation](https://www.linux-kvm.org/page/Documents) — документация KVM
- [Docker Documentation](https://docs.docker.com/) — официальная документация Docker
- [Linux namespaces man page](https://man7.org/linux/man-pages/man7/namespaces.7.html) — официальная документация

### Статьи и туториалы
- [GeeksforGeeks: Virtualization](https://www.geeksforgeeks.org/virtualization-cloud-computing-and-types/) — базовое объяснение
- [Julia Evans: Containers from scratch](https://jvns.ca/blog/2016/10/10/what-even-is-a-container/) — отличное объяснение
- [AWS: Firecracker](https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/) — официальный анонс

### Глубокое погружение
- [LWN: Namespaces series](https://lwn.net/Articles/531114/) — серия статей о namespaces
- [Brendan Gregg: Container Performance](https://www.brendangregg.com/blog/2017-11-14/container-performance-analysis-at-netflix.html) — performance analysis

---

*Обновлено: 2026-01-09 — добавлены педагогические секции (5 аналогий: отель/коворкинг, зеркальные комнаты/namespaces, счётчики/cgroups, прозрачные плёнки/OverlayFS, администратор/hypervisor; 6 типичных ошибок: контейнер≠VM, безопасность, root в контейнере, данные внутри контейнера, Docker Mac/Windows≠Linux, K8s overkill; 5 ментальных моделей: спектр изоляции, контейнер=процесс+изоляция, дерево решений, эволюция, overhead пирамида)*
