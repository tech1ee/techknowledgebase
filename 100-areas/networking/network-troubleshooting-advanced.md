---
title: "Advanced Network Troubleshooting"
created: 2025-12-26
modified: 2025-12-26
type: practical
level: senior
tags:
  - networking
  - troubleshooting
  - debugging
  - production
---

# Advanced Network Troubleshooting

> Systematic debugging для сложных сетевых проблем в production

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| **Debugging basics** | Базовая диагностика | [[network-debugging-basics]] |
| **Tools reference** | Знание инструментов | [[network-tools-reference]] |
| **TCP internals** | Понимание протоколов | [[network-transport-layer]] |
| **Packet analysis** | tcpdump/Wireshark | [[network-tcpdump-wireshark]] |

### Для кого этот материал

| Уровень | Подходит? | Рекомендация |
|---------|-----------|--------------|
| **Новичок** | ❌ Рано | Сначала debugging basics |
| **Intermediate** | ⚠️ Читать | Для понимания методологии |
| **Advanced** | ✅ Да | Основная аудитория |

### Терминология для новичков

> 💡 **Advanced Troubleshooting** = методология для сложных проблем, когда простые инструменты не помогают. Системный подход вместо угадывания.

| Термин | Значение | Аналогия для новичка |
|--------|----------|---------------------|
| **Intermittent** | Проблема то есть, то нет | **Мигающая лампочка** — иногда работает |
| **Root cause** | Настоящая причина проблемы | **Источник болезни** — не симптом, а причина |
| **Hypothesis** | Предположение для проверки | **"Думаю, проблема в..."** |
| **Isolation** | Отсечение возможных причин | **Метод исключения** |
| **Baseline** | Нормальное состояние для сравнения | **"Как было раньше"** |
| **MTTR** | Mean Time To Recovery | **Время на починку** |
| **Distributed tracing** | Путь запроса через сервисы | **GPS-трек посылки** |
| **Edge case** | Редкий случай | **Особый случай** — не у всех |
| **Reproduction** | Воспроизвести проблему | **Повторить ошибку специально** |
| **Post-mortem** | Разбор после инцидента | **Отчёт что пошло не так** |

---

## Часть 1: Интуиция без кода

> Прежде чем погружаться в команды, построим ментальные образы для понимания advanced troubleshooting

### Аналогия 1: Systematic troubleshooting как врач-диагност

**Плохой врач (хаотичная диагностика):**
```
Пациент: "Доктор, у меня болит голова"

Врач (random walk):
  → "Попробуйте аспирин" (не помогло)
  → "Сделаем МРТ мозга" (норма)
  → "Проверим зрение" (норма)
  → "Может, невролог?" (3 недели ожидания)
  → "А давление мерили?" (200/120!)
  → "Так это гипертония..."

Время: 3 недели + $5000 на ненужные тесты
```

**Хороший врач (systematic approach):**
```
┌─────────────────────────────────────────────────────┐
│  1. IDENTIFY: Собрать информацию                    │
│     • Когда болит? Как часто? Где именно?           │
│     • Что изменилось недавно? Стресс? Питание?      │
│                                                     │
│  2. THEORIZE: Гипотезы по частоте                   │
│     • 60% — напряжение/стресс                       │
│     • 25% — давление                                │
│     • 10% — зрение                                  │
│     • 5% — серьёзные причины                        │
│                                                     │
│  3. TEST: Начать с простого и вероятного            │
│     • Измерить давление (30 секунд, бесплатно)      │
│     → Давление 200/120! Гипотеза подтверждена       │
│                                                     │
│  4. FIX + VERIFY                                    │
│     • Назначить лечение, проверить через неделю     │
└─────────────────────────────────────────────────────┘

Время: 15 минут. Проблема найдена.
```

**То же самое в сетевой диагностике:**
```
Проблема: "Сервис иногда тормозит"

ПЛОХО (random walk):            ХОРОШО (systematic):
→ Перезагрузить сервер          → Когда? Как часто? Кому?
→ Почистить логи                → Что в метриках?
→ Обновить версию               → Какой процент запросов?
→ Может DNS?                    → Correlation с чем?
→ Позвать DevOps                → Гипотеза → тест → результат
```

---

### Аналогия 2: Hypothesis-driven approach как научный метод

**Научный метод в troubleshooting:**
```
┌─────────────────────────────────────────────────────┐
│             НАУЧНЫЙ МЕТОД ДИАГНОСТИКИ               │
│                                                     │
│  1. НАБЛЮДЕНИЕ                                      │
│     "Запросы к API иногда возвращают timeout"       │
│                                                     │
│  2. ВОПРОС                                          │
│     "Почему timeout? Что общего у этих запросов?"   │
│                                                     │
│  3. ГИПОТЕЗА (testable!)                            │
│     "Timeout происходит когда payload > 1400 bytes" │
│                                                     │
│  4. ЭКСПЕРИМЕНТ                                     │
│     curl с payload 1000 bytes → OK                  │
│     curl с payload 1500 bytes → TIMEOUT             │
│     curl с payload 1400 bytes → OK                  │
│     curl с payload 1450 bytes → TIMEOUT             │
│                                                     │
│  5. ВЫВОД                                           │
│     "MTU проблема на VPN туннеле (MTU = 1400)"      │
│                                                     │
│  6. VERIFICATION                                    │
│     Уменьшили MTU → проблема ушла                   │
└─────────────────────────────────────────────────────┘
```

**Правило хорошей гипотезы:**
```
✅ ХОРОШАЯ: "Проблема в MTU на VPN интерфейсе"
   → Можно проверить: ping -s 1472 -M do target
   → Можно опровергнуть: если ping проходит, гипотеза неверна

❌ ПЛОХАЯ: "Что-то с сетью"
   → Невозможно проверить конкретно
   → Невозможно опровергнуть
```

---

### Аналогия 3: Isolation как бинарный поиск

**Представьте: перегорела лампочка в гирлянде из 1000 лампочек**

**Линейный поиск (плохо):**
```
Проверяем: 1, 2, 3, 4, 5, 6, ... 847 (НАШЛИ!)
Время: O(n) — проверили 847 лампочек
```

**Бинарный поиск (хорошо):**
```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  1000 лампочек                                      │
│  [████████████████████████████████████████████████] │
│                   │                                 │
│  Делим пополам:   ▼                                 │
│  [████████████████]          [░░░░░░░░░░░░░░░░░░░░] │
│   Не горит!                        Горит           │
│        │                                            │
│        ▼                                            │
│  [████████]    [░░░░░░░░]                           │
│   Не горит!      Горит                             │
│        │                                            │
│        ▼                                            │
│  [████]  [░░░░]                                     │
│   Горит  Не горит!                                  │
│             │                                       │
│             ▼                                       │
│  ...                                                │
│  Лампочка 847 — перегорела!                         │
│                                                     │
│  Проверили: log₂(1000) ≈ 10 шагов                   │
└─────────────────────────────────────────────────────┘
```

**В сетевой диагностике:**
```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  Клиент → LB → Service A → Service B → DB           │
│                                                     │
│  Проблема: timeout где-то в цепочке                 │
│                                                     │
│  BINARY SEARCH:                                     │
│                                                     │
│  Шаг 1: Проверяем середину (Service A)              │
│         curl Service A напрямую → OK                │
│         Значит, проблема после Service A            │
│                                                     │
│  Шаг 2: Проверяем Service B                         │
│         curl Service B напрямую → TIMEOUT!          │
│         Проблема на Service B или дальше            │
│                                                     │
│  Шаг 3: Проверяем DB с Service B                    │
│         psql → OK                                   │
│         Проблема = Service B → DB                   │
│                                                     │
│  Шаг 4: tcpdump на Service B                        │
│         Connection pool exhausted!                  │
│                                                     │
│  Найдено за 4 шага вместо перебора всего            │
└─────────────────────────────────────────────────────┘
```

---

### Аналогия 4: MTU как грузовик в узком туннеле

**Проблема MTU простым языком:**
```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  Грузовик высотой 4м                                │
│       ┌─────┐                                       │
│       │█████│                                       │
│       │█████│ 4м                                    │
│       │█████│                                       │
│  ═════╧═════╧═════                                  │
│                                                     │
│  Едет по шоссе... всё ОК                            │
│                                                     │
│              ┌─────────────────┐                    │
│              │    ТУННЕЛЬ 3м   │                    │
│              │      ████       │                    │
│  ════════════╧═══════════════╧════════════          │
│                                                     │
│  СТОП! Грузовик не проходит в туннель!              │
│                                                     │
│  Варианты:                                          │
│  1. Разгрузить грузовик (фрагментация)              │
│  2. Узнать высоту туннеля заранее (PMTUD)           │
│  3. Изначально не грузить выше 3м (MSS clamping)    │
│                                                     │
└─────────────────────────────────────────────────────┘
```

**В сетевых терминах:**
```
┌─────────────────────────────────────────────────────┐
│  MTU = Maximum Transmission Unit                    │
│  "Максимальный размер посылки"                      │
│                                                     │
│  Ethernet: MTU = 1500 bytes (обычно)                │
│  VPN/IPSEC: MTU = 1400 bytes (overhead туннеля)     │
│  Docker VXLAN: MTU = 1450 bytes                     │
│                                                     │
│  ═══════════════════════════════════════════════    │
│                                                     │
│  Клиент отправляет пакет 1500 bytes                 │
│       │                                             │
│       ▼                                             │
│  [Ethernet: OK] → [VPN туннель: MTU 1400]           │
│                          │                          │
│                          ▼                          │
│                   ПАКЕТ НЕ ПРОХОДИТ!                │
│                                                     │
│  Если DF bit set:                                   │
│  → ICMP "Fragmentation Needed" (если firewall OK)   │
│  → Молчаливый DROP (если firewall блокирует ICMP)   │
│                                                     │
│  Симптомы:                                          │
│  • Маленькие запросы работают                       │
│  • Большие запросы timeout                          │
│  • ping работает, но wget/curl timeout              │
│                                                     │
└─────────────────────────────────────────────────────┘
```

**Как диагностировать:**
```bash
# Найти MTU до хоста (Linux)
ping -c 3 -M do -s 1472 target.com  # 1472 + 28 = 1500
# Если "Frag needed" — уменьшайте -s

# Windows
ping -f -l 1472 target.com

# Найти точный MTU
for size in 1500 1450 1400 1350; do
    ping -c 1 -M do -s $((size - 28)) target.com && echo "MTU >= $size"
done
```

---

### Аналогия 5: Intermittent problems как мигающая лампочка

**Почему intermittent проблемы самые сложные:**
```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  ПОСТОЯННАЯ ПРОБЛЕМА        INTERMITTENT ПРОБЛЕМА   │
│  ══════════════════         ════════════════════    │
│                                                     │
│  Сломано ВСЕГДА             Сломано ИНОГДА          │
│                                                     │
│  ████████████████           █░░█░░░░█░░█░░░░█░░     │
│                                                     │
│  Легко воспроизвести        Трудно поймать          │
│  Легко проверить fix        Fix работает... кажется │
│                                                     │
│  "Не работает"              "Иногда не работает"    │
│                                                     │
└─────────────────────────────────────────────────────┘
```

**Типичные причины intermittent проблем:**
```
┌─────────────────────────────────────────────────────┐
│  1. RACE CONDITIONS                                 │
│     Два процесса конкурируют за ресурс              │
│     Результат зависит от timing                     │
│                                                     │
│  2. RESOURCE EXHAUSTION                             │
│     Connection pool, file descriptors, memory       │
│     Проблема появляется под нагрузкой               │
│                                                     │
│  3. DNS CACHING                                     │
│     Разные клиенты видят разные IP                  │
│     TTL истекает в разное время                     │
│                                                     │
│  4. LOAD BALANCER ISSUES                            │
│     Один из backend-ов нездоров                     │
│     Round-robin попадает на него 1/N запросов       │
│                                                     │
│  5. NETWORK CONGESTION                              │
│     Packet loss при пиковой нагрузке                │
│     Retransmissions замедляют часть запросов        │
│                                                     │
│  6. TIMEOUTS RACING                                 │
│     Клиент timeout < сервер processing time         │
│     Иногда успевает, иногда нет                     │
└─────────────────────────────────────────────────────┘
```

**Подход к debugging:**
```
┌─────────────────────────────────────────────────────┐
│  1. CORRELATE (ищи паттерн)                         │
│     • Время суток?                                  │
│     • Конкретные users/IPs?                         │
│     • Размер payload?                               │
│     • После деплоя?                                 │
│                                                     │
│  2. INCREASE OBSERVABILITY                          │
│     • Добавь метрики                                │
│     • Увеличь детальность логов                     │
│     • Включи distributed tracing                    │
│                                                     │
│  3. REPRODUCE (провоцируй проблему)                 │
│     • Load testing                                  │
│     • Chaos engineering                             │
│     • Симуляция условий                             │
│                                                     │
│  4. CAPTURE (лови момент)                           │
│     • Continuous packet capture                     │
│     • Auto-triggered dumps                          │
│     • Alerting + auto-diagnostics                   │
└─────────────────────────────────────────────────────┘
```

---

## Часть 2: Почему это сложно

> 6 типичных ошибок при advanced troubleshooting

### Ошибка 1: Хаотичная диагностика без методологии

**СИМПТОМ:**
```
Инцидент: "Сервис недоступен"

Действия команды:
10:00  "Давайте перезагрузим сервер"
10:05  "Не помогло, проверим DNS"
10:10  "DNS ок, может firewall?"
10:20  "Firewall ок, позовём DBA"
10:30  "База ок, может CDN?"
10:45  "Может это DDoS?"
11:00  "А давайте откатим деплой"
11:15  Случайно нашли проблему: disk full

MTTR: 1 час 15 минут хаоса
```

**ПОЧЕМУ ЭТО НЕЭФФЕКТИВНО:**
```
• Нет приоритизации гипотез
• Нет документации что проверили
• Возможно проверяли одно и то же дважды
• Стресс → плохие решения
• Нет learning для будущего
```

**РЕШЕНИЕ:**
```
┌─────────────────────────────────────────────────────┐
│  INCIDENT COMMANDER назначен                        │
│                                                     │
│  10:00 IDENTIFY                                     │
│     • Что не работает? HTTP 503                     │
│     • Когда началось? 09:55                         │
│     • Что изменилось? Деплой в 09:50               │
│                                                     │
│  10:02 ГИПОТЕЗЫ (ranked by probability)             │
│     1. Деплой сломал что-то (95%)                  │
│     2. Инфраструктура (5%)                         │
│                                                     │
│  10:03 ТЕСТ гипотезы #1                            │
│     kubectl logs → OOMKilled                        │
│     Новая версия утечка памяти                      │
│                                                     │
│  10:05 ROLLBACK                                     │
│     kubectl rollout undo                            │
│                                                     │
│  10:07 VERIFY: сервис работает                      │
│                                                     │
│  MTTR: 7 минут (vs 1 час 15 минут)                  │
└─────────────────────────────────────────────────────┘
```

---

### Ошибка 2: Игнорирование MTU/fragmentation проблем

**СИМПТОМ:**
```
"Странная проблема"
• ping работает
• curl на маленькие endpoint-ы работает
• curl на endpoint с большим response — timeout
• VPN пользователи жалуются
• Иногда работает, иногда нет

"Это что-то с приложением!"
    ↓
3 дня debugging кода...
    ↓
Проблема была: MTU mismatch на VPN
```

**ПОЧЕМУ ПРОПУСКАЮТ:**
```
• MTU проблемы маскируются как "application timeout"
• Ping работает (маленькие пакеты)
• Небольшие запросы работают
• ICMP "Frag needed" блокируется firewall
• PMTUD молча ломается
```

**РЕШЕНИЕ:**
```bash
# 1. ВСЕГДА проверяй MTU при странных timeout-ах

# Тест PMTUD
ping -M do -s 1472 -c 3 target.com  # Должно работать для MTU 1500

# Если "Frag needed" или timeout:
for size in 1472 1400 1350 1300 1250; do
    if ping -M do -s $size -c 1 target.com >/dev/null 2>&1; then
        echo "Working MTU: $((size + 28))"
        break
    fi
done

# 2. Проверь реальный MTU интерфейсов
ip link show | grep mtu

# 3. На VPN/туннелях
# Ожидаемые значения:
# - IPSEC: 1400-1438
# - OpenVPN: 1400
# - WireGuard: 1420
# - VXLAN (Docker/K8s): 1450

# 4. TCP MSS clamping на роутере
iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN \
         -j TCPMSS --clamp-mss-to-pmtu
```

---

### Ошибка 3: Нет baseline для сравнения

**СИМПТОМ:**
```
"Сервис стал медленнее"

Вопрос: "Какой был latency раньше?"
Ответ: "Не знаем, не мерили"

Вопрос: "Сколько было retransmissions в норме?"
Ответ: "Не знаем"

Вопрос: "Какая была CPU utilization?"
Ответ: "Не смотрели"

→ Невозможно понять что изменилось
→ Невозможно понять когда "починили"
```

**ПОЧЕМУ ЭТО КРИТИЧНО:**
```
┌─────────────────────────────────────────────────────┐
│  БЕЗ BASELINE:                                      │
│                                                     │
│  Latency = 150ms                                    │
│  Это плохо? Хорошо? Нормально?                      │
│  → Не знаем                                         │
│                                                     │
│  С BASELINE:                                        │
│                                                     │
│  Baseline latency = 50ms                            │
│  Current latency = 150ms                            │
│  → 3x degradation! Явная проблема.                  │
│                                                     │
│  После fix:                                         │
│  Latency = 55ms                                     │
│  → Вернулись к норме (с погрешностью)              │
└─────────────────────────────────────────────────────┘
```

**РЕШЕНИЕ:**
```
Что записывать как baseline:

1. LATENCY
   • p50, p95, p99 response time
   • По endpoint-ам и регионам

2. THROUGHPUT
   • RPS в норме и под нагрузкой
   • Bandwidth utilization

3. ERROR RATES
   • % 5xx ошибок
   • % timeout-ов

4. RESOURCE USAGE
   • CPU, memory, disk I/O
   • Network I/O, connection count

5. NETWORK HEALTH
   • Retransmission rate
   • RTT to dependencies
   • DNS resolution time

Хранение: Prometheus + Grafana dashboards
Алерты: когда metric > baseline * 2
```

---

### Ошибка 4: Искать проблему в сети когда она в приложении

**СИМПТОМ:**
```
"Сеть тормозит!"
    ↓
tcpdump, Wireshark, traceroute...
    ↓
"Сеть в порядке, непонятно"
    ↓
Неделю ищем сетевую проблему
    ↓
Оказалось: N+1 query в коде
```

**ПРАВИЛО ДИФФЕРЕНЦИАЦИИ:**
```
┌─────────────────────────────────────────────────────┐
│  ГДЕ ПРОБЛЕМА?                                      │
│                                                     │
│  ВРЕМЯ ОТВЕТА                    ВЫВОД              │
│  ───────────────────────────────────────────────    │
│  Connect time высокий            → СЕТЬ             │
│  (handshake занимает долго)                         │
│                                                     │
│  TTFB высокий (после connect)    → BACKEND          │
│  (сервер думает)                                    │
│                                                     │
│  Download time высокий           → СЕТЬ или PAYLOAD │
│  (данные идут медленно)                             │
│                                                     │
└─────────────────────────────────────────────────────┘

Как проверить:
curl -w "
    DNS:        %{time_namelookup}s
    Connect:    %{time_connect}s
    TLS:        %{time_appconnect}s
    TTFB:       %{time_starttransfer}s
    Total:      %{time_total}s
" -o /dev/null -s https://api.example.com/slow-endpoint
```

**ТИПИЧНЫЕ "СЕТЕВЫЕ" ПРОБЛЕМЫ КОТОРЫЕ НА САМОМ ДЕЛЕ В ПРИЛОЖЕНИИ:**
```
1. N+1 queries             → Много мелких запросов к DB
2. Missing indexes         → Медленные queries
3. Sync I/O in async code  → Blocking
4. Memory pressure         → GC pauses
5. Connection pool exhaust → Ждём свободное соединение
6. Lock contention         → Threads ждут друг друга
```

---

### Ошибка 5: Забывать про DNS кэширование

**СИМПТОМ:**
```
Проблема: "Сервис недоступен!"

DevOps: "Поменяли DNS на новый IP"
Dev:    "У меня всё работает!"
User:   "У меня не работает!"

"Магия какая-то..."
```

**ПОЧЕМУ DNS КЭШИРОВАНИЕ КОВАРНО:**
```
┌─────────────────────────────────────────────────────┐
│  DNS КЭШИ ВЕЗДЕ:                                    │
│                                                     │
│  Браузер         → Свой кэш (минуты)                │
│  OS              → Свой кэш (до TTL)                │
│  Local resolver  → Свой кэш (TTL)                   │
│  ISP DNS         → Свой кэш (часто игнорирует TTL!) │
│  CDN/LB          → Свой кэш                         │
│  Application     → Может кэшировать (JVM, etc)      │
│                                                     │
│  Все видят РАЗНЫЕ IP в разное время!                │
│                                                     │
└─────────────────────────────────────────────────────┘
```

**РЕШЕНИЕ:**
```bash
# 1. Проверяй что видит конкретный клиент
dig @8.8.8.8 api.example.com +short   # Google DNS
dig @1.1.1.1 api.example.com +short   # Cloudflare
dig @local-dns api.example.com +short # Локальный

# 2. Смотри TTL
dig api.example.com +noall +answer
# Если TTL маленький (60s) — изменения быстрее
# Если TTL большой (86400s) — ждать до суток

# 3. Флаш кэша при тестировании
# macOS
sudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder
# Linux
sudo systemd-resolve --flush-caches
# Windows
ipconfig /flushdns

# 4. При миграции: сначала уменьши TTL до 60s, подожди старый TTL,
#    потом меняй IP, потом верни TTL обратно
```

---

### Ошибка 6: Не использовать distributed tracing

**СИМПТОМ:**
```
Микросервисная архитектура:
User → Gateway → Auth → UserService → DB
                    ↘ InventoryService → Cache

"Запрос медленный, но где?"

Логи Gateway: "Request took 5s"
Логи Auth: "Request took 100ms"
Логи UserService: "Request took 200ms"
Логи InventoryService: "Request took 150ms"

100 + 200 + 150 = 450ms
Но total = 5000ms

"Где остальные 4.5 секунды?!"
```

**ПОЧЕМУ ЛОГОВ НЕДОСТАТОЧНО:**
```
┌─────────────────────────────────────────────────────┐
│  ЛОГИ показывают:                                   │
│  • Время внутри сервиса                             │
│  • Не показывают время между сервисами              │
│  • Не показывают retries, queuing, network          │
│  • Трудно коррелировать между сервисами             │
│                                                     │
│  DISTRIBUTED TRACING показывает:                    │
│  • Полный путь запроса                              │
│  • Время каждого span-а                             │
│  • Gaps между spans (очереди, сеть)                 │
│  • Parent-child relationships                       │
└─────────────────────────────────────────────────────┘
```

**РЕШЕНИЕ:**
```
┌─────────────────────────────────────────────────────┐
│  JAEGER / ZIPKIN / TEMPO TRACE VIEW                 │
│                                                     │
│  Request abc-123                    Total: 5.2s     │
│  ├─ Gateway                         [====]  200ms   │
│  │  └─ Auth Service                 [=] 100ms       │
│  │     └─ JWT Validation            [=] 80ms        │
│  │                                                  │
│  ├─ UserService                     [==] 150ms      │
│  │  └─ DB Query                     [=] 100ms       │
│  │                                                  │
│  ├─ InventoryService                [=========] 4.5s│ ← BOTTLENECK!
│  │  └─ Redis GET (retry x3)         [===] 1.5s     │
│  │  └─ External API call            [======] 3s    │ ← ВОТ ОНО!
│  │                                                  │
│  └─ Response                        [=] 50ms        │
│                                                     │
│  ВЫВОД: External API call в InventoryService        │
│         занимает 3 секунды!                         │
└─────────────────────────────────────────────────────┘

Инструменты: Jaeger, Zipkin, AWS X-Ray, Datadog APM
Стандарт: OpenTelemetry (W3C Trace Context)
```

---

## Часть 3: Ментальные модели

### Модель 1: Scientific Method для troubleshooting

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│           НАУЧНЫЙ МЕТОД ДИАГНОСТИКИ                 │
│                                                     │
│  ┌─────────────┐                                    │
│  │ НАБЛЮДЕНИЕ  │ ← Симптомы, метрики, логи          │
│  └──────┬──────┘                                    │
│         │                                           │
│         ▼                                           │
│  ┌─────────────┐                                    │
│  │   ВОПРОС    │ ← "Почему X происходит?"           │
│  └──────┬──────┘                                    │
│         │                                           │
│         ▼                                           │
│  ┌─────────────┐                                    │
│  │  ГИПОТЕЗА   │ ← Testable, falsifiable            │
│  └──────┬──────┘                                    │
│         │                                           │
│         ▼                                           │
│  ┌─────────────┐                                    │
│  │ ЭКСПЕРИМЕНТ │ ← Тест гипотезы                    │
│  └──────┬──────┘                                    │
│         │                                           │
│    ┌────┴────┐                                      │
│    ▼         ▼                                      │
│  [ПОДТВ.]  [ОПРОВЕРГ.]                              │
│    │         │                                      │
│    ▼         └────► Новая гипотеза                  │
│  [FIX]                                              │
│    │                                                │
│    ▼                                                │
│  [VERIFY]                                           │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

### Модель 2: Divide and Conquer

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  ПРОБЛЕМА: "Запросы иногда timeout"                 │
│                                                     │
│  Система:                                           │
│  Client → CDN → LB → App → Cache → DB               │
│                                                     │
│  ШАГ 1: Раздели пополам                             │
│  ────────────────────                               │
│  Тест: curl App напрямую (минуя CDN, LB)            │
│  Результат: OK                                      │
│  → Проблема ДО App (CDN или LB)                     │
│                                                     │
│  ШАГ 2: Раздели оставшееся пополам                  │
│  ─────────────────────────────────                  │
│  Тест: curl LB напрямую (минуя CDN)                 │
│  Результат: TIMEOUT!                                │
│  → Проблема на LB                                   │
│                                                     │
│  ШАГ 3: Исследуй LB                                 │
│  ────────────────────                               │
│  Один из backend-ов в LB не отвечает                │
│  Health check слишком редкий                        │
│  → FIX: Увеличить частоту health checks             │
│                                                     │
│  Найдено за 3 шага (не 6)                           │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

### Модель 3: Timeline Reconstruction

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  ИНЦИДЕНТ: Сервис недоступен с 14:00 до 14:30       │
│                                                     │
│  TIMELINE:                                          │
│                                                     │
│  13:45  Deploy v2.3.1                               │
│  13:50  Metrics: CPU +20%, memory stable            │
│  13:55  First slow response in logs                 │
│  14:00  Alerts: Error rate > 5%                     │
│  14:05  Users report: "site down"                   │
│  14:10  On-call paged                               │
│  14:15  Investigation: OOMKilled pods               │
│  14:20  Rollback to v2.3.0                          │
│  14:25  Pods healthy                                │
│  14:30  All clear                                   │
│                                                     │
│  ROOT CAUSE: Memory leak в v2.3.1                   │
│  TRIGGER: Deploy в 13:45                            │
│  TIME TO IMPACT: 15 минут                           │
│  TIME TO DETECT: 25 минут                           │
│  TIME TO RESOLVE: 45 минут                          │
│                                                     │
│  ACTION ITEMS:                                      │
│  • Добавить memory alerts раньше порога OOM         │
│  • Canary deployment перед full rollout             │
│  • Load testing с профилированием памяти            │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

### Модель 4: Correlation vs Causation

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  КОРРЕЛЯЦИЯ ≠ ПРИЧИННОСТЬ                           │
│                                                     │
│  Наблюдение:                                        │
│  "После деплоя latency выросла"                     │
│                                                     │
│  Возможные объяснения:                              │
│                                                     │
│  1. Деплой ВЫЗВАЛ проблему (causation)              │
│     Deploy → New code → Bug → Latency               │
│                                                     │
│  2. Деплой СОВПАЛ с проблемой (correlation)         │
│     Deploy ──────────────────────┐                  │
│                                  │ совпадение       │
│     Traffic spike ───────────────┤                  │
│                                  ▼                  │
│                              Latency                │
│                                                     │
│  3. Общая причина (confounding)                     │
│     Release day → Deploy                            │
│                 → Marketing campaign                │
│                 → Traffic spike → Latency           │
│                                                     │
│  КАК ОТЛИЧИТЬ:                                      │
│  • Rollback: latency вернулась? → Causation         │
│  • Rollback: latency осталась? → Correlation        │
│  • Проверь другие изменения в это время             │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

### Модель 5: The 5 Whys (Пять Почему)

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  ИНЦИДЕНТ: Сайт был недоступен 2 часа               │
│                                                     │
│  ПОЧЕМУ #1: Почему сайт был недоступен?             │
│  → База данных не отвечала                          │
│                                                     │
│  ПОЧЕМУ #2: Почему БД не отвечала?                  │
│  → Диск заполнился на 100%                          │
│                                                     │
│  ПОЧЕМУ #3: Почему диск заполнился?                 │
│  → Логи не ротировались                             │
│                                                     │
│  ПОЧЕМУ #4: Почему логи не ротировались?            │
│  → logrotate был отключен после миграции            │
│                                                     │
│  ПОЧЕМУ #5: Почему отключение не заметили?          │
│  → Нет мониторинга disk usage                       │
│  → Нет checklist для миграций                       │
│                                                     │
│  ROOT CAUSE: Отсутствие процесса проверки           │
│              после миграций                         │
│                                                     │
│  FIXES:                                             │
│  • Immediate: включить logrotate                    │
│  • Short-term: мониторинг disk usage                │
│  • Long-term: migration checklist                   │
│                                                     │
└─────────────────────────────────────────────────────┘

ПРАВИЛО: Продолжай спрашивать "почему" пока не дойдёшь
         до системной/процессной проблемы
```

---

## Когда нужен advanced troubleshooting

### Почему базовых инструментов недостаточно

**Разница между junior и senior troubleshooting.** Junior-разработчик может запустить ping и curl. Senior знает, что делать, когда ping работает, curl возвращает 200, но пользователи жалуются на проблемы. Advanced troubleshooting — это умение диагностировать сложные, неочевидные, intermittent проблемы.

**Типы сложных проблем, которые требуют advanced подхода:**
- **Intermittent failures** — "иногда работает, иногда нет". Простые инструменты могут показать "всё ок", потому что проблема не воспроизводится в момент проверки
- **Performance degradation** — сервис работает, но медленнее чем обычно. Где bottleneck: сеть? приложение? база данных?
- **Edge cases** — проблема только у части пользователей, или только с определённым типом запросов
- **Distributed system issues** — проблема возникает на стыке нескольких сервисов

**Ключевой навык: системное мышление.** Вместо хаотичных проверок — структурированный подход. Вместо "попробую это" — гипотезы, их проверка, документирование результатов. Это экономит часы времени на сложных проблемах.

---

## Systematic Troubleshooting Methodology

### Почему методология важнее инструментов

Многие инженеры знают десятки команд, но не знают, в каком порядке их применять. Результат: часы хаотичных проверок вместо 15 минут структурированной диагностики.

**Проблема "хватания за соломинку".** Без методологии типичный подход: "попробую перезагрузить", "может DNS?", "давай проверим firewall". Это random walk — можете найти проблему случайно, можете не найти вообще.

**Системный подход гарантирует результат.** Когда вы идёте по методологии, вы либо находите проблему, либо точно знаете, что исключили. Это критически важно для сложных случаев.

### The Golden Framework

```
┌─────────────────────────────────────────────────────────────────────┐
│                    SYSTEMATIC TROUBLESHOOTING                       │
├─────────────────────────────────────────────────────────────────────┤
│ 1. IDENTIFY    │ Собери информацию: что, где, когда, как часто      │
│ 2. THEORIZE    │ Построй гипотезы на основе симптомов               │
│ 3. TEST        │ Проверь каждую гипотезу (начни с самой вероятной) │
│ 4. ISOLATE     │ Локализуй проблему до конкретного компонента       │
│ 5. FIX         │ Исправь и проверь что fix работает                 │
│ 6. DOCUMENT    │ Запиши root cause и решение                        │
└─────────────────────────────────────────────────────────────────────┘
```

### Подходы к диагностике

**Top-Down (Application → Physical):**
- Начинаем с L7 (HTTP errors, response times)
- Спускаемся к L4 (TCP connections, ports)
- Дальше к L3 (IP routing, firewall)
- До L1 (physical connectivity)

**Bottom-Up (Physical → Application):**
- Проверяем физику (кабели, интерфейсы)
- Затем L3 (ping, routing)
- TCP connectivity (ports, firewalls)
- Application level

**Divide and Conquer:**
- Разделяем network path пополам
- Тестируем середину
- Сужаем scope до problematic segment

---

## Case Study: 502/503/504 Gateway Errors

### Почему gateway ошибки так часты в современных архитектурах

**Анатомия типичного web-запроса.** В современной архитектуре между пользователем и приложением — несколько слоёв: CDN → Load Balancer → Ingress → Sidecar → Application → Database. Каждый слой — потенциальная точка отказа. Gateway errors (5xx) означают, что один из промежуточных слоёв не смог получить ответ от следующего.

**Почему это сложно диагностировать.** Когда nginx возвращает 502, проблема может быть:
- В самом nginx (конфигурация)
- В сети между nginx и backend
- В backend-приложении
- В зависимостях backend (база, внешние API)

И логи nginx просто скажут "upstream connection refused" — без деталей, что именно сломалось.

**Ключ к быстрой диагностике: понять цепочку.** Нарисуйте путь запроса от пользователя до базы данных. Потом проверяйте каждое звено: живо ли? отвечает ли? как быстро?

### Quick Reference

| Error | Значение | Типичные причины |
|-------|----------|------------------|
| **502** | Bad Gateway | Backend crashed, invalid response, connection refused |
| **503** | Service Unavailable | Overload, maintenance, circuit breaker open |
| **504** | Gateway Timeout | Backend не ответил вовремя, slow query, deadlock |

### 502 Bad Gateway — Debugging

```
┌──────────┐     ┌──────────┐     ┌──────────┐
│  Client  │────>│  Proxy   │────>│ Backend  │
│          │     │  (LB)    │     │ (crashed)│
└──────────┘     └──────────┘     └──────────┘
                       ↓
            502 Bad Gateway
```

**Checklist:**

```bash
# 1. Backend живой?
curl -v http://backend-server:8080/health
# Connection refused? → сервис не запущен
# Timeout? → firewall или сеть

# 2. Что в логах backend?
tail -f /var/log/app/application.log

# 3. Что в логах proxy/LB?
tail -f /var/log/nginx/error.log
# "connect() failed (111: Connection refused)" → backend down
# "upstream prematurely closed connection" → backend crashed mid-request

# 4. Health checks проходят?
# Nginx upstream status
curl http://lb.internal/nginx_status

# 5. Порты слушают?
ss -tuln | grep 8080

# 6. Процесс есть?
ps aux | grep java  # или python, node, etc.
```

**Типичные причины 502:**
- Backend crashed (OOM, segfault, unhandled exception)
- Backend не успел стартовать
- Backend слушает на wrong interface (127.0.0.1 vs 0.0.0.0)
- Misconfigured proxy upstream

### 503 Service Unavailable — Debugging

```bash
# 1. Сервер в maintenance mode?
# Check for maintenance files
ls /var/www/html/maintenance.html

# 2. Rate limiting сработал?
# Check response headers
curl -I https://api.example.com/endpoint
# Look for: X-RateLimit-Remaining, Retry-After

# 3. Circuit breaker open?
# Check application metrics/dashboard

# 4. Resource exhaustion?
# CPU
top -bn1 | head -20

# Memory
free -h

# Disk
df -h

# File descriptors
cat /proc/sys/fs/file-nr
```

### 504 Gateway Timeout — Debugging

```bash
# 1. Backend отвечает но медленно?
curl -w "Time: %{time_total}s\n" -o /dev/null \
  http://backend:8080/slow-endpoint
# >30s = backend too slow

# 2. Database lock?
# PostgreSQL
SELECT * FROM pg_stat_activity WHERE wait_event_type = 'Lock';

# MySQL
SHOW PROCESSLIST;
SHOW ENGINE INNODB STATUS;

# 3. Network latency?
mtr backend-server

# 4. Увеличить timeout (временное решение)
# Nginx
proxy_read_timeout 120s;
proxy_connect_timeout 10s;
```

---

## Case Study: DNS Problems

### Slow DNS Resolution

**Симптомы:**
- First request медленный, subsequent быстрые
- Периодические timeouts
- `curl -w "DNS: %{time_namelookup}s"` показывает >100ms

**Debugging:**

```bash
# 1. Проверить какой DNS server используется
cat /etc/resolv.conf
# или systemd-resolved
resolvectl status

# 2. Время резолвинга
time dig example.com
# Query time: >100ms = slow

# 3. Сравнить с public DNS
time dig @8.8.8.8 example.com
# Быстрее? → проблема с вашим DNS сервером

# 4. Проверить DNS сервер доступен
ping $(cat /etc/resolv.conf | grep nameserver | head -1 | awk '{print $2}')

# 5. Смотрим TTL
dig example.com +short +ttlid
# Низкий TTL (60s) = частые lookups
```

**Решения:**
- Использовать local DNS cache (dnsmasq, unbound)
- Увеличить TTL в записях
- Переключиться на faster DNS (1.1.1.1, 8.8.8.8)
- В Kubernetes — увеличить CoreDNS replicas

### DNS Cache Problems

```bash
# Flush DNS cache

# macOS
sudo dscacheutil -flushcache && sudo killall -HUP mDNSResponder

# Linux (systemd-resolved)
sudo systemd-resolve --flush-caches

# Windows
ipconfig /flushdns

# Проверить что кэш пустой
dig example.com  # первый запрос
dig example.com  # второй запрос должен быть из cache (если TTL>0)
```

### Negative Caching

**Проблема:** Посетили домен ДО создания DNS записи → "отсутствие записи" закэшировалось.

```bash
# Проверить negative caching TTL
dig example.com +short
# Пусто = NXDOMAIN cached

# Время жизни negative cache = SOA минимум
dig SOA example.com
# Смотрим последнее число (minimum TTL)
# example.com. 3600 IN SOA ... 300
# 300 = 5 минут ждать

# Решение: подождать TTL или flush cache
```

---

## Case Study: Connection Pool Exhaustion

### Симптомы

- Requests hang на 30+ секунд, потом timeout
- CPU/Memory нормальные
- Database connections исчерпаны
- В логах: "Timeout waiting for connection from pool"

### Диагностика

```bash
# 1. Сколько соединений к database?
# PostgreSQL
SELECT count(*) FROM pg_stat_activity;
SELECT max_connections FROM pg_settings WHERE name = 'max_connections';

# MySQL
SHOW STATUS LIKE 'Threads_connected';
SHOW VARIABLES LIKE 'max_connections';

# 2. Netstat — сколько ESTABLISHED connections?
netstat -an | grep ESTABLISHED | grep 5432 | wc -l  # PostgreSQL
netstat -an | grep ESTABLISHED | grep 3306 | wc -l  # MySQL

# 3. TIME_WAIT connections?
netstat -an | grep TIME_WAIT | wc -l
# Много TIME_WAIT = connections быстро открываются/закрываются

# 4. Где leak?
# Thread dump (Java)
jstack PID > thread_dump.txt
grep -A 10 "waiting for connection" thread_dump.txt
```

### Причины

| Причина | Как распознать | Решение |
|---------|----------------|---------|
| Connection leak | Connections растут со временем | Добавить connection tracking, найти код без finally/close |
| Pool too small | Peaks при load | Увеличить pool size |
| Slow queries | Connections долго заняты | Оптимизировать queries, добавить timeouts |
| No connection timeout | Threads ждут вечно | Установить connectionTimeout |

### Конфигурация pool (best practices)

```yaml
# HikariCP example
hikari:
  maximum-pool-size: 20
  minimum-idle: 5
  connection-timeout: 30000  # 30s — max wait for connection
  idle-timeout: 600000       # 10m — close idle connections
  max-lifetime: 1800000      # 30m — max connection age
  leak-detection-threshold: 60000  # 60s — log if not returned
```

---

## Case Study: TLS/SSL Failures

### Certificate Chain Issues

**Симптомы:**
- "SSL: CERTIFICATE_VERIFY_FAILED"
- "unable to get local issuer certificate"
- Works in browser, fails in curl/code

**Debugging:**

```bash
# 1. Посмотреть certificate chain
openssl s_client -connect example.com:443 -showcerts

# Ищем:
# - depth=0 — server cert
# - depth=1 — intermediate CA
# - depth=2 — root CA
# Нет intermediate? → incomplete chain

# 2. Проверить что chain полный
# Сервер должен отдавать: server cert + intermediate(s)
# Не должен отдавать: root CA

# 3. Qualys SSL Test (web)
# https://www.ssllabs.com/ssltest/

# 4. Проверить expiration
echo | openssl s_client -connect example.com:443 2>/dev/null | \
  openssl x509 -noout -dates
# notAfter=Dec 31 23:59:59 2024 GMT
```

**Fix incomplete chain:**

```nginx
# Nginx — объединить server cert + intermediate
ssl_certificate /etc/ssl/certs/server_chain.pem;
# Содержимое server_chain.pem:
# -----BEGIN CERTIFICATE-----
# (server cert)
# -----END CERTIFICATE-----
# -----BEGIN CERTIFICATE-----
# (intermediate cert)
# -----END CERTIFICATE-----
```

### TLS Version Mismatch

```bash
# Проверить supported версии
openssl s_client -connect example.com:443 -tls1_2
openssl s_client -connect example.com:443 -tls1_3

# Если TLS 1.2 не поддерживается
# → upgrade client или downgrade server requirement
```

### Common TLS Errors

| Error | Причина | Решение |
|-------|---------|---------|
| CERTIFICATE_EXPIRED | Cert истёк | Renew certificate |
| CERTIFICATE_UNKNOWN | Unknown CA | Add CA to trust store |
| HANDSHAKE_FAILURE | No common cipher | Check cipher compatibility |
| CERTIFICATE_REVOKED | Cert отозван | Issue new certificate |

---

## Case Study: TCP Performance Issues

### High Retransmission Rate

```bash
# 1. Проверить retransmissions
netstat -s | grep -i retrans
# RetransSegs: 12345
# % retransmissions = RetransSegs / OutSegs * 100
# >1% = проблема

# 2. Wireshark filter
tcp.analysis.retransmission

# 3. Где потери?
mtr -r -c 100 destination
# Смотрим Loss% на каждом hop
```

**Причины и решения:**

| Паттерн | Вероятная причина | Решение |
|---------|-------------------|---------|
| Retrans на первых hops | Local network issue | Check switches, cables |
| Retrans в середине | ISP/transit issue | Contact provider |
| Retrans в конце | Remote server issue | Contact remote admin |
| Sporadic retrans | Normal internet | Nothing |

### Zero Window

```bash
# Wireshark filter
tcp.analysis.zero_window

# Причина: receiver не успевает обрабатывать
# Application читает медленнее чем приходят данные
```

**Решения:**
- Увеличить receive buffer: `sysctl -w net.core.rmem_max=16777216`
- Оптимизировать application processing
- Проверить disk I/O (если пишет на диск)

---

## Case Study: Intermittent Failures

### Pattern Analysis

```bash
# 1. Время возникновения
# Периодические? → cron job, scheduled task
# Random? → race condition, resource exhaustion
# Under load? → capacity issue

# 2. Корреляция с метриками
# CPU spikes?
# Memory pressure?
# Disk I/O?
# Network traffic?

# 3. Автоматический мониторинг
while true; do
  curl -s -o /dev/null -w "%{http_code} %{time_total}\n" https://api.example.com/health
  sleep 5
done > health_log.txt
```

### Race Condition Detection

```bash
# 1. Thread dumps при проблеме
jstack PID > thread_dump_problem.txt

# 2. Сравнить с нормальным состоянием
diff thread_dump_normal.txt thread_dump_problem.txt

# 3. Искать deadlocks
grep -A 20 "deadlock" thread_dump_problem.txt
```

---

## Production Debugging Checklist

### Quick Triage (первые 5 минут)

```bash
# System health
uptime                    # Load average
free -h                   # Memory
df -h                     # Disk
top -bn1 | head -20       # CPU/processes

# Network health
ss -s                     # Socket summary
netstat -s | grep -i error  # Network errors

# Application health
curl -s localhost:8080/health  # Health endpoint
tail -100 /var/log/app/app.log  # Recent logs
```

### Deep Dive (если quick triage не помог)

```bash
# Network level
tcpdump -i any -c 1000 -w capture.pcap port 8080
mtr -r -c 100 backend-server

# DNS
dig +trace problematic-domain.com
dig @8.8.8.8 problematic-domain.com

# TLS
openssl s_client -connect host:443 -showcerts

# Connections
ss -tunapOe
lsof -i :8080
```

### Documentation Template

```markdown
## Incident: [Short description]

**Time:** YYYY-MM-DD HH:MM UTC
**Duration:** X hours
**Impact:** [What was affected]

### Symptoms
- [What users/systems observed]

### Timeline
- HH:MM — First alert
- HH:MM — Investigation started
- HH:MM — Root cause identified
- HH:MM — Fix applied
- HH:MM — Service restored

### Root Cause
[Technical description of the problem]

### Resolution
[What was done to fix]

### Prevention
- [Action items to prevent recurrence]
```

---

## Quick Reference

### Error → Likely Cause

| Symptom | First Check | Second Check |
|---------|-------------|--------------|
| Connection refused | Service running? | Firewall? |
| Connection timeout | Network path? | Security groups? |
| 502 Bad Gateway | Backend health? | Proxy config? |
| 503 Service Unavailable | Overloaded? | Rate limited? |
| 504 Gateway Timeout | Slow backend? | Timeout config? |
| SSL error | Cert expired? | Chain complete? |
| Slow response | DNS? | Network latency? | Backend? |

### Commands Cheat Sheet

```bash
# Connectivity
ping -c 4 host
mtr -r -c 50 host
telnet host port

# DNS
dig domain +short
dig @8.8.8.8 domain

# Ports/connections
ss -tuln
lsof -i :port
netstat -an | grep ESTABLISHED

# HTTP
curl -v -w "Time: %{time_total}s" url
curl -I url  # headers only

# TLS
openssl s_client -connect host:443 -showcerts

# Capture
tcpdump -i any -c 100 port 80 -w capture.pcap
```

---

## Ссылки и источники

### Методологии
- [CompTIA Troubleshooting Model](https://www.comptia.org/content/guides/a-guide-to-network-troubleshooting)
- [Network Troubleshooting Methodology](https://www.dnsstuff.com/network-troubleshooting-steps)

### Gateway Errors
- [502 Bad Gateway (MDN)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/502)
- [Azure App Gateway Troubleshooting](https://learn.microsoft.com/en-us/azure/application-gateway/application-gateway-troubleshooting-502)
- [Gateway Errors Guide (dev.to)](https://dev.to/yorgie7/understanding-gateway-errors-500-502-503-504-an-expert-end-to-end-guide-for-developers-2dp1)

### Connection Pools
- [Connection Pool Exhaustion](https://howtech.substack.com/p/connection-pool-exhaustion-the-silent)
- [TCP Socket Leak Debugging](https://hasura.io/blog/debugging-tcp-socket-leak-in-a-kubernetes-cluster-99171d3e654b)

### DNS
- [Slow DNS (Catchpoint)](https://www.catchpoint.com/dns-monitoring/slow-dns)
- [DNS Breaking Ways (Julia Evans)](https://jvns.ca/blog/2022/01/15/some-ways-dns-can-break/)

### TLS
- [SSL/TLS Debugging](https://maulwuff.de/research/ssl-debugging.html)
- [Incomplete Certificate Chains](https://www.svix.com/blog/ssl-tls-incomplete-certificate-chain/)

---

## Связанные материалы

### В этом разделе
→ [[network-debugging-basics]] — базовый troubleshooting
→ [[network-tcpdump-wireshark]] — packet analysis
→ [[network-tools-reference]] — справочник инструментов
→ [[network-performance-optimization]] — tuning

### Смежные темы
→ [[observability]] — мониторинг и алертинг
→ [[devops-incident-management]] — incident response

---

*Последнее обновление: 2026-01-09 — Добавлены педагогические секции: 5 аналогий (systematic troubleshooting как врач, hypothesis-driven как научный метод, isolation как бинарный поиск, MTU как грузовик в туннеле, intermittent как мигающая лампочка), 6 типичных ошибок advanced диагностики (хаотичная диагностика, игнорирование MTU, нет baseline, сеть vs приложение, DNS кэширование, нет distributed tracing), 5 ментальных моделей (scientific method, divide and conquer, timeline reconstruction, correlation vs causation, 5 Whys)*
