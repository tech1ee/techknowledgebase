---
title: "Сетевой стек ОС: Linux networking, сокеты, netfilter"
created: 2025-12-18
modified: 2025-12-18
type: concept
status: verified
confidence: high
sources_verified: true
tags:
  - networking/os
  - linux/kernel
  - networking/sockets
related:
  - "[[networking-overview]]"
  - "[[network-transport-layer]]"
  - "[[network-ip-routing]]"
  - "[[docker-for-developers]]"
  - "[[network-cloud-modern]]"
---

# Сетевой стек ОС: Linux networking, сокеты, netfilter

Socket — абстракция для сетевого I/O (файловый дескриптор). Netfilter — framework для фильтрации в ядре (iptables/nftables — userspace). sk_buff — структура ядра для пакета. eBPF — программируемость сети без модификации ядра.

---

## Prerequisites

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| **Linux basics** | Файловые дескрипторы, процессы | Linux fundamentals |
| **Сетевые основы** | TCP/IP, IP-адресация | [[network-fundamentals-for-developers]] |
| **C programming** | Для понимания syscalls (опционально) | C basics |

### Для кого этот материал

| Уровень | Подходит? | Рекомендация |
|---------|-----------|--------------|
| **Новичок** | ⚠️ Сложно | Сначала сетевые основы |
| **Intermediate** | ✅ Да | Основная аудитория |
| **Advanced** | ✅ Да | eBPF, XDP |

### Терминология для новичков

> 💡 **Socket** = "розетка" для сетевого подключения. Программа создаёт сокет чтобы отправлять/получать данные по сети.

| Термин | Значение | Аналогия для новичка |
|--------|----------|---------------------|
| **Socket** | Endpoint для сетевого соединения | **Телефонная трубка** — через неё говоришь |
| **File Descriptor** | Числовой ID открытого ресурса | **Номер в очереди** — ссылка на ресурс |
| **Netfilter** | Kernel framework для обработки пакетов | **Охрана на входе** — проверяет каждого |
| **iptables** | Утилита для настройки firewall | **Список правил для охраны** |
| **Network Namespace** | Изолированный сетевой стек | **Своя квартира** — своя сеть |
| **eBPF** | Программируемость ядра без перекомпиляции | **Плагин для ядра** |
| **XDP** | Express Data Path — быстрая обработка | **Экспресс-проверка на входе** |
| **veth** | Virtual Ethernet pair | **Виртуальный кабель** |
| **sk_buff** | Структура пакета в ядре | **Конверт с данными** внутри ядра |
| **bind** | Привязать сокет к адресу/порту | **Занять номер телефона** |

---

## Терминология

| Термин | Значение |
|--------|----------|
| **Socket** | Endpoint для сетевого соединения |
| **File descriptor** | Числовой идентификатор открытого ресурса |
| **sk_buff** | Socket buffer — структура пакета в ядре |
| **Netfilter** | Kernel framework для обработки пакетов |
| **iptables** | Userspace утилита для Netfilter |
| **nftables** | Современная замена iptables |
| **eBPF** | Extended Berkeley Packet Filter |
| **XDP** | eXpress Data Path (eBPF на NIC) |
| **Network namespace** | Изоляция сетевого стека |
| **veth** | Virtual Ethernet pair |

---

## Часть 1: Интуиция без кода

> 🎯 **Цель секции:** Понять сетевой стек ОС через знакомые аналогии, прежде чем погружаться в системные вызовы и структуры ядра.

### Аналогия 1: Socket — Телефонная розетка

**Путаница:** "Что такое сокет? Это порт? Соединение?"

```
АНАЛОГИЯ: Socket = Телефонный аппарат с розеткой

┌─────────────────────────────────────────────────────────────┐
│                      КВАРТИРА (Процесс)                      │
│                                                              │
│   ┌─────────┐     ┌─────────┐     ┌─────────┐              │
│   │Телефон 1│     │Телефон 2│     │Телефон 3│              │
│   │ fd = 3  │     │ fd = 4  │     │ fd = 5  │              │
│   └────┬────┘     └────┬────┘     └────┬────┘              │
│        │               │               │                    │
│   Розетка          Розетка         Розетка                  │
│   :8080            :8443           :3306                    │
│        │               │               │                    │
└────────┼───────────────┼───────────────┼────────────────────┘
         │               │               │
    ═════╧═══════════════╧═══════════════╧═════  (Сеть)

Socket = (IP:Port, Protocol)
File Descriptor = номер "телефона" в квартире
```

**Жизненный цикл сокета:**
| Этап | Системный вызов | Аналогия |
|------|-----------------|----------|
| Создать | `socket()` | Купить телефон |
| Привязать к номеру | `bind()` | Подключить к розетке, получить номер |
| Слушать | `listen()` | Ждать звонков |
| Принять звонок | `accept()` | Снять трубку |
| Позвонить | `connect()` | Набрать номер |
| Разговаривать | `send()/recv()` | Говорить/слушать |
| Повесить трубку | `close()` | Положить трубку |

**Ключевой инсайт:**
> "Сокет — это файловый дескриптор. Любой `read()` или `write()` работает с сокетом так же, как с файлом. Unix философия: всё есть файл!"

---

### Аналогия 2: Путь пакета — Почтовое отделение

**Проблема:** Как пакет проходит через ядро?

```
ПУТЬ ВХОДЯЩЕГО ПАКЕТА (Incoming)

         Интернет
            │
            ▼
    ┌───────────────┐
    │      NIC      │  Сетевая карта получила "конверт"
    │  (Почтальон)  │
    └───────┬───────┘
            │ DMA копирует в память
            ▼
    ┌───────────────┐
    │  Ring Buffer  │  Очередь писем в почтовом ящике
    │   (sk_buff)   │
    └───────┬───────┘
            │ Прерывание → NAPI
            ▼
    ┌───────────────┐
    │   Driver      │  Почтальон передаёт в сортировку
    │  (XDP hook)   │  ← Можно отбросить тут! (самый быстрый)
    └───────┬───────┘
            │
            ▼
    ┌───────────────┐
    │   L2 (MAC)    │  Проверка: это наш адрес?
    │   Ethernet    │
    └───────┬───────┘
            │
            ▼
    ┌───────────────┐
    │   L3 (IP)     │  Кому доставить? (routing table)
    │  Netfilter    │  ← Охрана проверяет (iptables)
    │  PREROUTING   │
    └───────┬───────┘
            │
    ┌───────┴───────┐
    │ LOCAL?  FORWARD? │  Это нам или пересылаем?
    └───────┬───────┘
            │ (если LOCAL)
            ▼
    ┌───────────────┐
    │   L4 (TCP)    │  Распаковка: какому разговору?
    │   UDP/SCTP    │
    └───────┬───────┘
            │
            ▼
    ┌───────────────┐
    │ Socket Buffer │  Положить в ящик получателя
    └───────┬───────┘
            │ read() / recv()
            ▼
    ┌───────────────┐
    │  Application  │  Наконец-то приложение читает!
    └───────────────┘
```

**Ключевой инсайт:**
> Чем раньше отбросить плохой пакет — тем лучше для производительности. XDP отбрасывает ДО создания sk_buff (дорогая операция!), iptables — уже после.

---

### Аналогия 3: Netfilter/iptables — Охрана с чек-листом

**Проблема:** Как работает firewall в Linux?

```
NETFILTER = Охрана на 5 КПП (hooks)

                        Пакет пришёл
                              │
                              ▼
                    ┌─────────────────┐
                    │   PREROUTING    │  КПП #1: На входе в здание
                    │  (raw, mangle,  │  "Кто такой? Куда?"
                    │      nat)       │
                    └────────┬────────┘
                             │
              ┌──────────────┴──────────────┐
              │ Это нам?                    │ Пересылаем?
              ▼                             ▼
    ┌─────────────────┐           ┌─────────────────┐
    │     INPUT       │           │    FORWARD      │  КПП #3
    │  (filter, nat)  │  КПП #2   │    (filter)     │  Транзитный
    └────────┬────────┘           └────────┬────────┘
             │                             │
             ▼                             │
    ┌─────────────────┐                    │
    │   Local Process │                    │
    │   (приложение)  │                    │
    └────────┬────────┘                    │
             │                             │
             ▼                             │
    ┌─────────────────┐                    │
    │    OUTPUT       │  КПП #4            │
    │(filter, nat,mangle)│ Исходящий       │
    └────────┬────────┘                    │
             │                             │
             └──────────────┬──────────────┘
                            │
                            ▼
                  ┌─────────────────┐
                  │  POSTROUTING    │  КПП #5: На выходе
                  │   (nat, mangle) │  SNAT/MASQUERADE тут!
                  └─────────────────┘
                            │
                            ▼
                        Пакет ушёл
```

**Tables в Netfilter:**
| Table | Назначение | Аналогия |
|-------|-----------|----------|
| **filter** | DROP/ACCEPT пакеты | Охранник: пустить или нет |
| **nat** | Менять src/dst IP:port | Ресепшн: переадресация звонков |
| **mangle** | Менять TTL, TOS, mark | Штамповщик: метки на конвертах |
| **raw** | До connection tracking | VIP-вход без очереди |

**Порядок правил:**
> "Первое совпавшее правило побеждает. Порядок важен! Если забыл ACCEPT перед DROP all — заблокируешь себя."

---

### Аналогия 4: Network Namespace — Квартиры в доме

**Проблема:** Как Docker изолирует сети контейнеров?

```
БЕЗ NAMESPACES:
┌─────────────────────────────────────────────────────────────┐
│                    ОДИН СЕТЕВОЙ СТЕК                        │
│                                                              │
│   App1 :80     App2 :80     ← КОНФЛИКТ! Оба хотят :80       │
│        │            │                                        │
│        └────────────┴──────── eth0: 192.168.1.100           │
└─────────────────────────────────────────────────────────────┘

С NETWORK NAMESPACES (Docker):
┌─────────────────────────────────────────────────────────────┐
│  HOST NAMESPACE (Default)                                    │
│                                                              │
│                    docker0 (bridge)                          │
│                    172.17.0.1                                │
│                        │                                     │
│            ┌──────────┴──────────┐                          │
│            │                     │                          │
│       veth-a                veth-b                          │
│            │                     │                          │
├────────────┼─────────────────────┼──────────────────────────┤
│  NS: container1 │          │  NS: container2 │              │
│  ┌─────────────┐│          │┌─────────────┐  │              │
│  │ eth0        ││          ││ eth0        │  │              │
│  │ 172.17.0.2  ││          ││ 172.17.0.3  │  │              │
│  │             ││          ││             │  │              │
│  │ App :80  ✓  ││          ││ App :80  ✓  │  │ ← ОБА :80 OK!│
│  └─────────────┘│          │└─────────────┘  │              │
└─────────────────┴──────────┴─────────────────┴──────────────┘
```

**Компоненты изоляции:**
| Компонент | Что изолирует | Аналогия |
|-----------|--------------|----------|
| Network NS | Сетевой стек, интерфейсы, routing | Своя квартира с сетью |
| veth pair | Виртуальный кабель между NS | Провод между квартирами |
| bridge | Коммутатор между veth | Домовой хаб |
| iptables NAT | Выход в интернет | Консьерж пересылает |

**Ключевой инсайт:**
> "Каждый контейнер думает, что у него свой компьютер с сетью. Network namespace — это граница иллюзии."

---

### Аналогия 5: eBPF/XDP — Программируемая охрана

**Проблема:** Почему eBPF — это революция?

```
ТРАДИЦИОННЫЙ ПОДХОД (iptables):
┌──────────────────────────────────────────────────────────────┐
│                                                               │
│  Правило 1: if src=1.2.3.4 DROP        ← Жёстко закодировано │
│  Правило 2: if dst_port=22 ACCEPT      ← Только простые      │
│  Правило 3: if ... DROP                   условия            │
│                                                               │
│  ❌ Нельзя: "DROP если >100 пакетов/сек с одного IP"         │
│  ❌ Нельзя: "Логировать паттерн DDoS атаки"                  │
│  ❌ Нельзя: кастомная логика без перекомпиляции ядра         │
└──────────────────────────────────────────────────────────────┘

eBPF ПОДХОД:
┌──────────────────────────────────────────────────────────────┐
│                                                               │
│  eBPF программа (загружается динамически):                   │
│                                                               │
│  int xdp_firewall(struct xdp_md *ctx) {                      │
│      // ЛЮБАЯ ЛОГИКА!                                        │
│      struct rate_limit *rl = bpf_map_lookup(&limits, &src);  │
│      if (rl && rl->pps > 100) {                              │
│          bpf_map_update(&blocked, &src, &now);               │
│          return XDP_DROP;  // Отбросить ДО создания sk_buff!│
│      }                                                        │
│      return XDP_PASS;                                         │
│  }                                                            │
│                                                               │
│  ✅ Кастомная логика                                         │
│  ✅ Карты (maps) для состояния                               │
│  ✅ Работает на скорости ядра                                │
│  ✅ Без перекомпиляции ядра                                  │
└──────────────────────────────────────────────────────────────┘
```

**Где eBPF может подключиться:**
```
                          User Space
                              │
═════════════════════════════════════════════════════
                              │
                        Kernel Space
                              │
                    ┌─────────┴─────────┐
                    │                   │
              ┌─────▼─────┐       ┌─────▼─────┐
              │   XDP     │       │   TC      │
              │ (ingress) │       │ (egress)  │
              │ 🚀 Fastest│       │           │
              └─────┬─────┘       └─────┬─────┘
                    │                   │
              ┌─────▼─────────────────▼─────┐
              │         Socket Layer         │
              │    (socket filters, etc)     │
              └──────────────────────────────┘
```

**Ключевой инсайт:**
> "eBPF = JavaScript для ядра. Ты пишешь код, загружаешь без перезагрузки, и ядро выполняет его на wire speed. XDP — это eBPF на максимальной скорости (до 10M pps)."

---

## Часть 2: Почему это сложно

> ⚠️ **Цель секции:** Разобрать типичные ошибки при работе с сетевым стеком Linux.

### Ошибка 1: Забыли bind() и получаем EADDRINUSE

**СИМПТОМ:**
```
bind: Address already in use
# Но netstat показывает TIME_WAIT, а не LISTEN!
```

**Проблема:**
```
Сервер упал → socket в TIME_WAIT (2MSL = ~60 секунд)
Перезапуск → bind() на тот же порт → EADDRINUSE!

netstat -tlnp | grep 8080
tcp  0  0  0.0.0.0:8080  0.0.0.0:*  TIME_WAIT  -

TIME_WAIT — это нормально! TCP ждёт потерянные пакеты.
Но это мешает быстрому перезапуску сервера.
```

**РЕШЕНИЕ:**
```c
// Перед bind() установить SO_REUSEADDR
int opt = 1;
setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

// Теперь bind() успешен даже если порт в TIME_WAIT
bind(sockfd, (struct sockaddr *)&addr, sizeof(addr));
```

```go
// Go: автоматически для net.Listen, но для raw:
syscall.SetsockoptInt(fd, syscall.SOL_SOCKET, syscall.SO_REUSEADDR, 1)
```

---

### Ошибка 2: iptables правила в неправильном порядке

**СИМПТОМ:**
```bash
# Добавили правило, но трафик всё равно блокируется
iptables -A INPUT -p tcp --dport 443 -j ACCEPT
# Не работает! Почему?
```

**Проблема:**
```bash
# Смотрим все правила:
iptables -L INPUT -n --line-numbers

Chain INPUT (policy ACCEPT)
num  target  prot  source    destination
1    DROP    all   0.0.0.0/0 0.0.0.0/0      ← DROP ALL первым!
2    ACCEPT  tcp   0.0.0.0/0 0.0.0.0/0  dport 443  ← Никогда не достигнем!

# Правило 1 срабатывает раньше, DROP побеждает.
```

**РЕШЕНИЕ:**
```bash
# Вставить ПЕРЕД DROP, а не после
iptables -I INPUT 1 -p tcp --dport 443 -j ACCEPT

# Или удалить и добавить в правильном порядке
iptables -D INPUT 1  # Удалить DROP
iptables -A INPUT -p tcp --dport 443 -j ACCEPT
iptables -A INPUT -j DROP  # DROP в конце!

# Проверяем:
iptables -L INPUT -n --line-numbers
1    ACCEPT  tcp   0.0.0.0/0 0.0.0.0/0  dport 443  ✓
2    DROP    all   0.0.0.0/0 0.0.0.0/0             ✓
```

---

### Ошибка 3: Заблокировал себе SSH (локаут)

**СИМПТОМ:**
```
Выполнил iptables -P INPUT DROP
Соединение SSH оборвалось
Теперь не могу подключиться к серверу! 💀
```

**Проблема:**
```bash
# Опасная последовательность:
iptables -P INPUT DROP      # Установили политику DROP
# ЗАБЫЛИ добавить ACCEPT для SSH!
# Текущее SSH соединение умирает (или следующий пакет дропается)
# Всё, сервер недоступен.
```

**РЕШЕНИЕ (ДО того как применять):**
```bash
# 1. СНАЧАЛА разрешаем SSH и established
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -p tcp --dport 22 -j ACCEPT

# 2. ТОЛЬКО ПОТОМ меняем политику
iptables -P INPUT DROP

# Или используем at для автоматического сброса:
echo "iptables -P INPUT ACCEPT" | at now + 5 minutes
# Если всё хорошо — отменяем: atrm <job_id>

# Или используем iptables-apply (ждёт подтверждения):
iptables-apply /etc/iptables.rules
# Если не подтвердишь за 10 сек — откатит!
```

---

### Ошибка 4: XDP в generic mode вместо native

**СИМПТОМ:**
```
XDP программа работает, но производительность не улучшилась.
Ожидали 10M pps, получили 500K pps.
```

**Проблема:**
```bash
# Загрузили XDP программу:
ip link set dev eth0 xdp obj prog.o

# Но проверяем режим:
ip link show eth0
# ... xdpgeneric ...  ← GENERIC MODE!

# Generic mode = sk_buff уже создан, преимущество потеряно!
# Это режим совместимости для драйверов без поддержки XDP.
```

**РЕШЕНИЕ:**
```bash
# Проверить поддержку драйвером:
ethtool -i eth0 | grep driver
# driver: mlx5_core  ← Поддерживает native XDP

# Загрузить в native mode:
ip link set dev eth0 xdp obj prog.o
# или явно:
ip link set dev eth0 xdpdrv obj prog.o

# Проверяем:
ip link show eth0
# ... xdpdrv/id:42 ...  ← NATIVE MODE! ✓

# Если драйвер не поддерживает — увидим ошибку.
# Популярные драйверы с native XDP: mlx5, i40e, ixgbe, virtio_net
```

---

### Ошибка 5: Не включили ip_forward для роутинга

**СИМПТОМ:**
```
Настроил NAT, добавил правила iptables MASQUERADE.
Но трафик между сетями не ходит!
```

**Проблема:**
```bash
# По умолчанию Linux не пересылает пакеты между интерфейсами!
cat /proc/sys/net/ipv4/ip_forward
0  ← Выключено!

# Пакет приходит на eth0, должен уйти на eth1.
# Ядро: "Это не мне? DROP."
```

**РЕШЕНИЕ:**
```bash
# Временно (до перезагрузки):
echo 1 > /proc/sys/net/ipv4/ip_forward

# Постоянно:
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
sysctl -p

# Для IPv6:
echo "net.ipv6.conf.all.forwarding = 1" >> /etc/sysctl.conf
sysctl -p

# Проверяем:
sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 1  ✓
```

---

### Ошибка 6: Утечка file descriptors сокетов

**СИМПТОМ:**
```
Too many open files
Сервер перестаёт принимать соединения после N клиентов.
```

**Проблема:**
```c
// Забыли close() при ошибке:
int client_fd = accept(server_fd, ...);
if (handle_client(client_fd) < 0) {
    return -1;  // 💀 client_fd не закрыт!
}
close(client_fd);

// Или в Go: defer не сработал из-за паники
func handleConn(conn net.Conn) {
    defer conn.Close()  // Не сработает если паника до defer!
    // ...
}
```

**РЕШЕНИЕ:**
```bash
# Диагностика:
lsof -p <pid> | wc -l           # Сколько fd у процесса
cat /proc/<pid>/limits | grep "open files"  # Лимит

# Увеличить лимит (временно):
ulimit -n 65535

# Постоянно (в /etc/security/limits.conf):
*  soft  nofile  65535
*  hard  nofile  65535
```

```go
// Правильно в Go — сначала defer, потом ошибки:
func handleConn(conn net.Conn) {
    defer func() {
        if err := conn.Close(); err != nil {
            log.Printf("close error: %v", err)
        }
    }()
    // Теперь defer сработает даже при панике
}
```

---

## Часть 3: Ментальные модели

> 🧠 **Цель секции:** Дать системные способы думать о сетевом стеке ОС.

### Модель 1: Иерархия скорости обработки пакетов

**Принцип:** Чем раньше обработать — тем быстрее. Но чем раньше — тем меньше информации.

```
ПИРАМИДА СКОРОСТИ (pps = packets per second)

                    ▲ Больше информации
                    │
           ┌────────┴────────┐
           │   Application   │  ~100K pps
           │   (userspace)   │  Полный контроль, но медленно
           │   recv()/send() │
           └────────┬────────┘
                    │
           ┌────────┴────────┐
           │  Socket Filter  │  ~500K pps
           │    (SO_ATTACH)  │  Можно фильтровать до приложения
           └────────┬────────┘
                    │
           ┌────────┴────────┐
           │   TC (Traffic   │  ~1M pps
           │    Control)     │  Ingress + Egress
           │  eBPF/cls_bpf   │
           └────────┬────────┘
                    │
           ┌────────┴────────┐
           │   iptables/     │  ~2M pps
           │   nftables      │  L3/L4 фильтрация
           │   (Netfilter)   │
           └────────┬────────┘
                    │
           ┌────────┴────────┐
           │      XDP        │  ~10-20M pps 🚀
           │ (eXpress Data   │  До создания sk_buff!
           │     Path)       │  Только ingress
           └────────┬────────┘
                    │
           ┌────────┴────────┐
           │   Hardware      │  ~100M+ pps
           │   Offload       │  SmartNIC (Память, Netronome)
           │                 │  Минимум гибкости
           └─────────────────┘
                    │
                    ▼ Больше скорости
```

**Когда что использовать:**
| Задача | Инструмент | Почему |
|--------|-----------|--------|
| Простой firewall | iptables/nftables | Достаточно быстро, понятно |
| DDoS mitigation | XDP | Нужна скорость, drop плохих |
| Traffic shaping | TC | Работает на egress |
| Deep inspection | Application | Нужен весь payload |

---

### Модель 2: File Descriptor как универсальный интерфейс

**Принцип:** В Unix всё — файл. Сокеты, pipes, устройства — всё через fd.

```
ЕДИНЫЙ ИНТЕРФЕЙС I/O

┌─────────────────────────────────────────────────────────────┐
│                        Application                           │
│                                                              │
│   read(fd, buf, n)     write(fd, buf, n)     close(fd)      │
│                                                              │
└──────────────────────────┬───────────────────────────────────┘
                           │
          ┌────────────────┼────────────────┐
          │                │                │
    ┌─────▼─────┐    ┌─────▼─────┐    ┌─────▼─────┐
    │  fd = 3   │    │  fd = 4   │    │  fd = 5   │
    │  Socket   │    │  File     │    │  Pipe     │
    │ TCP:8080  │    │ /var/log  │    │ stdin→    │
    └───────────┘    └───────────┘    └───────────┘

Преимущества:
• epoll()/select() работает со ВСЕМИ типами fd
• Один код для разных источников I/O
• Передача fd между процессами (Unix sockets)
```

**Практическое применение:**
```c
// Один event loop для всего:
int epfd = epoll_create1(0);

// Добавляем сокет
epoll_ctl(epfd, EPOLL_CTL_ADD, socket_fd, &ev);

// Добавляем файл (inotify)
epoll_ctl(epfd, EPOLL_CTL_ADD, inotify_fd, &ev);

// Добавляем таймер
epoll_ctl(epfd, EPOLL_CTL_ADD, timer_fd, &ev);

// Один wait для всех!
while (1) {
    int n = epoll_wait(epfd, events, MAX_EVENTS, -1);
    for (int i = 0; i < n; i++) {
        if (events[i].data.fd == socket_fd) handle_network();
        if (events[i].data.fd == inotify_fd) handle_file();
        if (events[i].data.fd == timer_fd) handle_timer();
    }
}
```

---

### Модель 3: Netfilter как конвейер с КПП

**Принцип:** Пакет проходит через цепочку hooks. Каждый hook может DROP/ACCEPT/изменить пакет.

```
МАТРИЦА: ЧТО ГДЕ ПРОИСХОДИТ

                    Входящий         Транзитный        Исходящий
                    (к нам)          (через нас)       (от нас)
                       │                  │                │
    PREROUTING ────────┼──────────────────┼────────────────┘
    (nat, mangle)      │                  │
                       │                  │
                       ▼                  ▼
                   ┌───────┐          ┌───────┐
    INPUT ─────────│ LOCAL │          │FORWARD│──────────────┐
    (filter)       │  ?    │          │       │              │
                   └───┬───┘          └───────┘              │
                       │                                     │
                       ▼                                     │
                  Application                                │
                       │                                     │
                       ▼                                     │
    OUTPUT ────────────┼─────────────────────────────────────┤
    (filter, nat)      │                                     │
                       │                                     │
    POSTROUTING ───────┴─────────────────────────────────────┘
    (nat, mangle)

ТИПИЧНЫЕ СЦЕНАРИИ:
┌────────────────────────────────────────────────────────────┐
│ Сценарий              │ Hooks                              │
├────────────────────────────────────────────────────────────┤
│ Firewall (блокировка) │ INPUT, FORWARD, OUTPUT (filter)    │
│ DNAT (port forward)   │ PREROUTING (nat)                   │
│ SNAT/Masquerade       │ POSTROUTING (nat)                  │
│ Изменить TTL          │ mangle (любой hook)                │
│ Маркировка пакетов    │ mangle → policy routing            │
└────────────────────────────────────────────────────────────┘
```

---

### Модель 4: Namespace как виртуализация на уровне ядра

**Принцип:** Namespace = иллюзия собственной системы без накладных расходов VM.

```
СРАВНЕНИЕ ИЗОЛЯЦИИ

ВИРТУАЛЬНАЯ МАШИНА:
┌─────────────────────────────────────────────────────────────┐
│  Host OS                                                     │
│  ┌─────────────────┐  ┌─────────────────┐                   │
│  │      VM 1       │  │      VM 2       │                   │
│  │  ┌───────────┐  │  │  ┌───────────┐  │                   │
│  │  │ Guest OS  │  │  │  │ Guest OS  │  │  ← Целая ОС!     │
│  │  │ (kernel)  │  │  │  │ (kernel)  │  │                   │
│  │  └───────────┘  │  │  └───────────┘  │                   │
│  │  App            │  │  App            │                   │
│  └─────────────────┘  └─────────────────┘                   │
│  Hypervisor                                                  │
└─────────────────────────────────────────────────────────────┘
Overhead: Гигабайты RAM, секунды на старт

КОНТЕЙНЕРЫ (Namespaces):
┌─────────────────────────────────────────────────────────────┐
│  Host OS (один kernel для всех!)                            │
│                                                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐                   │
│  │Container1│  │Container2│  │Container3│                   │
│  │  App     │  │  App     │  │  App     │  ← Только app    │
│  │          │  │          │  │          │                   │
│  │ NS: net, │  │ NS: net, │  │ NS: net, │  ← Изоляция через│
│  │ pid, mnt │  │ pid, mnt │  │ pid, mnt │     namespaces   │
│  └──────────┘  └──────────┘  └──────────┘                   │
│                                                              │
│  Shared Kernel                                               │
└─────────────────────────────────────────────────────────────┘
Overhead: Мегабайты RAM, миллисекунды на старт
```

**Типы Linux Namespaces:**
| Namespace | Что изолирует | Пример использования |
|-----------|--------------|---------------------|
| **net** | Сетевые интерфейсы, routing, iptables | Своя сеть в контейнере |
| **pid** | Process IDs | PID 1 в контейнере |
| **mnt** | Mount points | Свой /etc, /var |
| **user** | UID/GID | root в контейнере ≠ root на хосте |
| **uts** | Hostname | Своё имя хоста |
| **ipc** | IPC resources | Изоляция shared memory |

---

### Модель 5: sk_buff как "конверт" в ядре

**Принцип:** sk_buff — это структура, которая "обёртывает" пакет и несёт метаданные по всему стеку.

```
sk_buff (Socket Buffer) — СТРУКТУРА ПАКЕТА В ЯДРЕ

┌─────────────────────────────────────────────────────────────┐
│                        sk_buff                               │
├─────────────────────────────────────────────────────────────┤
│  Метаданные (дешёвые операции):                             │
│  ┌─────────────────────────────────────────────────────────┐│
│  │ • dev: указатель на net_device (откуда пришёл)          ││
│  │ • protocol: ETH_P_IP, ETH_P_IPV6...                     ││
│  │ • mark: метка для policy routing                        ││
│  │ • priority: QoS                                          ││
│  │ • timestamp: когда получен                               ││
│  │ • destructor: callback при освобождении                 ││
│  └─────────────────────────────────────────────────────────┘│
│                                                              │
│  Указатели на данные (zero-copy где возможно):              │
│  ┌─────────────────────────────────────────────────────────┐│
│  │  head ──► [headroom]                                    ││
│  │  data ──► [MAC header][IP header][TCP header][payload]  ││
│  │  tail ──► [tailroom]                                    ││
│  │  end  ──►                                               ││
│  └─────────────────────────────────────────────────────────┘│
│                                                              │
│  Почему headroom/tailroom?                                  │
│  • Добавление/удаление заголовков без копирования!          │
│  • Encapsulation: просто сдвигаем data назад               │
│  • Decapsulation: сдвигаем data вперёд                     │
└─────────────────────────────────────────────────────────────┘

ОПЕРАЦИИ (O(1), без копирования данных):
┌────────────────────────────────────────────────────────────┐
│ skb_push(skb, len)   — добавить заголовок (data -= len)    │
│ skb_pull(skb, len)   — убрать заголовок (data += len)      │
│ skb_put(skb, len)    — добавить данные в конец             │
│ skb_trim(skb, len)   — обрезать хвост                      │
└────────────────────────────────────────────────────────────┘
```

**Почему XDP быстрее:**
```
ТРАДИЦИОННЫЙ ПУТЬ:
  NIC → DMA → Driver → alloc_skb() ← ДОРОГО! (~200 циклов)
                          ↓
                       sk_buff
                          ↓
                    Network stack

XDP ПУТЬ:
  NIC → DMA → Driver → XDP program → DROP/PASS/REDIRECT
                       (xdp_buff — легче чем sk_buff!)

  sk_buff создаётся ТОЛЬКО если XDP_PASS!
```

---

## Linux Network Stack

### Почему всё через файловые дескрипторы?

Unix философия: "всё есть файл". Сокет — это файловый дескриптор, как и обычный файл. Это позволяет:
- Использовать одни и те же системные вызовы (`read`, `write`, `close`) для файлов, сокетов, pipes
- Передавать сокеты между процессами как файловые дескрипторы
- Использовать `select`/`poll`/`epoll` для мультиплексирования любых I/O источников

**Путь пакета через стек:** NIC прерывание → драйвер создаёт `sk_buff` → L2 (ethernet) → L3 (IP, routing decision, netfilter) → L4 (TCP/UDP) → socket buffer → приложение через `read()`.

```
┌─────────────────────────────────────────────────────────────────┐
│                  LINUX NETWORK STACK                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  USER SPACE                                                      │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Application (curl, nginx, Chrome...)                        ││
│  │  ┌─────────────────────────────────────────────────────────┐││
│  │  │  libc (glibc, musl)                                     │││
│  │  │  socket(), bind(), listen(), accept(), connect()        │││
│  │  │  send(), recv(), read(), write()                        │││
│  │  └─────────────────────────────────────────────────────────┘││
│  └─────────────────────────────────────────────────────────────┘│
│                          │ System calls                          │
│  ========================│=======================================│
│                          ▼                                       │
│  KERNEL SPACE                                                    │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Socket Layer                                                ││
│  │  ┌─────────────────────────────────────────────────────────┐││
│  │  │  struct socket, struct sock                             │││
│  │  │  Protocol families: AF_INET, AF_INET6, AF_UNIX         │││
│  │  └─────────────────────────────────────────────────────────┘││
│  └─────────────────────────────────────────────────────────────┘│
│                          │                                       │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Transport Layer (L4)                                        ││
│  │  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐   ││
│  │  │     TCP       │  │     UDP       │  │    SCTP       │   ││
│  │  │  tcp_v4_rcv() │  │  udp_rcv()    │  │               │   ││
│  │  └───────────────┘  └───────────────┘  └───────────────┘   ││
│  └─────────────────────────────────────────────────────────────┘│
│                          │                                       │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Network Layer (L3)                                          ││
│  │  ┌───────────────┐  ┌───────────────┐                       ││
│  │  │     IPv4      │  │     IPv6      │                       ││
│  │  │   ip_rcv()    │  │  ipv6_rcv()   │                       ││
│  │  │   ip_output() │  │               │                       ││
│  │  └───────────────┘  └───────────────┘                       ││
│  │  Routing: ip_route_input(), ip_route_output()               ││
│  │  Netfilter hooks: PREROUTING, FORWARD, POSTROUTING          ││
│  └─────────────────────────────────────────────────────────────┘│
│                          │                                       │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Link Layer (L2)                                             ││
│  │  ┌───────────────────────────────────────────────────────┐  ││
│  │  │  net_device structure                                  │  ││
│  │  │  dev_queue_xmit(), netif_receive_skb()                │  ││
│  │  │  Bridge, VLAN, Bonding                                 │  ││
│  │  └───────────────────────────────────────────────────────┘  ││
│  └─────────────────────────────────────────────────────────────┘│
│                          │                                       │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Device Driver                                               ││
│  │  ┌───────────────────────────────────────────────────────┐  ││
│  │  │  NIC Driver (e1000, ixgbe, mlx5...)                   │  ││
│  │  │  NAPI (New API) for interrupt coalescing              │  ││
│  │  │  XDP hook (before sk_buff allocation)                 │  ││
│  │  └───────────────────────────────────────────────────────┘  ││
│  └─────────────────────────────────────────────────────────────┘│
│                          │                                       │
│  ========================│=======================================│
│                          ▼                                       │
│  HARDWARE (NIC)                                                  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### sk_buff структура

**Почему sk_buff, а не просто массив байт?**

Обработка пакета требует добавления/удаления заголовков на разных уровнях. Наивный подход — копировать данные при каждом добавлении заголовка. `sk_buff` решает это через указатели: `head`, `data`, `tail`, `end`. Добавление заголовка — это просто сдвиг указателя `data` назад (`skb_push`), без копирования payload.

**Headroom/Tailroom:** При создании `sk_buff` резервируется место спереди (для будущих заголовков) и сзади (для padding). Это позволяет проходить весь стек без reallocations.

```
┌─────────────────────────────────────────────────────────────────┐
│                    sk_buff (socket buffer)                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  struct sk_buff — основная структура для пакетов в Linux        │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │                    sk_buff metadata                          ││
│  │  ┌────────────────────────────────────────────────────────┐ ││
│  │  │ *next, *prev      — linked list                        │ ││
│  │  │ *sk               — owning socket                      │ ││
│  │  │ *dev              — network device                     │ ││
│  │  │ len               — data length                        │ ││
│  │  │ protocol          — L3 protocol                        │ ││
│  │  │ pkt_type          — packet class                       │ ││
│  │  │ *head, *data      — buffer pointers                    │ ││
│  │  │ *tail, *end       — buffer limits                      │ ││
│  │  └────────────────────────────────────────────────────────┘ ││
│  └─────────────────────────────────────────────────────────────┘│
│                                                                  │
│  Buffer layout:                                                  │
│                                                                  │
│  head ──► ┌──────────────────────┐                              │
│           │     headroom         │  ← skb_reserve()             │
│  data ──► ├──────────────────────┤                              │
│           │                      │                              │
│           │    packet data       │  ← skb_put() extends         │
│           │                      │                              │
│  tail ──► ├──────────────────────┤                              │
│           │     tailroom         │                              │
│  end  ──► └──────────────────────┘                              │
│                                                                  │
│  Операции:                                                       │
│  • skb_reserve(skb, len)  — резервирует headroom                │
│  • skb_put(skb, len)      — добавляет данные в конец            │
│  • skb_push(skb, len)     — добавляет header спереди            │
│  • skb_pull(skb, len)     — убирает header спереди              │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Sockets API

### Почему API такой низкоуровневый?

Berkeley Sockets (1983) создавались для C на Unix. Цель — минимальная абстракция над TCP/IP. Каждый системный вызов — одна операция в протоколе:
- `socket()` — создать endpoint
- `bind()` — привязать к локальному адресу
- `listen()` — перевести в режим ожидания (только TCP)
- `accept()` — принять входящее соединение
- `connect()` — инициировать соединение

Современные языки (Go, Python, Node.js) оборачивают это в высокоуровневые абстракции, но под капотом всё те же системные вызовы.

### Системные вызовы

```c
// Основные socket системные вызовы

// 1. Создание сокета
int socket(int domain, int type, int protocol);
// domain: AF_INET (IPv4), AF_INET6 (IPv6), AF_UNIX (local)
// type: SOCK_STREAM (TCP), SOCK_DGRAM (UDP), SOCK_RAW
// protocol: обычно 0 (автовыбор)

// 2. Привязка к адресу (сервер)
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

// 3. Прослушивание (TCP сервер)
int listen(int sockfd, int backlog);
// backlog: размер очереди pending connections

// 4. Принятие соединения (TCP сервер)
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
// Блокирует до нового соединения
// Возвращает НОВЫЙ socket для клиента

// 5. Подключение (клиент)
int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

// 6. Отправка/получение
ssize_t send(int sockfd, const void *buf, size_t len, int flags);
ssize_t recv(int sockfd, void *buf, size_t len, int flags);

// Или универсальные read/write:
ssize_t read(int fd, void *buf, size_t count);
ssize_t write(int fd, const void *buf, size_t count);

// 7. UDP специфичные (без connect)
ssize_t sendto(int sockfd, const void *buf, size_t len, int flags,
               const struct sockaddr *dest_addr, socklen_t addrlen);
ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags,
                 struct sockaddr *src_addr, socklen_t *addrlen);

// 8. Закрытие
int close(int fd);
int shutdown(int sockfd, int how);
// how: SHUT_RD, SHUT_WR, SHUT_RDWR
```

### TCP Server Example (C)

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define PORT 8080
#define BUFFER_SIZE 1024

int main() {
    int server_fd, client_fd;
    struct sockaddr_in address;
    socklen_t addrlen = sizeof(address);
    char buffer[BUFFER_SIZE] = {0};

    // 1. Create socket
    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd < 0) {
        perror("socket failed");
        exit(EXIT_FAILURE);
    }

    // Socket options (reuse address)
    int opt = 1;
    setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // 2. Bind
    address.sin_family = AF_INET;
    address.sin_addr.s_addr = INADDR_ANY;  // 0.0.0.0
    address.sin_port = htons(PORT);        // host to network short

    if (bind(server_fd, (struct sockaddr *)&address, sizeof(address)) < 0) {
        perror("bind failed");
        exit(EXIT_FAILURE);
    }

    // 3. Listen
    if (listen(server_fd, 10) < 0) {  // backlog = 10
        perror("listen failed");
        exit(EXIT_FAILURE);
    }

    printf("Server listening on port %d\n", PORT);

    // 4. Accept loop
    while (1) {
        client_fd = accept(server_fd, (struct sockaddr *)&address, &addrlen);
        if (client_fd < 0) {
            perror("accept failed");
            continue;
        }

        // Get client info
        char client_ip[INET_ADDRSTRLEN];
        inet_ntop(AF_INET, &address.sin_addr, client_ip, INET_ADDRSTRLEN);
        printf("Client connected: %s:%d\n", client_ip, ntohs(address.sin_port));

        // 5. Read request
        ssize_t bytes_read = read(client_fd, buffer, BUFFER_SIZE - 1);
        if (bytes_read > 0) {
            buffer[bytes_read] = '\0';
            printf("Received: %s\n", buffer);

            // 6. Send response
            const char *response = "HTTP/1.1 200 OK\r\nContent-Length: 12\r\n\r\nHello World!";
            write(client_fd, response, strlen(response));
        }

        // 7. Close client
        close(client_fd);
    }

    close(server_fd);
    return 0;
}
```

### Non-blocking I/O и Event Loop

**Проблема:** Один поток = одно соединение. Для 10,000 соединений нужно 10,000 потоков. Каждый поток — ~1MB стека. Это не масштабируется.

**Решение:** Non-blocking I/O + event loop. Один поток обрабатывает тысячи соединений. Вместо блокирующего `read()` (ждёт данные) используем `epoll_wait()` — он сообщает, какие сокеты готовы к чтению.

**Почему epoll, а не select/poll?**
- `select`: O(n) на каждый вызов, лимит 1024 fd
- `poll`: O(n), без лимита, но всё равно медленно
- `epoll`: O(1) добавление/удаление, O(готовых) при wait. Nginx, Redis, Node.js используют epoll.

```c
// Multiplexing: обработка множества соединений

// 1. select() — старый способ (max 1024 fd)
int select(int nfds, fd_set *readfds, fd_set *writefds,
           fd_set *exceptfds, struct timeval *timeout);

// 2. poll() — улучшенный select
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

// 3. epoll — Linux-специфичный, масштабируется (миллионы fd)
int epoll_create1(int flags);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

```c
// epoll example (основа для nginx, Redis, Node.js)

#include <sys/epoll.h>

#define MAX_EVENTS 1000

int main() {
    int epoll_fd = epoll_create1(0);
    struct epoll_event ev, events[MAX_EVENTS];

    // Add server socket to epoll
    ev.events = EPOLLIN;  // Ready to read
    ev.data.fd = server_fd;
    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, server_fd, &ev);

    while (1) {
        // Wait for events (blocking)
        int nfds = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);

        for (int i = 0; i < nfds; i++) {
            if (events[i].data.fd == server_fd) {
                // New connection
                int client_fd = accept(server_fd, ...);

                // Make non-blocking
                fcntl(client_fd, F_SETFL, O_NONBLOCK);

                // Add to epoll
                ev.events = EPOLLIN | EPOLLET;  // Edge-triggered
                ev.data.fd = client_fd;
                epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &ev);
            } else {
                // Handle client data
                handle_client(events[i].data.fd);
            }
        }
    }
}
```

### Socket Options

**Зачем нужны socket options?**

Поведение сокета по умолчанию не всегда оптимально. Примеры:
- **SO_REUSEADDR**: После `close()` порт занят ~60 сек (TIME_WAIT). Без этой опции перезапуск сервера падает с "Address already in use".
- **TCP_NODELAY**: По умолчанию TCP буферизует маленькие пакеты (алгоритм Nagle) для эффективности. Для интерактивных протоколов (SSH, игры) это создаёт лаги — отключаем.
- **SO_KEEPALIVE**: TCP не знает, что соединение "умерло" (клиент выключился без FIN). Keepalive периодически проверяет.

```c
// Важные опции сокетов

// Reuse address (быстрый restart сервера)
int opt = 1;
setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

// Reuse port (multiple processes на одном порту)
setsockopt(fd, SOL_SOCKET, SO_REUSEPORT, &opt, sizeof(opt));

// TCP keepalive
setsockopt(fd, SOL_SOCKET, SO_KEEPALIVE, &opt, sizeof(opt));

// TCP_NODELAY (disable Nagle's algorithm)
setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &opt, sizeof(opt));

// Send/receive buffer sizes
int bufsize = 1024 * 1024;  // 1MB
setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &bufsize, sizeof(bufsize));
setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &bufsize, sizeof(bufsize));

// Timeout
struct timeval tv;
tv.tv_sec = 5;
tv.tv_usec = 0;
setsockopt(fd, SOL_SOCKET, SO_RCVTIMEO, &tv, sizeof(tv));

// Получение информации
int error;
socklen_t len = sizeof(error);
getsockopt(fd, SOL_SOCKET, SO_ERROR, &error, &len);
```

---

## Netfilter и iptables

### Почему Netfilter, а не фильтрация в приложении?

Firewall на уровне приложения: пакет проходит весь сетевой стек, создаётся `sk_buff`, копируется в userspace, приложение решает DROP — и всё это зря. Тысячи таких пакетов в секунду = DDoS успешен.

**Netfilter** — это hooks в ядре. Пакет проверяется ДО того, как достигнет приложения. DROP на уровне ядра — это наносекунды, не микросекунды.

**5 hooks = 5 точек перехвата:**
- PREROUTING — до routing decision (DNAT здесь)
- INPUT — для локальных процессов
- FORWARD — для пересылаемых пакетов (роутер)
- OUTPUT — от локальных процессов
- POSTROUTING — после routing (SNAT/MASQUERADE здесь)

### Netfilter Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                  NETFILTER HOOKS                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Network                                                         │
│  Interface                                                       │
│     │                                                            │
│     ▼                                                            │
│  ┌──────────────┐                                               │
│  │  PREROUTING  │  ← Первый hook (до routing decision)          │
│  │  (raw, nat,  │    DNAT происходит здесь                       │
│  │   mangle)    │                                                │
│  └──────┬───────┘                                               │
│         │                                                        │
│         ▼                                                        │
│  ┌──────────────┐    Routing                                    │
│  │   Routing    │    Decision                                    │
│  │   Decision   │    ─────────                                   │
│  └──────┬───────┘    For me? → INPUT                            │
│         │            Forward? → FORWARD                          │
│         │                                                        │
│    ┌────┴────┐                                                  │
│    │         │                                                   │
│    ▼         ▼                                                   │
│  ┌─────┐  ┌─────────┐                                           │
│  │INPUT│  │ FORWARD │  ← Пересылаемые пакеты (router/firewall)  │
│  │     │  │         │                                            │
│  └──┬──┘  └────┬────┘                                           │
│     │          │                                                 │
│     ▼          │                                                 │
│  ┌──────┐      │                                                │
│  │Local │      │                                                │
│  │Process      │                                                │
│  └──┬──┘      │                                                 │
│     │          │                                                 │
│     ▼          │                                                 │
│  ┌──────┐      │                                                │
│  │OUTPUT│      │  ← Локально сгенерированные пакеты             │
│  └──┬──┘      │                                                 │
│     │          │                                                 │
│     └────┬─────┘                                                │
│          │                                                       │
│          ▼                                                       │
│  ┌───────────────┐                                              │
│  │  POSTROUTING  │  ← Последний hook (после routing)            │
│  │  (nat, mangle)│    SNAT/MASQUERADE происходит здесь          │
│  └───────────────┘                                              │
│          │                                                       │
│          ▼                                                       │
│      Network                                                     │
│      Interface                                                   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### iptables Tables и Chains

```
┌─────────────────────────────────────────────────────────────────┐
│                 iptables TABLES                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. filter (default) — фильтрация пакетов                       │
│     Chains: INPUT, FORWARD, OUTPUT                               │
│     Actions: ACCEPT, DROP, REJECT                                │
│                                                                  │
│  2. nat — Network Address Translation                            │
│     Chains: PREROUTING (DNAT), OUTPUT, POSTROUTING (SNAT)        │
│     Actions: SNAT, DNAT, MASQUERADE, REDIRECT                    │
│                                                                  │
│  3. mangle — модификация пакетов                                │
│     Chains: все пять                                             │
│     Actions: TOS, TTL, MARK                                      │
│                                                                  │
│  4. raw — до connection tracking                                 │
│     Chains: PREROUTING, OUTPUT                                   │
│     Actions: NOTRACK                                             │
│                                                                  │
│  5. security — SELinux                                           │
│     Chains: INPUT, FORWARD, OUTPUT                               │
│                                                                  │
│  Порядок обработки (PREROUTING):                                 │
│  raw → mangle → nat                                              │
│                                                                  │
│  Порядок обработки (INPUT):                                      │
│  mangle → filter → security                                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### iptables Commands

```bash
# Просмотр правил
iptables -L                    # Все chains в filter
iptables -L -v -n              # С счётчиками, без DNS resolve
iptables -t nat -L             # NAT table
iptables -S                    # В формате команд

# Политика по умолчанию
iptables -P INPUT DROP         # Блокировать всё входящее
iptables -P FORWARD DROP       # Блокировать forwarding
iptables -P OUTPUT ACCEPT      # Разрешить исходящее

# Разрешить established/related connections
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

# Разрешить loopback
iptables -A INPUT -i lo -j ACCEPT

# Разрешить SSH
iptables -A INPUT -p tcp --dport 22 -j ACCEPT

# Разрешить HTTP/HTTPS
iptables -A INPUT -p tcp -m multiport --dports 80,443 -j ACCEPT

# Блокировать IP
iptables -A INPUT -s 192.168.1.100 -j DROP

# Rate limiting (защита от brute force)
iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW \
         -m recent --set --name SSH
iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW \
         -m recent --update --seconds 60 --hitcount 4 --name SSH -j DROP

# NAT: MASQUERADE для динамического IP
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE

# NAT: SNAT для статического IP
iptables -t nat -A POSTROUTING -o eth0 -j SNAT --to-source 203.0.113.1

# Port forwarding (DNAT)
iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 10.0.0.2:80
iptables -A FORWARD -p tcp -d 10.0.0.2 --dport 80 -j ACCEPT

# Удаление правила
iptables -D INPUT -p tcp --dport 22 -j ACCEPT
iptables -D INPUT 3    # По номеру

# Очистка
iptables -F            # Flush все chains
iptables -X            # Удалить custom chains
iptables -t nat -F     # Flush NAT

# Сохранение/восстановление
iptables-save > /etc/iptables.rules
iptables-restore < /etc/iptables.rules
```

### nftables — Modern Replacement

**Почему nftables вместо iptables?**

iptables создавался в 1998, когда IPv6 был экзотикой. Для IPv6 сделали отдельный `ip6tables`, для ARP — `arptables`, для bridging — `ebtables`. Четыре утилиты с похожим, но разным синтаксисом.

**nftables** (2014) — единая замена всего. Плюсы:
- Один синтаксис для IPv4/IPv6/ARP/bridging
- Атомарные обновления (нет race conditions при замене правил)
- Лучшая производительность (optimized matching)
- Встроенные sets и maps (в iptables нужен ipset)

```bash
# nftables — замена iptables/ip6tables/arptables/ebtables
# Один синтаксис для всего

# Просмотр
nft list ruleset
nft list tables
nft list table inet filter

# Создание таблицы
nft add table inet filter    # inet = IPv4 + IPv6

# Создание chain
nft add chain inet filter input { type filter hook input priority 0 \; policy drop \; }
nft add chain inet filter forward { type filter hook forward priority 0 \; policy drop \; }
nft add chain inet filter output { type filter hook output priority 0 \; policy accept \; }

# Правила
nft add rule inet filter input ct state established,related accept
nft add rule inet filter input iif lo accept
nft add rule inet filter input tcp dport 22 accept
nft add rule inet filter input tcp dport { 80, 443 } accept
nft add rule inet filter input ip saddr 192.168.1.0/24 accept

# Rate limiting
nft add rule inet filter input tcp dport 22 ct state new \
    limit rate 3/minute accept

# NAT
nft add table nat
nft add chain nat postrouting { type nat hook postrouting priority 100 \; }
nft add rule nat postrouting oifname "eth0" masquerade

# Port forwarding
nft add chain nat prerouting { type nat hook prerouting priority -100 \; }
nft add rule nat prerouting tcp dport 8080 dnat to 10.0.0.2:80
```

```
# nftables config file (/etc/nftables.conf)
#!/usr/sbin/nft -f

flush ruleset

table inet filter {
    chain input {
        type filter hook input priority 0; policy drop;

        # Established connections
        ct state established,related accept

        # Loopback
        iif lo accept

        # ICMP
        ip protocol icmp accept
        ip6 nexthdr icmpv6 accept

        # SSH (rate limited)
        tcp dport 22 ct state new limit rate 3/minute accept

        # HTTP/HTTPS
        tcp dport { 80, 443 } accept

        # Log dropped
        log prefix "nftables drop: " limit rate 5/minute
    }

    chain forward {
        type filter hook forward priority 0; policy drop;
    }

    chain output {
        type filter hook output priority 0; policy accept;
    }
}
```

---

## Network Namespaces

### Почему Docker использует namespaces?

Проблема: как дать контейнеру свой IP, свои iptables, свою routing table, не затрагивая хост?

**Network namespace** — изолированная копия сетевого стека. У каждого namespace свои:
- Интерфейсы (eth0 в контейнере ≠ eth0 на хосте)
- Routing table
- iptables/nftables rules
- /proc/net/*

**veth pair** — виртуальный "кабель" между namespaces. Пакет, отправленный в один конец, появляется на другом. Docker создаёт veth: один конец в контейнере (eth0), другой на хосте (подключен к docker0 bridge).

```
┌─────────────────────────────────────────────────────────────────┐
│                 NETWORK NAMESPACES                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Network namespace = изолированный сетевой стек                 │
│  • Свои интерфейсы                                               │
│  • Своя routing table                                            │
│  • Свои iptables                                                 │
│  • Используется в Docker, Kubernetes                             │
│                                                                  │
│  ┌───────────────────┐     ┌───────────────────┐                │
│  │   Host namespace  │     │  Container ns     │                │
│  │                   │     │                   │                │
│  │  eth0 (physical)  │     │  eth0 (veth)      │                │
│  │  docker0 (bridge) │     │  lo               │                │
│  │  veth123          │═════│  (connected)      │                │
│  │                   │     │                   │                │
│  └───────────────────┘     └───────────────────┘                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

```bash
# Управление network namespaces

# Создать namespace
ip netns add test_ns

# Список namespaces
ip netns list

# Выполнить команду в namespace
ip netns exec test_ns ip addr
ip netns exec test_ns ping 8.8.8.8

# Создать veth pair (виртуальный кабель)
ip link add veth0 type veth peer name veth1

# Переместить один конец в namespace
ip link set veth1 netns test_ns

# Настроить адреса
ip addr add 10.0.0.1/24 dev veth0
ip netns exec test_ns ip addr add 10.0.0.2/24 dev veth1

# Поднять интерфейсы
ip link set veth0 up
ip netns exec test_ns ip link set veth1 up
ip netns exec test_ns ip link set lo up

# Теперь можно пинговать
ping 10.0.0.2
ip netns exec test_ns ping 10.0.0.1

# Дать доступ в интернет
# На хосте:
echo 1 > /proc/sys/net/ipv4/ip_forward
iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -j MASQUERADE

# В namespace:
ip netns exec test_ns ip route add default via 10.0.0.1

# Удалить namespace
ip netns del test_ns
```

---

## eBPF и XDP

### Почему eBPF называют "революцией в Linux"?

Раньше для нового функционала в сетевом стеке нужно было:
1. Написать kernel module
2. Убедить maintainers принять его в mainline
3. Ждать годы, пока дистрибутивы обновятся

**eBPF** позволяет загружать программы в ядро БЕЗ модификации ядра. Программа на C компилируется в BPF bytecode, загружается через `bpf()` syscall, verifier проверяет безопасность (нет infinite loops, нет invalid memory access), JIT компилирует в native code.

**XDP (eXpress Data Path)** — самый ранний hook, на уровне драйвера NIC. Пакет ещё не прошёл через стек, `sk_buff` не создан. DROP на XDP — это 10-20 миллионов пакетов в секунду. Cloudflare использует XDP для защиты от DDoS.

```
┌─────────────────────────────────────────────────────────────────┐
│                    eBPF (extended BPF)                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  eBPF = программы в ядре без модификации ядра                   │
│                                                                  │
│  User Space                Kernel Space                          │
│  ┌────────────────┐       ┌────────────────────────────────────┐│
│  │                │       │                                    ││
│  │  BPF program   │──────►│  Verifier (safety check)           ││
│  │  (C → BPF      │       │         │                          ││
│  │   bytecode)    │       │         ▼                          ││
│  │                │       │  JIT compiler → native code        ││
│  └────────────────┘       │         │                          ││
│                           │         ▼                          ││
│  ┌────────────────┐       │  ┌──────────────────────────────┐  ││
│  │                │       │  │     BPF program (running)     │  ││
│  │  BPF maps      │◄─────►│  │                              │  ││
│  │  (shared data) │       │  │  Hooks:                       │  ││
│  │                │       │  │  • XDP (packet processing)    │  ││
│  └────────────────┘       │  │  • tc (traffic control)       │  ││
│                           │  │  • socket                     │  ││
│                           │  │  • tracepoints               │  ││
│                           │  │  • kprobes                    │  ││
│                           │  └──────────────────────────────┘  ││
│                           └────────────────────────────────────┘│
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  XDP (eXpress Data Path) — самый быстрый hook                   │
│                                                                  │
│  ┌─────────┐                                                    │
│  │   NIC   │                                                    │
│  └────┬────┘                                                    │
│       │                                                          │
│       ▼                                                          │
│  ┌─────────┐     XDP Actions:                                   │
│  │   XDP   │───► XDP_DROP    — отбросить (DDoS защита)          │
│  │  hook   │───► XDP_PASS    — передать в стек                  │
│  └────┬────┘───► XDP_TX      — отправить обратно                │
│       │     ───► XDP_REDIRECT— перенаправить                    │
│       ▼                                                          │
│  ┌─────────┐                                                    │
│  │ sk_buff │     До создания sk_buff!                           │
│  │ alloc   │     → Минимальные затраты                          │
│  └─────────┘                                                    │
│                                                                  │
│  Производительность:                                             │
│  • iptables: ~1M pps                                             │
│  • XDP: ~10-20M pps (10-20x быстрее)                            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Cilium — eBPF-based networking

```
┌─────────────────────────────────────────────────────────────────┐
│                    CILIUM (Kubernetes)                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Cilium заменяет:                                                │
│  • kube-proxy (iptables) → eBPF load balancing                  │
│  • CNI plugin → eBPF networking                                  │
│  • Network policies → eBPF enforcement                           │
│                                                                  │
│  Преимущества:                                                   │
│  • 30%+ higher throughput vs iptables                            │
│  • L7 visibility (HTTP, gRPC, Kafka)                             │
│  • Identity-based security (не IP-based)                         │
│  • Native support для Service Mesh                               │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  Pod A                      Pod B                         │   │
│  │  ┌──────┐                  ┌──────┐                      │   │
│  │  │ App  │                  │ App  │                      │   │
│  │  └──┬───┘                  └──┬───┘                      │   │
│  │     │                         │                          │   │
│  │  ┌──┴───┐                  ┌──┴───┐                      │   │
│  │  │ veth │                  │ veth │                      │   │
│  │  └──┬───┘                  └──┬───┘                      │   │
│  │     │      eBPF programs      │                          │   │
│  │     │      ┌─────────────┐    │                          │   │
│  │     └──────┤   Cilium    ├────┘                          │   │
│  │            │   eBPF      │                               │   │
│  │            └─────────────┘                               │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Tuning сетевого стека

### Почему дефолты не оптимальны?

Linux по умолчанию настроен для "среднего" сервера 2000-х годов: небольшой трафик, мало соединений. На современном сервере с 10,000+ соединений и 10 Gbps NIC дефолты становятся bottleneck.

**Типичные проблемы:**
- `net.core.somaxconn=128` — очередь accept() переполняется под нагрузкой
- `net.ipv4.tcp_max_syn_backlog=128` — SYN flood забивает очередь
- Маленькие буферы — потеря пакетов на high-bandwidth links
- TIME_WAIT накопление — исчерпание портов при частых соединениях

```bash
# Важные sysctl параметры для high-performance networking

# TCP backlog (pending connections)
sysctl -w net.core.somaxconn=65535
sysctl -w net.ipv4.tcp_max_syn_backlog=65535

# Buffer sizes
sysctl -w net.core.rmem_max=16777216
sysctl -w net.core.wmem_max=16777216
sysctl -w net.ipv4.tcp_rmem="4096 87380 16777216"
sysctl -w net.ipv4.tcp_wmem="4096 65536 16777216"

# TIME_WAIT connections
sysctl -w net.ipv4.tcp_tw_reuse=1
sysctl -w net.ipv4.tcp_fin_timeout=15

# Keepalive
sysctl -w net.ipv4.tcp_keepalive_time=600
sysctl -w net.ipv4.tcp_keepalive_intvl=60
sysctl -w net.ipv4.tcp_keepalive_probes=3

# Local port range
sysctl -w net.ipv4.ip_local_port_range="1024 65535"

# Enable IP forwarding (для роутинга)
sysctl -w net.ipv4.ip_forward=1

# Disable source routing (security)
sysctl -w net.ipv4.conf.all.accept_source_route=0

# Enable SYN cookies (SYN flood protection)
sysctl -w net.ipv4.tcp_syncookies=1

# Reverse path filtering (anti-spoofing)
sysctl -w net.ipv4.conf.all.rp_filter=1

# Persistent (add to /etc/sysctl.conf or /etc/sysctl.d/)
```

### File Descriptor Limits

```bash
# Лимиты на open files (сокеты = файлы)

# Проверить текущие лимиты
ulimit -n           # soft limit
ulimit -Hn          # hard limit

# Увеличить для текущей сессии
ulimit -n 65535

# Permanent: /etc/security/limits.conf
*               soft    nofile          65535
*               hard    nofile          65535

# Для systemd сервисов: /etc/systemd/system/myservice.service.d/limits.conf
[Service]
LimitNOFILE=65535

# System-wide limit
sysctl -w fs.file-max=2097152
```

---

## Диагностика

```bash
# ss — modern replacement для netstat
ss -tuln              # TCP/UDP listening sockets
ss -s                 # Summary statistics
ss -p                 # Show process
ss -t state established  # Established TCP
ss -o state time-wait    # TIME_WAIT connections

# Network statistics
netstat -s            # Protocol statistics
cat /proc/net/snmp    # SNMP counters
cat /proc/net/sockstat  # Socket statistics

# Interface statistics
ip -s link            # Packets, bytes, errors
ethtool -S eth0       # NIC-level stats

# Connection tracking
conntrack -L          # List tracked connections
conntrack -S          # Statistics

# Routing
ip route              # Routing table
ip route get 8.8.8.8  # Path to destination

# ARP
ip neigh              # ARP cache

# tcpdump — packet capture
tcpdump -i eth0 port 80
tcpdump -i any -nn 'tcp port 443'
tcpdump -w capture.pcap  # Save to file

# Netfilter debugging
iptables -L -v -n --line-numbers
iptables -t nat -L -v -n
dmesg | grep nf       # Netfilter logs
```

---

## Подводные камни

### Проблема 1: Connection refused vs timeout

```
Connection refused (ECONNREFUSED):
─────────────────────────────────
• Порт закрыт (no listener)
• iptables REJECT
• TCP RST получен

Connection timeout:
───────────────────
• Пакеты DROP (iptables DROP)
• Неправильный routing
• Firewall без response
```

### Проблема 2: Too many open files

```
Error: "Too many open files"
───────────────────────────

1. Проверить лимит: ulimit -n
2. Проверить использование: ls /proc/<pid>/fd | wc -l
3. Увеличить лимит (см. выше)

Причины:
• Socket leaks (не закрытые соединения)
• TIME_WAIT накопление
• Слишком много concurrent connections
```

### Проблема 3: NAT и connection tracking

```
conntrack table full:
─────────────────────

dmesg: "nf_conntrack: table full, dropping packet"

Решение:
sysctl -w net.netfilter.nf_conntrack_max=262144
sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=30

Или использовать eBPF вместо iptables NAT
```

---

## Actionable

**Для разработчиков:**
- Используйте SO_REUSEADDR для серверов
- epoll для high-concurrency (не select/poll)
- TCP_NODELAY для low-latency протоколов

**Для DevOps:**
- nftables вместо iptables для новых систем
- Tune sysctl для production нагрузок
- Network namespaces для изоляции

**Для SRE:**
- eBPF/Cilium для Kubernetes networking
- Мониторинг conntrack table
- XDP для DDoS protection

---

## Связи

- Обзор сетей: [[networking-overview]]
- Transport layer: [[network-transport-layer]]
- IP и маршрутизация: [[network-ip-routing]]
- Docker networking: [[docker-for-developers]]
- Service Mesh и eBPF: [[network-cloud-modern]]

---

## Источники

- [Linux Kernel Networking Documentation](https://www.kernel.org/doc/html/latest/networking/index.html) — проверено 2025-12-18
- [Beej's Guide to Network Programming](https://beej.us/guide/bgnet/) — проверено 2025-12-18
- [nftables Wiki](https://wiki.nftables.org/) — проверено 2025-12-18
- [eBPF.io](https://ebpf.io/) — проверено 2025-12-18
- [Cilium Documentation](https://docs.cilium.io/) — проверено 2025-12-18
- [Linux Performance: Network](https://www.brendangregg.com/linuxperf.html) — проверено 2025-12-18

---

**Последняя верификация**: 2025-12-18
**Уровень достоверности**: high

---

*Последнее обновление: 2026-01-09 — Добавлены педагогические секции: 5 аналогий (socket как телефонная розетка с fd, путь пакета как почтовое отделение NIC→Driver→L2→L3→L4→Socket, netfilter/iptables как охрана с 5 КПП, network namespace как квартиры в доме для Docker, eBPF/XDP как программируемая охрана), 6 типичных ошибок с СИМПТОМ/РЕШЕНИЕ (EADDRINUSE и SO_REUSEADDR, iptables правила в неправильном порядке, SSH локаут при DROP policy, XDP generic vs native mode, забытый ip_forward для NAT, утечка file descriptors), 5 ментальных моделей (пирамида скорости обработки пакетов XDP→iptables→TC→Application, fd как универсальный интерфейс с epoll, netfilter hooks матрица, namespace vs VM изоляция, sk_buff как конверт с headroom/tailroom)*
