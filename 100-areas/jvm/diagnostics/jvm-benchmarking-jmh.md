---
title: "JMH: правильные бенчмарки в Java"
created: 2025-11-25
modified: 2025-12-02
tags:
  - topic/jvm
  - jmh
  - benchmarking
  - performance
  - type/deep-dive
  - level/intermediate
type: deep-dive
status: published
area: programming
confidence: high
prerequisites:
  - "[[jvm-performance-overview]]"
  - "[[jvm-jit-compiler]]"
related:
  - "[[jvm-performance-overview]]"
  - "[[jvm-jit-compiler]]"
  - "[[jvm-profiling]]"
---

# JMH: правильные бенчмарки в Java

JMH (Java Microbenchmark Harness) --- официальный фреймворк от OpenJDK для измерения производительности Java-кода. Он был создан людьми, которые пишут сам JIT-компилятор, и это не случайность: только разработчики виртуальной машины знают все оптимизации, которые JIT применяет к пользовательскому коду, и могут гарантировать, что бенчмарк измеряет именно то, что задумано, а не побочный эффект оптимизаций компилятора.

Наивный подход --- обернуть вызов в `System.nanoTime()` и вычислить разницу --- не просто неточен, а фундаментально некорректен. JIT-компилятор может полностью удалить измеряемый код (Dead Code Elimination), вычислить результат на этапе компиляции (Constant Folding), или заинлайнить вызов так, что overhead метода исчезнет. Первые 10--15 тысяч итераций выполняются в интерпретаторе и работают в 50--100 раз медленнее скомпилированного кода. GC-паузы вносят случайные задержки от микросекунд до секунд. Одно измерение статистически бессмысленно. JMH решает все эти проблемы системно.

---

## Зачем это знать

Каждый Java-разработчик рано или поздно встаёт перед вопросом: "ArrayList или LinkedList? HashMap или TreeMap? Stream или цикл?" Без корректного бенчмарка ответ --- лишь догадка. Хуже того --- наивный бенчмарк даёт **уверенно неправильный** ответ, потому что JIT-оптимизации систематически искажают результаты в одну сторону.

Понимание JMH и принципов бенчмаркинга на JVM --- это не просто навык работы с инструментом. Это понимание того, как JIT-компилятор трансформирует ваш код, как GC влияет на latency, и почему микроизмерения требуют научной методологии. Без этого понимания невозможно принимать обоснованные решения об оптимизации.

---

## Терминология

| Термин | Значение | Аналогия из жизни |
|--------|----------|-------------------|
| **Microbenchmark** | Измерение производительности изолированного фрагмента кода | Замер 0--100 км/ч автомобиля на полигоне (не в городском трафике) |
| **Warmup** | Прогревочные итерации до начала измерений | Разминка спортсмена перед соревнованием |
| **Fork** | Запуск бенчмарка в отдельном JVM-процессе | Каждый краш-тест на новом автомобиле, а не на уже побитом |
| **Blackhole** | Механизм предотвращения Dead Code Elimination | "Чёрная дыра", которая поглощает результат --- компилятор не знает, нужен ли он |
| **Throughput** | Количество операций в единицу времени (ops/sec) | Пропускная способность кассы: сколько покупателей за час |
| **Latency** | Время выполнения одной операции | Время обслуживания одного покупателя |
| **Percentile (p99)** | Значение, ниже которого попадает 99% измерений | Из 100 автомобилей 99 проехали быстрее этого времени |
| **Dead Code Elimination** | JIT удаляет код, результат которого не используется | Архитектор убирает комнату из проекта, если никто ею не пользуется |
| **Constant Folding** | JIT вычисляет выражения с константами на этапе компиляции | Калькулятор заранее подставляет ответ вместо формулы |

---

## Историческая справка

История правильного бенчмаркинга на JVM начинается с осознания проблемы. В начале 2000-х Java получила репутацию "медленного языка" во многом из-за некорректных бенчмарков, которые измеряли работу интерпретатора, а не скомпилированного кода. Разработчики писали цикл с `System.currentTimeMillis()`, получали числа в десятки раз хуже C++, и делали выводы о производительности языка.

В 2008 году Google выпустил **Caliper** --- первую попытку создать фреймворк для корректных Java-бенчмарков. Caliper решал часть проблем: запускал warmup, делал несколько итераций, форматировал результаты. Однако Caliper не знал внутренностей HotSpot JVM и не мог предотвратить все оптимизации JIT-компилятора. Кроме того, проект постепенно потерял активных разработчиков.

В 2013 году **Aleksey Shipilev** из Oracle (позже Red Hat), один из ведущих разработчиков HotSpot JVM, создал JMH. Это было принципиально важно: JMH создан человеком, который буквально пишет те оптимизации, от которых нужно защитить бенчмарк. Shipilev знал каждый трюк JIT-компилятора и заложил защиту от них прямо в архитектуру фреймворка. JMH стал частью OpenJDK project и де-факто стандартом бенчмаркинга на JVM.

Сегодня JMH используется командами Oracle, Red Hat, Azul, Jetbrains и практически всеми, кто серьёзно работает с производительностью Java. Результаты JMH принимаются в quality-gate процессах OpenJDK и являются доказательной базой при принятии решений об оптимизациях в самой виртуальной машине.

---

## Почему бенчмарки без JMH бессмысленны

### JIT Elimination: компилятор умнее вашего бенчмарка

JIT-компилятор HotSpot --- это одна из самых агрессивных оптимизирующих систем в мире. Его задача --- сделать код максимально быстрым, и он делает это блестяще. Но именно эта агрессивность делает наивные бенчмарки бесполезными.

Рассмотрим простой пример: вы хотите измерить скорость вычисления квадратного корня. Пишете цикл, вызываете `Math.sqrt(42)` миллион раз, замеряете время. Результат: 0 наносекунд. JIT увидел, что результат `Math.sqrt(42)` --- константа (6.48...), вычислил её один раз и заменил весь цикл на одно присваивание. Вы измерили не скорость `Math.sqrt`, а скорость присваивания константы.

Ещё хуже: если результат вычисления нигде не используется, JIT просто удалит весь код. Dead Code Elimination --- одна из первых оптимизаций, которую применяет C2-компилятор. Ваш бенчмарк покажет время пустого цикла --- несколько наносекунд. Вы заключите, что `sqrt` невероятно быстрый, и это будет полной неправдой.

Inlining добавляет ещё один уровень сложности. JIT может заинлайнить метод, который вы хотите измерить, объединить его с вызывающим кодом, и оптимизировать результат. В итоге вы измеряете не тот метод, а трансформированную версию, которая не существует в реальном приложении.

### GC Interference: сборщик мусора как генератор шума

Garbage Collector --- источник случайного шума, который может исказить результаты бенчмарка на порядки. Young GC пауза занимает 5--50 мс, Full GC --- от 100 мс до нескольких секунд. Если ваш бенчмарк длится 1 секунду, и в этот момент произошла Full GC пауза в 200 мс --- вы измерили 80% GC и 20% полезной работы.

Проблема усугубляется тем, что GC недетерминирован. Он может сработать на второй итерации, на десятой, или не сработать вовсе. Два последовательных запуска одного и того же кода дадут разные результаты не из-за разницы в производительности кода, а из-за разницы в моментах срабатывания GC. Без статистической обработки множества измерений отделить сигнал от шума невозможно.

JMH решает эту проблему несколькими способами. Во-первых, множество итераций и статистическая обработка: если из 20 измерений 18 показывают 50 нс и 2 показывают 50 мс --- JMH корректно обработает выбросы. Во-вторых, fork --- каждый fork стартует с чистым heap, что уменьшает влияние предыдущих аллокаций. В-третьих, JMH может интегрироваться с GC-профайлером (`-prof gc`), чтобы показать, сколько аллокаций делает бенчмарк.

### OS Scheduling: операционная система добавляет хаос

Операционная система --- ещё один источник недетерминизма. Планировщик процессов может приостановить ваш JVM-процесс в любой момент, отдать CPU другому процессу, и вернуть управление через неопределённое время. На загруженном сервере это может добавить миллисекунды к каждому измерению.

Кэш-эффекты CPU --- ещё один фактор. Если между итерациями бенчмарка ОС вытеснила ваш процесс и другой процесс заполнил L1/L2 кэш своими данными --- первая итерация после возврата будет значительно медленнее из-за cache miss'ов. На микробенчмарках, где речь идёт о наносекундах, cache miss (50--100 нс для L3, 100+ нс для RAM) может исказить результат на порядок.

Thermal throttling --- ещё одна неочевидная проблема. Если процессор перегревается во время длительного бенчмарка, он снижает частоту. Первые итерации работают на полной частоте, последние --- на пониженной. Без fork (перезапуска JVM) и достаточного количества итераций этот эффект невидим, но систематически искажает результаты.

> **Аналогия:** JMH --- это лаборатория для краш-тестов автомобилей. Нельзя проверить безопасность машины, выехав на дорогу и врезавшись в первый попавшийся столб. Нужны контролируемые условия: одинаковая скорость, одинаковый угол, одинаковая стена, калиброванные датчики, и серия повторных тестов для статистической значимости. Наивный бенчмарк с `System.nanoTime()` --- это как проверять безопасность машины по ощущениям после поездки.

---

## JMH Annotations: зачем каждая нужна

### @Benchmark: точка входа в измерение

Аннотация `@Benchmark` --- это маркер метода, который JMH будет измерять. Но её роль гораздо глубже, чем просто "пометить метод для запуска". JMH генерирует вокруг `@Benchmark`-метода специальный код: обёртку, которая управляет warmup-итерациями, собирает timestamps с наносекундной точностью, обрабатывает результаты статистически, и гарантирует, что JIT-компилятор не может оптимизировать измерительную инфраструктуру.

Важная деталь: метод с `@Benchmark` должен либо возвращать значение, либо использовать `Blackhole`. Если метод `void` и не использует `Blackhole` --- JIT может удалить весь код внутри как dead code. Возврат значения --- самый простой способ предотвратить DCE: JMH гарантирует, что возвращённое значение будет "использовано" (через Blackhole внутри сгенерированного кода).

Ещё один нюанс: JMH не вызывает ваш метод напрямую. Он генерирует подкласс, который наследуется от вашего бенчмарка, и переопределяет измерительную логику. Это позволяет JMH вставить barrier'ы для JIT-компилятора и предотвратить нежелательные оптимизации между итерациями.

### @State: изоляция данных между потоками

`@State` определяет, как данные (поля класса) разделяются между потоками бенчмарка. Это критично для многопоточных бенчмарков, но важно и для однопоточных.

`Scope.Thread` --- каждый поток получает свой экземпляр State-объекта. Это значит, что потоки не конкурируют за данные, нет false sharing (когда разные поля попадают в одну cache line), нет contention. Используйте для измерения чистой производительности алгоритма без overhead'а синхронизации.

`Scope.Benchmark` --- один экземпляр на все потоки. Используйте для измерения производительности при конкурентном доступе: ConcurrentHashMap, synchronized блоки, атомарные операции. Это показывает реальную производительность под нагрузкой, включая cache coherence overhead и lock contention.

`Scope.Group` --- один экземпляр на группу потоков. Позволяет моделировать producer-consumer сценарии: одна группа потоков пишет в структуру данных, другая читает. Это самый реалистичный режим для бенчмарков concurrent-структур.

Без `@State` JMH не знает, как инициализировать данные для бенчмарка, и вам придётся создавать их внутри `@Benchmark`-метода --- а это включит overhead аллокации в измерения.

### @Setup и @TearDown: подготовка и очистка

`@Setup` и `@TearDown` --- методы, которые выполняются до и после бенчмарка. Параметр `Level` определяет гранулярность:

`Level.Trial` --- выполняется один раз перед всеми итерациями. Используйте для тяжёлой инициализации: загрузка данных из файла, создание большой коллекции, инициализация connection pool. Overhead `@Setup(Level.Trial)` не включается в измерения.

`Level.Iteration` --- выполняется перед каждой итерацией (iteration = batch из множества вызовов). Используйте для сброса состояния между итерациями: если бенчмарк модифицирует данные (например, добавляет элементы в коллекцию), `Level.Iteration` позволяет вернуть коллекцию в исходное состояние.

`Level.Invocation` --- выполняется перед каждым вызовом `@Benchmark`-метода. Это опасный уровень: overhead самого `@Setup` может быть сопоставим с измеряемой операцией. Используйте только когда каждый вызов ДОЛЖЕН начинаться с чистого состояния, и вы понимаете, что overhead `@Setup` включается в результат.

### @BenchmarkMode: что именно измеряем

Выбор режима измерения --- это не формальность, а принципиальное решение о том, какой аспект производительности вас интересует.

`Mode.Throughput` отвечает на вопрос "сколько операций система может выполнить за секунду". Это метрика пропускной способности. Подходит для сравнения реализаций batch-обработки, сортировок, алгоритмов, где важен общий объём работы.

`Mode.AverageTime` отвечает на вопрос "сколько в среднем длится одна операция". Это самый распространённый режим, подходящий для большинства микробенчмарков. Результат в наносекундах или микросекундах на операцию интуитивно понятен.

`Mode.SampleTime` показывает распределение: p50, p90, p95, p99, p99.9. Это критично для latency-sensitive систем, где средняя задержка не имеет значения, а p99 определяет пользовательский опыт. Если средняя --- 1 мс, но p99 --- 500 мс, каждый сотый запрос будет неприемлемо медленным.

`Mode.SingleShotTime` измеряет холодный старт --- одно выполнение без warmup. Подходит для Lambda-функций, CLI-утилит, и других сценариев, где каждый запуск "холодный".

---

## Benchmark Pitfalls: подводные камни измерений

### Dead Code Elimination: JIT удаляет ваш бенчмарк

Dead Code Elimination (DCE) --- самая коварная проблема бенчмаркинга. JIT-компилятор анализирует граф зависимостей и удаляет любой код, результат которого не влияет на наблюдаемое поведение программы. В контексте бенчмарка это означает: если результат вычисления никуда не передаётся и никем не читается --- JIT его удалит.

Проблема в том, что DCE невидима. Бенчмарк не упадёт с ошибкой. Он покажет результат --- просто неправильный. Вы увидите время выполнения пустого метода и примете его за время вашего алгоритма. Хуже того, результат будет стабильным и воспроизводимым --- потому что пустой метод каждый раз работает одинаково быстро.

JMH предлагает два решения. Первое --- вернуть значение из `@Benchmark`-метода: JMH гарантирует, что оно будет "использовано". Второе --- `Blackhole.consume()`: специальный метод, который JIT не может оптимизировать, потому что он реализован с использованием `@CompilerControl` и volatile-семантики. Blackhole "поглощает" значение так, что JIT вынужден его вычислить, но без реального side-effect.

Ниже --- демонстрация проблемы и решения. Метод `bad()` возвращает `void`, и JIT удалит вычисление. Метод `good()` возвращает результат, заставляя JIT сохранить вычисление:

```java
@Benchmark
public void bad() {
    Math.sqrt(42);           // JIT удалит: результат никому не нужен
}

@Benchmark
public double good() {
    return Math.sqrt(42);    // JMH "использует" результат через Blackhole
}
```

Без этой защиты метод `bad()` покажет ~1 нс (время пустого вызова), тогда как реальная стоимость `Math.sqrt` --- ~10-20 нс. Ошибка в 10-20 раз --- и это для простейшего случая.

### Constant Folding: компилятор решает задачу за вас

Constant Folding --- оптимизация, при которой JIT вычисляет выражения с известными значениями на этапе компиляции и заменяет их результатом. Если все входные данные бенчмарка --- константы, JIT может вычислить результат один раз и подставить готовое значение.

Это означает, что бенчмарк `fibonacci(10)` может показать 0 нс --- не потому что Fibonacci быстрый, а потому что JIT вычислил `fibonacci(10) = 55` и заменил весь вызов на `return 55`. Вы измеряете не алгоритм, а возврат константы.

Решение --- использовать `@State`. Поля State-объекта не являются compile-time константами для JIT (потому что они могут быть изменены через reflection, другими потоками, или в `@Setup`-методе). Когда вход приходит из `@State`, JIT вынужден выполнять вычисление каждый раз.

Тонкий момент: даже `final` поле в `@State` не гарантирует constant folding, потому что JMH создаёт State-объекты через reflection и JIT не может доказать неизменность значения. Но `static final` поле в обычном классе --- может быть свёрнуто. Поэтому все данные для бенчмарка должны приходить через `@State`, а не через статические поля.

### Loop Optimization: JIT видит паттерны циклов

JIT-компилятор отлично оптимизирует циклы: Loop Unrolling (развёртка), Loop Vectorization (SIMD-инструкции), Loop Peeling (вынос инвариантных операций). Если вы вручную пишете цикл внутри `@Benchmark`-метода, JIT может его трансформировать так, что результат будет нерепрезентативен.

Например, цикл суммирования массива может быть vectorized: вместо поэлементного сложения JIT использует SIMD-инструкции, обрабатывающие 4-8 элементов за такт. Это ускоряет бенчмарк в 4-8 раз по сравнению с тем, что произойдёт в реальном приложении, где массив маленький или размер не кратен ширине SIMD-регистра.

JMH не решает эту проблему автоматически. Разработчик должен понимать, что именно он измеряет, и осознанно решать, хочет ли он видеть векторизованный результат или нет. Если нужно измерить "скорость без векторизации", можно использовать `@CompilerControl(Mode.DONT_INLINE)` или JVM-флаг `-XX:-UseSuperWord`.

---

## JIT Warmup: почему первые итерации врут

> **Аналогия:** Представьте спортсмена перед соревнованием. Первый забег "на холодную" покажет результат на 10--20% хуже, чем после полноценной разминки. Мышцы не разогреты, нервная система не активирована, координация движений не вышла на пиковый уровень. Никому не придёт в голову оценивать результат спортсмена по первому забегу после выхода из раздевалки. JIT warmup --- та же разминка для JVM.

HotSpot JVM использует tiered compilation --- многоуровневую компиляцию. Код начинает выполняться в интерпретаторе (level 0), который медленнее скомпилированного кода в 50--100 раз. После ~200 вызовов метода JIT компилирует его C1-компилятором (level 1--3) --- быстрая компиляция с базовыми оптимизациями, ускорение ~5x. После ~10,000 вызовов C2-компилятор (level 4) перекомпилирует метод с агрессивными оптимизациями: inlining, escape analysis, loop vectorization. Только после C2-компиляции код достигает пиковой производительности.

Это означает, что первые тысячи итераций бенчмарка измеряют не скомпилированный код, а интерпретатор и C1-компилятор. Если вы измерите первые 1000 итераций и последние 1000 --- результаты могут отличаться в 50 раз. Это не погрешность, это фундаментально разный код: интерпретированный vs нативный.

JMH решает эту проблему через warmup iterations --- прогревочные итерации, которые выполняются перед измерениями и не включаются в результат. Стандартное значение --- 5 warmup iterations по 1 секунде. Этого обычно достаточно для C2-компиляции. Однако для сложных методов с глубоким inlining или OSR (On-Stack Replacement) может потребоваться больше warmup. Если результаты первых измерительных итераций значительно отличаются от последних --- увеличьте количество warmup.

---

## Интерпретация результатов: что значат числа

### Score и Error

Результат JMH выглядит так: `Score = 15234.567 ± 234.12 ns/op`. Score --- это среднее значение по всем измерительным итерациям и fork'ам. Error (±234.12) --- это 99.9% confidence interval, вычисленный на основе t-распределения. Это значит: с вероятностью 99.9% истинное значение лежит в интервале [15000.45, 15468.69].

Ключевое правило: если confidence intervals двух бенчмарков перекрываются --- разница статистически незначима. Если бенчмарк A показывает 100 ± 15 ns/op, а бенчмарк B --- 110 ± 20 ns/op, нельзя утверждать, что A быстрее. Интервалы [85, 115] и [90, 130] перекрываются, и наблюдаемая разница может быть случайной.

Высокий Error (скажем, ±30% от Score) --- признак нестабильности. Причины: GC-паузы во время измерений, OS scheduling на загруженной машине, или недостаточное количество fork'ов. Увеличьте количество fork'ов (3--5 вместо 2), убедитесь что машина не нагружена, и проверьте GC-логи.

### Percentiles

В режиме `SampleTime` JMH показывает распределение: p50, p90, p95, p99, p99.9, p100. Для latency-sensitive систем средняя задержка --- бессмысленная метрика. Если p50 = 1 мс (половина запросов быстрее), но p99 = 200 мс (каждый сотый запрос --- 200 мс), пользователь заметит эти "каждые сотые" запросы, особенно если один пользовательский сценарий включает десятки API-вызовов.

Правило "nines": если один запрос пользователя порождает N backend-вызовов, и каждый имеет p99 latency = L, то пользовательская p99 latency примерно N * L. Для 50 backend-вызовов с p99 = 5 мс, пользовательская p99 ~ 250 мс. Именно поэтому Amazon и Google фокусируются на p99.9 и выше.

---

## Распространённые заблуждения

**"System.currentTimeMillis() достаточно для бенчмарков."** Нет. Разрешение `currentTimeMillis()` на многих платформах --- 10--15 мс. Для микробенчмарков, где операция занимает наносекунды, это бессмысленно. Даже `nanoTime()` недостаточен без защиты от DCE, constant folding и без статистической обработки. JMH использует `nanoTime()` внутри, но оборачивает его в инфраструктуру, которая гарантирует корректность.

**"Больше итераций = точнее результат."** Частично верно, но после определённого количества итераций точность перестаёт расти. Гораздо важнее количество fork'ов --- каждый fork создаёт новый JVM-процесс с чистым JIT-кэшем и heap'ом. Разные fork'и могут показать различную производительность из-за разного layout'а объектов в памяти (а значит разных cache-эффектов). Fork = 1 --- опасно, потому что вы измеряете один конкретный "расклад" JIT-компиляции.

**"JMH автоматически делает всё правильно."** JMH предотвращает многие ошибки, но не все. Неправильный `@State`, забытый Blackhole, constant folding через static final поля --- всё это приведёт к некорректным результатам, несмотря на использование JMH. JMH --- инструмент, а не магия. Нужно понимать, от чего он защищает и от чего нет.

**"Микробенчмарки предсказывают реальную производительность."** Микробенчмарк --- это изолированная среда. В реальном приложении работают десятки потоков, GC собирает мусор, кэши процессора shared между всеми потоками, I/O блокирует выполнение. Операция, которая занимает 50 нс в микробенчмарке, может занять 5 мкс в production из-за cache pollution от других потоков. Микробенчмарки отвечают на вопрос "A быстрее B?", но не "насколько быстро будет в production?".

**"Warmup в 1 итерацию достаточно."** Tiered compilation (C1 -> C2) требует тысяч вызовов для полной оптимизации. Одна warmup-итерация в 1 секунду может не дать C2 достаточно данных для профилирования. Стандартные 5 итераций --- разумный минимум, но для сложных методов может потребоваться 10--20.

---

## Когда НЕ применяется

JMH не подходит для макробенчмарков --- измерения производительности целого приложения или сложного workflow из множества компонентов. Для этого используйте нагрузочное тестирование (Gatling, JMeter) с реальным приложением.

JMH не заменяет профилирование. Если вы не знаете, где bottleneck, JMH не поможет --- вы будете оптимизировать не то место. Сначала профилирование (async-profiler, JFR) для поиска hotspot, потом JMH для сравнения вариантов оптимизации.

JMH не подходит для бенчмарков, зависящих от внешних систем: базы данных, сетевые вызовы, файловый I/O. Latency этих операций определяется внешней системой, а не вашим кодом. JMH покажет вам latency I/O, а не производительность вашего алгоритма.

---

## Подводные камни

**False sharing в многопоточных бенчмарках:** если State-объекты разных потоков попадают в одну cache line (64 байта), потоки будут invalidate cache друг друга, даже если пишут в разные поля. JMH использует `@State(Scope.Thread)` и padding для предотвращения этого, но если вы используете `Scope.Benchmark` с отдельными полями для разных потоков --- false sharing может исказить результат.

**Benchmark interference:** предыдущий бенчмарк влияет на следующий. Например, первый бенчмарк аллоцировал много объектов, которые попали в Old Gen. Следующий бенчмарк страдает от Full GC, вызванной предыдущим. Fork решает эту проблему, но только если fork > 1.

**Power management и turbo boost:** современные процессоры динамически меняют частоту. Первые итерации могут работать на turbo-частоте, но при длительной нагрузке CPU снижает частоту из-за нагрева. Фиксируйте частоту CPU перед бенчмарками на серверных машинах.

---

## Связь с другими темами

**[[jvm-performance-overview]]** --- JMH --- один из инструментов в арсенале performance engineering, описанном в обзоре производительности JVM. Benchmarking (микро-измерения) дополняет profiling (анализ реального приложения) и monitoring (наблюдение в production). JMH подходит для изолированных вопросов ("ArrayList.get vs LinkedList.get"), но не заменяет профилирование реального приложения, где результат определяется взаимодействием компонентов, GC паузами и I/O. Рекомендуется: JMH для ответа на конкретный вопрос о performance, profiling для поиска bottlenecks в реальном коде.

**[[jvm-jit-compiler]]** --- JIT-компилятор --- главная причина, по которой наивные бенчмарки дают неправильные результаты, и JMH существует именно для борьбы с JIT-оптимизациями. Dead Code Elimination, Constant Folding, Loop Unrolling, Inlining --- все эти оптимизации могут "оптимизировать" измеряемый код до нуля. JMH использует Blackhole (предотвращает DCE), @State (предотвращает Constant Folding), Warmup iterations (даёт JIT время на компиляцию C1->C2). Понимание JIT необходимо для интерпретации результатов JMH и написания корректных бенчмарков.

**[[jvm-profiling]]** --- JMH и профилирование (JFR, async-profiler) --- взаимодополняющие инструменты. JMH отвечает на вопрос "что быстрее: A или B?" в изолированном контексте. Profiling отвечает на вопрос "где bottleneck в моём приложении?" в реальных условиях. Типичный workflow: профилирование находит hot method -> JMH измеряет варианты оптимизации -> профилирование подтверждает улучшение в production. JMH даже интегрируется с профайлерами через `-prof` опцию (gc, stack, perf).

---

## Что читать дальше

1. [[jvm-jit-compiler]] --- понимание JIT необходимо для написания корректных бенчмарков
2. [[jvm-profiling]] --- профилирование дополняет бенчмаркинг: сначала найти bottleneck, потом измерить варианты
3. [[jvm-performance-overview]] --- общая картина performance engineering на JVM

---

## Источники и дальнейшее чтение

- Shipilev A. (2013--2023). *JMH documentation, samples, and conference talks.* --- Aleksey Shipilev, автор JMH и один из ведущих разработчиков HotSpot JVM, создал обширную коллекцию примеров (jmh-samples) и выступлений, объясняющих каждый pitfall микробенчмаркинга. Это первоисточник: каждый аспект JMH объяснён человеком, который его написал.
- Oaks S. (2014). *Java Performance: The Definitive Guide.* --- Глава о бенчмарках объясняет методологию правильного измерения производительности Java-кода, включая статистическую интерпретацию результатов, влияние JIT и GC на измерения, и практические рецепты JMH.
- Herlihy M., Shavit N. (2012). *The Art of Multiprocessor Programming.* --- Глава о benchmark methodology описывает теоретические основы измерения производительности concurrent-кода: почему наивные подходы не работают и как проектировать эксперименты, дающие воспроизводимые результаты.
- Evans B., Gough J. (2014). *Optimizing Java: Practical Techniques for Improving JVM Application Performance.* --- Подробное описание JIT-компиляции, GC и их влияния на бенчмарки, с практическими примерами JMH и объяснением взаимодействия измерительного кода с оптимизациями компилятора.

---

*Проверено: 2026-02-11 --- Педагогический контент проверен*
