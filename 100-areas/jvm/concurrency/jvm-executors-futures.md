---
title: "JVM Executors & Futures: управление потоками"
created: 2025-11-25
modified: 2025-12-02
tags:
  - topic/jvm
  - concurrency
  - executors
  - completablefuture
  - type/deep-dive
  - level/intermediate
type: deep-dive
status: published
area: programming
confidence: high
prerequisites:
  - "[[jvm-concurrency-overview]]"
  - "[[jvm-synchronization]]"
related:
  - "[[jvm-concurrency-overview]]"
  - "[[jvm-synchronization]]"
  - "[[java-modern-features]]"
---

# JVM Executors & Futures: управление потоками

Создание потока в JVM — это системный вызов, аллокация ~1MB стека, регистрация в планировщике ОС. При 10 000 запросах в секунду создание нового потока на каждый запрос убьёт любой сервер. Executor Framework решает эту проблему: потоки создаются один раз и переиспользуются, задачи попадают в очередь. CompletableFuture добавляет функциональную композицию: цепочки преобразований, параллельное комбинирование, обработку ошибок — без блокирующих вызовов. Fork/Join Framework распределяет вычислительные задачи по ядрам. А Virtual Threads (Java 21) переворачивают модель: вместо пула из N потоков — миллионы легковесных виртуальных потоков.

---

## Зачем это знать

Ручное управление потоками через `new Thread()` — это ассемблер многопоточности: полный контроль, но слишком низкий уровень для production-кода. Неправильный размер пула потоков превращает мощный сервер в черепаху. Неограниченная очередь задач тихо съедает память до OutOfMemoryError. Забытый `shutdown()` — и программа висит вечно. Понимание Executor Framework — это разница между «работает на моей машине» и «стабильно обрабатывает 100K rps в production».

---

## Терминология

| Термин | Значение | Аналогия из жизни |
|--------|----------|-------------------|
| **Thread Pool** | Набор переиспользуемых потоков, берущих задачи из общей очереди | Бригада рабочих на стройке: фиксированная команда, задачи приходят — свободный берёт следующую |
| **Work Queue** | Очередь задач, ожидающих свободного потока | Стопка заказов на кухне ресторана |
| **Future** | Обещание результата: «будет готово позже» | Чек из химчистки: предъявишь — получишь готовую вещь |
| **CompletableFuture** | Future + возможность строить цепочки и композиции | Конвейер на почте: посылка проходит сортировку, упаковку, маркировку — каждый этап запускается автоматически |
| **Work Stealing** | Свободный поток «крадёт» задачи из очереди занятого соседа | Бригада грузчиков: кто разгрузил свою часть — помогает соседу |
| **Rejection Policy** | Стратегия поведения при переполнении пула и очереди | Полная парковка: развернуть машину, предложить другую парковку, или водитель сам ставит машину у дома |
| **Virtual Thread** | Легковесный поток, управляемый JVM, а не ОС | Зелёные потоки: тысячи «виртуальных рабочих», которых JVM распределяет по реальным ядрам |

---

## Историческая справка: от Thread к Virtual Threads

История управления потоками в Java — это история постепенного повышения уровня абстракции. Каждый шаг решал проблемы предыдущего подхода.

**Java 1.0 (1996): голые потоки.** Единственный способ — `new Thread(runnable).start()`. Разработчик сам управлял жизненным циклом потоков, синхронизацией, завершением. Потоки создавались и умирали на каждую задачу. При высокой нагрузке это означало тысячи системных вызовов в секунду.

**Java 5 (2004): JSR-166 и java.util.concurrent.** Дуг Ли (Doug Lea) и его команда создали Executor Framework — пулы потоков, Future, BlockingQueue. Впервые появилось разделение между «что выполнить» (Runnable/Callable) и «как выполнить» (Executor). Потоки стали переиспользоваться, задачи — ставиться в очередь. Это был фундаментальный сдвиг: от императивного «создай поток» к декларативному «отправь задачу».

**Java 7 (2011): Fork/Join Framework.** JSR-166 получил расширение — ForkJoinPool с work stealing. Вместо одной общей очереди — у каждого потока своя. Свободный поток «крадёт» задачи у занятого. Это решило проблему неравномерного распределения нагрузки в вычислительных задачах.

**Java 8 (2014): CompletableFuture.** `Future.get()` блокирует поток. CompletableFuture добавил функциональный API: цепочки (`thenApply`), комбинирование (`thenCombine`), обработку ошибок (`exceptionally`). Асинхронный код перестал быть callback hell — стал читаемой цепочкой преобразований.

**Java 21 (2023): Virtual Threads (Project Loom).** Революция. Вместо тяжёлых OS-потоков (~1MB стека, системный вызов на создание) — легковесные виртуальные потоки, управляемые JVM. Создание virtual thread — наносекунды, не миллисекунды. Можно иметь миллионы одновременных потоков. Для I/O-bound задач это устраняет необходимость в пулах потоков и CompletableFuture-цепочках.

> **Ключевая идея:** Каждый шаг эволюции не отменяет предыдущий. Virtual Threads не убивают thread pools — они убивают thread pools для I/O-bound задач. Для CPU-bound задач классические пулы по-прежнему оптимальны.

---

## Prerequisites

| Что нужно знать | Где изучить |
|-----------------|-------------|
| Основы потоков: создание, жизненный цикл, context switching | [[jvm-concurrency-overview]] |
| Примитивы синхронизации: synchronized, volatile, CAS, locks | [[jvm-synchronization]] |
| BlockingQueue: producer-consumer, backpressure | [[jvm-concurrent-collections]] |

---

## Thread Pool Sizing: сколько потоков нужно

Размер пула потоков — один из самых важных и часто неправильно выбираемых параметров. Слишком мало потоков — ядра процессора простаивают. Слишком много — overhead от context switching съедает всю выгоду. Правильный размер зависит от характера задач.

### CPU-bound задачи: ядра решают

CPU-bound задачи (вычисления, парсинг, сериализация) загружают процессор на 100%. Добавление потоков сверх количества ядер не ускоряет работу — наоборот, каждое переключение контекста тратит ~1-10 микросекунд впустую. Потоки конкурируют за одни и те же ядра, и вместо полезной работы процессор тратит время на сохранение/восстановление регистров.

Формула для CPU-bound:

```
Оптимальный размер пула = N_cores + 1
```

Почему `+1`? Если один поток заблокируется (page fault, GC pause), дополнительный поток подхватит работу, и ядро не простаивает. Это рекомендация из книги Goetz (2006), подтверждённая многочисленными бенчмарками. Больше потоков — хуже: context switching начинает доминировать.

### I/O-bound задачи: ожидание доминирует

I/O-bound задачи (HTTP-запросы, чтение из базы, файловые операции) большую часть времени ждут ответа. Пока поток ждёт сетевой ответ (50-500мс), ядро процессора простаивает. Здесь больше потоков — лучше: пока одни ждут I/O, другие выполняют полезную работу.

Формула для I/O-bound (Little's Law adaptation):

```
Оптимальный размер пула = N_cores * (1 + W/C)

W = среднее время ожидания (wait time)
C = среднее время вычислений (compute time)
```

Например, при 8 ядрах, среднем времени ожидания 200мс и времени вычислений 20мс: `8 * (1 + 200/20) = 8 * 11 = 88 потоков`. Это значительно больше, чем количество ядер, потому что каждый поток «занимает» ядро лишь малую долю времени.

### Практические соображения

Формулы — отправная точка, не финальный ответ. В реальности задачи редко бывают чисто CPU-bound или чисто I/O-bound. Лучший подход — начать с формулы, затем нагрузочно тестировать и корректировать. Ключевые метрики: CPU utilization (должна быть 70-80%, не 100%), queue size (должен быть стабильным, не расти), и latency (p99, не среднее).

> **Подводный камень:** Общий ForkJoinPool (`ForkJoinPool.commonPool()`) имеет размер `Runtime.getRuntime().availableProcessors() - 1`. Если все потоки заблокированы на I/O (например, в `CompletableFuture.supplyAsync`), весь commonPool парализован — включая parallel streams. Всегда используйте отдельный пул для I/O операций.

Мы разобрали, сколько потоков нужно. Теперь разберём, какой тип пула выбрать — каждый из четырёх типов оптимизирован для своего класса задач.

---

## Типы Executor'ов: какой для какой задачи

Представьте стройку. Бригада рабочих — это thread pool. Но бригады бывают разные: фиксированная команда из 10 человек, бригада с наймом временных рабочих на пиковые дни, один мастер для деликатной последовательной работы, или бригада с будильником для периодических задач. Каждый тип Executor в Java — это определённый тип бригады.

### FixedThreadPool: стабильная бригада

`Executors.newFixedThreadPool(N)` создаёт пул из ровно N потоков. Задачи, для которых нет свободного потока, ставятся в неограниченную `LinkedBlockingQueue`. Потоки никогда не умирают, даже если нет задач — они ждут в `take()`.

Это самый простой и предсказуемый тип пула. Он подходит для стабильной, равномерной нагрузки: веб-сервер, обрабатывающий примерно одинаковое количество запросов. Проблема — неограниченная очередь. При перегрузке задачи копятся в очереди, потребляя память, вместо того чтобы сигнализировать о проблеме.

Следующий код показывает создание фиксированного пула и подводный камень — неограниченная очередь может расти бесконечно.

```java
// Простой, но опасный: неограниченная очередь
ExecutorService fixed = Executors.newFixedThreadPool(10);

// Безопаснее: явный ThreadPoolExecutor с bounded queue
ThreadPoolExecutor safe = new ThreadPoolExecutor(
    10, 10, 0L, TimeUnit.MILLISECONDS,
    new ArrayBlockingQueue<>(1000),       // Макс. 1000 задач в очереди
    new ThreadPoolExecutor.CallerRunsPolicy()  // При переполнении — backpressure
);
```

Вторая версия с `ThreadPoolExecutor` явно ограничивает очередь. При переполнении `CallerRunsPolicy` заставляет вызывающий поток сам выполнить задачу — это естественный backpressure, который замедляет producer вместо того, чтобы тихо съедать память.

### CachedThreadPool: найм временных рабочих

`Executors.newCachedThreadPool()` — полная противоположность FixedThreadPool. Пул начинается с 0 потоков. Каждая новая задача: если есть свободный поток — отдать ему (через `SynchronousQueue`), если нет — создать новый. Поток, простаивающий 60 секунд, уничтожается.

Этот тип идеален для коротких, бурстовых задач: пришло 100 запросов одновременно — создалось 100 потоков, обработали, через минуту без нагрузки — все уничтожились. Опасность: при стабильно высокой нагрузке пул создаёт неограниченное количество потоков. 10 000 одновременных медленных задач = 10 000 потоков = 10GB стека = OutOfMemoryError или полная деградация от context switching.

### WorkStealingPool: бригада взаимопомощи

`Executors.newWorkStealingPool()` (Java 8+) создаёт `ForkJoinPool` с количеством потоков, равным количеству доступных ядер. Главное отличие — work stealing: у каждого потока своя двусторонняя очередь (deque). Поток берёт задачи из своей очереди. Если очередь пуста — «крадёт» задачу из хвоста очереди соседа.

Представьте бригаду грузчиков, разгружающих несколько фур. Каждый грузчик работает со своей фурой. Кто разгрузил первым — не стоит без дела, а идёт помогать соседу, начиная с дальнего конца его очереди (чтобы не мешать). Это автоматическая балансировка нагрузки без центрального координатора.

Work stealing эффективен для задач с неравномерным временем выполнения. Если один поток получил тяжёлую задачу — другие не простаивают, а перехватывают его мелкие задачи. Важное ограничение: work stealing не помогает, если все задачи одинаково тяжёлые.

### ScheduledExecutorService: бригада с будильником

`Executors.newScheduledThreadPool(N)` — пул для отложенных и периодических задач. Заменяет устаревший `java.util.Timer`, который имел критический недостаток: один поток на все задачи. Если одна задача бросала исключение или зависала — все остальные останавливались.

Следующий код показывает два режима периодического выполнения и принципиальную разницу между ними.

```java
ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(2);

// Fixed rate: каждые 10 сек от НАЧАЛА предыдущего запуска
scheduler.scheduleAtFixedRate(() -> checkHealth(), 0, 10, TimeUnit.SECONDS);

// Fixed delay: 10 сек ПОСЛЕ ЗАВЕРШЕНИЯ предыдущего запуска
scheduler.scheduleWithFixedDelay(() -> syncData(), 0, 10, TimeUnit.SECONDS);
```

`scheduleAtFixedRate` — для метрик и мониторинга: «проверяй здоровье каждые 10 секунд». Если проверка занимает 3 секунды, следующая начнётся через 7 секунд. Если проверка занимает больше 10 секунд — следующая начнётся сразу после завершения текущей (но не параллельно). `scheduleWithFixedDelay` — для синхронизации: «подожди 10 секунд после завершения, потом запусти снова». Гарантирует паузу между запусками.

```
scheduleAtFixedRate (каждые 10 сек от старта):
|──задача 3с──|       |──задача 3с──|
0            3   ...  10           13

scheduleWithFixedDelay (10 сек после завершения):
|──задача 3с──|          |──задача 3с──|
0            3──10 сек──13           16
```

---

## Конфигурация ThreadPoolExecutor вручную

Фабричные методы `Executors.*` удобны, но скрывают опасные дефолты. В production рекомендуется создавать пулы вручную через `ThreadPoolExecutor`, явно контролируя каждый параметр.

Следующий код показывает полную конфигурацию production-ready пула с объяснением каждого параметра.

```java
ThreadPoolExecutor executor = new ThreadPoolExecutor(
    5,                                // corePoolSize: минимум потоков (всегда живы)
    10,                               // maximumPoolSize: макс. при пиках
    60L, TimeUnit.SECONDS,            // keepAliveTime: жизнь потоков сверх core
    new LinkedBlockingQueue<>(100),   // очередь С ОГРАНИЧЕНИЕМ
    new ThreadPoolExecutor.CallerRunsPolicy()  // политика при переполнении
);
```

Логика расширения работает так: сначала создаются `corePoolSize` потоков. Новые задачи идут в очередь. Если очередь заполнилась — создаются дополнительные потоки до `maximumPoolSize`. Если и максимум достигнут, и очередь полная — срабатывает rejection policy.

### Политики отказа

Что делать, когда пул и очередь переполнены? Четыре стандартных стратегии:

| Политика | Что делает | Когда использовать |
|----------|-----------|-------------------|
| `AbortPolicy` | Бросает `RejectedExecutionException` | Нужно явно знать о перегрузке (дефолт) |
| `CallerRunsPolicy` | Выполняет задачу в вызывающем потоке | Естественный backpressure: замедляет producer |
| `DiscardPolicy` | Молча отбрасывает задачу | Задачи не критичны (метрики, логи) |
| `DiscardOldestPolicy` | Отбрасывает самую старую задачу в очереди | Актуальность важнее полноты |

`CallerRunsPolicy` — часто лучший выбор для production. Когда пул перегружен, вызывающий поток (например, поток, принимающий HTTP-запросы) сам выполняет задачу. Это замедляет приём новых запросов — естественный backpressure без потери данных и без исключений.

Мы разобрали, как создавать и конфигурировать пулы потоков. Но `Future.get()` блокирует вызывающий поток. Как строить асинхронные цепочки без блокировки? Для этого в Java 8 появился CompletableFuture.

---

## CompletableFuture: асинхронная композиция

`Future` из Java 5 — это чек из химчистки. Ты отдал вещь (задачу), получил чек (Future), и можешь прийти за результатом (`get()`). Но чтобы получить результат, ты должен прийти и ждать у стойки. Нельзя сказать: «когда будет готово — позвоните мне». Нельзя сказать: «когда будет готово — сразу отнесите в ателье».

`CompletableFuture` (Java 8) — это чек с автоматической доставкой. Можно сказать: «когда готово — преобразуй результат (`thenApply`), скомбинируй с другим (`thenCombine`), при ошибке — верни запасной вариант (`exceptionally`)». Всё это без блокировки вызывающего потока.

### Цепочки преобразований (chaining)

CompletableFuture позволяет строить конвейер преобразований, где каждый шаг автоматически запускается после завершения предыдущего. Это декларативный подход: описываешь «что», а не «как».

Следующий код демонстрирует цепочку: запрос данных, преобразование, использование результата — всё без единого блокирующего вызова.

```java
CompletableFuture.supplyAsync(() -> fetchUser(userId), ioExecutor)
    .thenApply(user -> user.getName().toUpperCase())   // Преобразовать
    .thenApply(name -> "Hello, " + name)                // Ещё раз
    .thenAccept(greeting -> sendNotification(greeting)); // Использовать
```

Каждый `thenApply` создаёт новый CompletableFuture. Важный нюанс: без суффикса `Async` (как `thenApply`) шаг выполняется в том же потоке, что и предыдущий. С суффиксом `Async` (`thenApplyAsync`) — в ForkJoinPool.commonPool() или в указанном executor. Для CPU-лёгких преобразований `thenApply` эффективнее (нет overhead'а на переключение потока). Для тяжёлых — `thenApplyAsync` с отдельным пулом.

### Композиция: параллельные операции

Часто нужно запустить несколько операций параллельно и скомбинировать результаты. CompletableFuture предлагает несколько способов.

Следующий код показывает параллельный запуск трёх HTTP-запросов и комбинирование результатов — суммарное время равно времени самого медленного запроса, а не сумме всех трёх.

```java
CompletableFuture<User> userF = fetchUserAsync(id);        // 200мс
CompletableFuture<List<Order>> ordersF = fetchOrdersAsync(id); // 300мс
CompletableFuture<Settings> settingsF = fetchSettingsAsync(id); // 150мс

// Все три запущены параллельно. Общее время: ~300мс, не 650мс
CompletableFuture<Dashboard> dashboard = userF
    .thenCombine(ordersF, (user, orders) -> new Pair<>(user, orders))
    .thenCombine(settingsF, (pair, s) ->
        new Dashboard(pair.first(), pair.second(), s));
```

`thenCombine` ждёт завершения обоих futures и комбинирует результаты. `allOf` ждёт завершения всех. `anyOf` — завершения любого (для fallback/racing стратегий).

### Обработка ошибок

Ошибки в асинхронном коде — особая головная боль. В синхронном коде исключение поднимается по стеку вызовов. В асинхронном — исключение происходит в другом потоке, в другом контексте. CompletableFuture предлагает три стратегии.

Следующий код демонстрирует `exceptionally` для fallback-значения и `handle` для обработки обоих случаев (успех и ошибка) в одном месте.

```java
CompletableFuture.supplyAsync(() -> fetchFromPrimary())
    .exceptionally(ex -> {
        log.warn("Primary failed: {}", ex.getMessage());
        return fetchFromFallback();  // Запасной источник
    })
    .thenAccept(data -> process(data));

// Или handle — для обоих случаев
.handle((result, ex) -> {
    if (ex != null) return defaultValue;
    return transform(result);
});
```

`exceptionally` — это `catch` для асинхронного мира. `handle` — это `try/catch` в одной лямбде. `whenComplete` — аналог `finally`: выполняется всегда, но не меняет результат.

Важное предупреждение: исключение внутри `supplyAsync` не логируется и не падает — оно «запаковывается» в CompletableFuture. Если никто не вызовет `get()`, `join()`, `exceptionally()` или `handle()` — ошибка будет тихо проглочена. Это одна из самых частых причин «молчаливых» багов в асинхронном коде.

---

## Fork/Join Framework: разделяй и властвуй

Fork/Join Framework (Java 7) — реализация паттерна «разделяй и властвуй» (divide and conquer) для параллельных вычислений. Идея: разбить большую задачу на подзадачи, выполнить их параллельно, скомбинировать результаты.

Представьте бригаду грузчиков, разгружающих огромный склад. Бригадир делит склад на секции — каждый грузчик берёт свою. Кто закончил раньше — не стоит без дела, а идёт к соседу и берёт часть его работы с дальнего конца (чтобы не мешать). Это work stealing — ключевой механизм Fork/Join.

### Work Stealing: автоматическая балансировка

В обычном пуле потоков одна очередь задач на всех. В ForkJoinPool — у каждого потока своя двусторонняя очередь (deque). Поток добавляет подзадачи в голову своей deque и берёт оттуда же (LIFO — последняя добавленная первой). Когда deque пуста — поток «крадёт» задачу из хвоста deque другого потока (FIFO — самую старую).

```
Поток A (deque):         Поток B (deque):
  голова                   голова
  ┌─────────┐             ┌─────────┐
  │ задача 5│ ← берёт     │         │ (пусто!)
  │ задача 4│              │         │
  │ задача 3│              └─────────┘
  │ задача 2│                  │
  │ задача 1│ ← B крадёт ─────┘
  └─────────┘
  хвост                    B крадёт самую старую задачу A
```

Почему LIFO для своих задач и FIFO для кражи? Свои задачи — мелкие (только что fork'нутые подзадачи текущей задачи), они быстро выполняются и нужны для join. Чужие задачи — крупные (давно лежат в очереди), их выполнение имеет больший эффект. Кроме того, LIFO/FIFO-доступ с разных концов deque минимизирует конкуренцию между владельцем и «вором».

### Выбор порога (threshold)

Fork/Join эффективен, когда задача делится на подзадачи. Но деление тоже стоит ресурсов: создание объекта задачи, помещение в deque, overhead fork/join. Если делить слишком мелко — overhead доминирует. Если слишком крупно — параллелизм не используется.

Правило: порог (threshold) должен давать 100-10000 элементов на подзадачу. Для массива из 1 000 000 элементов и 8 ядер: `1_000_000 / (8 * 4) = ~30_000` — разумный порог. Множитель 4 обеспечивает запас для work stealing.

> **Практический совет:** Начните с `N / (parallelism * 4)` и подстройте по бенчмарку. Слишком мелкий порог хуже, чем слишком крупный: overhead создания задач растёт линейно, а выигрыш от параллелизма — логарифмически.

---

## Virtual Threads (Java 21): новая эра

Virtual Threads — самое значительное изменение модели concurrency в Java за 20 лет. Они переворачивают фундаментальное ограничение: «потоки дорогие, экономьте их».

### Проблема, которую решают Virtual Threads

Platform thread (обычный Java-поток) — это обёртка над потоком операционной системы. Каждый platform thread занимает ~1MB стека и требует системного вызова для создания. При 10 000 одновременных I/O-операций нужно 10 000 потоков — это 10GB только на стеки. Поэтому появились пулы потоков (ограничиваем количество) и асинхронные API (CompletableFuture, реактивные фреймворки) — чтобы обходиться меньшим числом потоков.

Но асинхронный код сложнее синхронного. CompletableFuture-цепочки трудно читать и отлаживать. Stack trace показывает lambda вместо бизнес-логики. Точки останова в debugger не работают привычным образом.

### Как работают Virtual Threads

Virtual thread — это объект в куче JVM, а не поток ОС. Он не привязан к конкретному OS-потоку. Когда virtual thread выполняет I/O (сетевой запрос, чтение файла, `Thread.sleep`), JVM автоматически «отсоединяет» его от carrier thread (OS-потока) и отдаёт carrier другому virtual thread. Когда I/O завершается — virtual thread «присоединяется» к любому свободному carrier.

```
Carrier threads (OS):    [C1]  [C2]  [C3]  [C4]

Virtual threads:         VT1 выполняется на C1
                         VT2 ждёт I/O (отсоединён)
                         VT3 выполняется на C2
                         VT4 ждёт I/O (отсоединён)
                         ...
                         VT10000 выполняется на C3

Результат: 10000 virtual threads на 4 carrier threads
```

Создание virtual thread — наносекунды. Переключение — микросекунды (внутри JVM, без системного вызова). Стек растёт и сжимается динамически. Можно создать миллион virtual threads без проблем.

### Как Virtual Threads меняют подход к Executors

С virtual threads для I/O-bound задач не нужен пул — создавайте новый virtual thread на каждую задачу. Паттерн «thread-per-request» снова становится жизнеспособным.

Следующий код показывает, как virtual threads упрощают код: вместо CompletableFuture-цепочек — простой синхронный код в virtual thread.

```java
// До Virtual Threads: CompletableFuture-цепочка
CompletableFuture.supplyAsync(() -> fetchUser(id), ioPool)
    .thenApply(user -> enrichUser(user))
    .thenAccept(user -> saveUser(user));

// С Virtual Threads: простой синхронный код
Thread.startVirtualThread(() -> {
    User user = fetchUser(id);     // Блокирует VT, не carrier
    User enriched = enrichUser(user);
    saveUser(enriched);
});
```

Второй вариант проще, читабельнее, и имеет нормальный stack trace при отладке. Блокирующий вызов `fetchUser(id)` приостанавливает virtual thread, но не блокирует carrier — JVM автоматически переключает carrier на другой virtual thread.

### Когда Virtual Threads НЕ помогают

Virtual Threads оптимизированы для I/O-bound задач. Для CPU-bound задач они не дают преимущества и могут быть вредны. Если задача непрерывно использует CPU, virtual thread не может «отсоединиться» от carrier — он занимает OS-поток точно так же, как platform thread.

Также virtual threads плохо работают с `synchronized` блоками и `ReentrantLock`: при блокировке на мониторе virtual thread «закрепляется» (pinning) на carrier thread и не может быть отсоединён. Это нивелирует преимущества virtual threads. Рекомендуется использовать `java.util.concurrent.locks.Lock` вместо `synchronized`.

```
Virtual Threads — использовать:
  ✅ HTTP-серверы (тысячи одновременных запросов)
  ✅ Работа с базами данных (JDBC блокирует)
  ✅ Файловый I/O
  ✅ Микросервисная коммуникация

Virtual Threads — НЕ использовать:
  ❌ CPU-intensive вычисления (используйте ForkJoinPool)
  ❌ Код с synchronized блоками (pinning)
  ❌ Задачи, использующие ThreadLocal для кэширования (миллионы копий!)
```

---

## Типичные ошибки

### Забытый shutdown

Non-daemon потоки в пуле не дают JVM завершиться. Без `shutdown()` программа висит бесконечно, даже если `main()` давно завершился.

```java
// ❌ Программа не завершится — потоки пула не демоны
ExecutorService executor = Executors.newFixedThreadPool(10);
executor.submit(task);
// main() завершается, но JVM висит — ждёт потоки пула

// ✅ Всегда shutdown в finally или try-with-resources (Java 19+)
try { executor.submit(task); }
finally { executor.shutdown(); }
```

Без `shutdown()` потоки пула продолжают ждать задачи в `take()`, и JVM не может завершиться. Это одна из самых частых причин «зависших» Java-процессов в production.

### Бесконечная очередь

```java
// ❌ newFixedThreadPool: неограниченная LinkedBlockingQueue
ExecutorService bad = Executors.newFixedThreadPool(2);
// При 10000 задач/сек и 2 потоках — очередь растёт бесконечно → OOM

// ✅ Явный bounded queue + rejection policy
ThreadPoolExecutor good = new ThreadPoolExecutor(
    2, 2, 0L, TimeUnit.MILLISECONDS,
    new ArrayBlockingQueue<>(100),
    new ThreadPoolExecutor.CallerRunsPolicy()
);
```

Это фундаментальная проблема фабричных методов `Executors.*`: они скрывают конфигурацию очереди. `newFixedThreadPool` использует `LinkedBlockingQueue` без лимита. Многие production-аварии начинались с безобидного `Executors.newFixedThreadPool(10)`.

### Проглоченные исключения

Исключение в задаче, отправленной через `execute()`, печатается в stderr. Исключение в задаче через `submit()` — запаковывается в Future. Если никто не вызовет `get()` — ошибка потеряна навсегда.

```java
// ❌ Исключение проглочено — никто не вызовет get()
executor.submit(() -> { throw new RuntimeException("Ошибка"); });

// ✅ Обработка через Future.get()
Future<?> f = executor.submit(() -> { throw new RuntimeException(); });
try { f.get(); }
catch (ExecutionException e) { log.error("Task failed", e.getCause()); }
```

Альтернативный подход — переопределить `afterExecute()` в подклассе `ThreadPoolExecutor` и логировать все исключения централизованно.

---

## Распространённые заблуждения

| Заблуждение | Почему это неправильно |
|-------------|----------------------|
| «Executors.newFixedThreadPool безопасен из коробки» | Неограниченная очередь (`LinkedBlockingQueue` без лимита) может вызвать OutOfMemoryError при перегрузке. В production используйте `ThreadPoolExecutor` с явной bounded queue |
| «Больше потоков = быстрее» | Для CPU-bound задач оптимум — N_cores + 1. Сверх этого context switching доминирует: каждое переключение ~1-10 мкс, при 10000 потоков — миллисекунды overhead |
| «CompletableFuture всегда асинхронный» | Без суффикса `Async` (`thenApply`, `thenAccept`) шаг выполняется в потоке, завершившем предыдущий шаг. `thenApplyAsync` — в ForkJoinPool.commonPool() |
| «Virtual Threads решают все проблемы concurrency» | Virtual Threads оптимальны для I/O-bound. CPU-bound задачи по-прежнему нуждаются в platform thread pool с ограниченным размером. Synchronized блоки вызывают pinning |
| «Future.get() — антипаттерн» | `get()` нормален для простых случаев. Проблема — блокировать пул потоков вызовами `get()`. CompletableFuture — когда нужны цепочки и композиция |
| «ForkJoinPool.commonPool() подходит для всего» | commonPool имеет размер `N_cores - 1`. Если заблокировать его I/O-задачами — парализуются и parallel streams, и все CompletableFuture без явного executor |

---

## Подводные камни и когда НЕ применяется

**ThreadLocal в пулах потоков.** `ThreadLocal` хранит значение per-thread. В пуле потоков один поток обрабатывает тысячи запросов — ThreadLocal из предыдущего запроса «утекает» в следующий. При использовании virtual threads проблема усугубляется: миллион virtual threads = миллион копий ThreadLocal, что может исчерпать память.

**CompletableFuture и исключения в цепочках.** Исключение на любом этапе цепочки пропускает все последующие `thenApply` и попадает в ближайший `exceptionally` или `handle`. Если обработчика ошибок нет — исключение запаковано в CompletableFuture и молчит. Всегда завершайте цепочку обработчиком ошибок.

**ScheduledExecutorService и исключения.** Если задача в `scheduleAtFixedRate` бросает исключение — все последующие запуски отменяются без уведомления. Всегда оборачивайте тело периодической задачи в try/catch.

---

## CS-фундамент

| CS-концепция | Применение в Executors |
|--------------|------------------------|
| **Thread Pool Pattern** | Переиспользование потоков вместо создания новых. Амортизация стоимости создания |
| **Producer-Consumer** | submit (producer) → BlockingQueue → worker thread (consumer) |
| **Work Stealing** | ForkJoinPool: свободный поток крадёт задачи из deque занятого. Автобалансировка |
| **Future/Promise** | Future = placeholder для будущего результата. CompletableFuture = Future + Promise |
| **Backpressure** | Bounded queue + rejection policy предотвращают перегрузку системы |
| **Little's Law** | L = λW — связь между количеством потоков, пропускной способностью и latency |

---

## Связь с другими темами

**[[jvm-concurrency-overview]]** — Executors и CompletableFuture — высокоуровневые абстракции над потоками и синхронизацией, описанными в обзоре concurrency. ThreadPoolExecutor инкапсулирует создание потоков, их lifecycle management и распределение задач, избавляя от ручного управления Thread'ами. Понимание low-level концепций (thread states, context switching, memory visibility) помогает правильно настраивать пулы: corePoolSize зависит от характера задач (CPU-bound vs I/O-bound), а rejection policy — от требований к надёжности. Рекомендуется изучить основы потоков, затем executors как production-ready абстракцию.

**[[jvm-synchronization]]** — Executors используют внутренне примитивы синхронизации: ThreadPoolExecutor основан на ReentrantLock для управления состоянием пула, BlockingQueue для очереди задач, и volatile для shutdown flag. CompletableFuture использует CAS для атомарного перехода между состояниями (pending → completed → consumed). Понимание синхронизации помогает диагностировать проблемы executor'ов: почему задача «зависла» (deadlock в пуле), почему исключение проглочено (Future.get не вызван), почему программа не завершается (non-daemon потоки в пуле).

**[[java-modern-features]]** — Virtual Threads (Java 21) — эволюция executor model: вместо пула из N platform threads — миллионы lightweight virtual threads. Для I/O-bound задач Virtual Threads устраняют необходимость в CompletableFuture цепочках — можно писать блокирующий код, а JVM автоматически yield'ит virtual thread при I/O. Для CPU-bound задач platform thread pools по-прежнему необходимы. Structured Concurrency (preview в Java 21) идёт ещё дальше, связывая жизненный цикл дочерних задач с родительским scope. Изучите executors для понимания фундамента, затем Virtual Threads как современную альтернативу для I/O-intensive приложений.

---

## Источники и дальнейшее чтение

- Goetz B. et al. (2006). *Java Concurrency in Practice*. — Каноническое руководство по concurrency на JVM. Главы 6 «Task Execution» и 8 «Applying Thread Pools» детально разбирают executors: выбор типа пула, настройка ThreadPoolExecutor, thread pool sizing формулы, rejection и saturation policies. Обязательна к прочтению.
- Lea D. (2000). *Concurrent Programming in Java: Design Principles and Patterns*, 2nd Edition. — Книга от создателя java.util.concurrent и Fork/Join Framework. Описывает теоретические основы work stealing, design decisions behind ExecutorService, и эволюцию от голых потоков к высокоуровневым абстракциям.
- Oaks S. (2020). *Java Performance: In-Depth Advice for Tuning and Programming Java 8, 11, and Beyond*, 2nd Edition. — Главы о threading и ForkJoinPool описывают performance characteristics разных типов пулов, sizing strategies, влияние на GC, и практические бенчмарки для выбора оптимальной конфигурации.

---

*Проверено: 2026-02-11 — Педагогический контент проверен*
