---
title: "JIT Compiler: как JVM ускоряет код"
created: 2025-11-25
modified: 2026-02-11
tags:
  - topic/jvm
  - jit
  - performance
  - compilation
  - type/deep-dive
  - level/beginner
type: deep-dive
status: published
area: programming
confidence: high
related:
  - "[[jvm-performance-overview]]"
  - "[[jvm-profiling]]"
  - "[[jvm-benchmarking-jmh]]"
---

# JIT Compiler: как JVM ускоряет код

> **TL;DR:** JIT компилирует bytecode в native код во время выполнения. Tiered compilation: Interpreter (медленно) → C1 (~2000 вызовов, 10x ускорение) → C2 (~15000 вызовов, 100x ускорение). Ключевые оптимизации: inlining, escape analysis (объекты на стеке), loop unrolling. Warmup = 10-60 сек. После прогрева JVM часто быстрее C++ благодаря profile-guided оптимизациям.

---

## Пререквизиты

| Тема | Зачем нужно | Где изучить |
|------|-------------|-------------|
| Как работает JVM | Понимать путь от bytecode к выполнению | [[jvm-basics-history]] |
| Bytecode basics | Что компилирует JIT | [[jvm-virtual-machine-concept]] |
| CPU architecture basics | Понимать native code, регистры, cache | [[os-overview]] |

---

## Зачем это знать

JIT Compiler — причина, по которой Java-приложения могут конкурировать по производительности с C/C++. Без понимания JIT невозможно объяснить, почему первые запросы к сервису медленнее последующих, почему микробенчмарки без JMH бессмысленны, и почему "Java медленная" — устаревшее заблуждение.

Для инженеров, работающих с latency-sensitive системами (trading, real-time, gaming), знание JIT критично: warmup стратегии, deoptimization, tiered compilation — всё это напрямую влияет на production-поведение приложения.

---

## Терминология для новичков

| Термин | Что это | Аналогия из жизни |
|--------|---------|-------------------|
| **JIT** | Just-In-Time — компиляция во время выполнения | Синхронный переводчик на конференции |
| **Interpreter** | Построчное выполнение байткода (медленно, но сразу) | Читать инструкцию и делать по шагу |
| **C1 (Client)** | Быстрый компилятор с базовыми оптимизациями | Черновой перевод — быстро, но неидеально |
| **C2 (Server)** | Медленный компилятор с агрессивными оптимизациями | Литературный перевод — долго, но качественно |
| **Warmup** | Период прогрева, пока JIT компилирует код | Разогрев спортсмена перед соревнованием |
| **Inlining** | Замена вызова метода на его тело | Вместо "см. стр. 42" вставить текст прямо сюда |
| **Escape Analysis** | Анализ: "убегает" ли объект из метода | Проверка: нужно ли упаковывать подарок, если он останется дома |
| **Deoptimization** | Откат скомпилированного кода в интерпретатор | Отмена оптимистичного решения, когда оно оказалось неверным |
| **OSR** | On-Stack Replacement — замена кода во время выполнения | Замена двигателя на ходу |
| **Profile-Guided** | Оптимизации на основе реального поведения | Настройка маршрута на основе пробок |

---

## Историческая справка

Идея JIT-компиляции не родилась вместе с Java. Её корни уходят в исследовательский проект Self VM (Chambers, Ungar, 1989) в Стэнфордском университете. Self — динамический объектно-ориентированный язык без классов, где всё основано на прототипах. Для достижения приемлемой производительности в Self были придуманы ключевые техники, ставшие фундаментом современных JIT: adaptive compilation (компиляция горячего кода), type feedback (использование информации о типах в runtime) и method inlining на основе профилирования.

Когда Sun Microsystems создавала Java в начале 1990-х, первые версии JVM использовали только интерпретатор — каждая инструкция байткода исполнялась пошагово. Java 1.0 (1996) была действительно медленной: в 10-50 раз медленнее C. Это породило стигму "Java is slow", которая преследует язык до сих пор, несмотря на то что ситуация радикально изменилась.

В 1999 году Sun приобрела компанию Animorphic Systems, создателей Self VM. Команда Self (Urs Hölzle, Lars Bak и другие) адаптировала свои технологии для Java и создала **HotSpot VM** — JVM с адаптивной компиляцией. Название "HotSpot" буквально описывает подход: VM определяет "горячие точки" (hot spots) — часто выполняемый код — и компилирует именно их. HotSpot стал default JVM в Java 1.3 (2000) и остаётся основной реализацией до сих пор.

Изначально HotSpot содержал два отдельных JIT-компилятора: C1 (Client) для быстрого старта десктопных приложений и C2 (Server) для максимальной производительности серверных. В Java 7 (2011) появилась tiered compilation — объединение обоих компиляторов в единый pipeline, где C1 даёт быстрый буст, а C2 доводит горячий код до максимальной производительности.

В 2012 году начался проект **Graal** (Oracle Labs), который предложил радикально иной подход: написать JIT-компилятор **на Java**, а не на C++ как C2. Graal стал основой GraalVM и был включён как экспериментальный JIT-компилятор в JDK 10 (2018). Graal проще в разработке и поддержке, а для некоторых workloads показывает производительность на 10-15% выше C2.

---

## Аналогия: система обучения пилотов

Tiered compilation можно понять через аналогию с системой подготовки пилотов гражданской авиации.

На первом этапе курсант проходит теоретическое обучение и летает на тренажёре (**Interpreter**, Level 0). Тренажёр медленный и не передаёт все нюансы реального полёта, но позволяет начать немедленно, без риска. Ошибки безопасны, и инструктор собирает данные: какие манёвры курсант выполняет чаще, где допускает ошибки.

На втором этапе курсант пересаживается на учебный самолёт с инструктором (**C1**, Level 3). Полёт уже реальный и быстрее, но инструктор рядом — продолжает наблюдать и записывать: какие маршруты летает курсант, какие погодные условия встречает, где теряет время. Этот этап — компромисс между скоростью и сбором информации.

На третьем этапе курсант сдаёт экзамен и летает самостоятельно (**C2**, Level 4). Теперь он оптимизирует полёт на основе всего накопленного опыта: знает лучшие маршруты, оптимальные высоты, привычки конкретных диспетчеров. Полёт максимально эффективен, но потребовал месяцев подготовки. Если условия резко меняются (новый аэропорт, незнакомый тип воздушного пространства), пилот может временно вернуться к полётам с инструктором (**deoptimization**).

---

## Tiered Compilation: от интерпретатора к native

Современная HotSpot JVM использует многоуровневую компиляцию. Каждый метод проходит через несколько стадий, постепенно ускоряясь.

### Пять уровней компиляции

JVM Specification определяет пять уровней (levels) компиляции, хотя на практике не все из них используются для каждого метода.

**Level 0: Interpreter** — JVM начинает с интерпретации байткода. Каждая инструкция байткода (iload, iadd, invokevirtual и т.д.) читается по одной, декодируется и выполняется. Это медленно — в 10-100 раз медленнее native кода — но начинается мгновенно, без затрат на компиляцию. Интерпретатор также собирает базовую статистику: invocation counter (сколько раз вызван метод) и back-edge counter (сколько раз выполнены обратные переходы в циклах).

**Level 1: C1 без профилирования** — простая компиляция C1 без встраивания инструментации. Используется для методов, которые JVM считает "тривиальными" — getter/setter, методы из нескольких строк. JVM определяет, что собирать профиль для таких методов бессмысленно: они либо будут inline'ены, либо слишком просты для агрессивных оптимизаций.

**Level 2: C1 с ограниченным профилированием** — компиляция C1 с простым счётчиком вызовов. Этот уровень используется как промежуточный, когда очередь компиляции C2 перегружена. Вместо того чтобы заставлять метод ждать на интерпретаторе, JVM компилирует его C1 с минимальной инструментацией.

**Level 3: C1 с полным профилированием** — основной уровень C1. После ~2000 вызовов метод компилируется C1, и в генерированный код встраивается детальная инструментация. Эта инструментация собирает type profiles (какие типы объектов встречаются в call sites), branch profiles (какие ветви if/else выполняются чаще), и exception profiles. Именно эти данные C2 использует для своих агрессивных оптимизаций.

**Level 4: C2 с полной оптимизацией** — после ~15000 вызовов и на основе собранного профиля C2 перекомпилирует метод с агрессивными оптимизациями. Компиляция занимает 100-500 мс, но результат — код, близкий по скорости к ручному C/C++, а иногда и быстрее благодаря profile-guided оптимизациям, которые недоступны статическим компиляторам.

```
Метод вызывается впервые
       │
       ▼
Level 0: Interpreter
       │ Собирает invocation + back-edge counters
       │ ~2000 вызовов
       ▼
Level 3: C1 + профилирование
       │ Генерирует native code + инструментация
       │ Собирает type/branch/exception profiles
       │ ~15000 вызовов
       ▼
Level 4: C2 (агрессивные оптимизации)
       │ Использует собранный профиль
       ▼
Максимальная производительность
```

### Почему не сразу C2?

Это компромисс между временем запуска и пиковой производительностью. Если бы JVM ждала 15000 вызовов перед первой компиляцией, короткоживущие программы работали бы только на интерпретаторе. C1 даёт быстрый буст (10x к интерпретатору), пока C2 собирает данные для финальной оптимизации.

Кроме того, C2 **нуждается** в данных профилирования для эффективной работы. Без type profiles C2 не может делать speculative inlining. Без branch profiles не может оптимизировать layout кода. Level 3 (C1 с профилированием) — не просто "промежуточный шаг", а необходимый этап сбора данных.

Tiered compilation можно отключить (`-XX:-TieredCompilation`), и тогда JVM будет использовать только C2 (или только C1 с `-client`). Это иногда делают для trading-систем, где предсказуемость важнее быстрого старта: лучше медленный, но стабильный warmup, чем перекомпиляция кода в критический момент.

Важный нюанс: переход между уровнями не линеен. Метод может перескочить с Level 0 сразу на Level 1 (для тривиальных методов), или остаться на Level 2, если очередь C2-компиляции переполнена. JVM динамически адаптирует стратегию на основе текущей нагрузки: если потоки компиляции C2 заняты, JVM предпочтёт быструю компиляцию C1 (Level 2), чтобы код не оставался на интерпретаторе.

### Наблюдение за компиляцией

```bash
java -XX:+PrintCompilation MyApp

# Вывод:
#  76   3   java.lang.String::hashCode (55 bytes)
#        ↑   ↑
#  время уровень  метод (размер байткода)

#  77   4   MyApp::compute (10 bytes)
#        ↑
#   Level 4 = C2, максимальная оптимизация
```

---

## Ключевые оптимизации JIT

JIT применяет десятки оптимизаций, но несколько из них дают основной выигрыш. Понимание этих оптимизаций помогает писать код, который JIT может эффективно ускорить.

### Method Inlining

Inlining — замена вызова метода на его тело. Это, пожалуй, самая важная оптимизация JIT, и вот почему: inlining не просто устраняет накладные расходы вызова (сохранение регистров, переход, возврат) — он **открывает возможности для дальнейших оптимизаций**.

> **Аналогия:** Представьте кулинарную книгу, где рецепт пирога говорит: "для крема см. стр. 42, для теста см. стр. 67". Каждый раз вам нужно листать, искать, возвращаться. Inlining — это вклеить рецепт крема и теста прямо в рецепт пирога. Теперь, видя весь рецепт целиком, вы замечаете: "тесто и крем используют одинаковое масло — могу отмерить один раз". Эта оптимизация (constant folding) была бы невозможна, пока рецепты были на разных страницах.

Когда код разбит на маленькие методы, каждый метод оптимизируется изолированно. JIT не видит, что `getValue()` всегда возвращает константу, потому что это другой метод. После inlining'а код объединяется, и становятся возможны constant folding, dead code elimination и другие оптимизации.

Inlining виртуальных методов — особенно мощная техника. В Java почти все методы виртуальные (не static, не final, не private), и каждый вызов теоретически требует virtual dispatch — поиска нужной реализации через vtable. Это дорого: чтение из vtable, непрямой переход, невозможность предсказания ветвления. JIT анализирует type profile и определяет: если в данном call site всегда встречался один тип (monomorphic), можно inline'ить конкретную реализацию напрямую, добавив guard-проверку типа.

Если call site bimorphic (два типа), JIT может сделать два inline'а с проверками. Если megamorphic (три и более типа) — inlining невозможен, и используется virtual dispatch. Именно поэтому megamorphic call sites — одна из главных причин потери производительности в Java.

**Ограничения inlining:**
- Методы > 35 байт байткода не inline'ятся по умолчанию (`-XX:MaxInlineSize=35`)
- Для частых методов порог выше — до 325 байт (`-XX:FreqInlineSize=325`)
- Рекурсивные методы inline'ятся ограниченно (1-2 уровня)
- Общая глубина inline-дерева ограничена (`-XX:MaxInlineLevel=15`)

### Escape Analysis

JVM анализирует, "убегает" ли объект из метода — то есть сохраняется ли ссылка на него где-то, откуда её можно получить после возврата из метода.

Escape Analysis — это анализ потока данных, определяющий scope жизни объекта. Результат анализа классифицирует объект на три уровня: **NoEscape** (объект не покидает метод и не виден другим потокам), **ArgEscape** (объект передан в вызываемый метод, но не сохранён глобально), **GlobalEscape** (объект сохранён в heap и доступен глобально).

Для NoEscape-объектов JVM может применить три мощных оптимизации.

**Scalar Replacement** — объект "разбирается" на отдельные переменные (скаляры). Вместо создания объекта Point с полями x и y на heap, JIT создаёт две локальные переменные `point_x` и `point_y`. Нет объекта — нет heap allocation, нет GC pressure, нет cache miss при обращении к полям. Это самая частая и самая эффективная оптимизация EA.

**Stack Allocation** — объект выделяется на стеке вместо heap. В отличие от scalar replacement, объект остаётся объектом, но живёт на стеке и автоматически освобождается при выходе из метода. На практике в HotSpot JVM stack allocation реализована через scalar replacement, а не как отдельный механизм — JIT разбирает объект на скаляры и размещает их в регистрах или на стеке.

**Lock Elision** — если синхронизированный объект не убегает из потока, блокировку можно убрать. Зачем захватывать lock, если ни один другой поток не может получить доступ к объекту? Это важно для legacy-кода, использующего `StringBuffer` (синхронизированный) вместо `StringBuilder`: если StringBuffer локален для метода, JIT уберёт всю синхронизацию.

```java
// Исходный код
public int sumPoints() {
    Point p = new Point(10, 20);  // Объект не убегает!
    return p.x + p.y;
}

// Что делает JIT (scalar replacement)
public int sumPoints() {
    int p_x = 10;  // Нет объекта, нет allocation
    int p_y = 20;  // Нет GC pressure
    return p_x + p_y;  // → 30 (constant folding)
}
```

**Когда объект "убегает":**
- Возвращается из метода (`return new Point(x, y)`)
- Присваивается в поле объекта (`this.cache = new Point(x, y)`)
- Передаётся в метод, который JIT не смог inline'ить
- Сохраняется в коллекцию (`list.add(new Point(x, y))`)

Важный нюанс: если вызываемый метод inline'ен, JIT может проанализировать его код и определить, что объект не убегает и внутри него. Поэтому inlining и escape analysis работают синергически: чем больше методов inline'ено, тем больше объектов JIT может оптимизировать.

### Loop Unrolling

Цикл имеет накладные расходы: проверка условия, инкремент счётчика, переход. При разворачивании JIT копирует тело цикла несколько раз, уменьшая количество итераций и overhead.

Кроме уменьшения branch overhead, loop unrolling позволяет процессору лучше использовать instruction-level parallelism (ILP). Современные CPU имеют несколько execution units, которые могут выполнять независимые инструкции параллельно. В развёрнутом цикле больше независимых операций доступны одновременно, и pipeline процессора загружен эффективнее.

JIT также может применить loop vectorization после unrolling: если тело цикла — простая арифметическая операция над массивом, развёрнутые итерации могут быть объединены в одну SIMD-инструкцию (SSE, AVX, AVX-512), обрабатывающую 4-16 элементов за такт.

Loop unrolling контролируется флагом `-XX:LoopUnrollLimit`. Слишком агрессивный unrolling может увеличить размер кода и ухудшить instruction cache hit rate, поэтому JIT балансирует factor unrolling на основе размера тела цикла и доступного кэша.

### Dead Code Elimination

JIT удаляет код, результат которого не используется. Это звучит тривиально, но имеет огромное значение для бенчмаркинга. Если результат вычисления не используется (не выводится, не сохраняется, не возвращается), JIT имеет полное право удалить всё вычисление — включая циклы, вызовы методов и создание объектов.

Именно поэтому наивные бенчмарки показывают бессмысленные результаты. Если вы вычисляете что-то в цикле, но не используете результат, JIT может удалить весь цикл. Время выполнения — 0 наносекунд. JMH (Java Microbenchmark Harness) решает эту проблему через `Blackhole` — специальный объект, который "потребляет" значения, не давая JIT их устранить.

Dead code elimination работает совместно с другими оптимизациями, образуя каскад. После inlining JIT может обнаружить, что inline'енный метод всегда возвращает константу — и заменить весь вызов на эту константу (constant propagation). Затем код, зависящий от этой "переменной", упрощается (constant folding), и некоторые ветви становятся unreachable (dead code). Этот каскад — inlining → constant propagation → constant folding → dead code elimination → дальнейший inlining — объясняет, почему маленькие методы в Java не имеют overhead: JIT устраняет не только вызов, но и весь redundant код, который становится виден после объединения.

Ещё одна важная оптимизация — **null check elimination**. Java гарантирует NullPointerException при обращении к null-ссылке. Это означает, что перед каждым обращением к полю или вызовом метода JVM должна проверить, не null ли ссылка. JIT оптимизирует это: если профиль показывает, что ссылка никогда не была null, проверка заменяется на uncommon trap. На уровне машинного кода null check вообще отсутствует — вместо этого используется hardware trap (segfault при обращении к адресу ~0), который JVM перехватывает и конвертирует в NullPointerException. Это zero-cost в нормальном случае.

---

## Deoptimization: когда оптимизации отменяются

JIT делает предположения на основе наблюдаемого поведения. Если предположение нарушается, скомпилированный код становится некорректным и должен быть отменён. Этот процесс называется deoptimization — откат из скомпилированного кода обратно в интерпретатор.

### Speculative Optimization и её цена

JIT наблюдает, что виртуальный метод всегда вызывается на объектах одного типа (monomorphic call site). Он встраивает код конкретной реализации напрямую, избегая виртуального вызова. Но это **предположение**, а не гарантия: Java — динамический язык, и новый подкласс может появиться в любой момент (через загрузку нового класса, reflection, или динамический прокси).

```java
List<Animal> animals = getAnimals();
for (Animal a : animals) {
    a.sound();  // JIT видит: всегда Dog → inline Dog::sound()
}
// Позже появляется Cat в коллекции
// → guard check fails → DEOPTIMIZATION
```

При deoptimization происходит следующее: JIT вставляет проверку (guard) перед speculative кодом. Если guard проверка обнаруживает неожиданный тип, выполнение останавливается, состояние стека восстанавливается до эквивалентного состоянию интерпретатора (через mapping, сохранённый при компиляции), и выполнение продолжается в интерпретаторе. Затем метод перекомпилируется — на этот раз с учётом нового типа.

Deoptimization — не катастрофа, но имеет ощутимую цену. Во-первых, сам процесс отката и восстановления стека занимает время. Во-вторых, метод возвращается на интерпретатор и должен заново пройти tiered compilation. В-третьих, перекомпилированная версия может быть менее оптимальной (bimorphic dispatch вместо monomorphic inline). Для hot path в latency-sensitive системах deoptimization может вызвать latency spike в десятки миллисекунд.

### Uncommon Traps

JIT оптимизирует под частый путь выполнения, а редкие ветви заменяет на uncommon traps — специальные маркеры, которые при срабатывании вызывают deoptimization.

```java
public int divide(int a, int b) {
    if (b == 0) {  // Профиль: 0.001% случаев
        throw new ArithmeticException();  // → uncommon trap
    }
    return a / b;  // Основной путь — оптимизирован
}
```

Код для `b == 0` не генерируется в скомпилированной версии. Вместо него JIT вставляет trap — если условие срабатывает, выполнение переходит в интерпретатор. Это позволяет компилятору генерировать более компактный и быстрый код для основного пути: нет ветвления, нет кода обработки ошибки, процессор лучше предсказывает переходы.

Если uncommon trap срабатывает достаточно часто (порог: ~100 раз для одного trap), JIT "учится" и перекомпилирует метод, включая ранее редкий путь в оптимизированный код. Это адаптивная система: JIT подстраивается под изменяющееся поведение программы.

Типичные причины deoptimization:
- **Class hierarchy change:** загрузка нового подкласса, нарушающего monomorphic assumption
- **Unstable if:** ветвь, которая раньше никогда не выполнялась, начинает выполняться
- **Null check:** ссылка, которая никогда не была null, стала null
- **Array bounds:** обращение за границы массива

### Мониторинг deoptimization

```bash
java -XX:+PrintDeoptimization MyApp

# Вывод:
# Deoptimization: reason='bimorphic' action='recompile' method=...
#                        ↑
#           Причина: было 2 типа, стало больше
```

Частые deoptimizations указывают на проблемы в коде (megamorphic calls) или нестабильные паттерны использования. В production мониторинг deoptimization — часть observability: если количество deoptimizations резко возрастает, это может объяснить деградацию latency.

---

## GraalVM JIT: новое поколение

GraalVM — проект Oracle Labs, начатый в 2012 году, предлагающий альтернативный JIT-компилятор, написанный на Java вместо C++ (как C2). Это не просто технический рефакторинг — это фундаментальное изменение подхода к разработке компиляторов.

C2 написан на C++ и содержит ~250 000 строк кода, накопленных за 20+ лет. Код сложен, плохо документирован, и добавление новых оптимизаций требует глубокого знания внутренней архитектуры. Баги в C2 могут привести к segfault всей JVM — а не к Java-exception, который можно перехватить и обработать.

Graal, написанный на Java, использует Truffle framework для построения AST (Abstract Syntax Tree) и partial evaluation для его компиляции. Graal проще в разработке: новые оптимизации можно добавлять как обычные Java-классы, отлаживать стандартными Java-инструментами (IntelliJ IDEA, debugger), и тестировать unit-тестами. Баги в Graal приводят к Java-exception, а не к крашу JVM.

Для некоторых workloads Graal показывает производительность на 10-15% выше C2, особенно для Scala, Kotlin и кода с большим количеством allocation. Graal лучше справляется с escape analysis в сложных случаях (partial escape analysis), лучше inline'ит через несколько уровней абстракции, и лучше оптимизирует functional-style код.

Ключевое преимущество Graal — **partial escape analysis**. Стандартный escape analysis в C2 бинарный: объект либо escapes, либо нет. Graal может определить, что объект escapes только по одному из путей выполнения, и материализовать его на heap только когда (и если) execution дойдёт до этого пути. На основном пути объект остаётся scalar-replaced. Для кода с большим количеством short-lived объектов (Scala, Kotlin, functional-style Java) это даёт заметный выигрыш.

Однако Graal — не серебряная пуля. Для некоторых workloads он не показывает преимущества перед C2 — особенно для low-level кода с минимальным allocation. Graal сам потребляет больше памяти: JIT-компилятор на Java создаёт объекты в heap для представления IR-графа, оптимизаций и метаданных. Это увеличивает GC pressure во время компиляции. На машинах с ограниченной памятью это может быть проблемой.

GraalVM Native Image (AOT-компиляция) — отдельная технология, хотя и использует тот же компилятор. Native Image компилирует Java-приложение в standalone native binary до запуска. Результат: мгновенный старт (< 50 мс vs 5-30 сек для JIT), минимальное потребление памяти, отсутствие warmup. Но цена — потеря runtime-оптимизаций и жёсткие ограничения: все классы должны быть известны на этапе компиляции, reflection требует explicit configuration, dynamic class loading невозможен. Для microservices и serverless (AWS Lambda, Cloud Functions) это часто приемлемый trade-off; для долгоживущих серверов JIT обычно даёт лучший throughput.

---

## Warmup: период прогрева

### Почему первые запросы медленные

При старте приложения весь код выполняется интерпретатором. JIT начинает компилировать методы только после накопления статистики.

```
Типичный warmup веб-приложения:

0-5 сек:    Загрузка классов, interpreter
5-30 сек:   C1 компиляция основных путей
30-60 сек:  C2 компиляция горячего кода
60+ сек:    Стабильная производительность
```

Для latency-critical систем это означает, что нельзя направлять production traffic на только что запущенный инстанс. Trading-системы используют pre-warming: перед открытием рынка прогоняют синтетические ордера, чтобы JIT скомпилировал все критические пути.

### Стратегии ускорения warmup

**1. Снижение порогов компиляции:**
```bash
-XX:Tier3InvocationThreshold=100   # C1 после 100 вызовов
-XX:Tier4InvocationThreshold=1000  # C2 после 1000
```
Быстрее warmup, но C2 получает меньше данных для оптимизации — возможна менее эффективная компиляция.

**2. Class Data Sharing (CDS):** ускоряет загрузку классов (но не JIT).

**3. AOT / Native Image (GraalVM):** компилирует в native код до запуска. Мгновенный старт, но теряются runtime оптимизации.

---

## Практические рекомендации

### Пишите маленькие методы

JIT лучше оптимизирует маленькие методы — они inline'ятся, открывая возможности для дальнейших оптимизаций.

Парадокс: разбиение на методы добавляет вызовы, но JIT их устраняет через inlining, а взамен получает лучший код. Длинный метод (200+ строк) не inline'ится и оптимизируется изолированно — JIT не может связать его с вызывающим кодом.

### Избегайте megamorphic calls

Когда виртуальный метод вызывается на объектах трёх и более типов (megamorphic), JIT не может inline'ить и вынужден использовать virtual dispatch каждый раз. Это в 3-5 раз медленнее, чем monomorphic inline.

### Помогайте Escape Analysis

Локальные объекты, не убегающие из метода, могут размещаться на стеке. Старайтесь не "выпускать" временные объекты: не возвращайте их из методов, не сохраняйте в поля, не передавайте в методы, которые JIT не может inline'ить.

### Используйте final

`final` классы и методы дают JIT больше свободы — не нужно учитывать возможные override'ы. Для `final` метода JIT гарантированно знает реализацию и может inline'ить без guard-проверки. Это устраняет и overhead проверки, и риск deoptimization.

---

## Распространённые заблуждения

| Заблуждение | Почему это неверно |
|-------------|-------------------|
| "Java медленная потому что интерпретируется" | JIT компилирует hot code в **native machine code**. После warmup Java может быть быстрее C++ благодаря profile-guided optimizations, недоступным статическим компиляторам |
| "Warmup — это проблема Java" | Warmup — **цена за runtime optimizations**. JIT видит реальный профиль и делает оптимизации, невозможные для AOT. Результат после warmup часто превосходит статическую компиляцию |
| "Native Image всегда лучше JIT" | Native Image = fast startup, но **меньше runtime оптимизаций**. Для долгоживущих серверов JIT обычно даёт лучший throughput |
| "C2 компилятор всегда лучше C1" | C1 быстрее компилирует, даёт **быстрый старт**. Tiered compilation (C1 → C2) — лучший баланс. Без C1 warmup был бы на порядок длиннее |
| "Inlining всегда улучшает performance" | Aggressive inlining может **ухудшить** performance: code bloat увеличивает размер кода, что ведёт к instruction cache misses. JIT балансирует inline size threshold |
| "Одинаковый код работает одинаково быстро" | JIT оптимизирует на основе **профиля исполнения**. Один метод в разных call sites компилируется по-разному. Monomorphic site → inline, megamorphic → virtual dispatch |
| "Достаточно запустить приложение под нагрузкой для warmup" | Warmup должен покрывать **все code paths**. Если редкий error path не вызывался при warmup, он будет интерпретироваться при первом вызове |
| "GraalVM = замена HotSpot" | GraalVM — **альтернативный JIT** или отдельная VM. Для некоторых workloads +10-15%, для других без разницы. Не silver bullet |
| "Deoptimization = баг" | Deoptimization — **нормальная часть** adaptive compilation. JIT speculates, если speculation wrong — deopt и recompile. Проблема — только если deopt происходит часто |
| "Микробенчмарки показывают реальную производительность" | JIT может **удалить dead code** целиком. Без JMH и Blackhole результаты бессмысленны. Warmup iterations обязательны |

---

## Диагностика JIT

### Что компилируется

```bash
java -XX:+PrintCompilation MyApp

#   123   4   com.example.Service::process (45 bytes)
#    ↑    ↑   ↑
#  время уровень  метод (размер)
```

### Решения об inlining

```bash
java -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining MyApp

# @ 12   com.example.Helper::getValue (5 bytes)   inline (hot)
# @ 34   com.example.Helper::compute (120 bytes)  too big
```

### Детали компиляции

```bash
# Сохранить ассемблерный код
java -XX:+UnlockDiagnosticVMOptions \
     -XX:+PrintAssembly \
     -XX:LogFile=hotspot.log \
     MyApp
```

---

## Кто использует и реальные примеры

| Компания/Проект | Как используют JIT | Результаты |
|-----------------|-------------------|------------|
| **Netflix** | Тюнинг C2 для streaming сервисов | Peak performance после 30-60 сек warmup |
| **Twitter** | Scala на JVM, агрессивный inlining | Миллионы запросов в секунду |
| **Alibaba** | Dragonwell JDK с улучшенным JIT | Оптимизации под их workload |
| **GraalVM** | Новый JIT компилятор на Java | +10-15% производительность vs C2 |
| **AWS Lambda** | SnapStart = сохранение warmed JVM | Cold start с 5сек до <1сек |

### Реальные цифры warmup

| Тип приложения | Warmup до peak | Когда достаточно C1 |
|----------------|----------------|---------------------|
| Web API (Spring Boot) | 30-60 сек | ~10 сек для базовой производительности |
| Batch processing | 1-5 мин | Не критично, важен throughput |
| Trading system | 10-20 мин (требуют pre-warming) | Недопустимо, нужен C2 |
| AWS Lambda | SnapStart или Native Image | C1 недостаточно |

---

## Связь с другими темами

**[[jvm-performance-overview]] — общая карта оптимизации JVM.** JIT — один из трёх столпов производительности JVM наряду с GC и memory management. Понимание JIT необходимо, чтобы правильно интерпретировать результаты профилирования: если метод горячий, но уже скомпилирован C2, дальнейшая оптимизация JIT не поможет — нужно оптимизировать алгоритм. Рекомендуется читать performance overview как введение, затем углубляться в JIT.

**[[jvm-benchmarking-jmh]] — как правильно измерять с учётом warmup.** JMH — неотъемлемый инструмент для работы с JIT. Без JMH невозможно написать корректный микробенчмарк: JIT может удалить dead code, constant-fold целые вычисления, или показать нерепрезентативные результаты из-за OSR. Понимание JIT объясняет, *зачем* JMH делает то, что делает (warmup iterations, Blackhole, fork).

**[[jvm-profiling]] — найти горячие методы для оптимизации.** Профилирование показывает, где JVM тратит время, а знание JIT позволяет интерпретировать результаты. Метод, помеченный как горячий в профилировщике, может быть: (1) действительно медленным, (2) горячим потому что вызывается миллионы раз (но каждый вызов быстр), или (3) не скомпилированным C2 (deoptimization, megamorphic calls). Без понимания JIT невозможно отличить эти случаи.

---

## Источники и дальнейшее чтение

- **Würthinger T. et al. (2013). One VM to Rule Them All.** — фундаментальная paper, описывающая архитектуру GraalVM и Truffle framework. Объясняет, как один JIT-компилятор может обслуживать множество языков через partial evaluation. Важна для понимания будущего JIT-компиляции.

- **Paleczny M. et al. (2001). The Java HotSpot Server Compiler.** — оригинальная paper, описывающая архитектуру C2 (Server Compiler). Объясняет "sea of nodes" IR, graph-based оптимизации, и design decisions, определившие производительность JVM на два десятилетия.

- **Oaks S. (2014). Java Performance: The Definitive Guide.** — практическое руководство по JIT tuning. Подробно разбирает tiered compilation, inlining, escape analysis с примерами из production. Лучшая книга для инженера, которому нужно оптимизировать Java-приложение.

- **Chambers C., Ungar D. (1989). Customization: Optimizing Compiler Technology for Self.** — paper, заложившая основы adaptive compilation. Техники из Self (type feedback, speculative optimization) стали фундаментом HotSpot JIT.

- [JITWatch](https://github.com/AdoptOpenJDK/jitwatch) — визуализация JIT-компиляции: что было inline'ено, что скомпилировано, где deoptimization
- [GraalVM Reference](https://www.graalvm.org/latest/reference-manual/java/compiler/) — официальная документация Graal JIT

---

*Проверено: 2026-02-11 | Источники: Paleczny (2001), Würthinger (2013), Oaks (2014), Chambers & Ungar (1989), Oracle docs*
