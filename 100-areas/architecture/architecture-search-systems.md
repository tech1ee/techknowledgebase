---
title: "Search Systems: Elasticsearch, Full-Text Search, Relevance"
created: 2025-12-22
modified: 2026-01-02
type: concept
status: published
confidence: high
tags:
  - topic/architecture
  - search
  - elasticsearch
  - full-text
  - relevance
  - type/concept
  - level/intermediate
related:
  - "[[architecture-overview]]"
  - "[[databases-overview]]"
  - "[[caching-strategies]]"
  - "[[ai-ml-overview]]"
---

# Search Systems: Elasticsearch, Full-Text Search, Relevance

> Поиск — не просто WHERE LIKE '%query%'. Это инвертированные индексы, анализаторы, relevance scoring. Netflix запускает 150+ кластеров Elasticsearch. SQL LIKE в 6-39 раз медленнее full-text search.

---

## Что нужно знать перед изучением

Прежде чем изучать поисковые системы, убедитесь, что понимаете:

- **SQL базы данных** — как работают индексы и запросы → [[databases-overview]]
- **JSON** — Elasticsearch использует JSON для документов и запросов
- **REST API** — все операции через HTTP запросы
- **Распределённые системы** (для продвинутого уровня) — шардирование, репликация

---

## Почему SQL LIKE недостаточно

### Главная аналогия: Указатель в книге

Представьте, что вам нужно найти все страницы в книге, где упоминается слово "elasticsearch":

```
СПОСОБ 1: SQL LIKE (читаем всю книгу)
┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│  📖 Книга на 1000 страниц                                            │
│                                                                     │
│  SELECT * FROM pages WHERE content LIKE '%elasticsearch%'           │
│                                                                     │
│  Что происходит:                                                    │
│  1. Открываем страницу 1 → читаем ВЕСЬ текст → ищем слово          │
│  2. Открываем страницу 2 → читаем ВЕСЬ текст → ищем слово          │
│  3. ...                                                             │
│  1000. Открываем страницу 1000 → читаем ВЕСЬ текст → ищем слово    │
│                                                                     │
│  Время: O(n × m) где n = страниц, m = средняя длина страницы       │
│  Для миллиона страниц = ОЧЕНЬ МЕДЛЕННО 🐌                          │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘

СПОСОБ 2: Inverted Index (смотрим в указатель)
┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│  📖 Книга на 1000 страниц + 📑 Алфавитный указатель в конце          │
│                                                                     │
│  Указатель:                                                         │
│  ───────────────────────────────────────────                        │
│  database ............... 15, 42, 89, 156                           │
│  elasticsearch .......... 23, 45, 67, 234, 567, 890  ← НАШЛИ!       │
│  index .................. 12, 34, 56, 78                            │
│  ...                                                                │
│                                                                     │
│  Что происходит:                                                    │
│  1. Открываем указатель на букву "E" → находим "elasticsearch"     │
│  2. Видим страницы: 23, 45, 67, 234, 567, 890                       │
│  3. Готово!                                                         │
│                                                                     │
│  Время: O(log n) для поиска + O(k) где k = количество результатов  │
│  Для миллиона страниц = МГНОВЕННО ⚡                                │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Реальные цифры: SQL LIKE vs Full-Text Search

| Метрика | SQL LIKE | Elasticsearch |
|---------|----------|---------------|
| 1 млн записей | 2-5 секунд | 5-50 мс |
| 100 млн записей | 30-60+ секунд | 50-200 мс |
| Разница | Baseline | **6-39x быстрее** |

**Почему такая разница?**

| Проблема SQL LIKE | Как решает Elasticsearch |
|-------------------|--------------------------|
| Сканирует каждую строку | Ищет в индексе за O(1) |
| Нет понимания языка | Анализаторы, стемминг, синонимы |
| Нет ранжирования | BM25 сортирует по релевантности |
| Плохо масштабируется | Распределённый по кластеру |
| `%fox%` ≠ `foxes` | "fox" найдёт и "foxes", и "foxy" |

### Реальный пример: Почему "туфли" не находит "туфля"

```sql
-- SQL LIKE: точное совпадение подстроки
SELECT * FROM products WHERE name LIKE '%туфли%';

-- Найдёт:  "Красные туфли Nike"
-- НЕ найдёт: "Туфля кожаная" (единственное число)
-- НЕ найдёт: "Обувь классическая" (синоним)
-- НЕ найдёт: "тУфЛи" если без LOWER()
```

```json
// Elasticsearch: понимает язык
{
  "query": {
    "match": {
      "name": "туфли"
    }
  }
}

// Найдёт ВСЁ:
// - "Красные туфли Nike" (точное совпадение)
// - "Туфля кожаная" (стемминг: туфля → туфл)
// - "Классические туфельки" (однокоренное)
// И отсортирует по релевантности!
```

### Компании, которые перешли на Elasticsearch

**Netflix:**
- **150+ кластеров** Elasticsearch
- **3,500 серверов**
- Используют для: поиска контента, customer support, обнаружения ошибок
- Latency поиска: миллисекунды

**Spotify:**
- Индексирует **100+ миллионов** треков, подкастов, плейлистов
- Elasticsearch + AI для понимания запросов
- Транскрибирует подкасты для поиска по словам

**Stack Overflow:**
- Весь поиск по 23+ миллионам вопросов
- Full-text search с пониманием кода

**LinkedIn, GitHub, Uber, Slack** — все используют Elasticsearch или его форки.

---

## Терминология (с аналогиями)

| Термин | Значение | Аналогия |
|--------|----------|----------|
| **Document** | Единица данных (JSON объект) | Страница в книге |
| **Index** | Коллекция документов | Сама книга |
| **Inverted Index** | Индекс: слово → документы | Алфавитный указатель в конце книги |
| **Analyzer** | Pipeline обработки текста | Редактор, исправляющий текст |
| **Tokenizer** | Разбивает текст на слова | Разделитель на слова по пробелам |
| **Token Filter** | Преобразует слова (lowercase, стемминг) | Корректор: "Running" → "run" |
| **TF-IDF** | Term Frequency × Inverse Doc Frequency | Редкие слова важнее частых |
| **BM25** | Улучшенный TF-IDF (default в ES) | Умный калькулятор релевантности |
| **Shard** | Часть индекса (для распределения) | Том энциклопедии (А-К, Л-Я) |
| **Replica** | Копия шарда | Запасная копия тома |
| **Aggregation** | Аналитика по результатам | "Сколько книг по годам?" |
| **Query DSL** | Язык запросов Elasticsearch | SQL для поисковых систем |

### Ключевые метрики

| Метрика | Хорошее значение | Плохое значение |
|---------|------------------|-----------------|
| Search latency P99 | < 100 мс | > 500 мс |
| Indexing throughput | 10,000+ docs/sec | < 1,000 docs/sec |
| Hit rate (если кэш) | > 80% | < 50% |
| Shard size | 10-50 GB | > 100 GB или < 1 GB |

---

## Inverted Index: Как это работает

### Шаг за шагом

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    СОЗДАНИЕ INVERTED INDEX                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ШАГ 1: Имеем 3 документа                                               │
│  ─────────────────────────                                              │
│                                                                         │
│  Doc 1: "The quick brown fox jumps over the lazy dog"                  │
│  Doc 2: "Quick brown rabbits jump over lazy cats"                      │
│  Doc 3: "The fox is quick and smart"                                   │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ШАГ 2: Анализ текста (tokenization + normalization)                   │
│  ───────────────────────────────────────────────────                   │
│                                                                         │
│  Doc 1: [the, quick, brown, fox, jump, over, the, lazi, dog]           │
│         └─ lowercase ─┘  └─ stemming: jumps→jump, lazy→lazi ─┘         │
│                                                                         │
│  Doc 2: [quick, brown, rabbit, jump, over, lazi, cat]                  │
│                                                                         │
│  Doc 3: [the, fox, is, quick, and, smart]                              │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ШАГ 3: Строим inverted index                                          │
│  ────────────────────────────                                          │
│                                                                         │
│  ОБЫЧНЫЙ индекс (forward):     INVERTED индекс:                        │
│  Doc ID → Слова                 Слово → Doc IDs                        │
│                                                                         │
│  1 → [quick, brown, fox...]    brown  → [1, 2]                         │
│  2 → [quick, brown, rabbit...] cat    → [2]                            │
│  3 → [fox, quick, smart...]    dog    → [1]                            │
│                                 fox    → [1, 3]    ← Поиск "fox"       │
│                                 jump   → [1, 2]                         │
│                                 lazi   → [1, 2]                         │
│                                 quick  → [1, 2, 3]                      │
│                                 rabbit → [2]                            │
│                                 smart  → [3]                            │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ШАГ 4: Поиск "fox"                                                    │
│  ──────────────────                                                    │
│                                                                         │
│  1. Ищем "fox" в индексе → Находим: [1, 3]                             │
│  2. Время поиска: O(1) — один lookup!                                  │
│  3. Возвращаем документы 1 и 3                                         │
│                                                                         │
│  БЕЗ индекса:                                                          │
│  1. Читаем Doc 1 полностью → ищем "fox" → нашли                        │
│  2. Читаем Doc 2 полностью → ищем "fox" → не нашли                     │
│  3. Читаем Doc 3 полностью → ищем "fox" → нашли                        │
│  4. Время: O(n × m) — читаем ВСЁ                                       │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Дополнительные данные в индексе

Inverted index хранит не только Doc IDs, но и:

```
┌─────────────────────────────────────────────────────────────────────────┐
│  Term     │ Doc IDs  │ Positions           │ Term Frequency            │
│  ─────────┼──────────┼─────────────────────┼───────────────────────────│
│  quick    │ [1,2,3]  │ [1:1, 2:0, 3:3]     │ [1:1, 2:1, 3:1]          │
│           │          │  ↑                   │  ↑                       │
│           │          │  Doc 1, позиция 1    │  Doc 1, встречается 1 раз│
│           │          │                      │                          │
│  fox      │ [1, 3]   │ [1:3, 3:1]          │ [1:1, 3:1]               │
│           │          │  ↑                   │                          │
│           │          │  Doc 1, позиция 3    │                          │
└─────────────────────────────────────────────────────────────────────────┘

Зачем это нужно:

• Positions — для phrase queries: "quick fox" (слова рядом)
• Term Frequency — для scoring: чем чаще слово, тем релевантнее
• Document Frequency — сколько документов содержат слово
                       (редкие слова важнее)
```

---

## Text Analysis Pipeline

Прежде чем попасть в индекс, текст проходит обработку:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     PIPELINE АНАЛИЗА ТЕКСТА                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Входной текст:                                                         │
│  "The Quick Brown Fox's jumping over lazy dogs!!!"                     │
│                                                                         │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │  ЭТАП 1: CHARACTER FILTERS                                        │  │
│  │                                                                   │  │
│  │  Что делает:                                                      │  │
│  │  • Удаляет HTML теги (<b>text</b> → text)                         │  │
│  │  • Заменяет символы (& → and)                                     │  │
│  │  • Удаляет специальные символы                                    │  │
│  │                                                                   │  │
│  │  Результат:                                                       │  │
│  │  "The Quick Brown Fox's jumping over lazy dogs"                   │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                               ↓                                         │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │  ЭТАП 2: TOKENIZER                                                │  │
│  │                                                                   │  │
│  │  Что делает:                                                      │  │
│  │  • Разбивает текст на отдельные токены (обычно слова)             │  │
│  │  • Разные токенайзеры: по пробелам, по пунктуации, для URL        │  │
│  │                                                                   │  │
│  │  Результат:                                                       │  │
│  │  ["The", "Quick", "Brown", "Fox's", "jumping", "over",           │  │
│  │   "lazy", "dogs"]                                                 │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                               ↓                                         │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │  ЭТАП 3: TOKEN FILTERS (применяются последовательно)              │  │
│  │                                                                   │  │
│  │  a) Lowercase Filter                                              │  │
│  │     ["the", "quick", "brown", "fox's", "jumping", ...]           │  │
│  │                                                                   │  │
│  │  b) Stopwords Filter (удаляем "the", "is", "and", ...)           │  │
│  │     ["quick", "brown", "fox's", "jumping", "lazy", "dogs"]       │  │
│  │                                                                   │  │
│  │  c) Stemming/Lemmatization (приводим к корню)                     │  │
│  │     "jumping" → "jump"                                            │  │
│  │     "dogs" → "dog"                                                │  │
│  │     "fox's" → "fox"                                               │  │
│  │     "lazy" → "lazi" (Porter stemmer)                              │  │
│  │                                                                   │  │
│  │     ["quick", "brown", "fox", "jump", "lazi", "dog"]             │  │
│  │                                                                   │  │
│  │  d) Synonyms Filter (опционально)                                 │  │
│  │     "quick" → ["quick", "fast", "rapid"]                          │  │
│  │                                                                   │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                               ↓                                         │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │  ФИНАЛЬНЫЕ ТОКЕНЫ (идут в inverted index):                        │  │
│  │                                                                   │  │
│  │  ["quick", "brown", "fox", "jump", "lazi", "dog"]                │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                                                         │
│  ВАЖНО: Тот же анализатор применяется к поисковому запросу!            │
│  Поэтому запрос "jumping" тоже станет "jump" и найдёт документ.        │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## BM25: Как считается релевантность

BM25 (Best Matching 25) — алгоритм ранжирования по умолчанию в Elasticsearch.

### Интуиция: Что делает слово "важным"?

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    ИНТУИЦИЯ BM25                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ФАКТОР 1: Term Frequency (TF) — частота слова в документе             │
│  ─────────────────────────────────────────────────────────              │
│                                                                         │
│  Документ A: "dog dog dog dog dog"   → TF("dog") = 5                   │
│  Документ B: "dog"                    → TF("dog") = 1                   │
│                                                                         │
│  Документ A релевантнее? Не совсем...                                  │
│                                                                         │
│  Проблема: Если слово встречается 100 раз vs 200 раз —                 │
│           документ НЕ в 2 раза релевантнее.                            │
│           После какого-то момента важность "насыщается".               │
│                                                                         │
│  BM25 решение: TF / (TF + k1) — чем больше TF, тем меньше прирост      │
│                                                                         │
│  TF=1:   1/(1+1.2) = 0.45                                              │
│  TF=5:   5/(5+1.2) = 0.81                                              │
│  TF=100: 100/(100+1.2) = 0.99  ← Почти не растёт после TF=10           │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ФАКТОР 2: Inverse Document Frequency (IDF) — редкость слова           │
│  ───────────────────────────────────────────────────────────           │
│                                                                         │
│  Коллекция: 10,000 документов                                          │
│                                                                         │
│  Слово "the" встречается в 9,500 документах → IDF очень низкий        │
│  Слово "elasticsearch" встречается в 50 документах → IDF высокий      │
│                                                                         │
│  Почему: Редкие слова БОЛЬШЕ говорят о теме документа.                 │
│          "the" ничего не значит, "elasticsearch" — очень специфично.   │
│                                                                         │
│  IDF = log((N - n + 0.5) / (n + 0.5) + 1)                              │
│  где N = всего документов, n = документов со словом                    │
│                                                                         │
│  IDF("the"):           log((10000 - 9500 + 0.5)/(9500 + 0.5) + 1) ≈ 0.05│
│  IDF("elasticsearch"): log((10000 - 50 + 0.5)/(50 + 0.5) + 1) ≈ 5.3    │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ФАКТОР 3: Document Length — длина документа                           │
│  ───────────────────────────────────────────                           │
│                                                                         │
│  Документ A: 100 слов, "dog" встречается 5 раз → 5%                    │
│  Документ B: 1000 слов, "dog" встречается 5 раз → 0.5%                 │
│                                                                         │
│  Документ A релевантнее! Слово занимает бОльшую часть.                 │
│                                                                         │
│  BM25 нормализует по средней длине документов:                         │
│  dl/avgdl — если документ длиннее среднего, его score уменьшается     │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### BM25: Пошаговый расчёт на примере

```
ЗАДАЧА: Поиск "quick fox" в 3 документах

Документы:
Doc 1: "The quick brown fox jumps" (5 слов)
Doc 2: "Quick brown rabbits jump over lazy cats today" (8 слов)
Doc 3: "The fox is quick and smart" (6 слов)

Параметры BM25 (по умолчанию):
k1 = 1.2 (saturation)
b = 0.75 (length normalization)
avgdl = (5 + 8 + 6) / 3 = 6.33

────────────────────────────────────────────────────────────
ШАГ 1: Считаем IDF для каждого слова запроса
────────────────────────────────────────────────────────────

N = 3 (всего документов)

"quick": встречается в docs [1, 2, 3] → n = 3
IDF("quick") = log((3 - 3 + 0.5) / (3 + 0.5) + 1)
             = log(0.5 / 3.5 + 1)
             = log(1.14)
             = 0.13

"fox": встречается в docs [1, 3] → n = 2
IDF("fox") = log((3 - 2 + 0.5) / (2 + 0.5) + 1)
           = log(1.5 / 2.5 + 1)
           = log(1.6)
           = 0.47

────────────────────────────────────────────────────────────
ШАГ 2: Считаем score для каждого документа
────────────────────────────────────────────────────────────

Формула для каждого term:
score(term) = IDF × (TF × (k1 + 1)) / (TF + k1 × (1 - b + b × dl/avgdl))

--- Doc 1: "The quick brown fox jumps" (dl = 5) ---

TF("quick") = 1, TF("fox") = 1

score("quick", Doc1) = 0.13 × (1 × 2.2) / (1 + 1.2 × (1 - 0.75 + 0.75 × 5/6.33))
                     = 0.13 × 2.2 / (1 + 1.2 × (0.25 + 0.59))
                     = 0.13 × 2.2 / (1 + 1.01)
                     = 0.13 × 2.2 / 2.01
                     = 0.14

score("fox", Doc1)   = 0.47 × 2.2 / 2.01
                     = 0.51

TOTAL Doc1 = 0.14 + 0.51 = 0.65

--- Doc 2: "Quick brown rabbits..." (dl = 8) ---

TF("quick") = 1, TF("fox") = 0

score("quick", Doc2) = 0.13 × 2.2 / (1 + 1.2 × (0.25 + 0.75 × 8/6.33))
                     = 0.13 × 2.2 / (1 + 1.2 × (0.25 + 0.95))
                     = 0.13 × 2.2 / 2.44
                     = 0.12

score("fox", Doc2)   = 0 (слова нет)

TOTAL Doc2 = 0.12

--- Doc 3: "The fox is quick and smart" (dl = 6) ---

TF("quick") = 1, TF("fox") = 1

score("quick", Doc3) = 0.13 × 2.2 / (1 + 1.2 × (0.25 + 0.75 × 6/6.33))
                     = 0.13 × 2.2 / (1 + 1.2 × 0.96)
                     = 0.13 × 2.2 / 2.15
                     = 0.13

score("fox", Doc3)   = 0.47 × 2.2 / 2.15
                     = 0.48

TOTAL Doc3 = 0.13 + 0.48 = 0.61

────────────────────────────────────────────────────────────
РЕЗУЛЬТАТ (отсортировано по релевантности):
────────────────────────────────────────────────────────────

1. Doc 1: score = 0.65 ✓ (короче + оба слова)
2. Doc 3: score = 0.61   (оба слова, но чуть длиннее)
3. Doc 2: score = 0.12   (только "quick", нет "fox")
```

---

## Elasticsearch Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    ELASTICSEARCH CLUSTER                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Кластер = группа узлов (nodes), работающих вместе                     │
│                                                                         │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                                                                   │  │
│  │  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐              │  │
│  │  │   Node 1    │   │   Node 2    │   │   Node 3    │              │  │
│  │  │   (Master)  │   │   (Data)    │   │   (Data)    │              │  │
│  │  │             │   │             │   │             │              │  │
│  │  │ ┌─────────┐ │   │ ┌─────────┐ │   │ ┌─────────┐ │              │  │
│  │  │ │ Shard 0 │ │   │ │ Shard 1 │ │   │ │ Shard 2 │ │  Primary    │  │
│  │  │ │ PRIMARY │ │   │ │ PRIMARY │ │   │ │ PRIMARY │ │  Shards     │  │
│  │  │ └─────────┘ │   │ └─────────┘ │   │ └─────────┘ │              │  │
│  │  │             │   │             │   │             │              │  │
│  │  │ ┌─────────┐ │   │ ┌─────────┐ │   │ ┌─────────┐ │              │  │
│  │  │ │ Shard 1 │ │   │ │ Shard 2 │ │   │ │ Shard 0 │ │  Replica    │  │
│  │  │ │ REPLICA │ │   │ │ REPLICA │ │   │ │ REPLICA │ │  Shards     │  │
│  │  │ └─────────┘ │   │ └─────────┘ │   │ └─────────┘ │              │  │
│  │  └─────────────┘   └─────────────┘   └─────────────┘              │  │
│  │                                                                   │  │
│  │  Index: "products"                                                │  │
│  │  • 3 primary shards (данные разделены на 3 части)                │  │
│  │  • 1 replica на каждый primary (для отказоустойчивости)          │  │
│  │                                                                   │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                                                         │
│  РОЛИ УЗЛОВ:                                                           │
│  • Master — управляет кластером (создание индексов, распределение)     │
│  • Data — хранит данные и выполняет запросы                            │
│  • Coordinating — маршрутизирует запросы, агрегирует результаты        │
│  • Ingest — предобработка документов перед индексацией                 │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  FLOW ПОИСКОВОГО ЗАПРОСА (Scatter-Gather):                             │
│                                                                         │
│  1. Клиент → любой узел (coordinating node)                            │
│  2. Coordinating → рассылает запрос на ВСЕ shards (scatter)            │
│  3. Каждый shard ищет локально, возвращает top N результатов           │
│  4. Coordinating собирает, сортирует, возвращает финальный top N       │
│                                                                         │
│  ┌─────────┐      ┌─────────┐                                          │
│  │ Client  │─────▶│ Node 1  │                                          │
│  └─────────┘      └────┬────┘                                          │
│                        │ scatter                                       │
│              ┌─────────┼─────────┐                                     │
│              ▼         ▼         ▼                                     │
│         ┌────────┐┌────────┐┌────────┐                                 │
│         │Shard 0 ││Shard 1 ││Shard 2 │                                 │
│         │ top 10 ││ top 10 ││ top 10 │                                 │
│         └────┬───┘└────┬───┘└────┬───┘                                 │
│              └─────────┼─────────┘ gather                              │
│                        ▼                                               │
│                   Final top 10                                         │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Elasticsearch: Практика

### Создание индекса с маппингом

```json
// PUT /products — создаём индекс "products"
//
// Каждая строка объяснена ниже

PUT /products
{
  "settings": {
    // Сколько шардов для распределения данных
    // Правило: 1 shard = 10-50 GB данных
    // НЕЛЬЗЯ изменить после создания!
    "number_of_shards": 3,

    // Сколько копий каждого шарда
    // 1 replica = каждый шард дублируется на другом узле
    "number_of_replicas": 1,

    // Настройка анализаторов
    "analysis": {
      "analyzer": {
        // Создаём кастомный анализатор для русского языка
        "russian_custom": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "lowercase",        // КОШКА → кошка
            "russian_stop",     // Удалить "и", "в", "на"
            "russian_stemmer"   // кошки → кошк
          ]
        }
      },
      "filter": {
        "russian_stop": {
          "type": "stop",
          "stopwords": "_russian_"
        },
        "russian_stemmer": {
          "type": "stemmer",
          "language": "russian"
        }
      }
    }
  },

  // Маппинг — схема документа (какие поля, какие типы)
  "mappings": {
    "properties": {

      // TEXT — для полнотекстового поиска
      // Анализируется, разбивается на токены
      "name": {
        "type": "text",
        "analyzer": "russian_custom",
        // Дополнительное поле для exact match и сортировки
        "fields": {
          "keyword": { "type": "keyword" }
        }
      },

      "description": {
        "type": "text",
        "analyzer": "russian_custom"
      },

      // KEYWORD — для точного совпадения, фильтров, агрегаций
      // НЕ анализируется, хранится as-is
      "category": {
        "type": "keyword"
      },

      "brand": {
        "type": "keyword"
      },

      // Числовые типы
      "price": {
        "type": "float"
      },

      "rating": {
        "type": "float"
      },

      // Дата
      "created_at": {
        "type": "date"
      },

      // Массив keyword (теги)
      "tags": {
        "type": "keyword"
      },

      // Вложенный объект
      "attributes": {
        "type": "nested",  // Для сложных вложенных запросов
        "properties": {
          "name": { "type": "keyword" },
          "value": { "type": "keyword" }
        }
      }
    }
  }
}
```

### TEXT vs KEYWORD: Когда что использовать

| Тип | Когда использовать | Примеры |
|-----|-------------------|---------|
| **text** | Полнотекстовый поиск | Название, описание, статьи |
| **keyword** | Точное совпадение, фильтры | Категория, статус, email, ID |
| **text + keyword** | И то, и другое | Название (поиск + сортировка) |

```json
// Пример: поле "title" для поиска И сортировки
"title": {
  "type": "text",        // Для match запросов
  "fields": {
    "keyword": {
      "type": "keyword"  // Для сортировки и aggregations
    }
  }
}

// Использование:
// Поиск:     { "match": { "title": "running shoes" } }
// Сортировка: { "sort": [{ "title.keyword": "asc" }] }
```

### Базовые запросы

```json
// =============================================================
// 1. MATCH — полнотекстовый поиск
// =============================================================
// Ищет документы, содержащие слова из запроса
// Слова анализируются тем же анализатором, что и при индексации

GET /products/_search
{
  "query": {
    "match": {
      "description": "удобные беговые кроссовки"
      // Результат: документы с любым из этих слов
      // Чем больше слов совпало, тем выше score
    }
  }
}

// =============================================================
// 2. MULTI_MATCH — поиск по нескольким полям
// =============================================================

GET /products/_search
{
  "query": {
    "multi_match": {
      "query": "nike кроссовки",
      "fields": [
        "name^3",        // ^3 = boost в 3 раза (название важнее)
        "description",   // boost = 1 (по умолчанию)
        "brand^2"        // ^2 = boost в 2 раза
      ],
      "type": "best_fields"  // Выбирает лучшее совпадение из полей
    }
  }
}

// =============================================================
// 3. BOOL — комбинация условий
// =============================================================
// must = AND (обязательно)
// should = OR (желательно, повышает score)
// must_not = NOT (исключить)
// filter = AND без влияния на score (быстрее, кэшируется)

GET /products/_search
{
  "query": {
    "bool": {
      // Обязательные условия (влияют на score)
      "must": [
        { "match": { "description": "кроссовки беговые" } }
      ],

      // Фильтры — НЕ влияют на score, кэшируются
      "filter": [
        { "term": { "category": "footwear" } },  // Точное совпадение
        { "range": { "price": { "lte": 10000 } } }  // Цена <= 10000
      ],

      // Желательные условия (повышают score если совпали)
      "should": [
        { "term": { "tags": "sale" } },  // Бонус если на распродаже
        { "range": { "rating": { "gte": 4.5 } } }  // Бонус если рейтинг высокий
      ],
      "minimum_should_match": 0,  // should не обязательны

      // Исключить
      "must_not": [
        { "term": { "brand": "fake_brand" } }
      ]
    }
  }
}

// =============================================================
// 4. FUZZY — поиск с опечатками
// =============================================================

GET /products/_search
{
  "query": {
    "fuzzy": {
      "name": {
        "value": "кросовки",  // Опечатка! (должно быть "кроссовки")
        "fuzziness": "AUTO"   // AUTO = 1-2 символа в зависимости от длины
        // Для слов 3-5 символов: 1 ошибка
        // Для слов 6+ символов: 2 ошибки
      }
    }
  }
}

// =============================================================
// 5. PHRASE — поиск точной фразы
// =============================================================

GET /products/_search
{
  "query": {
    "match_phrase": {
      "description": "беговые кроссовки nike"
      // Слова должны идти ИМЕННО в таком порядке и рядом друг с другом
    }
  }
}

// С допуском на расстояние между словами
GET /products/_search
{
  "query": {
    "match_phrase": {
      "description": {
        "query": "беговые nike",
        "slop": 2  // Между словами может быть до 2 других слов
      }
    }
  }
}
```

### Aggregations (Аналитика)

```json
// =============================================================
// AGGREGATIONS — группировка и статистика
// =============================================================

GET /products/_search
{
  "size": 0,  // Не возвращать документы, только агрегации

  "query": {
    "match": { "category": "electronics" }
  },

  "aggs": {
    // 1. TERMS — группировка по значениям (для faceted search)
    "brands": {
      "terms": {
        "field": "brand",  // Должен быть keyword!
        "size": 10         // Top 10 брендов
      }
    },

    // 2. RANGE — группировка по диапазонам
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          { "key": "budget", "to": 5000 },
          { "key": "mid", "from": 5000, "to": 15000 },
          { "key": "premium", "from": 15000, "to": 50000 },
          { "key": "luxury", "from": 50000 }
        ]
      }
    },

    // 3. STATS — статистика по числовому полю
    "price_stats": {
      "stats": { "field": "price" }
      // Вернёт: count, min, max, avg, sum
    },

    // 4. NESTED AGGREGATION — агрегация внутри агрегации
    "categories_with_avg_price": {
      "terms": { "field": "category" },
      "aggs": {
        "avg_price": {
          "avg": { "field": "price" }
        },
        "top_rated": {
          "top_hits": {
            "size": 3,
            "sort": [{ "rating": "desc" }]
          }
        }
      }
    }
  }
}

// Результат:
{
  "aggregations": {
    "brands": {
      "buckets": [
        { "key": "Apple", "doc_count": 150 },
        { "key": "Samsung", "doc_count": 120 },
        { "key": "Xiaomi", "doc_count": 90 }
      ]
    },
    "price_ranges": {
      "buckets": [
        { "key": "budget", "doc_count": 500 },
        { "key": "mid", "doc_count": 300 },
        { "key": "premium", "doc_count": 150 },
        { "key": "luxury", "doc_count": 50 }
      ]
    },
    "price_stats": {
      "count": 1000,
      "min": 990.0,
      "max": 199990.0,
      "avg": 25500.0,
      "sum": 25500000.0
    }
  }
}
```

---

## Python Client

```python
from elasticsearch import Elasticsearch
from typing import List, Dict, Optional
import logging

# =============================================================
# ELASTICSEARCH SEARCH SERVICE
# Полный пример с комментариями
# =============================================================

class ProductSearchService:
    """
    Сервис поиска продуктов через Elasticsearch.

    Пример использования:
        search = ProductSearchService(["http://localhost:9200"])
        results = search.search_products(
            query="беговые кроссовки",
            filters={"category": "footwear", "price_max": 10000},
            page=1,
            size=20
        )
    """

    def __init__(self, hosts: List[str]):
        """
        Инициализация клиента.

        Args:
            hosts: Список URL'ов узлов Elasticsearch
                   Пример: ["http://localhost:9200", "http://node2:9200"]
        """
        self.es = Elasticsearch(
            hosts,
            # Настройки таймаутов
            timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
        self.logger = logging.getLogger(__name__)

    def search_products(
        self,
        query: str,
        filters: Optional[Dict] = None,
        page: int = 1,
        size: int = 20,
        sort_by: str = "_score"  # По релевантности по умолчанию
    ) -> Dict:
        """
        Поиск продуктов с фильтрами, пагинацией и facets.

        Args:
            query: Поисковый запрос пользователя
            filters: Словарь фильтров
                     {"category": "footwear", "price_min": 1000, "price_max": 10000}
            page: Номер страницы (начиная с 1)
            size: Количество результатов на странице
            sort_by: Поле для сортировки

        Returns:
            {
                "total": 150,
                "results": [...],
                "facets": {...}
            }
        """

        # ШАГ 1: Строим основной поисковый запрос
        # multi_match ищет по нескольким полям с разными весами
        must_clause = [{
            "multi_match": {
                "query": query,
                "fields": [
                    "name^3",        # Название в 3 раза важнее
                    "description",   # Описание
                    "brand^2",       # Бренд в 2 раза важнее
                    "tags"           # Теги
                ],
                "type": "best_fields",  # Выбираем лучшее совпадение
                "fuzziness": "AUTO",    # Толерантность к опечаткам
                "prefix_length": 2      # Первые 2 буквы должны совпадать точно
            }
        }]

        # ШАГ 2: Строим фильтры (НЕ влияют на score, кэшируются)
        filter_clauses = []

        if filters:
            # Фильтр по категории (точное совпадение)
            if "category" in filters:
                filter_clauses.append({
                    "term": {"category": filters["category"]}
                })

            # Фильтр по цене (диапазон)
            if "price_min" in filters or "price_max" in filters:
                price_range = {}
                if "price_min" in filters:
                    price_range["gte"] = filters["price_min"]  # >=
                if "price_max" in filters:
                    price_range["lte"] = filters["price_max"]  # <=
                filter_clauses.append({"range": {"price": price_range}})

            # Фильтр по брендам (несколько значений)
            if "brands" in filters and filters["brands"]:
                filter_clauses.append({
                    "terms": {"brand": filters["brands"]}  # IN (brand1, brand2)
                })

            # Фильтр по рейтингу
            if "min_rating" in filters:
                filter_clauses.append({
                    "range": {"rating": {"gte": filters["min_rating"]}}
                })

        # ШАГ 3: Собираем финальный запрос
        body = {
            "query": {
                "bool": {
                    "must": must_clause,
                    "filter": filter_clauses
                }
            },

            # Пагинация
            "from": (page - 1) * size,  # Offset
            "size": size,

            # Подсветка найденных слов
            "highlight": {
                "fields": {
                    "name": {},
                    "description": {
                        "fragment_size": 150,  # Длина фрагмента
                        "number_of_fragments": 3  # Сколько фрагментов
                    }
                },
                "pre_tags": ["<mark>"],   # Тег начала подсветки
                "post_tags": ["</mark>"]  # Тег конца подсветки
            },

            # Агрегации для faceted search (фильтры слева)
            "aggs": {
                "categories": {
                    "terms": {"field": "category", "size": 20}
                },
                "brands": {
                    "terms": {"field": "brand", "size": 30}
                },
                "price_stats": {
                    "stats": {"field": "price"}
                },
                "price_ranges": {
                    "range": {
                        "field": "price",
                        "ranges": [
                            {"key": "До 5000", "to": 5000},
                            {"key": "5000-15000", "from": 5000, "to": 15000},
                            {"key": "15000-50000", "from": 15000, "to": 50000},
                            {"key": "50000+", "from": 50000}
                        ]
                    }
                }
            }
        }

        # Сортировка
        if sort_by != "_score":
            body["sort"] = [{sort_by: {"order": "desc"}}]

        # ШАГ 4: Выполняем запрос
        try:
            response = self.es.search(index="products", body=body)
        except Exception as e:
            self.logger.error(f"Elasticsearch error: {e}")
            raise

        # ШАГ 5: Форматируем ответ
        return {
            "total": response["hits"]["total"]["value"],
            "results": [
                {
                    "id": hit["_id"],
                    "score": hit["_score"],
                    **hit["_source"],
                    "highlights": hit.get("highlight", {})
                }
                for hit in response["hits"]["hits"]
            ],
            "facets": {
                "categories": response["aggregations"]["categories"]["buckets"],
                "brands": response["aggregations"]["brands"]["buckets"],
                "price": response["aggregations"]["price_stats"],
                "price_ranges": response["aggregations"]["price_ranges"]["buckets"]
            }
        }

    def autocomplete(self, prefix: str, size: int = 10) -> List[str]:
        """
        Автодополнение для поисковой строки.

        Args:
            prefix: Что пользователь уже набрал
            size: Сколько подсказок вернуть

        Returns:
            ["кроссовки nike", "кроссовки adidas", ...]
        """

        body = {
            "suggest": {
                "product_suggest": {
                    "prefix": prefix,
                    "completion": {
                        "field": "name_suggest",  # Специальное поле типа completion
                        "size": size,
                        "skip_duplicates": True,
                        "fuzzy": {
                            "fuzziness": "AUTO"  # Толерантность к опечаткам
                        }
                    }
                }
            }
        }

        response = self.es.search(index="products", body=body)

        suggestions = response["suggest"]["product_suggest"][0]["options"]
        return [s["text"] for s in suggestions]

    def index_product(self, product: Dict) -> str:
        """
        Индексация одного продукта.

        Args:
            product: Данные продукта

        Returns:
            ID документа в Elasticsearch
        """
        response = self.es.index(
            index="products",
            id=product.get("id"),  # Можно задать свой ID
            body=product,
            refresh="wait_for"  # Ждать пока документ станет searchable
        )
        return response["_id"]

    def bulk_index(self, products: List[Dict]) -> tuple:
        """
        Массовая индексация продуктов.
        Значительно быстрее чем по одному!

        Args:
            products: Список продуктов

        Returns:
            (success_count, error_list)
        """
        from elasticsearch.helpers import bulk

        # Формируем actions для bulk API
        actions = [
            {
                "_index": "products",
                "_id": p.get("id"),
                "_source": p
            }
            for p in products
        ]

        success, errors = bulk(
            self.es,
            actions,
            chunk_size=1000,  # Отправлять по 1000 документов
            request_timeout=60
        )

        return success, errors
```

---

## Vector Search (Semantic Search)

Обычный поиск (BM25) ищет по ключевым словам. Vector search понимает СМЫСЛ.

```
Проблема BM25:
  Запрос: "недорогой ноутбук"
  Документ: "бюджетный лэптоп за 30000 рублей"

  BM25 НЕ найдёт! Нет общих слов.

Vector Search:
  "недорогой ноутбук" → вектор [0.2, 0.5, 0.1, ...]
  "бюджетный лэптоп"  → вектор [0.2, 0.5, 0.1, ...]

  Векторы ПОХОЖИ → документ найден!
```

### Настройка Vector Search

```json
// Создаём индекс с поддержкой векторов
PUT /products_semantic
{
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "description": { "type": "text" },

      // Поле для векторов (embeddings)
      "embedding": {
        "type": "dense_vector",
        "dims": 384,           // Размерность вектора (зависит от модели)
        "index": true,         // Индексировать для быстрого поиска
        "similarity": "cosine" // Метрика сходства
        // Варианты: cosine, dot_product, l2_norm
      }
    }
  }
}

// KNN поиск (K Nearest Neighbors)
GET /products_semantic/_search
{
  "knn": {
    "field": "embedding",
    "query_vector": [0.1, 0.2, ...],  // Вектор запроса (от ML модели)
    "k": 10,                           // Вернуть 10 ближайших
    "num_candidates": 100              // Рассмотреть 100 кандидатов
  }
}

// HYBRID SEARCH: BM25 + Vector (лучший результат!)
GET /products_semantic/_search
{
  // Текстовый поиск
  "query": {
    "match": {
      "description": "удобные беговые кроссовки"
    }
  },

  // Семантический поиск
  "knn": {
    "field": "embedding",
    "query_vector": [0.1, 0.2, ...],
    "k": 10,
    "num_candidates": 100,
    "boost": 0.5  // Вес семантического поиска
  }
}
```

---

## Оптимизация производительности

| Оптимизация | Описание | Когда применять |
|------------|----------|-----------------|
| **Размер shard** | 10-50 GB на shard | При создании индекса |
| **Replicas** | Больше реплик = больше read throughput | Read-heavy нагрузка |
| **Refresh interval** | Увеличить до 30s+ | Write-heavy (логи) |
| **Bulk indexing** | Пакетная индексация | Загрузка данных |
| **Filter context** | Использовать filter вместо must | Для точных совпадений |
| **Doc values** | Для aggregations и sorting | По умолчанию включено |
| **Source filtering** | Запрашивать только нужные поля | Большие документы |

```json
// Пример: оптимизированный запрос
GET /products/_search
{
  // Запрашиваем только нужные поля (не весь _source)
  "_source": ["name", "price", "category"],

  "query": {
    "bool": {
      // must влияет на score
      "must": [
        { "match": { "name": "кроссовки" } }
      ],
      // filter НЕ влияет на score, кэшируется
      "filter": [
        { "term": { "category": "footwear" } },
        { "range": { "price": { "lte": 10000 } } }
      ]
    }
  },

  // Pagination через search_after (не offset!)
  // Для глубокой пагинации
  "size": 20,
  "sort": [
    { "_score": "desc" },
    { "_id": "asc" }  // Tie-breaker
  ],
  "search_after": [0.95, "product_12345"]  // Значения из предыдущей страницы
}
```

---

## Распространённые ошибки

| Ошибка | Почему плохо | Как правильно |
|--------|--------------|---------------|
| Over-sharding | Overhead, медленные запросы | 10-50 GB на shard |
| Маппинг без типов | Dynamic mapping угадывает неправильно | Явный маппинг |
| text для ID | Анализируется, занимает место | keyword для ID |
| Игнорирование analyzers | "туфли" ≠ "туфля" | Настроить под язык |
| Deep pagination (offset) | from: 10000 очень медленно | search_after |
| Нет реплик | Потеря данных при падении | Минимум 1 replica |

---

## Actionable: Чеклист внедрения

```
□ Определить требования
  ├── Какие данные индексируем?
  ├── Какой язык(и)?
  ├── Ожидаемый объём данных?
  └── Требования к latency?

□ Спроектировать маппинг
  ├── Определить поля и типы
  ├── text vs keyword для каждого поля
  ├── Настроить анализаторы под язык
  └── Добавить поля для автодополнения

□ Рассчитать shards
  ├── Ожидаемый объём / 30 GB = количество shards
  └── Минимум 1 replica

□ Реализовать индексацию
  ├── Bulk API для массовой загрузки
  ├── Инкрементальное обновление
  └── Синхронизация с primary database

□ Реализовать поиск
  ├── multi_match для основного поиска
  ├── Фильтры через filter context
  ├── Aggregations для facets
  └── Highlight для подсветки

□ Мониторинг
  ├── Search latency
  ├── Indexing rate
  ├── Cluster health
  └── JVM heap usage
```

---

## Связи

- [[architecture-overview]] — обзор архитектуры
- [[databases-overview]] — сравнение с databases
- [[caching-strategies]] — кэширование результатов поиска
- [[ai-ml-overview]] — embeddings для vector search

---

## Источники

| # | Источник | Тип | Достоверность | Вклад |
|---|----------|-----|---------------|-------|
| 1 | [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/) | Документация | 0.95 | Official reference |
| 2 | [Elastic Blog: From the Bottom Up](https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up) | Статья | 0.90 | Inverted index internals |
| 3 | [BM25 Algorithm - Elastic](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables) | Статья | 0.95 | BM25 параметры |
| 4 | [Netflix Tech Blog: Studio Search](https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf) | Статья | 0.90 | Real-world Netflix architecture |
| 5 | [Okapi BM25 - Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25) | Энциклопедия | 0.85 | BM25 формула |
| 6 | [TF-IDF and BM25 for RAG](https://www.ai-bites.net/tf-idf-and-bm25-for-rag-a-complete-guide/) | Статья | 0.85 | TF-IDF vs BM25 сравнение |
| 7 | [Inverted Index - GeeksforGeeks](https://www.geeksforgeeks.org/dbms/inverted-index/) | Статья | 0.80 | Базовые концепции |
| 8 | [OpenSearch Project](https://opensearch.org/) | Документация | 0.90 | Open source fork |
| 9 | [Relevant Search - Manning](https://www.manning.com/books/relevant-search) | Книга | 0.95 | Relevance engineering |
| 10 | [Full-Text Search vs LIKE](https://learnsql.com/blog/full-text-search-in-sql/) | Статья | 0.80 | Performance comparison |

---

**Последняя верификация**: 2026-01-02
**Уровень достоверности**: high

---

*Проверено: 2026-01-09*
