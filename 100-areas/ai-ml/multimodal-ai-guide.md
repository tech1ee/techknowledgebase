---
title: "Multimodal AI: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º (2025)"
type: guide
status: published
tags:
  - topic/ai-ml
  - type/guide
  - level/intermediate
---

# Multimodal AI: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º (2025)

> –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –î–µ–∫–∞–±—Ä—å 2025
>
> –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π AI - —ç—Ç–æ —Å–∏—Å—Ç–µ–º—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤: —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ. –î–∞–Ω–Ω—ã–π –≥–∞–π–¥ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ LLM** | –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ | [[llm-fundamentals]] |
| **Python** | –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å API | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **REST API** | –í—Å–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —á–µ—Ä–µ–∑ API | [[ai-api-integration]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ AI** | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ | –ù–∞—á–Ω–∏—Ç–µ —Å [[ai-ml-overview-v2]], –∑–∞—Ç–µ–º —Å—é–¥–∞ |
| **AI Engineer** | ‚úÖ –î–∞ | –ü–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π |
| **Product Manager** | ‚úÖ –î–∞ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ |
| **Creative Professional** | ‚úÖ –î–∞ | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Multimodal AI** = AI, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –∏ —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞—É–¥–∏–æ, –≤–∏–¥–µ–æ)

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Modality** | –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∑–≤—É–∫) | **–Ø–∑—ã–∫** ‚Äî —Ç–µ–∫—Å—Ç, –∫–∞—Ä—Ç–∏–Ω–∫–∏, –º—É–∑—ã–∫–∞ ‚Äî —Ä–∞–∑–Ω—ã–µ "—è–∑—ã–∫–∏" –¥–ª—è AI |
| **Vision** | –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è | **–ó—Ä–µ–Ω–∏–µ** ‚Äî AI "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –ø–æ–Ω–∏–º–∞–µ—Ç —á—Ç–æ —Ç–∞–º |
| **Omni-model** | –ú–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ | **–£–Ω–∏–≤–µ—Ä—Å–∞–ª** ‚Äî –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö |
| **Image Generation** | –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É | **–•—É–¥–æ–∂–Ω–∏–∫ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é** ‚Äî –æ–ø–∏—à–∏, –ø–æ–ª—É—á–∏ –∫–∞—Ä—Ç–∏–Ω—É |
| **TTS** | Text-to-Speech (—Ç–µ–∫—Å—Ç –≤ —Ä–µ—á—å) | **–î–∏–∫—Ç–æ—Ä** ‚Äî —á–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –≥–æ–ª–æ—Å–æ–º |
| **STT/ASR** | Speech-to-Text (—Ä–µ—á—å –≤ —Ç–µ–∫—Å—Ç) | **–°—Ç–µ–Ω–æ–≥—Ä–∞—Ñ–∏—Å—Ç** ‚Äî –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ—á—å —Ç–µ–∫—Å—Ç–æ–º |
| **Video Understanding** | –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ | **–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∑—Ä–∏—Ç–µ–ª—å** ‚Äî –ø–æ–Ω–∏–º–∞–µ—Ç —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏–¥–µ–æ |
| **Computer Use** | AI —É–ø—Ä–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫ | **–£–¥–∞–ª—ë–Ω–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫** ‚Äî –∫–ª–∏–∫–∞–µ—Ç –º—ã—à–∫–æ–π, –ø–µ—á–∞—Ç–∞–µ—Ç |

---

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–û–±–∑–æ—Ä –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ AI](#–æ–±–∑–æ—Ä-–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ-ai)
2. [Vision Models - –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π](#vision-models---–ø–æ–Ω–∏–º–∞–Ω–∏–µ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
3. [Audio & Speech - –ê—É–¥–∏–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏](#audio--speech---–∞—É–¥–∏–æ-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
4. [Video Understanding - –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ](#video-understanding---–∞–Ω–∞–ª–∏–∑-–≤–∏–¥–µ–æ)
5. [Image Generation - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π](#image-generation---–≥–µ–Ω–µ—Ä–∞—Ü–∏—è-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
6. [Video Generation - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ](#video-generation---–≥–µ–Ω–µ—Ä–∞—Ü–∏—è-–≤–∏–¥–µ–æ)
7. [Voice Synthesis - –°–∏–Ω—Ç–µ–∑ –≥–æ–ª–æ—Å–∞](#voice-synthesis---—Å–∏–Ω—Ç–µ–∑-–≥–æ–ª–æ—Å–∞)
8. [Computer Use & Automation](#computer-use--automation)
9. [Production Integration](#production-integration)
10. [–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã](#—Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ-—Ç–∞–±–ª–∏—Ü—ã)

---

## –û–±–∑–æ—Ä –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ AI

### –ß—Ç–æ —Ç–∞–∫–æ–µ Multimodal AI?

**Multimodal AI** - —ç—Ç–æ —Å–∏—Å—Ç–µ–º—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö (—Ñ–æ—Ä–º–∞—Ç–∞—Ö –¥–∞–Ω–Ω—ã—Ö) –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ:

- **Text** - —Ç–µ–∫—Å—Ç–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- **Vision** - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- **Audio** - –∑–≤—É–∫, —Ä–µ—á—å, –º—É–∑—ã–∫–∞
- **Video** - –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–æ–π

### –†—ã–Ω–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (2025)

```
–†–∞–∑–º–µ—Ä —Ä—ã–Ω–∫–∞ Multimodal AI:
- 2025: $2.51 –º–∏–ª–ª–∏–∞—Ä–¥–∞
- 2034 (–ø—Ä–æ–≥–Ω–æ–∑): $42.38 –º–∏–ª–ª–∏–∞—Ä–¥–∞

–ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –æ–¥–∏–Ω API-–≤—ã–∑–æ–≤ –≤–º–µ—Å—Ç–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
Whisper + CLIP + GPT –æ—Ç–¥–µ–ª—å–Ω–æ. –ö–æ–º–ø–∞–Ω–∏–∏ —Å–æ–æ–±—â–∞—é—Ç –æ
—Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –Ω–∞ 50%.
```

### –≠–≤–æ–ª—é—Ü–∏—è –ø–æ–¥—Ö–æ–¥–æ–≤

```
2022: –û—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏
      CLIP (vision) + GPT-3 (text) + Whisper (audio)

2023: –†–∞–Ω–Ω—è—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
      GPT-4V, LLaVA, Claude 3 Vision

2024: Omni-–º–æ–¥–µ–ª–∏
      GPT-4o - –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è text/vision/audio

2025: –ü–æ–ª–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å
      GPT-5, Gemini 3, Claude Opus 4.5, Sora 2
      Video understanding –¥–æ 6 —á–∞—Å–æ–≤, Realtime Audio,
      Computer Use, –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
```

### –ö–ª—é—á–µ–≤—ã–µ –∏–≥—Ä–æ–∫–∏ —Ä—ã–Ω–∫–∞ (–î–µ–∫–∞–±—Ä—å 2025)

| –ö–æ–º–ø–∞–Ω–∏—è | –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ | –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è |
|----------|----------------|---------------|
| **OpenAI** | GPT-4o, GPT-5, Whisper, Sora 2, gpt-realtime | Full-stack multimodal |
| **Anthropic** | Claude Opus 4.5, Sonnet 4.5, Haiku 4.5 | Vision, Computer Use |
| **Google** | Gemini 3 Pro/Flash | Long video (6+ —á–∞—Å–æ–≤) |
| **Meta** | LLaMA 4, Llama 3.2 Vision | Open-source multimodal |
| **Black Forest Labs** | FLUX.2 Pro/Dev/Klein | Image generation SOTA |
| **Runway** | Gen-4.5, Gen-4, Gen-3 Alpha | Video generation |
| **ElevenLabs** | Multilingual v2, Flash v2.5 | Voice synthesis leader |
| **Allen AI** | Molmo 2 (4B-8B) | Open-source video SOTA |

---

## Vision Models - –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

### GPT-4 Vision / GPT-4o

**GPT-4o** ("o" = omni) - flagship –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å OpenAI, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è text, vision –∏ audio –≤ –µ–¥–∏–Ω–æ–º neural network.

#### –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

```
–†–µ–ª–∏–∑: –ú–∞–π 2024
Response time: 320ms (—Å—Ä–∞–≤–Ω–∏–º–æ —Å —á–µ–ª–æ–≤–µ–∫–æ–º)
–ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: text + image + audio input/output
Native image generation: –ú–∞—Ä—Ç 2025 (–∑–∞–º–µ–Ω–∏–ª DALL-E 3 –≤ ChatGPT)
File uploads: –¥–æ 512MB, 20 —Ñ–∞–π–ª–æ–≤ –Ω–∞ —á–∞—Ç
```

#### –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Vision

| –§—É–Ω–∫—Ü–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ö–∞—á–µ—Å—Ç–≤–æ |
|---------|----------|----------|
| **Object Detection** | –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å—Ü–µ–Ω | Excellent |
| **OCR (Text Extraction)** | –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π | Excellent |
| **Mathematical Analysis** | –ê–Ω–∞–ª–∏–∑ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª | Good |
| **Chart Interpretation** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –¥–∏–∞–≥—Ä–∞–º–º | Excellent |
| **Spatial Reasoning** | –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è | Good |

#### Benchmark Performance (–ò—é–ª—å 2025)

–ù–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è "How Well Does GPT-4o Understand Vision?":

```
GPT-4o vs –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 6 CV tasks:
- –õ—É—á—à–∏–π —Å—Ä–µ–¥–∏ non-reasoning –º–æ–¥–µ–ª–µ–π
- Top-1 –≤ 4 –∏–∑ 6 –∑–∞–¥–∞—á
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ >> –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ
- Reasoning models (o3) —É–ª—É—á—à–∞—é—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é

–¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏: GPT-4o, o4-mini, Gemini 1.5 Pro,
Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2
```

#### API Usage

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image? Analyze in detail."},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://example.com/image.jpg",
                        # –ò–ª–∏ base64: "data:image/jpeg;base64,..."
                        "detail": "high"  # low, high, auto
                    }
                }
            ]
        }
    ],
    max_tokens=1000
)
```

#### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

- –ù–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –ª–∏—Ü–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω (safety)
- Image comprehension error rate ~20% –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö
- –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ —Å–ª–æ–∂–Ω–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö
- –ê–ø—Ä–µ–ª—å 2025: rollback –∏–∑-–∑–∞ excessive sycophancy

### Claude Vision (Anthropic)

**Claude 3.5 Sonnet** –∏ **Claude Opus 4.5** - —Å–∏–ª—å–Ω–µ–π—à–∏–µ vision-–º–æ–¥–µ–ª–∏ Anthropic —Å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ–º –≤ document understanding.

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

```
–§–æ—Ä–º–∞—Ç—ã: JPEG, PNG, GIF, WebP
–õ–∏–º–∏—Ç—ã:
  - claude.ai: –¥–æ 20 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/–∑–∞–ø—Ä–æ—Å
  - API: –¥–æ 100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/–∑–∞–ø—Ä–æ—Å
  - Request size: 32MB max

–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: –¥–ª–∏–Ω–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ <= 1568px
Token cost: ~1600 tokens –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
```

#### –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

1. **Chart/Graph Analysis** - "step-change improvements" –¥–ª—è visual reasoning
2. **Text Transcription** - —Ç–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
3. **Multi-image Reasoning** - –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
4. **Document Understanding** - shipping manifests, invoices, forms

#### Benchmark Performance

```
Claude 3.5 Sonnet –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç:
- Claude 3 Opus –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö vision benchmarks
- GPT-4o –∏ Gemini 1.5 Pro –Ω–∞ MathVista, AI2D

–û—Å–æ–±–∞—è —Å–∏–ª–∞: retail, logistics, financial services
–≥–¥–µ –Ω—É–∂–Ω–æ "glean more insights from an image than from text alone"
```

#### API Usage

```python
import anthropic

client = anthropic.Anthropic()

message = client.messages.create(
    model="claude-sonnet-4-5-20250514",
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data,
                    },
                },
                {
                    "type": "text",
                    "text": "Extract all data from this invoice in JSON format"
                }
            ],
        }
    ],
)
```

### Open-Source Vision Models

#### LLaVA (Large Language-and-Vision Assistant)

```
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: CLIP ViT-L/14 + LLM (Vicuna/Llama/Qwen)
–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 7B - 110B
Training:
  - Stage 1: 558K feature alignment (frozen encoder + LLM)
  - Stage 2: 665K instruction tuning (150K GPT + 515K VQA)

Performance: 85.1% relative score vs GPT-4 on synthetic benchmark
Science QA: 92.53% accuracy (LLaVA + GPT-4 synergy)
```

**–ö–ª—é—á–µ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ (2025):**

| –í–µ—Ä—Å–∏—è | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ |
|--------|-------------|
| **LLaVA-NeXT** | Llama-3 (8B), Qwen-1.5 (72B/110B), zero-shot video |
| **LLaVA-CoT** (ICCV 2025) | Chain-of-thought reasoning, beats GPT-4o-mini |
| **LLaVA-GM** | Lightweight, low-resource deployment |
| **LLaViT** | LLM –∫–∞–∫ vision encoder, 2x performance vs baseline |

```python
# LLaVA —á–µ—Ä–µ–∑ Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫)
import ollama

response = ollama.chat(
    model='llava:13b',
    messages=[{
        'role': 'user',
        'content': 'Describe this image in detail',
        'images': ['./photo.jpg']
    }]
)
```

#### –î—Ä—É–≥–∏–µ Open-Source Leaders (2025)

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ |
|--------|-----------|-------------|
| **Molmo 2** (Allen AI) | 4B-8B | SOTA open-source, video tracking |
| **Qwen2.5-VL** | 32B | 256K-1M context, frame-by-frame |
| **GLM-4.5V** (Zhipu) | 106B (12B active) | MoE, 3D-RoPE, 128K context |
| **InternVL** | Various | Strong Chinese support |
| **Gemma 3** (Google) | Lightweight | Text, image, short video |

---

## Audio & Speech - –ê—É–¥–∏–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

### OpenAI Whisper

**Whisper** - state-of-the-art –º–æ–¥–µ–ª—å –¥–ª—è automatic speech recognition (ASR), –≤—ã–ø—É—â–µ–Ω–Ω–∞—è –≤ —Å–µ–Ω—Ç—è–±—Ä–µ 2022.

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
Type: Encoder-Decoder Transformer
Input: 30-second audio chunks -> Log-Mel spectrogram
Output: Text tokens + special tokens (language, timestamps)

Training Data:
- Whisper large: 680,000 hours supervised
- Whisper large-v3: 1M hours weakly labeled + 4M pseudo-labeled

Multilingual: 100+ —è–∑—ã–∫–æ–≤
Zero-shot: —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è
```

#### –ú–æ–¥–µ–ª–∏ Whisper

| Model | Parameters | Speed | Quality | Use Case |
|-------|------------|-------|---------|----------|
| tiny | 39M | Fastest | Lower | Edge devices |
| base | 74M | Fast | OK | Quick transcription |
| small | 244M | Medium | Good | Balanced |
| medium | 769M | Slower | Better | Accuracy focus |
| large-v3 | 1.5B | Slowest | Best | Production quality |
| large-v3-turbo | ~800M | Fast | Great | Best balance |

#### Key Features

```
Robustness:
- 50% fewer errors vs specialized models
- Works with accents, background noise, technical language

Capabilities:
- Multilingual speech recognition
- Speech translation (to English)
- Language identification
- Voice activity detection

Limitations:
- Hallucinations (generates text not in audio)
- Repetition tendency (mitigated by beam search)
- File size: max 25 MB via API
- Formats: mp3, mp4, mpeg, mpga, m4a, wav, webm
```

#### API Usage

```python
from openai import OpenAI

client = OpenAI()

# Transcription with timestamps
audio_file = open("meeting.mp3", "rb")
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="verbose_json",
    timestamp_granularities=["word", "segment"]
)

for segment in transcript.segments:
    print(f"[{segment['start']:.2f}s] {segment['text']}")

# Translation to English
translation = client.audio.translations.create(
    model="whisper-1",
    file=audio_file
)
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ STT –º–æ–¥–µ–ª–µ–π (2025)

#### Commercial Leaders

| Model | WER | Latency | Languages | Streaming | Best For |
|-------|-----|---------|-----------|-----------|----------|
| **AssemblyAI Universal-2** | 14.5% | Low | 99+ | Yes | Most consistent |
| **Deepgram Nova-3** | ~18% | <300ms | Many | Yes | Real-time, noisy |
| **Google Chirp** | 11.6% | Medium | 125+ | Batch | Accuracy focus |
| **GPT-4o-Transcribe** | Low | Medium | Many | No | Accents, noise |
| **ElevenLabs Scribe** | ~3.3% | Low | 99 | Yes | Highest accuracy |

#### Open-Source Champions

| Model | Speed (RTFx) | Accuracy | Best For |
|-------|--------------|----------|----------|
| **NVIDIA Canary Qwen 2.5B** | Fast | Lowest WER | Medical, Financial |
| **NVIDIA Parakeet TDT** | >2000 | Good | Real-time apps |
| **Whisper large-v3-turbo** | Fast | Good | General purpose |
| **groq-distil-whisper** | Fastest | Good | English only |

**Trend 2025:** Multi-model strategies - leading companies combine multiple STT models for virtually error-free transcription.

### OpenAI Realtime API

**Realtime API** - low-latency speech-to-speech –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ, GA —Å 28 –∞–≤–≥—É—Å—Ç–∞ 2025.

#### –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

```
Model: gpt-realtime
Latency: ~320ms (human-like)
Connection: WebRTC, WebSocket, SIP

Benchmark (Big Bench Audio):
- gpt-realtime: 82.8% accuracy
- December 2024 model: 65.6% accuracy

No session limits: –° 3 —Ñ–µ–≤—Ä–∞–ª—è 2025
```

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–∞–¥ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º

```
–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–∞–π–ø–ª–∞–π–Ω (1-3 —Å–µ–∫—É–Ω–¥—ã):
User Audio -> STT (Whisper) -> LLM (GPT-4) -> TTS -> Response
   500ms        500ms          800ms      500ms

Realtime API (300-500ms):
User Audio -> GPT-4o Native Audio -> Audio Response
              –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å
```

**Capabilities:**

- Native audio processing (no intermediate text)
- Non-verbal cues (laughs, sighs)
- Mid-sentence language switching
- Tone adaptation
- Interruption handling
- Accurate alphanumeric detection in multiple languages

#### –ì–æ–ª–æ—Å–∞

```
Built-in voices (13):
alloy, ash, ballad, coral, echo, fable, onyx,
nova, sage, shimmer, verse, marin, cedar

New in 2025: Marin, Cedar (most natural-sounding)

Custom voices: Available via reference audio sample
```

#### Pricing

```
gpt-realtime (20% cheaper than gpt-4o-realtime-preview):
- Input: $32/1M audio tokens
- Cached input: $0.40/1M tokens
- Output: $64/1M audio tokens
```

#### API Usage

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()

async def realtime_conversation():
    async with client.realtime.connect(model="gpt-realtime") as connection:
        await connection.session.update(
            voice="sage",
            instructions="You are a helpful barista assistant",
            turn_detection={
                "type": "server_vad",
                "threshold": 0.5,
                "silence_duration_ms": 500
            }
        )

        # Send audio
        await connection.input_audio_buffer.append(audio_bytes)
        await connection.input_audio_buffer.commit()

        # Receive streaming response
        async for event in connection:
            if event.type == "response.audio.delta":
                play_audio(event.delta)
            elif event.type == "input_audio_buffer.speech_started":
                # User interrupted - cancel response
                await connection.send({"type": "response.cancel"})
```

---

## Video Understanding - –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ

### Google Gemini Video

**Gemini 3 Pro/Flash** (–î–µ–∫–∞–±—Ä—å 2025) - –ª–∏–¥–µ—Ä—ã –≤ video understanding —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–æ 6 —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ.

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

```
Gemini 3 Pro: "most capable multimodal model"
- SOTA on: document, spatial, screen, video understanding
- Benchmarks: MMMU Pro, Video MMMU

Context Windows:
- 2M tokens: up to 6 hours video (low res)
- 1M tokens: up to 3 hours video (low res)

Sampling: 1 frame per second
Fast-paced actions: >1 fps for better understanding

Token cost:
- Default: ~300 tokens/second (258 per frame + 32 audio)
- Low res: ~100 tokens/second (66 per frame + 32 audio)
```

#### Supported Formats

```
Video: MP4, MPEG, MOV, AVI, FLV, MPG, WebM, WMV, 3GPP

Input methods:
- Files API: videos >20MB or >1 minute
- Inline data: videos <20MB
- YouTube URLs: public videos (8-hour daily limit free)

Multi-video:
- Gemini 2.5+: up to 10 videos per request
- Earlier models: 1 video per request
```

#### Key Capabilities

```
Visual Analysis:
- Content description
- Scene segmentation
- Object tracking
- Speaker identification

Audio Integration:
- Synchronized audio-visual understanding
- Timeline correlation

Special Features:
- Time-specific Q&A with timestamps
- AI-generated video detection (SynthID watermark)
- Coach-level sports analysis
- Video-to-Learning-App transformation
```

#### API Usage

```python
import google.generativeai as genai
import time

genai.configure(api_key="YOUR_API_KEY")

# Upload video
video_file = genai.upload_file(path="video.mp4")

# Wait for processing
while video_file.state.name == "PROCESSING":
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

# Analyze with Gemini 3
model = genai.GenerativeModel(model_name="gemini-3-pro")
response = model.generate_content(
    [video_file, """
    Analyze this video:
    1. Key events with timestamps
    2. Speaker identification
    3. Main topics discussed
    4. Action items mentioned
    """],
    generation_config={"media_resolution": "low"}  # for 6h videos
)

print(response.text)
```

### Other Video Understanding Models

#### Molmo 2 (Allen AI, –î–µ–∫–∞–±—Ä—å 2025)

```
Status: SOTA open-source for video

Benchmarks (leads open-weight models):
- Image QA
- Short-video QA
- Video counting
- Video tracking
- Human preference

Comparison: Just behind GPT-5/GPT-5 mini, ahead of Gemini 2.5 Pro
```

#### Microsoft MMCTAgent (–ù–æ—è–±—Ä—å 2025)

```
Purpose: Long-form video reasoning (hours of content)
Architecture: Multi-agent Planner-Critic on AutoGen

Approach:
- Structured reasoning over large-scale visual data
- Handles minutes-to-hours of video context
- Built for real-world reasoning tasks
```

#### Qwen3-VL

```
Context: 256K native, expandable to 1M tokens

Features:
- Frame-by-frame description
- Second-level video indexing
- Hours of video processing
- Detailed Q&A across long content
```

### Video Understanding Benchmarks (2025)

| Benchmark | Purpose | Notes |
|-----------|---------|-------|
| **Video-MME** | Comprehensive eval | CVPR 2025 |
| **VideoMind** | Long-form reasoning | Chain-of-LoRA |
| **MVU-Eval** | Multi-video understanding | - |
| **CrossVid** | Cross-video reasoning | AAAI 2026 |
| **OmniVideoBench** | Audio-visual in omni MLLMs | - |

---

## Image Generation - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

### DALL-E 3 (OpenAI)

**DALL-E 3** - –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π OpenAI, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤ ChatGPT. –í –º–∞—Ä—Ç–µ 2025 –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ GPT Image –¥–ª—è –Ω–∞—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

#### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

```
Resolution: 1024x1024 (base)
Aspect ratios: horizontal, square, vertical
Quality modes: standard, hd
Style modes: vivid (hyper-real), natural

Integration: Built on ChatGPT (prompt enhancement)
Iterative editing: "Make his nose bigger", etc.

Safety:
- No public figures
- No living artist styles
- C2PA watermark (February 2024+)

Still available: Microsoft Copilot (via Bing Image Creator)
```

#### API Usage

```python
from openai import OpenAI

client = OpenAI()

response = client.images.generate(
    model="dall-e-3",
    prompt="A futuristic city with flying cars at sunset, photorealistic",
    size="1024x1024",
    quality="hd",
    style="vivid",
    n=1
)

image_url = response.data[0].url
revised_prompt = response.data[0].revised_prompt

# Editing capabilities (DALL-E 2/3)
# - Variations: generate variants of existing image
# - Inpainting: fill missing areas
# - Outpainting: expand image boundaries
```

#### Pricing

```
API: $0.02-0.08 per image (size/quality dependent)
ChatGPT Plus: ~100 images/day included ($20/month)
```

### FLUX (Black Forest Labs)

**FLUX.2** (–ù–æ—è–±—Ä—å 2025) - state-of-the-art image generation –æ—Ç —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π Stable Diffusion.

#### –ú–æ–¥–µ–ª–∏ FLUX.2

| Model | License | Resolution | Use Case |
|-------|---------|------------|----------|
| **FLUX.2 Pro** | Commercial | 4MP | Production quality |
| **FLUX.2 Flex** | - | 4MP | Flexible usage |
| **FLUX.2 Dev** | Non-commercial | 4MP | Development |
| **FLUX.2 Klein** | Apache 2.0 | - | Open-source |

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

```
Architecture: Latent Flow Matching + Rectified Flow
VLM: Mistral-3 (24B parameters)
VAE: Open-source Apache 2.0

Features:
- Photorealistic output (no "AI look")
- Character/style consistency across references
- Complex text rendering
- Brand guideline adherence
- 4MP editing with detail preservation
- Real-world lighting and physics
```

#### Key 2025 Milestones

```
January 2025: NVIDIA Blackwell partnership
May 2025: Flux.1 Kontext (in-context editing)
September 2025: Adobe Photoshop integration
November 2025: FLUX.2 release with NVIDIA ComfyUI optimization
```

#### Availability

```
APIs: Replicate, fal.ai, mystic
Local: ComfyUI with NVIDIA weight streaming
Adobe: Photoshop (beta) - Flux.1 Kontext Pro for generative fill
```

### Stable Diffusion 3.5 (Stability AI)

#### –ú–æ–¥–µ–ª–∏

```
SD 3.5 Large: 8B parameters, up to 1MP
SD 3.5 Large Turbo: Distilled, faster
SD 3.5 Medium: Edge devices, 0.25-2MP

Architecture: Diffusion Transformer + Flow Matching
Backbone: Rectified Flow (new in 3.0)

Enterprise: Available on Amazon Bedrock
```

#### Hardware Requirements

```
Recommended VRAM:
- 12GB: RTX 3060 (minimum)
- 16GB: RTX 4060 Ti (comfortable)
- 24GB: RTX 3090/4090 (best performance)

Higher VRAM = faster generation + higher resolution
NVIDIA GPUs preferred (better software compatibility)
```

### Midjourney vs DALL-E 3 (2025)

| Aspect | DALL-E 3 | Midjourney V7 |
|--------|----------|---------------|
| **Strength** | Prompt accuracy, text in images | Artistic quality, aesthetics |
| **Style** | Photorealistic, clean | Emotional, atmospheric |
| **Interface** | ChatGPT, API | Discord (learning curve) |
| **API Access** | Yes (official) | No official API |
| **Pricing** | $20/mo (Plus) + API | $10-60/mo tiers |
| **Best For** | Marketing, product viz | Art, storytelling |

#### When to Choose

```
DALL-E 3:
- Professional marketing materials
- Accurate product visualizations
- ChatGPT integration
- Clear commercial licensing
- API for automation

Midjourney V7:
- Artistic quality priority
- Creative exploration
- Style consistency
- Community inspiration
- High-volume, cost-effective
```

---

## Video Generation - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ

### OpenAI Sora 2

**Sora 2** (30 —Å–µ–Ω—Ç—è–±—Ä—è 2025) - flagship video/audio generation model, –æ–ø–∏—Å—ã–≤–∞–µ–º–∞—è –∫–∞–∫ "GPT-3.5 moment for video".

#### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

```
Platforms: sora.com, iOS app, Android (November 2025)
API: Available (sora-2, sora-2-2025-10-06, sora-2-2025-12-08)

Capabilities:
- Accurate physics simulation
- Basketball rebounds correctly
- Buoyancy, rigidity modeling
- Olympic gymnastics routines
- Backflips on paddleboard

Audio:
- Synchronized dialogue
- Sound effects
- Automatic music generation

Safety:
- Visible watermark
- C2PA metadata
- Content moderation
```

#### Notable Events (2025)

```
December 2025: Disney $1B investment
- Access to 200+ characters (Disney, Pixar, Marvel, Star Wars)

October 2025: Japan CODA request
- Stop using copyrighted content (Studio Ghibli, Square Enix)
```

#### Earlier Sora History

```
December 2024: Gradual public release (ChatGPT Pro/Plus, US/Canada)
February 2025: Integration plans with ChatGPT
```

### Runway Gen-4.5

**Runway** - –ø–∏–æ–Ω–µ—Ä AI video generation.

#### –ú–æ–¥–µ–ª–∏ (2025)

| Model | Purpose | Duration | Notes |
|-------|---------|----------|-------|
| **Gen-4.5** | SOTA quality | - | "World's best video model" |
| **Gen-4** | High fidelity | 10s | Text-to-Video |
| **Gen-3 Alpha** | Standard | 10-11s | Most features |
| **Gen-3 Alpha Turbo** | Fast/cheap | 10s | Blocking/prototyping |
| **Aleph** | Video-to-Video | - | Recommended for V2V |

#### Gen-3 Alpha Features

```
Core:
- Text to Video
- Image to Video
- Video to Video (with Aleph)

Controls:
- Motion Brush
- Advanced Camera Controls
- Director Mode
- Structure, style, motion control

Safety: Visual moderation + C2PA standards
Recognition: TIME "200 Best Inventions" 2024
```

### Pika Labs

**Pika 2.5** - short-form video creation platform —Å 11M+ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.

#### Features (2025)

```
Pika 2.5 Core:
- 10-second videos in 1080p
- Camera control
- Character/style consistency
- Multiple styles: 3D animation, anime, cinematic, live action

Special Tools:
- Pikaframes: Image-to-video with first/last frame
- Pikaformance: Lip-sync, facial expressions (TikTok, Reels)
- Pikatwists: Twist endings
- Pikaswaps: Object replacement in video
- Pikadditions: Add elements

Pricing: Free tier (480p), paid for full features
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Video Generators

| Feature | Sora 2 | Runway Gen-4 | Pika 2.5 |
|---------|--------|--------------|----------|
| **Physics** | Best | Good | Good |
| **Audio** | Native sync | No | Limited |
| **Duration** | Variable | 10s | 10s |
| **API** | Yes | Yes | No |
| **Price** | High | Medium | Low |
| **Best For** | Quality | Professional | Short-form |

---

## Voice Synthesis - –°–∏–Ω—Ç–µ–∑ –≥–æ–ª–æ—Å–∞

### ElevenLabs

**ElevenLabs** - –ª–∏–¥–µ—Ä –≤ realistic voice synthesis —Å deep learning.

#### –ü—Ä–æ–¥—É–∫—Ç—ã

```
Core Products:
- Text-to-Speech (TTS): 32 languages, nuanced expression
- Speech-to-Speech (STS): Voice conversion
- Voice Cloning: Instant from seconds of audio
- Voice Library: 1000+ community voices
- Conversational AI: Interactive voice agents

2025 Additions:
- February 2025: AI audiobook platform (Reader app)
- August 2025: Eleven Music (studio-grade from text)
```

#### TTS Models

| Model | Latency | Quality | Best For |
|-------|---------|---------|----------|
| **Flash v2.5** | 75ms | Good | Real-time apps |
| **Multilingual v2** | Higher | Best | Nuanced expression |

#### Key Features

```
Voice Cloning:
- Instant cloning from seconds of audio
- Multilingual cloning (speak in different language)
- Only requires few seconds to minutes of reference

Voice Settings:
- stability: Consistency (higher = more consistent)
- similarity_boost: Closeness to original
- style: Expressiveness
- use_speaker_boost: Clarity enhancement
```

#### API Usage

```python
from elevenlabs import ElevenLabs

client = ElevenLabs(api_key="YOUR_API_KEY")

# Text-to-Speech
audio = client.text_to_speech.convert(
    voice_id="21m00Tcm4TlvDq8ikWAM",  # Rachel
    text="Welcome to the future of voice technology.",
    model_id="eleven_multilingual_v2",
    voice_settings={
        "stability": 0.5,
        "similarity_boost": 0.75,
        "style": 0.5,
        "use_speaker_boost": True
    }
)

# Voice Cloning
voice = client.voices.clone(
    name="My Voice",
    files=[open("sample.mp3", "rb")],
    description="Professional, warm voice"
)

cloned_audio = client.text_to_speech.convert(
    voice_id=voice.voice_id,
    text="This sounds like me!"
)
```

### OpenAI TTS

```python
from openai import OpenAI

client = OpenAI()

response = client.audio.speech.create(
    model="tts-1-hd",  # or "tts-1" for speed
    voice="alloy",     # alloy, echo, fable, onyx, nova, shimmer
    input="The quick brown fox jumps over the lazy dog."
)

response.stream_to_file("output.mp3")
```

#### Comparison: ElevenLabs vs OpenAI TTS

| Feature | ElevenLabs | OpenAI TTS |
|---------|------------|------------|
| **Voices** | 1000+ community | 6 built-in |
| **Cloning** | Yes (instant) | Custom API |
| **Languages** | 32 | ~10 |
| **Quality** | Premium | Good |
| **Latency** | 75ms+ | ~500ms |
| **Pricing** | Usage-based | $15/1M chars |

---

## Computer Use & Automation

### Claude Computer Use

**Computer Use** - beta feature –¥–ª—è desktop automation —á–µ—Ä–µ–∑ vision (Anthropic).

#### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç

```
Process:
1. Screenshot capture -> Claude analyzes
2. Vision identifies UI elements
3. Coordinate-based mouse/keyboard actions
4. Pixel-perfect accuracy

Technology: Advanced vision + coordinate-based interaction
```

#### Supported Models (December 2025)

| Model | Tool Version | Special Features |
|-------|--------------|------------------|
| **Claude Opus 4.5** | computer_20251124 | zoom action for detail |
| Other models | computer_20250124 | Standard capabilities |

#### Benchmark Performance

```
OSWorld (screenshot-only):
- Claude 3.5 Sonnet: 14.9% (2x better than next AI)
- With more steps: 22.0%

Note: Next best AI system scored only 7.8%
```

#### API Usage

```python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5-20250514",
    max_tokens=1024,
    tools=[
        {
            "type": "computer_20250124",
            "name": "computer",
            "display_width_px": 1920,
            "display_height_px": 1080
        }
    ],
    messages=[
        {
            "role": "user",
            "content": "Open browser and search for 'weather today'"
        }
    ]
)
```

#### Use Cases

```
Production Applications:
- E2E UI testing (natural language test cases)
- Background information gathering
- Automated software testing
- App evaluation during development (Replit Agent)

Best For:
- Non-speed-critical tasks
- Trusted environments
- Complex UI interactions
```

#### Safety Measures

```
- Prompt injection classifiers on screenshots
- User confirmation for suspicious actions
- Sandboxed environments recommended
- May hallucinate coordinates
```

---

## Production Integration

### Top Multimodal AI Platforms

#### 1. OpenAI Platform

```python
from openai import OpenAI

client = OpenAI()

# Unified multimodal: text + image + audio
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Analyze this..."},
                {"type": "image_url", "image_url": {"url": "..."}},
                {"type": "input_audio", "input_audio": {...}}
            ]
        }
    ]
)
```

#### 2. Google Vertex AI / Gemini API

```python
import google.generativeai as genai

model = genai.GenerativeModel("gemini-3-pro")

# Multimodal input
response = model.generate_content([
    "Describe what's happening",
    uploaded_image,
    uploaded_video,
    uploaded_audio
])
```

#### 3. Azure OpenAI Service

```
Enterprise Benefits:
- SSO integration
- VNet isolation
- Compliance certifications
- Same OpenAI models
```

#### 4. Amazon Bedrock

```
Available Models:
- Claude 3.5 Sonnet
- Stable Diffusion 3.5 Large
- Amazon Titan Multimodal
- Llama 3.2 Vision
```

### LangChain Multimodal Integration

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(model="gpt-4o")

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe these images"},
        {"type": "image_url", "image_url": {"url": "image1.jpg"}},
        {"type": "image_url", "image_url": {"url": "image2.jpg"}}
    ]
)

response = model.invoke([message])
```

### Best Practices

#### 1. Image Optimization

```python
from PIL import Image
import io

def optimize_image(image_path, max_size=1568):
    """Reduce tokens by resizing large images."""
    img = Image.open(image_path)

    if max(img.size) > max_size:
        ratio = max_size / max(img.size)
        new_size = (int(img.width * ratio), int(img.height * ratio))
        img = img.resize(new_size, Image.LANCZOS)

    buffer = io.BytesIO()
    img.convert("RGB").save(buffer, format="JPEG", quality=85)
    return buffer.getvalue()
```

#### 2. Caching

```python
import hashlib
from functools import lru_cache

def hash_content(content):
    return hashlib.md5(str(content).encode()).hexdigest()

@lru_cache(maxsize=1000)
def cached_vision_analysis(content_hash, prompt):
    return api_call(prompt)
```

#### 3. Error Handling

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def generate_with_retry(client, prompt, image):
    try:
        return await client.chat.completions.create(
            model="gpt-4o",
            messages=[...],
            timeout=30
        )
    except RateLimitError:
        await asyncio.sleep(60)
        raise
```

#### 4. Monitoring

```python
from dataclasses import dataclass
import time

@dataclass
class MultimodalMetrics:
    latency_ms: float
    tokens_used: int
    modalities: list[str]
    model: str
    success: bool

def track_call(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        metrics = MultimodalMetrics(
            latency_ms=(time.time() - start) * 1000,
            tokens_used=result.usage.total_tokens,
            modalities=["text", "image"],
            model=kwargs.get("model"),
            success=True
        )
        log_metrics(metrics)
        return result
    return wrapper
```

---

## –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã

### Vision Models Comparison

| Model | Provider | Context | Strengths | Best For |
|-------|----------|---------|-----------|----------|
| **GPT-4o** | OpenAI | 128K | Best overall, omni | General |
| **Claude Opus 4.5** | Anthropic | 200K | Computer use, charts | Documents |
| **Gemini 3 Pro** | Google | 2M | Long video, 6h+ | Video analysis |
| **LLaVA-CoT** | Open | - | Reasoning | Research |
| **Qwen2.5-VL** | Alibaba | 1M | Multi-lang | Chinese apps |
| **Molmo 2** | Allen AI | - | Open SOTA, tracking | Open-source |

### Audio Models Comparison

| Model | Type | Languages | Latency | Open |
|-------|------|-----------|---------|------|
| **Whisper large-v3** | STT | 100+ | Medium | Yes |
| **gpt-realtime** | S2S | Many | 320ms | No |
| **ElevenLabs Flash** | TTS | 32 | 75ms | No |
| **NVIDIA Canary** | STT | Many | Low | Yes |
| **AssemblyAI Universal-2** | STT | 99+ | Low | No |

### Image Generation Comparison

| Model | Style | Resolution | API | Open |
|-------|-------|------------|-----|------|
| **DALL-E 3** | Realistic | 1024px | Yes | No |
| **FLUX.2 Pro** | Photo | 4MP | Yes | No |
| **FLUX.2 Klein** | Various | - | Yes | Yes |
| **SD 3.5 Large** | Various | 1MP | Yes | Partial |
| **Midjourney V7** | Artistic | High | No | No |

### Video Generation Comparison

| Model | Duration | Audio | Physics | API |
|-------|----------|-------|---------|-----|
| **Sora 2** | Variable | Yes | Best | Yes |
| **Runway Gen-4** | 10s | No | Good | Yes |
| **Pika 2.5** | 10s | Limited | Good | No |

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

### –ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã 2025

1. **Unified Models** - –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π (GPT-4o, Gemini 3)
2. **Long Context** - –¥–æ 6 —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ (Gemini), 1M+ —Ç–æ–∫–µ–Ω–æ–≤
3. **Realtime Processing** - sub-second latency –¥–ª—è voice/video
4. **Computer Use** - AI –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ vision
5. **Open-Source Catch-Up** - Molmo 2, FLUX.2 Klein, LLaVA-CoT

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É

```
–î–ª—è Vision:
- Production: GPT-4o –∏–ª–∏ Claude Opus 4.5
- Open-source: Molmo 2 –∏–ª–∏ LLaVA-CoT
- Long video: Gemini 3 Pro

–î–ª—è Audio:
- STT accuracy: AssemblyAI Universal-2 –∏–ª–∏ ElevenLabs Scribe
- Open-source STT: Whisper large-v3
- Realtime voice: gpt-realtime
- Voice synthesis: ElevenLabs

–î–ª—è Image Generation:
- API integration: DALL-E 3 –∏–ª–∏ FLUX.2
- Artistic: Midjourney V7
- Open-source: FLUX.2 Klein, SD 3.5

–î–ª—è Video Generation:
- Quality: Sora 2
- Professional: Runway Gen-4
- Short-form: Pika 2.5
```

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

### Vision & Multimodal
- [GPT-4 Vision Guide - DataCamp](https://www.datacamp.com/tutorial/gpt-4-vision-comprehensive-guide)
- [Claude Vision Documentation](https://docs.claude.com/en/docs/build-with-claude/vision)
- [Gemini Video Understanding](https://ai.google.dev/gemini-api/docs/video-understanding)
- [LLaVA Project](https://llava-vl.github.io/)
- [Molmo 2 - Allen AI](https://allenai.org/blog/molmo2)
- [Anthropic Claude 3.5 Announcement](https://www.anthropic.com/news/claude-3-5-sonnet)

### Audio & Speech
- [Introducing Whisper - OpenAI](https://openai.com/index/whisper/)
- [OpenAI Realtime API](https://platform.openai.com/docs/guides/realtime)
- [Best Speech-to-Text Models 2025](https://nextlevel.ai/best-speech-to-text-models/)
- [ElevenLabs Documentation](https://elevenlabs.io/docs)
- [gpt-realtime Introduction](https://openai.com/index/introducing-gpt-realtime/)

### Image Generation
- [DALL-E 3 Cookbook - OpenAI](https://cookbook.openai.com/articles/what_is_new_with_dalle_3)
- [FLUX.2 - Black Forest Labs](https://bfl.ai/blog/flux-2)
- [Stable Diffusion 3 - Stability AI](https://stability.ai/news/stable-diffusion-3)
- [Midjourney vs DALL-E 2025](https://vertu.com/lifestyle/midjourney-vs-dall-e-3-the-ultimate-ai-image-generation-showdown-for-2025/)

### Video Generation
- [Sora 2 - OpenAI](https://openai.com/index/sora-2/)
- [Runway Gen-3 Alpha](https://runwayml.com/research/introducing-gen-3-alpha)
- [Pika Labs Features 2025](https://pikalabs.net/pika-ai-features/)

### Computer Use & Automation
- [Claude Computer Use Documentation](https://docs.claude.com/en/docs/agents-and-tools/tool-use/computer-use-tool)
- [Anthropic Computer Use Announcement](https://www.anthropic.com/news/3-5-models-and-computer-use)

### Production & Integration
- [Top Platforms for Multimodal AI 2025](https://thirdeyedata.ai/top-18-tools-and-platforms-for-multimodal-ai-solutions-development-in-2025-26/)
- [Best Multimodal Chat APIs - Eden AI](https://www.edenai.co/post/best-multimodal-chat-apis)
- [Google Cloud Multimodal AI](https://cloud.google.com/use-cases/multimodal-ai)
- [Gemini Video Understanding Blog](https://developers.googleblog.com/en/gemini-2-5-video-understanding/)

---

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
