---
title: –õ–æ–∫–∞–ª—å–Ω—ã–µ LLM –∏ Self-Hosting - –ü–æ–ª–Ω–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ
tags: [ai, llm, local, ollama, llama-cpp, self-hosting, open-source, deepseek, qwen, llama, mobile, on-device, edge, mediapipe, executorch, mlx]
category: ai-ml
level: advanced
created: 2025-01-15
updated: 2025-12-29
sources: [ollama.com, lmstudio.ai, github.com/ggerganov/llama.cpp, huggingface.co, mistral.ai, deepseek.com, ai.google.dev/edge, pytorch.org/executorch, developer.apple.com/mlx]
related:
  - "[[mobile-ai-ml-guide]]"
---

# –õ–æ–∫–∞–ª—å–Ω—ã–µ LLM: Ollama, LM Studio, llama.cpp

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ LLM** | –ß—Ç–æ —Ç–∞–∫–æ–µ –º–æ–¥–µ–ª–∏, —Ç–æ–∫–µ–Ω—ã, inference | [[llm-fundamentals]] |
| **Linux/Terminal** | –ó–∞–ø—É—Å–∫, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ | –ë–∞–∑–æ–≤—ã–π –∫—É—Ä—Å Linux |
| **Hardware** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ GPU, VRAM, quantization | –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤–µ–Ω–¥–æ—Ä–æ–≤ |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ AI** | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ | –ù–∞—á–Ω–∏—Ç–µ —Å Ollama ‚Äî —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç |
| **AI Engineer** | ‚úÖ –î–∞ | –ü–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä self-hosting –æ–ø—Ü–∏–π |
| **DevOps/SRE** | ‚úÖ –î–∞ | Production deployment, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ |
| **Privacy-focused** | ‚úÖ –î–∞ | –î–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç –≤–∞—à —Å–µ—Ä–≤–µ—Ä |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Self-hosting LLM** = –∑–∞–ø—É—Å–∫ AI-–º–æ–¥–µ–ª–∏ –Ω–∞ —Å–≤–æ—ë–º –∂–µ–ª–µ–∑–µ (–Ω–µ —á–µ—Ä–µ–∑ API)

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Ollama** | –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ LLM | **Docker –¥–ª—è LLM** ‚Äî –æ–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç |
| **llama.cpp** | –ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π inference engine | **–î–≤–∏–∂–æ–∫** ‚Äî –º–∞–∫—Å–∏–º—É–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ |
| **GGUF** | –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–µ–π | **–§–æ—Ä–º–∞—Ç –≤–∏–¥–µ–æ** ‚Äî –∫–∞–∫ MP4 –¥–ª—è –≤–∏–¥–µ–æ, GGUF –¥–ª—è –º–æ–¥–µ–ª–µ–π |
| **Quantization** | –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ—Ç–µ—Ä–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ | **–°–∂–∞—Ç–∏–µ JPEG** ‚Äî –º–µ–Ω—å—à–µ –º–µ—Å—Ç–∞, —á—É—Ç—å —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| **Q4_K_M** | –ü–æ–ø—É–ª—è—Ä–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ | **–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å** ‚Äî —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —É–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä |
| **VRAM** | –í–∏–¥–µ–æ–ø–∞–º—è—Ç—å GPU | **–û–ø–µ—Ä–∞—Ç–∏–≤–∫–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã** ‚Äî —Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ —Ç—É–¥–∞ –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è |
| **tok/s** | –¢–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É | **–°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏** ‚Äî —á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –±—ã—Å—Ç—Ä–µ–µ –æ—Ç–≤–µ—Ç—ã |
| **Context Length** | –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤—Ö–æ–¥ | **–†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å** ‚Äî —Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å |

---

## TL;DR

> **–õ–æ–∫–∞–ª—å–Ω—ã–µ LLM** - –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–≤–æ–µ–º –∂–µ–ª–µ–∑–µ –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏, —Å–∫–æ—Ä–æ—Å—Ç–∏ (sub-10ms vs 200-800ms API), –∏ —ç–∫–æ–Ω–æ–º–∏–∏.
>
> **Desktop/Server:** Ollama (–ø—Ä–æ—Å—Ç–µ–π—à–∏–π setup), LM Studio (GUI + MLX), llama.cpp (max performance). –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏: DeepSeek R1 (reasoning, MIT), Qwen 3 (multilingual, 1M context), Llama 3.3/4 (general), Mistral Small 3.1 (24B, 150 tok/s). RTX 5090 (32GB) - –ª–∏–¥–µ—Ä 2025 —Å 213 tok/s.
>
> **Mobile/Edge (NEW 2025):** Gemma 3n (60-70 tok/s –Ω–∞ Pixel, multimodal), Llama 3.2 1B/3B (iPhone/Android), Qwen3 0.6B (edge). Frameworks: MediaPipe (Android), MLX Swift (iOS), ExecuTorch (cross-platform). –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: 1-4B –º–æ–¥–µ–ª–∏, 50% –±–∞—Ç–∞—Ä–µ–∏ –∑–∞ 90 –º–∏–Ω, Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞.

---

## –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è Local LLM (2023-2025)

```
+----------------------------------------------------------------------------+
|                    –≠–≤–æ–ª—é—Ü–∏—è Open-Source LLM —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã                     |
+----------------------------------------------------------------------------+
|                                                                            |
|  2023: –ó–ê–†–û–ñ–î–ï–ù–ò–ï                                                          |
|  ~~~~~~~~~~~~~~~~                                                          |
|  - Meta –≤—ã–ø—É—Å–∫–∞–µ—Ç LLaMA 1 –∏ LLaMA 2 (–ø–µ—Ä–≤—ã–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ open-weight)     |
|  - –ü–æ—è–≤–ª–µ–Ω–∏–µ llama.cpp (Georgi Gerganov) - inference –Ω–∞ CPU               |
|  - –§–æ—Ä–º–∞—Ç GGML –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π                                    |
|  - –ü–µ—Ä–≤—ã–µ –≤–µ—Ä—Å–∏–∏ Ollama —É–ø—Ä–æ—â–∞—é—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫                         |
|  - Ratio: –¥–æ 284 tokens/parameter (Llama 2)                               |
|                                                                            |
|  2024: –£–°–ö–û–†–ï–ù–ò–ï                                                           |
|  ~~~~~~~~~~~~~~~                                                           |
|  - GGUF –∑–∞–º–µ–Ω—è–µ—Ç GGML (–ª—É—á—à–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)                              |
|  - LM Studio –ø–æ–ª—É—á–∞–µ—Ç MLX –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–ª—è Apple Silicon                     |
|  - Mixtral 8x7B (MoE) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã                |
|  - Llama 3: ratio 1,875 tok/param (8B –Ω–∞ 15T —Ç–æ–∫–µ–Ω–æ–≤)                     |
|  - Qwen 2.5 –∏ DeepSeek-Coder –ø–æ–¥–Ω–∏–º–∞—é—Ç –ø–ª–∞–Ω–∫—É –¥–ª—è coding                  |
|  - vLLM –∏ SGLang –¥–ª—è production high-throughput                           |
|                                                                            |
|  2025: –ü–ê–†–ò–¢–ï–¢ –° CLOSED-SOURCE                                             |
|  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                              |
|  - DeepSeek R1 (671B MoE) - —Å—Ä–∞–≤–Ω–∏–º —Å OpenAI o1, MIT license              |
|  - Qwen 3: 1M context, thinking mode, Apache 2.0                          |
|  - Llama 4: –Ω–∞—Ç–∏–≤–Ω—ã–π multimodal (Scout, Maverick)                         |
|  - Mistral Small 3.1: 24B —Å 150 tok/s, vision                             |
|  - RTX 5090 (32GB VRAM) - 213 tok/s –Ω–∞ 8B                                 |
|  - "–ì–æ–¥ –∞–≥–µ–Ω—Ç–æ–≤" - –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –¥–ª—è tool use                            |
|  - Open-weight –¥–æ–≥–Ω–∞–ª–∏ closed –ø–æ –∫–∞—á–µ—Å—Ç–≤—É                                 |
|                                                                            |
+----------------------------------------------------------------------------+
```

**–ö–ª—é—á–µ–≤–æ–π —Å–¥–≤–∏–≥ 2025**: —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–º–µ—Å—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏. –í–æ–ø—Ä–æ—Å –∏–∑–º–µ–Ω–∏–ª—Å—è —Å "–ö–∞–∫–∞—è LLM –ª—É—á—à–∞—è?" –Ω–∞ "–ö–∞–∫–∞—è –º–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏?"

---

## –ì–ª–æ—Å—Å–∞—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤

| –¢–µ—Ä–º–∏–Ω | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ |
|--------|-------------|
| **GGUF** | –§–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è llama.cpp (–∑–∞–º–µ–Ω–∞ GGML), –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è inference |
| **Quantization** | –°–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Å–æ–≤ (Q2-Q8) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è |
| **K-Quants** | –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (Q4_K_M, Q5_K_M) —Å –ª—É—á—à–∏–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ |
| **VRAM** | –í–∏–¥–µ–æ–ø–∞–º—è—Ç—å GPU –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ |
| **Unified Memory** | –û–±—â–∞—è –ø–∞–º—è—Ç—å CPU/GPU –Ω–∞ Apple Silicon |
| **Context Length** | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö |
| **Tokens/sec** | –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ |
| **MoE** | Mixture of Experts - –∞–∫—Ç–∏–≤–∞—Ü–∏—è —á–∞—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤) |
| **Instruct** | –ú–æ–¥–µ–ª—å fine-tuned –¥–ª—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º |
| **Thinking Mode** | –†–µ–∂–∏–º "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á (chain-of-thought) |
| **MLX** | Apple framework –¥–ª—è ML –Ω–∞ Apple Silicon |

---

## –ó–∞—á–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ LLM?

```
+----------------------------------------------------------------------------+
|                        Cloud vs Local LLM (2025)                           |
+----------------------------------------------------------------------------+
|                                                                            |
|  CLOUD API (GPT-4o, Claude 3.5)      LOCAL LLM (Ollama, LM Studio)        |
|                                                                            |
|  + –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–ø–æ–∫–∞)            + 100% –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö            |
|  + –ù–µ—Ç –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –∂–µ–ª–µ–∑–æ              + –ù–µ—Ç costs per token                |
|  + –í—Å–µ–≥–¥–∞ latest version             + Latency sub-10ms (vs 200-800ms)    |
|  + –ü—Ä–æ—Å—Ç–æ–π —Å—Ç–∞—Ä—Ç                     + –†–∞–±–æ—Ç–∞–µ—Ç –æ—Ñ–ª–∞–π–Ω                    |
|                                       + –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∏ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è    |
|  - $0.01-0.15/1K tokens              + Fine-tuning –≤–æ–∑–º–æ–∂–µ–Ω               |
|  - Data –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –Ω–∞—Ä—É–∂—É                                               |
|  - Rate limits                        - –¢—Ä–µ–±—É–µ—Ç GPU/–∂–µ–ª–µ–∑–æ                |
|  - Latency 200-800ms                  - –ö–∞—á–µ—Å—Ç–≤–æ —á—É—Ç—å –Ω–∏–∂–µ GPT-4o         |
|  - –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞          - –ù—É–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –º–æ–¥–µ–ª–∏            |
|                                                                            |
|  –ö–û–ì–î–ê –í–´–ë–ò–†–ê–¢–¨ LOCAL:                                                     |
|  - –ü—Ä–∏–≤–∞—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ–∏–Ω–∞–Ω—Å—ã, –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)           |
|  - –í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º –∑–∞–ø—Ä–æ—Å–æ–≤ (1-10M+ —Ç–æ–∫–µ–Ω–æ–≤/–¥–µ–Ω—å - ROI 6-12 –º–µ—Å—è—Ü–µ–≤)       |
|  - –ù–∏–∑–∫–∞—è latency –∫—Ä–∏—Ç–∏—á–Ω–∞ (real-time applications)                       |
|  - –û—Ñ–ª–∞–π–Ω —Ä–∞–±–æ—Ç–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞                                              |
|  - –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è/fine-tuning –ø–æ–¥ –¥–æ–º–µ–Ω                                     |
|  - Compliance —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É)              |
|                                                                            |
+----------------------------------------------------------------------------+
```

**–≠–∫–æ–Ω–æ–º–∏–∫–∞ 2025**: –ü—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ 1-10M —Ç–æ–∫–µ–Ω–æ–≤ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ, single RTX 4090/5090 —Å quantized –º–æ–¥–µ–ª—è–º–∏ –æ–∫—É–ø–∞–µ—Ç—Å—è –∑–∞ 6-12 –º–µ—Å—è—Ü–µ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å API.

---

## 1. Hardware Requirements

### –†–∞—Å—á–µ—Ç VRAM

```
+----------------------------------------------------------------------------+
|                       VRAM Calculation Formula                              |
+----------------------------------------------------------------------------+
|                                                                            |
|  VRAM (GB) = (Parameters x Bits / 8) x 1.2 + KV Cache                     |
|                                                                            |
|  –ü—Ä–∏–º–µ—Ä: Llama 3.3 70B –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö:                                 |
|                                                                            |
|  FP16 (16 bit): 70 x 16/8 x 1.2 = 168 GB   (datacenter only)             |
|  INT8 (8 bit):  70 x 8/8 x 1.2  = 84 GB    (multi-GPU)                   |
|  Q4 (4 bit):    70 x 4/8 x 1.2  = 42 GB    (2x RTX 4090)                 |
|  Q3 (3 bit):    70 x 3/8 x 1.2  = 32 GB    (single RTX 5090)             |
|                                                                            |
|  + KV Cache –¥–ª—è context (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)                      |
|                                                                            |
|  –ö–†–ò–¢–ò–ß–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å + KV cache > VRAM:                                 |
|  –°–∫–æ—Ä–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç —Å 50-100 tok/s –¥–æ 2-5 tok/s (CPU spill)                 |
|  –õ—É—á—à–µ –º–µ–Ω—å—à–∞—è –º–æ–¥–µ–ª—å –≤ VRAM, —á–µ–º –±–æ–ª—å—à–∞—è —Å offload                      |
|                                                                            |
+----------------------------------------------------------------------------+
```

### GPU Recommendations (2025)

| GPU | VRAM | –ú–æ–¥–µ–ª–∏ | Tokens/sec | –¶–µ–Ω–∞ USD | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
|-----|------|--------|-----------|----------|------------|
| RTX 4060 Ti 16GB | 16GB | 7-13B (Q4) | 25-35 | ~$500 | Entry-level –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ |
| RTX 3090 | 24GB | 13-32B (Q4) | 25-40 | ~$800-900 (used) | Best value 24GB |
| RTX 4090 | 24GB | 32B (Q4) | 30-50 | ~$1,600 | –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä |
| **RTX 5090** | **32GB** | **32B (Q5), 49B (Q4)** | **61-213** | **~$2,000** | **–ù–æ–≤—ã–π –ª–∏–¥–µ—Ä 2025** |
| 2x RTX 4090 | 48GB | 70B (Q4) | 20-30 | ~$3,200 | Multi-GPU setup |
| Intel Arc B580 | 12GB | 7-8B | 15-25 | ~$250 | Budget experimentation |

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ RTX 5090 vs RTX 4090:**
- VRAM: 32GB vs 24GB (+33%)
- Bandwidth: 1,792 GB/s vs 1,010 GB/s (+77%)
- 8B –º–æ–¥–µ–ª–∏: 213 tok/s vs 128 tok/s (+67%)
- 32B –º–æ–¥–µ–ª–∏: 61 tok/s (RTX 5090 –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å)

### Apple Silicon Performance (2025)

```
+----------------------------------------------------------------------------+
|                  Apple Silicon + MLX Performance                            |
+----------------------------------------------------------------------------+
|                                                                            |
|  UNIFIED MEMORY ADVANTAGE:                                                 |
|  CPU –∏ GPU –¥–µ–ª—è—Ç –ø–∞–º—è—Ç—å –±–µ–∑ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è - –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è LLM                |
|                                                                            |
|  Chip          Memory BW    8B Model (Q4)    –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏         |
|  ----------------------------------------------------------------         |
|  M2            100 GB/s     ~6.5 tok/s       3-7B                         |
|  M2 Pro        200 GB/s     ~13 tok/s        7-8B                         |
|  M2 Max        400 GB/s     ~25 tok/s        8-14B                        |
|  M3 Pro        150 GB/s     ~12 tok/s        7-8B                         |
|  M3 Max        400 GB/s     ~28 tok/s        8-14B                        |
|  M4            120 GB/s     ~10 tok/s        7-8B                         |
|  M4 Pro        200 GB/s     ~16 tok/s        8-14B                        |
|  M4 Max        550 GB/s     ~35 tok/s        14-32B                       |
|  M5 (2025)     153+ GB/s    19-27% faster    +4x TTFT —Å Neural Accel     |
|                                                                            |
|  MLX vs llama.cpp –Ω–∞ Apple Silicon:                                       |
|  MLX (~230 tok/s) > llama.cpp (~150 tok/s) > Ollama (20-40 tok/s)        |
|                                                                            |
|  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: LM Studio —Å MLX backend –¥–ª—è Mac                            |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ 2025

```
+----------------------------------------------------------------------------+
|                       Recommended Setups 2025                               |
+----------------------------------------------------------------------------+
|                                                                            |
|  ENTRY ($500-800):                                                         |
|  - GPU: RTX 4060 Ti 16GB –∏–ª–∏ Intel Arc B580                               |
|  - Model: Qwen3 8B, Mistral 7B, DeepSeek R1 7B                            |
|  - Tool: Ollama                                                            |
|  - Speed: 25-35 tok/s                                                      |
|  - Use: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏                            |
|                                                                            |
|  DEVELOPER ($1,000-1,500):                                                 |
|  - GPU: Used RTX 3090 (24GB) –∏–ª–∏ Mac M3/M4 Pro                            |
|  - Model: Qwen3 14B, DeepSeek R1 14B, Mistral Small 3.1                   |
|  - Tool: Ollama + Open WebUI / LM Studio (Mac)                            |
|  - Speed: 30-50 tok/s                                                      |
|  - Use: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞, RAG, coding assistant                                 |
|                                                                            |
|  POWER USER ($2,000-3,000):                                                |
|  - GPU: RTX 5090 (32GB) –∏–ª–∏ RTX 4090                                      |
|  - Model: Qwen3 32B, DeepSeek R1 32B, Llama 3.3 70B (Q3)                  |
|  - Tool: Ollama + Open WebUI                                               |
|  - Speed: 30-60 tok/s                                                      |
|  - Use: Production-ready local inference                                   |
|                                                                            |
|  ENTERPRISE:                                                               |
|  - GPU: 2x RTX 4090/5090, H100, MI300X                                    |
|  - Model: Llama 3.3 70B, DeepSeek R1 (full), Qwen3 235B                   |
|  - Tool: vLLM, TGI (production throughput)                                |
|  - Speed: 15-30 tok/s (70B), optimized for throughput                     |
|                                                                            |
+----------------------------------------------------------------------------+
```

---

## 2. –¢–æ–ø Open-Source –º–æ–¥–µ–ª–∏ (2025)

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```
+----------------------------------------------------------------------------+
|                   Open Source LLM Comparison 2025                           |
+----------------------------------------------------------------------------+
|                                                                            |
|  Model               Size      License      Best For                       |
|  -------------------------------------------------------------------------  |
|  DeepSeek R1         671B*     MIT          Reasoning, Math, Code         |
|  DeepSeek V3.2       ~200B*    MIT          Code, large projects          |
|  Llama 4             ~400B*    Llama        Multimodal, general           |
|  Llama 3.3           70B       Llama        General, —à–∏—Ä–æ–∫–∞—è ecosystem    |
|  Qwen3               235B*     Apache 2     Multilingual, 1M context      |
|  Qwen3-Coder         ~480B*    Apache 2     Code generation (SOTA)        |
|  Mistral Small 3.1   24B       Apache 2     Fast (150 tok/s), vision      |
|  Phi-4               14B       MIT          Efficient, research           |
|  Gemma 2             27B       Apache 2     Instruction following         |
|                                                                            |
|  * MoE - –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ                           |
|                                                                            |
|  Quality Ranking (–±–µ–Ω—á–º–∞—Ä–∫–∏ 2025):                                        |
|  DeepSeek R1 ~ GPT-4o > Qwen3 235B > Llama 3.3 70B > Mistral Small 3.1   |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –ü–æ –∑–∞–¥–∞—á–∞–º (2025)

| –ó–∞–¥–∞—á–∞ | –õ—É—á—à–∏–π –≤—ã–±–æ—Ä | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ | VRAM (Q4) |
|--------|--------------|--------------|-----------|
| General Chat | Llama 3.3 70B | Qwen3 32B | 40GB / 18GB |
| Coding | DeepSeek V3.2, Qwen3-Coder | NVIDIA Nemotron 9B | varies / 6GB |
| Reasoning/Math | DeepSeek R1 | Qwen3 (thinking mode) | 40GB+ |
| Multilingual | Qwen3 (29+ —è–∑—ã–∫–æ–≤) | Llama 3.3 | varies |
| Long Context | Qwen3 (1M tokens) | Llama 3.1 (128K) | varies |
| Vision | Mistral Small 3.1, Qwen3-VL | Llava | 16GB |
| Edge/Mobile | Phi-4, Gemma 2B | Qwen3 0.5B | 2-4GB |
| Fast Response | Mistral Small 3.1 | Qwen3 8B | 16GB |
| RAG | Qwen3, Mistral | Llama 3.3 | varies |

### DeepSeek R1 - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä

```bash
# DeepSeek R1 - –ª—É—á—à–∏–π open-source reasoning model (MIT license)
# –°—Ä–∞–≤–Ω–∏–º —Å OpenAI o1, –Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π

# Distilled –≤–µ—Ä—Å–∏–∏ (–Ω–∞ –±–∞–∑–µ Qwen/Llama):
ollama run deepseek-r1:1.5b    # ~1GB  VRAM, –±–∞–∑–æ–≤—ã–π reasoning
ollama run deepseek-r1:7b      # ~4GB  VRAM, —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å
ollama run deepseek-r1:8b      # ~5GB  VRAM, Llama-based
ollama run deepseek-r1:14b     # ~8GB  VRAM, —Å–∏–ª—å–Ω—ã–π reasoning
ollama run deepseek-r1:32b     # ~18GB VRAM, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π
ollama run deepseek-r1:70b     # ~40GB VRAM, —Ç–æ–ø–æ–≤—ã–π distilled

# –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è (671B MoE, 37B –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤):
# –¢—Ä–µ–±—É–µ—Ç 48GB+ VRAM –¥–ª—è Q4 (–∏–ª–∏ datacenter: 8x H100)
# –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ Mac Studio M2 Ultra 192GB (~$5.6k)
# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 1.58-bit quantization (131GB)
```

**Hardware requirements –¥–ª—è DeepSeek R1:**
- 1.5B: 6GB VRAM (RTX 3060)
- 7B/8B: 8GB VRAM (RTX 4060)
- 14B: 12-16GB VRAM (RTX 4070 Ti)
- 32B: 24GB VRAM (RTX 4090)
- 70B distilled: 48GB+ (2x RTX 4090)
- 671B full: datacenter –∏–ª–∏ Mac Studio 192GB

### Qwen3 - –∫–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

```bash
# Qwen3 - Apache 2.0, thinking mode, –¥–æ 1M context

# Dense –º–æ–¥–µ–ª–∏:
ollama run qwen3:0.6b     # Edge/mobile
ollama run qwen3:4b       # –ë—ã—Å—Ç—Ä—ã–π, 8GB RAM –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
ollama run qwen3:8b       # 25+ tok/s –Ω–∞ –Ω–æ—É—Ç–±—É–∫–µ
ollama run qwen3:14b      # –ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞/—Å–∫–æ—Ä–æ—Å—Ç–∏
ollama run qwen3:32b      # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ

# MoE –º–æ–¥–µ–ª–∏ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ):
ollama run qwen3:30b-a3b  # 30B total, 3B active

# –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
# Thinking Mode - –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á (math, reasoning)
# Non-thinking Mode - –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã (chat)
# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏!

# Qwen3-2507 (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≤–≥—É—Å—Ç–∞ 2025):
# - 1M —Ç–æ–∫–µ–Ω–æ–≤ context
# - –£–ª—É—á—à–µ–Ω–Ω—ã–π thinking
# - –í–∞—Ä–∏–∞–Ω—Ç—ã: 235B-A22B, 30B-A3B, 4B
```

### Mistral Small 3.1 - sweet spot

```bash
# 24B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, 150 tok/s, vision, 128K context
# –ü–æ–º–µ—â–∞–µ—Ç—Å—è –Ω–∞ RTX 4090 –∏–ª–∏ 32GB MacBook (quantized)

ollama run mistral-small:24b

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - 81% MMLU (—Å—Ä–∞–≤–Ω–∏–º–æ —Å GPT-4o Mini)
# - 3x –±—ã—Å—Ç—Ä–µ–µ —á–µ–º Llama 3.3 70B –Ω–∞ —Ç–æ–º –∂–µ –∂–µ–ª–µ–∑–µ
# - Vision –ø–æ–Ω–∏–º–∞–Ω–∏–µ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
# - Apache 2.0 license
# - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è function calling

# –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è:
# - Fast-response chatbots
# - Low-latency tool calling
# - Fine-tuning –ø–æ–¥ –¥–æ–º–µ–Ω
# - –õ–æ–∫–∞–ª—å–Ω—ã–π inference —Å –ø—Ä–∏–≤–∞—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –¥–ª—è 8GB VRAM

```
+----------------------------------------------------------------------------+
|                    Best Models for 8GB VRAM (2025)                          |
+----------------------------------------------------------------------------+
|                                                                            |
|  TOP PICKS (–ø–æ –¥–∞–Ω–Ω—ã–º r/LocalLLaMA –∏ –±–µ–Ω—á–º–∞—Ä–∫–∞–º):                         |
|                                                                            |
|  1. Qwen3 8B (Standard)    - –ª—É—á—à–∏–π all-around                            |
|  2. Qwen3 8B (Reasoning)   - math –∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏                        |
|  3. NVIDIA Nemotron Nano 9B - –ª—É—á—à–∏–π –¥–ª—è –∫–æ–¥–∞                             |
|  4. DeepSeek R1 0528 Qwen3 8B - —Å–∏–ª—å–Ω—ã–π reasoning                         |
|  5. Granit 3.3 Instruct 8B - —Ö–æ—Ä–æ—à–∏–π general purpose                      |
|  6. Ministral 8B           - –±—ã—Å—Ç—Ä—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π                       |
|                                                                            |
|  –ü–û –ó–ê–î–ê–ß–ê–ú:                                                               |
|  - Math/Reasoning: Qwen3 8B (Reasoning) –∏–ª–∏ DeepSeek R1 8B                |
|  - Coding: NVIDIA Nemotron Nano 9B (Reasoning)                            |
|  - General: Qwen3 8B (Standard) –∏–ª–∏ DeepSeek R1 Distill Llama 8B          |
|                                                                            |
+----------------------------------------------------------------------------+
```

---

## 3. Ollama

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# macOS / Linux (–æ–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞)
curl -fsSL https://ollama.com/install.sh | sh

# Windows - —Å–∫–∞—á–∞—Ç—å installer —Å ollama.com

# Docker (–±–∞–∑–æ–≤—ã–π)
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Docker —Å NVIDIA GPU
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
ollama --version
```

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –°–∫–∞—á–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å (–æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π)
ollama run llama3.3:70b
ollama run qwen3:32b
ollama run deepseek-r1:14b
ollama run mistral-small:24b

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
ollama list                    # –°–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö
ollama pull qwen3:8b           # –°–∫–∞—á–∞—Ç—å –±–µ–∑ –∑–∞–ø—É—Å–∫–∞
ollama rm llama3.3:70b         # –£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å
ollama show qwen3:8b           # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
ollama cp qwen3:8b my-qwen     # –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å/–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å

# –°–µ—Ä–≤–µ—Ä
ollama serve                   # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

# API –∑–∞–ø—Ä–æ—Å
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:8b",
  "prompt": "Explain quantum computing",
  "stream": false
}'
```

### –ù–æ–≤—ã–µ —Ñ–∏—á–∏ Ollama 2025

```
+----------------------------------------------------------------------------+
|                      Ollama Updates 2025                                    |
+----------------------------------------------------------------------------+
|                                                                            |
|  –°–ï–ù–¢–Ø–ë–†–¨ 2025:                                                            |
|  - New Model Scheduling - –º–µ–Ω—å—à–µ OOM crashes, –ª—É—á—à–µ multi-GPU             |
|  - Cloud Models Preview - –∑–∞–ø—É—Å–∫ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ datacenter            |
|  - Web Search API - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (free tier)                          |
|                                                                            |
|  –ú–ê–ô 2025:                                                                 |
|  - Streaming with Tool Calling - real-time tools                          |
|  - Thinking Toggle - –∫–æ–Ω—Ç—Ä–æ–ª—å reasoning mode                              |
|  - Structured Outputs - JSON schema constraints                           |
|                                                                            |
|  –û–ö–¢–Ø–ë–†–¨ 2025:                                                             |
|  - OpenAI gpt-oss-safeguard (20B/120B safety models)                      |
|  - MiniMax M2 - coding –∏ agentic workflows                                |
|  - Qwen3-Coder-480B, GLM-4.6, Qwen3-VL multimodal                        |
|  - NVIDIA DGX Spark optimization                                          |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏

```dockerfile
# Modelfile
FROM qwen3:14b

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM """
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.
–í—Å–µ–≥–¥–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–π —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏.
–û–±—ä—è—Å–Ω—è–π –∫–æ–¥ –ø–æ–Ω—è—Ç–Ω—ã–º —è–∑—ã–∫–æ–º. –ò—Å–ø–æ–ª—å–∑—É–π type hints.
"""

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 8192
PARAMETER stop "<|im_end|>"

# Template (–¥–ª—è Qwen3)
TEMPLATE """
{{- if .System }}
<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""
```

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
ollama create python-expert -f Modelfile
ollama run python-expert "Write async web scraper with aiohttp"
```

### Python SDK

```python
import ollama

# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
response = ollama.chat(
    model='qwen3:14b',
    messages=[
        {'role': 'user', 'content': 'Explain transformer architecture'}
    ]
)
print(response['message']['content'])

# Streaming
for chunk in ollama.chat(
    model='qwen3:14b',
    messages=[{'role': 'user', 'content': 'Write a haiku about AI'}],
    stream=True
):
    print(chunk['message']['content'], end='', flush=True)

# Thinking mode (Qwen3)
response = ollama.chat(
    model='qwen3:14b',
    messages=[
        {'role': 'user', 'content': 'Solve: what is 23 * 47 + 156 / 12?'}
    ],
    options={'think': True}  # –í–∫–ª—é—á–∏—Ç—å reasoning
)

# Structured Output (JSON)
response = ollama.chat(
    model='qwen3:14b',
    messages=[{'role': 'user', 'content': 'List 3 programming languages'}],
    format={
        'type': 'object',
        'properties': {
            'languages': {
                'type': 'array',
                'items': {'type': 'string'}
            }
        }
    }
)

# Embeddings
embedding = ollama.embeddings(
    model='nomic-embed-text',
    prompt='The quick brown fox'
)
print(f"Embedding dimension: {len(embedding['embedding'])}")

# List models
models = ollama.list()
for model in models['models']:
    size_gb = model['size'] / (1024**3)
    print(f"{model['name']}: {size_gb:.1f} GB")
```

### OpenAI-compatible API

```python
from openai import OpenAI

# Ollama –∫–∞–∫ drop-in –∑–∞–º–µ–Ω–∞ OpenAI
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="not-needed"  # –õ—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
)

# Chat completions
response = client.chat.completions.create(
    model="qwen3:14b",
    messages=[
        {"role": "system", "content": "You are a helpful coding assistant."},
        {"role": "user", "content": "Write a Python function for binary search"}
    ],
    temperature=0.7,
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")

# Function calling (tool use)
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City name"}
            },
            "required": ["location"]
        }
    }
}]

response = client.chat.completions.create(
    model="mistral-small:24b",
    messages=[{"role": "user", "content": "What's the weather in Tokyo?"}],
    tools=tools,
    tool_choice="auto"
)
```

---

## 4. LM Studio

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

```
+----------------------------------------------------------------------------+
|                       LM Studio Features 2025                               |
+----------------------------------------------------------------------------+
|                                                                            |
|  CORE FEATURES:                                                            |
|  + –ö—Ä–∞—Å–∏–≤—ã–π GUI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏                                   |
|  + –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Hugging Face (–ø–æ–∏—Å–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ)                         |
|  + –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π chat interface                                               |
|  + Local server —Å OpenAI-compatible API                                   |
|  + –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–¥ –∂–µ–ª–µ–∑–æ                            |
|  + –ú—É–ª—å—Ç–∏–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π (Mac, Windows, Linux)                              |
|                                                                            |
|  2025 UPDATES:                                                             |
|  + 1000+ pre-configured –º–æ–¥–µ–ª–µ–π –≤ Enhanced Model Library                  |
|  + Team collaboration - multi-user management                             |
|  + Advanced monitoring - performance metrics                              |
|  + Plugin Ecosystem - third-party –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏                              |
|  + Mobile Companion - iOS/Android –¥–ª—è remote management                   |
|  + RTX 50-series support (CUDA 12.8)                                      |
|                                                                            |
|  APPLE SILICON (MLX):                                                      |
|  + –ù–∞—Ç–∏–≤–Ω–∞—è MLX –ø–æ–¥–¥–µ—Ä–∂–∫–∞ - –±—ã—Å—Ç—Ä–µ–µ —á–µ–º llama.cpp –Ω–∞ Mac                 |
|  + MLX –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏                                    |
|  + –î–æ 230 tok/s (vs 150 tok/s llama.cpp)                                 |
|                                                                            |
|  –õ–£–ß–®–ï –í–°–ï–ì–û –î–õ–Ø:                                                          |
|  - –ù–∞—á–∏–Ω–∞—é—â–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π                                               |
|  - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π                                            |
|  - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π Mac (MLX)                                                |
|  - –í–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–π                                      |
|  - –ë—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è                                              |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

1. –°–∫–∞—á–∞—Ç—å —Å [lmstudio.ai](https://lmstudio.ai)
2. Search -> –ù–∞–π—Ç–∏ –º–æ–¥–µ–ª—å (Hugging Face –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
3. –í—ã–±—Ä–∞—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é:
   - Q4_K_M - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞
   - Q5_K_M - –µ—Å–ª–∏ VRAM –ø–æ–∑–≤–æ–ª—è–µ—Ç
   - Q8_0 - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
4. Download -> Load –≤ Chat –∏–ª–∏ Server mode

### API Server

```python
# LM Studio server mode
# 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
# 2. Local Server tab -> Start Server
# 3. Default port: 1234

from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio"  # –õ—é–±–æ–π –∫–ª—é—á
)

# –†–∞–±–æ—Ç–∞–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω–æ OpenAI API
response = client.chat.completions.create(
    model="local-model",  # –ò–º—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ],
    temperature=0.7,
    max_tokens=1000
)

print(response.choices[0].message.content)
```

### Ollama vs LM Studio - –∫–æ–≥–¥–∞ —á—Ç–æ –≤—ã–±–∏—Ä–∞—Ç—å

| –ö—Ä–∏—Ç–µ—Ä–∏–π | Ollama | LM Studio |
|----------|--------|-----------|
| Interface | CLI | GUI |
| License | MIT (open-source) | Proprietary freeware |
| Apple Silicon | –•–æ—Ä–æ—à–æ | –õ—É—á—à–µ (MLX) |
| NVIDIA GPU | –•–æ—Ä–æ—à–æ | –•–æ—Ä–æ—à–æ |
| Server mode | –í—Å—Ç—Ä–æ–µ–Ω | –í—Å—Ç—Ä–æ–µ–Ω |
| Model management | CLI –∫–æ–º–∞–Ω–¥—ã | Visual browser |
| Scripting/Automation | –ò–¥–µ–∞–ª—å–Ω–æ | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ |
| Privacy concerns | Open-source | Closed-source |
| Learning curve | –°—Ä–µ–¥–Ω—è—è | –ù–∏–∑–∫–∞—è |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
- **Developer/Automation**: Ollama
- **Visual tinkerer/Mac user**: LM Studio
- **Privacy-conscious**: Ollama (open-source)

---

## 5. llama.cpp

### –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp

# –°–±–æ—Ä–∫–∞ —Å CUDA (NVIDIA)
make GGML_CUDA=1

# –°–±–æ—Ä–∫–∞ —Å Metal (Apple Silicon)
make GGML_METAL=1

# –°–±–æ—Ä–∫–∞ —Å ROCm (AMD)
make GGML_HIP=1

# –°–±–æ—Ä–∫–∞ —Å Vulkan (–∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π)
make GGML_VULKAN=1

# CPU only (AVX2 recommended)
make
```

### –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```bash
# –°–∫–∞—á–∞—Ç—å GGUF –º–æ–¥–µ–ª—å —Å Hugging Face
# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: TheBloke, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–µ–ø–æ

# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
./llama-cli \
    -m models/qwen3-14b-q4_k_m.gguf \
    -p "Write a Python function for" \
    -n 512 \
    --n-gpu-layers 40 \
    --ctx-size 8192 \
    --temp 0.7

# Server mode (OpenAI-compatible)
./llama-server \
    -m models/qwen3-14b-q4_k_m.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    -ngl 40 \
    -c 8192 \
    --flash-attn

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
# -ngl: layers –Ω–∞ GPU (–±–æ–ª—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ, –±–æ–ª—å—à–µ VRAM)
# -c: context size
# -t: threads –¥–ª—è CPU
# --mlock: –¥–µ—Ä–∂–∞—Ç—å –≤ RAM
# --flash-attn: Flash Attention (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
```

### Performance Tips 2025

```bash
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è multi-GPU (asymmetric)
# –ü—Ä–∏–º–µ—Ä: RTX 4090 + RTX 3080
./llama-cli \
    -m model.gguf \
    --tensor-split 97,3 \  # 97% –Ω–∞ 4090, 3% –Ω–∞ 3080
    -ngl 99 \
    -t 2  # –ú–µ–Ω—å—à–µ threads –¥–ª—è GPU inference

# AMD GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (wavefront 64)
# llama.cpp 2025 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç

# Flash Attention –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
./llama-server \
    -m model.gguf \
    --flash-attn \
    -c 32768  # 32K context
```

### –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

```bash
# HuggingFace -> GGUF
python convert_hf_to_gguf.py \
    /path/to/hf-model \
    --outfile model.gguf \
    --outtype q4_k_m

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ (2025):
# q2_k    - 2-bit, –º–∏–Ω–∏–º—É–º (70-80% quality)
# q3_k_m  - 3-bit, –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è (85-90%)
# q4_k_m  - 4-bit, –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø (92-95%)
# q5_k_m  - 5-bit, –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (95-97%)
# q6_k   - 6-bit, –ø–æ—á—Ç–∏ FP16 (98%)
# q8_0   - 8-bit, –º–∞–∫—Å–∏–º—É–º GGUF (99%)

# I-Quants (2025) - –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ —Ç–æ–º –∂–µ —Ä–∞–∑–º–µ—Ä–µ
# iq2_xxs, iq3_xxs, iq4_xs - improved quantization
```

---

## 6. GGUF Quantization - –≥–ª—É–±–æ–∫–∏–π —Ä–∞–∑–±–æ—Ä

```
+----------------------------------------------------------------------------+
|                    GGUF Quantization Quality vs Size                        |
+----------------------------------------------------------------------------+
|                                                                            |
|  Quantization   Quality    Size (70B)   Speed    Recommended For          |
|  -------------------------------------------------------------------------  |
|  FP16           100%       140 GB       1x       Production (datacenter)  |
|  Q8_0           99%        74 GB        1.2x     Max quality GGUF         |
|  Q6_K           98%        57 GB        1.4x     High quality             |
|  Q5_K_M         95-97%     48 GB        1.6x     Best balance             |
|  Q4_K_M         92-95%     40 GB        1.8x     * DEFAULT CHOICE         |
|  Q4_K_S         90-93%     38 GB        1.9x     Smaller Q4               |
|  Q3_K_M         85-90%     32 GB        2.0x     VRAM limited             |
|  Q2_K           70-80%     26 GB        2.2x     Last resort              |
|                                                                            |
|  K-QUANTS vs LEGACY:                                                       |
|  Q4_K_M > Q4_0 (K-quants –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤–∞–∂–Ω—ã–µ –≤–µ—Å–∞)                     |
|  Q5_K_M > Q5_0 (–º–µ–Ω—å—à–µ "derails" –≤ reasoning)                             |
|                                                                            |
|  I-QUANTS (2025):                                                          |
|  iq4_xs, iq3_xxs - –µ—â–µ –ª—É—á—à–µ quality/size ratio                          |
|                                                                            |
|  –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:                                                |
|  - Coding tasks: –∏—Å–ø–æ–ª—å–∑—É–π Q5_K_M+ (quantization –±–æ–ª—å—à–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–æ–¥)   |
|  - Creative writing: Q4_K_M –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ                                    |
|  - Math/Reasoning: Q5_K_M+ –∏–ª–∏ larger model —Å Q4                         |
|                                                                            |
|  LADDER: Q4_K_M -> Q5_K_M -> Q8_0 (–ø–æ –º–µ—Ä–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ VRAM)             |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –í—ã–±–æ—Ä quantization –º–µ—Ç–æ–¥–∞

| –ú–µ—Ç–æ–¥ | –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ | –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|-------|-----------|--------------|-------------------|
| **GGUF** | CPU, Apple Silicon | –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, K-quants | Default –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ |
| **GPTQ** | NVIDIA GPU | –ë—ã—Å—Ç—Ä—ã–π –Ω–∞ GPU | High-throughput serving |
| **AWQ** | NVIDIA GPU | –õ—É—á—à–µ GPTQ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É | Production inference |
| **MLX** | Apple Silicon | –ù–∞—Ç–∏–≤–Ω—ã–π –¥–ª—è Mac | LM Studio –Ω–∞ Mac |

---

## 7. Open WebUI

–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM:

```bash
# Docker (—Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º Ollama)
docker run -d -p 3000:8080 \
    --add-host=host.docker.internal:host-gateway \
    -v open-webui:/app/backend/data \
    --name open-webui \
    --restart always \
    ghcr.io/open-webui/open-webui:main

# –î–æ—Å—Ç—É–ø: http://localhost:3000
```

```yaml
# docker-compose.yml - –ø–æ–ª–Ω—ã–π —Å—Ç–µ–∫
version: '3.8'
services:
  ollama:
    image: ollama/ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama

volumes:
  ollama:
  open-webui:
```

### –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Open WebUI

- ChatGPT-like –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –ù–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- RAG —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (upload PDF, txt)
- –ü—Ä–æ–º–ø—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
- –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–æ–≤
- –ú—É–ª—å—Ç–∏–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ä–µ–∂–∏–º
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Ollama, OpenAI, Azure
- Web search integration
- Code execution sandbox

---

## 8. Production Self-Hosting

### vLLM –¥–ª—è –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏

```python
# vLLM - –¥–ª—è production —Å –≤—ã—Å–æ–∫–∏–º throughput
# Continuous batching, PagedAttention, tensor parallelism

from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen3-32B-Instruct",
    tensor_parallel_size=2,  # 2 GPU
    quantization="awq",
    max_model_len=8192,
    gpu_memory_utilization=0.9
)

sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=1024
)

outputs = llm.generate(["Explain quantum entanglement"], sampling_params)
```

```bash
# vLLM OpenAI-compatible server
vllm serve Qwen/Qwen3-32B-Instruct \
    --tensor-parallel-size 2 \
    --quantization awq \
    --port 8000 \
    --max-model-len 8192
```

### vLLM vs llama.cpp - –∫–æ–≥–¥–∞ —á—Ç–æ

| –°—Ü–µ–Ω–∞—Ä–∏–π | vLLM | llama.cpp |
|----------|------|-----------|
| High-throughput serving | +++ | + |
| Multi-user concurrent | +++ | ++ |
| Single-user latency | ++ | +++ |
| Portability | + | +++ |
| Memory efficiency | ++ | +++ |
| Startup time | -- | +++ |
| Consumer hardware | + | +++ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
- **vLLM**: Production API, –º–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, datacenter
- **llama.cpp**: –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞, embedded, consumer GPU

### Text Generation Inference (TGI)

```bash
# Hugging Face TGI
docker run --gpus all -p 8080:80 \
    -v /data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id Qwen/Qwen3-32B-Instruct \
    --quantize awq \
    --max-input-length 4096 \
    --max-total-tokens 8192 \
    --max-batch-prefill-tokens 4096
```

### Kubernetes deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
          requests:
            memory: "16Gi"
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama
        env:
        - name: OLLAMA_KEEP_ALIVE
          value: "24h"
        - name: OLLAMA_NUM_PARALLEL
          value: "4"
      volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: ollama-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
spec:
  selector:
    app: ollama
  ports:
  - port: 11434
    targetPort: 11434
  type: ClusterIP
```

### Security Best Practices

```
+----------------------------------------------------------------------------+
|                    Security for Self-Hosted LLM                             |
+----------------------------------------------------------------------------+
|                                                                            |
|  NETWORK:                                                                  |
|  - VPN (Tailscale, headscale, netbird) –¥–ª—è remote access                  |
|  - Firewall - –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å exposed services                                 |
|  - Reverse proxy —Å authentication (2FA)                                   |
|                                                                            |
|  ACCESS CONTROL:                                                           |
|  - Open WebUI: enable authentication, user management                     |
|  - API keys –¥–ª—è production endpoints                                      |
|  - Rate limiting per user/IP                                              |
|                                                                            |
|  DATA:                                                                     |
|  - –ù–µ —Ö—Ä–∞–Ω–∏—Ç—å sensitive data –≤ prompts/logs                               |
|  - Encryption at rest –¥–ª—è model storage                                   |
|  - Audit logging –¥–ª—è compliance                                           |
|                                                                            |
+----------------------------------------------------------------------------+
```

---

## 9. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (2025)

| –ö—Ä–∏—Ç–µ—Ä–∏–π | Ollama | LM Studio | llama.cpp | vLLM | TGI |
|----------|--------|-----------|-----------|------|-----|
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | +++++ | +++++ | ++ | +++ | +++ |
| **Performance** | ++++ | +++ | +++++ | +++++ | ++++ |
| **GUI** | - | +++++ | Web | - | - |
| **API** | +++++ | ++++ | ++++ | +++++ | ++++ |
| **Production** | +++ | ++ | +++ | +++++ | +++++ |
| **Model support** | ++++ | +++++ | +++++ | ++++ | ++++ |
| **Apple Silicon** | ++++ | +++++ | ++++ | + | + |
| **Multi-GPU** | +++ | ++ | ++++ | +++++ | +++++ |

### –ö–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

```
+----------------------------------------------------------------------------+
|                       Tool Selection Guide 2025                             |
+----------------------------------------------------------------------------+
|                                                                            |
|  "–•–æ—á—É –ø—Ä–æ—Å—Ç–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å LLM –ª–æ–∫–∞–ª—å–Ω–æ"                                   |
|  -> Ollama (curl install, ollama run qwen3:8b)                            |
|                                                                            |
|  "–•–æ—á—É GUI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"                               |
|  -> LM Studio                                                              |
|                                                                            |
|  "–£ –º–µ–Ω—è Mac, —Ö–æ—á—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å"                                 |
|  -> LM Studio —Å MLX backend                                               |
|                                                                            |
|  "–ù—É–∂–µ–Ω –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∏ performance"                              |
|  -> llama.cpp                                                              |
|                                                                            |
|  "Production API —Å –≤—ã—Å–æ–∫–∏–º —Ç—Ä–∞—Ñ–∏–∫–æ–º"                                      |
|  -> vLLM –∏–ª–∏ TGI                                                          |
|                                                                            |
|  "–ö—Ä–∞—Å–∏–≤—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∫–æ–º–∞–Ω–¥—ã"                                     |
|  -> Ollama + Open WebUI                                                   |
|                                                                            |
|  "–•–æ—á—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ (OpenAI SDK)"                     |
|  -> Ollama –∏–ª–∏ LM Studio (–æ–±–∞ OpenAI-compatible)                          |
|                                                                            |
|  "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å, open-source only"                             |
|  -> Ollama + llama.cpp                                                    |
|                                                                            |
+----------------------------------------------------------------------------+
```

---

## 10. On-Device LLM (Mobile, Edge) ‚Äî 2025 State

### –ó–∞—á–µ–º LLM –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ?

```
+----------------------------------------------------------------------------+
|                    On-Device vs Cloud LLM (2025)                            |
+----------------------------------------------------------------------------+
|                                                                            |
|  –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê ON-DEVICE:                                                   |
|  + 100% –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å ‚Äî –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ                       |
|  + Latency sub-50ms (vs 200-800ms cloud)                                  |
|  + –†–∞–±–æ—Ç–∞–µ—Ç –æ—Ñ–ª–∞–π–Ω                                                         |
|  + –ù–µ—Ç costs per token                                                     |
|  + Compliance (HIPAA, GDPR) ‚Äî –ø—Ä–æ—â–µ, –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã                      |
|                                                                            |
|  –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:                                                              |
|  - –ú–æ–¥–µ–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã 1-4B –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (—Ä–µ–∞–ª—å–Ω–æ usable)                    |
|  - –ö–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∂–µ GPT-4o / Claude 3.5                                      |
|  - –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –±–∞—Ç–∞—Ä–µ–∏: 50% –∑–∞ 90 –º–∏–Ω—É—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è           |
|  - –ù–∞–≥—Ä–µ–≤ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ                                |
|  - –ó–∞–Ω–∏–º–∞–µ—Ç 2-6GB storage                                                  |
|                                                                            |
|  –ö–û–ì–î–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨:                                                       |
|  ‚úì –ú–µ–¥–∏—Ü–∏–Ω–∞/–∑–¥–æ—Ä–æ–≤—å–µ ‚Äî –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞                               |
|  ‚úì –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏: summarization, transcription, Q&A                      |
|  ‚úì Offline —Ä–µ–∂–∏–º –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω                                                |
|  ‚úì –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –æ—Ç–∫–ª–∏–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω                                             |
|  ‚úó –°–ª–æ–∂–Ω—ã–π reasoning, math ‚Äî –ª—É—á—à–µ cloud                                  |
|  ‚úó Long-form generation ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ —Å–∫–æ—Ä–æ—Å—Ç–∏                        |
|                                                                            |
+----------------------------------------------------------------------------+
```

### –§—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è Mobile (2025)

| Framework | –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è | –õ—É—á—à–µ –¥–ª—è | GitHub Stars |
|-----------|-----------|-------------|-----------|--------------|
| **MediaPipe LLM** | Android, iOS, Web | GPU, NPU | Google models (Gemma) | 27k+ |
| **ExecuTorch** | iOS, Android | CPU, GPU, NPU | Llama, Qwen | 2.5k+ |
| **MLX Swift** | iOS, macOS | Apple Silicon | Apple devices | 18k+ |
| **llama.cpp** | Everywhere | CPU, GPU | GGUF models | 72k+ |
| **MLC-LLM** | All | GPU optimized | High performance | 19k+ |
| **CoreML** | iOS, macOS | Neural Engine | Apple native | Apple SDK |

### –ú–æ–¥–µ–ª–∏ –¥–ª—è Mobile (–¥–µ–∫–∞–±—Ä—å 2025)

```
+----------------------------------------------------------------------------+
|                    Best Models for Mobile 2025                              |
+----------------------------------------------------------------------------+
|                                                                            |
|  GEMMA 3n (Google) ‚Äî –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –¥–ª—è Android                            |
|  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                              |
|  - E2B: 5B params, ~2GB RAM (effective 2B)                                |
|  - E4B: 8B params, ~3GB RAM (effective 4B)                                |
|  - Multimodal: text + image + audio + video                               |
|  - 60-70 tok/s –Ω–∞ Pixel, 0.3s time-to-first-token                        |
|  - LMArena 1338 ‚Äî –ø–µ—Ä–≤–∞—è <10B –º–æ–¥–µ–ª—å —Å 1300+ score                       |
|  - 140 —è–∑—ã–∫–æ–≤ —Ç–µ–∫—Å—Ç, 35 —è–∑—ã–∫–æ–≤ multimodal                                 |
|                                                                            |
|  LLAMA 3.2 1B/3B (Meta) ‚Äî –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –í–´–ë–û–†                             |
|  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                              |
|  - 1B: –ª—é–±–æ–π iOS (6GB+ RAM), mid-range Android                           |
|  - 3B: iPhone 12 Pro+, Samsung S21 Ultra+                                 |
|  - 128K context, instruction-tuned                                        |
|  - Quantized: 2-4x –±—ã—Å—Ç—Ä–µ–µ, -56% —Ä–∞–∑–º–µ—Ä, -41% –ø–∞–º—è—Ç—å                     |
|  - 8-10 tok/s –Ω–∞ Snapdragon 8 Gen 2                                      |
|                                                                            |
|  PHI-4 MINI (Microsoft) ‚Äî REASONING                                       |
|  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                              |
|  - 3.8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤                                                        |
|  - –û—Ç–ª–∏—á–Ω—ã–π –¥–ª—è coding –∏ math                                             |
|  - MIT license                                                             |
|                                                                            |
|  QWEN3 0.6B (Alibaba) ‚Äî EDGE/IoT                                          |
|  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                              |
|  - –£–ª—å—Ç—Ä–∞-–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π                                                      |
|  - ~40 tok/s –Ω–∞ Pixel 8, iPhone 15 Pro                                   |
|  - –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è wearables                                                 |
|                                                                            |
+----------------------------------------------------------------------------+
```

### Performance Benchmarks (–¥–µ–∫–∞–±—Ä—å 2025)

| –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ | –ú–æ–¥–µ–ª—å | Tokens/sec | RAM | Framework |
|------------|--------|------------|-----|-----------|
| **iPhone 17 Pro** | LFM2-VL-450m | 136 | 2GB | Cactus |
| **iPhone 15 Pro** | Llama 3.2 1B | 17 (CPU) / 12.8 (GPU) | 4GB | llama.cpp |
| **iPhone 15 Pro** | Qwen3 0.6B | ~40 | 2GB | ExecuTorch |
| **Pixel 8 Pro** | Gemma 3n E2B | 60-70 | 2GB | MediaPipe |
| **Samsung S25 Ultra** | LFM2-VL-450m | 91 | 2GB | Cactus |
| **Snapdragon 8 Gen 2** | Llama 3.2 3B | 8-10 | 6GB | llama.cpp |
| **Galaxy A54 (6GB)** | 2B model | ~5 | 6GB | MediaPipe |
| **Mac M4 Pro** | ‚Äî | 173 | ‚Äî | Cactus |

**–í–∞–∂–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:**
- CPU –º–æ–∂–µ—Ç –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ GPU –Ω–∞ iPhone (17 vs 12.8 tok/s) –∏–∑-–∑–∞ overhead –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
- Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –±–µ–∑ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- NPU (Neural Engine) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–æ –±–∞—Ç–∞—Ä–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤

### –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è Mobile

```
+----------------------------------------------------------------------------+
|                    Mobile Quantization Guide                                |
+----------------------------------------------------------------------------+
|                                                                            |
|  –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –§–û–†–ú–ê–¢–´:                                                    |
|                                                                            |
|  Q4_K_M ‚Äî –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è mobile                                    |
|  - 3.4x –±—ã—Å—Ç—Ä–µ–µ, 3.2x –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ vs FP16                              |
|  - 68% —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏                                          |
|  - 84-87% —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ GSM8K (math)                            |
|                                                                            |
|  –†–ê–ó–ú–ï–†–´ –ú–û–î–ï–õ–ï–ô –ü–û –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–ò (3B –º–æ–¥–µ–ª—å):                              |
|  FP16: ~6GB -> Q4_K_M: ~1.9GB -> Q2_K: ~1.2GB                            |
|                                                                            |
|  TRADE-OFFS:                                                               |
|  - Q4: —Ö–æ—Ä–æ—à–æ –¥–ª—è chat, summarization                                     |
|  - Q5+: –Ω—É–∂–µ–Ω –¥–ª—è math, coding (–±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã)                     |
|  - Q2-Q3: —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–µ–Ω—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ RAM                             |
|                                                                            |
|  –§–ê–ö–¢–´ (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 2025):                                                |
|  - INT4 —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç perplexity –Ω–∞ 5-15% vs FP32                          |
|  - MMLU —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ ‚Äî –∏–∑–±–µ–≥–∞—Ç—å Q3 –∏ –Ω–∏–∂–µ                  |
|  - GSM8K (math) —É—Å—Ç–æ–π—á–∏–≤ ‚Äî Q4_K_M —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 84-87%                       |
|                                                                            |
+----------------------------------------------------------------------------+
```

### iOS: MLX Swift + CoreML

```swift
// MLX Swift ‚Äî –Ω–∞—Ç–∏–≤–Ω—ã–π –¥–ª—è Apple Silicon
// –î–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ mlx-swift-lm package

import MLXLM
import MLXLMCommon

// 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
let modelContainer = try await LLMModelFactory.shared.loadContainer(
    configuration: .init(id: "mlx-community/Llama-3.2-1B-Instruct-4bit")
)

// 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
let prompt = "Explain machine learning in simple terms"
let result = try await modelContainer.perform { context in
    let output = try await context.generate(
        prompt: prompt,
        maxTokens: 256
    )
    return output.text
}

// 3. Streaming
for try await token in modelContainer.generate(prompt: prompt) {
    print(token, terminator: "")
}
```

```swift
// CoreML + Transformers ‚Äî –¥–ª—è Neural Engine
import CoreML
import NaturalLanguage

// 1. –ó–∞–≥—Ä—É–∑–∫–∞ CoreML –º–æ–¥–µ–ª–∏ (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π)
let config = MLModelConfiguration()
config.computeUnits = .all  // CPU + GPU + Neural Engine

let model = try MLModel(
    contentsOf: modelURL,
    configuration: config
)

// 2. State management –¥–ª—è KV-cache (WWDC'24 feature)
// CoreML –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç stateful models –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ inference
```

**–†–µ—Å—É—Ä—Å—ã –¥–ª—è iOS:**
- [ml-explore/mlx-swift](https://github.com/ml-explore/mlx-swift) ‚Äî MLX Swift API
- [swift-transformers](https://github.com/huggingface/swift-transformers) ‚Äî HuggingFace Swift
- [Private LLM](https://privatellm.app/) ‚Äî –≥–æ—Ç–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

### Android: MediaPipe + llama.cpp

```kotlin
// MediaPipe LLM Inference ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Gemma
// build.gradle: implementation("com.google.mediapipe:tasks-genai:0.10.27")

import com.google.mediapipe.tasks.genai.llminference.LlmInference

class OnDeviceLLM(private val context: Context) {
    private lateinit var llmInference: LlmInference

    suspend fun initialize() {
        val options = LlmInference.LlmInferenceOptions.builder()
            .setModelPath("/path/to/gemma-3n-e2b.task")
            .setMaxTokens(1024)
            .setResultListener { partialResult, done ->
                // Streaming callback
                println(partialResult)
            }
            .build()

        llmInference = LlmInference.createFromOptions(context, options)
    }

    suspend fun generate(prompt: String): String {
        return llmInference.generateResponse(prompt)
    }

    // Multimodal (Gemma 3n)
    suspend fun generateWithImage(prompt: String, image: Bitmap): String {
        val mpImage = BitmapImageBuilder(image).build()
        return llmInference.generateResponse(prompt, listOf(mpImage))
    }
}
```

```kotlin
// llama.cpp —á–µ—Ä–µ–∑ Kotlin binding
// Kotlin-LlamaCpp –∏–ª–∏ InferKt

// 1. Gradle dependency
// implementation("com.github.user:kotlin-llama-cpp:version")

import com.example.llamacpp.LlamaModel

class LlamaInference {
    private val model = LlamaModel()

    fun load(modelPath: String) {
        model.loadModel(
            path = modelPath,
            contextSize = 2048,
            gpuLayers = 0  // CPU only –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        )
    }

    fun generate(prompt: String, maxTokens: Int = 256): Flow<String> = flow {
        model.generateStream(prompt, maxTokens).collect { token ->
            emit(token)
        }
    }
}
```

**–†–µ—Å—É—Ä—Å—ã –¥–ª—è Android:**
- [MediaPipe LLM Samples](https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/llm_inference/android)
- [Kotlin-LlamaCpp](https://github.com/ljcamargo/kotlinllamacpp)
- [SmolChat](https://github.com/user/smolchat) ‚Äî open-source GGUF runner

### ExecuTorch (Cross-Platform –æ—Ç Meta)

```bash
# ExecuTorch ‚Äî production framework –æ—Ç Meta
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Instagram, WhatsApp, Messenger

# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install executorch

# 2. –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏
python -m executorch.examples.models.llama.export_llama \
    --model Qwen/Qwen3-0.6B-Instruct \
    --output qwen3-0.6b.pte \
    --quantize int4

# 3. –†–∞–∑–º–µ—Ä runtime: 50KB base footprint
```

```kotlin
// Android —Å ExecuTorch AAR
// build.gradle: implementation("org.pytorch:executorch:1.0.0")

import org.pytorch.executorch.Module

val module = Module.load("qwen3-0.6b.pte")
val output = module.forward(inputTensor)
```

```swift
// iOS —Å ExecuTorch
import ExecuTorch

let module = try Module(fileAtPath: "qwen3-0.6b.pte")
let output = try module.forward([inputTensor])
```

### React Native —Å ExecuTorch

```typescript
// React Native ExecuTorch ‚Äî JS API –¥–ª—è mobile
// npm install react-native-executorch

import { useLLM } from 'react-native-executorch';

function ChatComponent() {
    const { generate, isLoading } = useLLM({
        modelPath: 'qwen3-0.6b.pte',
        maxTokens: 256
    });

    const handleChat = async (prompt: string) => {
        const response = await generate(prompt);
        console.log(response);
    };

    // Streaming
    const handleStreamChat = async (prompt: string) => {
        for await (const token of generate(prompt, { stream: true })) {
            console.log(token);
        }
    };
}
```

### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ Gotchas (2025)

```
+----------------------------------------------------------------------------+
|                    Mobile LLM ‚Äî –†–µ–∞–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è                        |
+----------------------------------------------------------------------------+
|                                                                            |
|  –ü–ê–ú–Ø–¢–¨:                                                                   |
|  - iOS –ª–∏–º–∏—Ç bundle: 4GB (–º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –∫–∞—á–∞—Ç—å –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏)            |
|  - 7B –º–æ–¥–µ–ª—å = 28GB (FP32) / 14GB (FP16) / 3.5GB (Q4)                    |
|  - –ï—Å–ª–∏ –º–æ–¥–µ–ª—å + KV cache > RAM ‚Üí crash –∏–ª–∏ swap (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)        |
|  - TinyLlama 1B –∑–∞–Ω–∏–º–∞–µ—Ç >50% RAM iPhone 14 (6GB)                        |
|                                                                            |
|  –ë–ê–¢–ê–†–ï–Ø:                                                                  |
|  - 7B –º–æ–¥–µ–ª—å: ~2 —á–∞—Å–∞ –¥–æ –ø–æ–ª–Ω–æ–≥–æ —Ä–∞–∑—Ä—è–¥–∞                                  |
|  - 50% –±–∞—Ç–∞—Ä–µ–∏ –∑–∞ 90 –º–∏–Ω—É—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è                        |
|  - GPU = –∫–∞–∫ –∏–≥—Ä–∞: —Å–∏–ª—å–Ω—ã–π –Ω–∞–≥—Ä–µ–≤, –±—ã—Å—Ç—Ä—ã–π —Ä–∞–∑—Ä—è–¥                         |
|  - NPU —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤                       |
|                                                                            |
|  –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:                                                       |
|  - 8-10 tok/s –Ω–∞ flagship (vs 50-100 tok/s desktop GPU)                  |
|  - Cold start 5-15 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏                            |
|  - Mid-range (Galaxy A54): –¥–∞–∂–µ 2B –º–æ–¥–µ–ª—å "—Ç–æ—Ä–º–æ–∑–∏—Ç"                     |
|                                                                            |
|  GPU vs CPU:                                                               |
|  - GPU –Ω–µ –≤—Å–µ–≥–¥–∞ –±—ã—Å—Ç—Ä–µ–µ! (iPhone 15: CPU 17 tok/s > GPU 12.8 tok/s)     |
|  - –ü—Ä–∏—á–∏–Ω–∞: overhead –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ GPU                             |
|  - GPU –∑–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏ (UI rendering)                          |
|                                                                            |
|  –¢–ï–ü–õ–û:                                                                    |
|  - –¢–µ–ª–µ—Ñ–æ–Ω –ø–µ—Ä–µ–≥—Ä–µ–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º inference                         |
|  - Thermal throttling —Å–Ω–∏–∂–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å                          |
|  - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ—Å—Å–∏–∏, –Ω–µ –±–æ–ª–µ–µ 5-10 –º–∏–Ω—É—Ç                   |
|                                                                            |
|  –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´:                                                      |
|  1. –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö (—ç–º—É–ª—è—Ç–æ—Ä—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç)           |
|  2. –ò–∑–º–µ—Ä—è–π—Ç–µ: cold start, inference latency, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É        |
|  3. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: 2B –Ω–∞ —Å—Ç–∞—Ä—ã—Ö, 4B –Ω–∞ –Ω–æ–≤—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö           |
|  4. Cloud fallback: –Ω–∞ —Å–ª–∞–±—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö ‚Üí cloud API                    |
|  5. Caching —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —ç–∫–æ–Ω–æ–º–∏—Ç –±–∞—Ç–∞—Ä–µ—é                             |
|                                                                            |
+----------------------------------------------------------------------------+
```

### Production Apps —Å On-Device LLM (2025)

| –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ | –ú–æ–¥–µ–ª—å | –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ |
|------------|--------|-----------|---------------|
| **Apple Intelligence** | 3B (Apple) | iOS 18+ | Siri, Writing Tools |
| **Samsung Galaxy AI** | On-device + Cloud | Galaxy S24+ | Translation, Summary |
| **Google AI Edge Gallery** | Gemma 3n | Android | Experimental demos |
| **Private LLM** | GGUF models | iOS | Privacy-first chat |
| **Enclave AI** | Local models | iOS/Mac | Zero tracking |
| **SmolChat** | GGUF models | Android | Open-source |
| **Meta Apps** | ExecuTorch | iOS/Android | Instagram, WhatsApp |

### Decision Tree: Cloud vs On-Device

```
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ   –ù—É–∂–Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å?     ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚ñº                               ‚ñº
                      –î–ê                              –ù–ï–¢
                        ‚îÇ                               ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
            ‚îÇ –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏?     ‚îÇ                   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
                        ‚îÇ                               ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
           ‚ñº                         ‚ñº                  ‚îÇ
      –ü–†–û–°–¢–ê–Ø                     –°–õ–û–ñ–ù–ê–Ø               ‚îÇ
    (summary, Q&A)             (reasoning, code)        ‚îÇ
           ‚îÇ                         ‚îÇ                  ‚îÇ
           ‚ñº                         ‚ñº                  ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ON-DEVICE   ‚îÇ         ‚îÇ HYBRID:      ‚îÇ   ‚îÇ    CLOUD     ‚îÇ
    ‚îÇ  Gemma 3n    ‚îÇ         ‚îÇ On-device +  ‚îÇ   ‚îÇ  GPT-4o /    ‚îÇ
    ‚îÇ  Llama 3.2   ‚îÇ         ‚îÇ Cloud backup ‚îÇ   ‚îÇ  Claude 3.5  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 11. Quick Start Guides

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (5 –º–∏–Ω—É—Ç)

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç)
ollama run qwen3:8b

# 3. –ì–æ—Ç–æ–≤–æ! –ß–∞—Ç –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
# –î–ª—è –≤—ã—Ö–æ–¥–∞: /bye
```

### –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ Setup (30 –º–∏–Ω—É—Ç)

```bash
# 1. Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
ollama pull qwen3:14b           # General purpose
ollama pull deepseek-r1:14b     # Reasoning
ollama pull qwen3-coder:7b      # Coding (–µ—Å–ª–∏ –µ—Å—Ç—å)

# 3. Open WebUI –¥–ª—è UI
docker run -d -p 3000:8080 \
    --add-host=host.docker.internal:host-gateway \
    -v open-webui:/app/backend/data \
    --name open-webui \
    ghcr.io/open-webui/open-webui:main

# 4. Python integration
pip install ollama openai

# 5. Test
python -c "import ollama; print(ollama.chat(model='qwen3:14b', messages=[{'role':'user','content':'Hello!'}])['message']['content'])"
```

### Production Setup

```bash
# 1. Docker Compose stack
cat > docker-compose.yml << 'EOF'
version: '3.8'
services:
  ollama:
    image: ollama/ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - webui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
  webui_data:
EOF

# 2. –ó–∞–ø—É—Å–∫
docker compose up -d

# 3. Preload –º–æ–¥–µ–ª–µ–π
docker exec ollama ollama pull qwen3:32b
docker exec ollama ollama pull mistral-small:24b

# 4. Health check
curl http://localhost:11434/api/tags
```

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

1. **Q: –°–∫–æ–ª—å–∫–æ VRAM –Ω—É–∂–Ω–æ –¥–ª—è Llama 3.3 70B –≤ Q4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏?**
   A: ~40-42GB. –§–æ—Ä–º—É–ª–∞: 70B x 4 bits / 8 x 1.2 = 42GB. –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ 2x RTX 4090 (48GB) –∏–ª–∏ single RTX 5090 (32GB) —Å Q3.

2. **Q: –ö–∞–∫—É—é –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –≤—ã–±—Ä–∞—Ç—å –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞?**
   A: Q4_K_M - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤—ã–±–æ—Ä (92-95% –∫–∞—á–µ—Å—Ç–≤–∞). Q5_K_M –µ—Å–ª–∏ VRAM –ø–æ–∑–≤–æ–ª—è–µ—Ç (95-97%). –î–ª—è coding –∑–∞–¥–∞—á –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ Q5_K_M+.

3. **Q: –ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è Ollama –æ—Ç llama.cpp?**
   A: Ollama - –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ llama.cpp —Å —É–¥–æ–±–Ω—ã–º CLI, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –º–æ–¥–µ–ª—è–º–∏ –∏ OpenAI-compatible API. llama.cpp - –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

4. **Q: –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ –¥–ª—è coding –≤ 2025?**
   A: DeepSeek V3.2, Qwen3-Coder, –∏–ª–∏ NVIDIA Nemotron Nano 9B (–¥–ª—è 8GB VRAM). –í—Å–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç GPT-4 –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –¥–æ—Å—Ç—É–ø–Ω—ã —Å MIT/Apache –ª–∏—Ü–µ–Ω–∑–∏–µ–π.

5. **Q: RTX 5090 vs RTX 4090 - —Å—Ç–æ–∏—Ç –ª–∏ –∞–ø–≥—Ä–µ–π–¥?**
   A: RTX 5090 –¥–∞–µ—Ç +33% VRAM (32 vs 24GB), +67% —Å–∫–æ—Ä–æ—Å—Ç–∏ –Ω–∞ 8B –º–æ–¥–µ–ª—è—Ö (213 vs 128 tok/s), –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—É—Å–∫–∞—Ç—å 49B –º–æ–¥–µ–ª–∏. –°—Ç–æ–∏—Ç –µ—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å 30B+ –º–æ–¥–µ–ª—è–º–∏.

6. **Q: –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–µ–∂–¥—É Ollama –∏ vLLM –¥–ª—è production?**
   A: Ollama - –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö deployments —Å –Ω–∏–∑–∫–∏–º —Ç—Ä–∞—Ñ–∏–∫–æ–º. vLLM - –¥–ª—è –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ (continuous batching, PagedAttention), throughput –≤ 10-20x –≤—ã—à–µ.

7. **Q: –ß—Ç–æ —Ç–∞–∫–æ–µ thinking mode –≤ Qwen3?**
   A: –†–µ–∂–∏–º "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" (chain-of-thought) –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á (math, reasoning). –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç process –º—ã—à–ª–µ–Ω–∏—è. –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å –≤ runtime –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞.

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

### –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [Ollama](https://ollama.com/) - [GitHub](https://github.com/ollama/ollama)
- [LM Studio](https://lmstudio.ai/)
- [llama.cpp](https://github.com/ggml-org/llama.cpp)
- [Open WebUI](https://github.com/open-webui/open-webui)
- [vLLM](https://github.com/vllm-project/vllm)
- [Hugging Face TGI](https://github.com/huggingface/text-generation-inference)

### –ú–æ–¥–µ–ª–∏
- [DeepSeek](https://github.com/deepseek-ai) - [DeepSeek R1](https://huggingface.co/deepseek-ai)
- [Meta Llama](https://llama.meta.com/) - [Llama 3.3](https://huggingface.co/meta-llama)
- [Qwen](https://github.com/QwenLM/Qwen3) - [Qwen3 Blog](https://qwenlm.github.io/blog/qwen3/)
- [Mistral AI](https://mistral.ai/) - [Mistral Small 3.1](https://mistral.ai/news/mistral-small-3-1)

### –°—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –≥–∞–π–¥—ã (2025)
- [LM Studio vs Ollama - HyScaler](https://hyscaler.com/insights/ollama-vs-lm-studio/)
- [Local LLM Deployment Guide - n8n](https://blog.n8n.io/local-llm/)
- [Best GPUs for LLM Inference 2025](https://localllm.in/blog/best-gpus-llm-inference-2025)
- [GGUF Quantization Guide](https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9)
- [DeepSeek R1 Local Deployment](https://dev.to/askyt/deepseek-r1-architecture-training-local-deployment-and-hardware-requirements-3mf8)
- [Apple MLX Performance](https://machinelearning.apple.com/research/exploring-llms-mlx-m5)
- [vLLM vs llama.cpp - Red Hat](https://developers.redhat.com/articles/2025/09/30/vllm-or-llamacpp-choosing-right-llm-inference-engine-your-use-case)

### On-Device / Mobile (NEW 2025)
- [MediaPipe LLM Inference API](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference) - Google AI Edge
- [ExecuTorch](https://pytorch.org/executorch/) - [GitHub](https://github.com/pytorch/executorch) - Meta PyTorch mobile
- [MLX Swift](https://github.com/ml-explore/mlx-swift) - Apple Machine Learning framework
- [Gemma 3n Developer Guide](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/) - Google
- [Llama 3.2 for Mobile](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) - Meta
- [CoreML On-Device Llama](https://machinelearning.apple.com/research/core-ml-on-device-llama) - Apple Research
- [React Native ExecuTorch](https://docs.swmansion.com/react-native-executorch/) - Cross-platform
- [Kotlin-LlamaCpp](https://github.com/ljcamargo/kotlinllamacpp) - Android binding
- [Are Local LLMs on Mobile a Gimmick?](https://www.callstack.com/blog/local-llms-on-mobile-are-a-gimmick) - Realistic assessment
- [Production-Grade Local LLM Inference Study (arXiv)](https://arxiv.org/abs/2511.05502) - Comparative benchmarks
- [Mobile LLM Quantization (arXiv)](https://arxiv.org/html/2512.06490) - INT4 optimization
- [MobileAIBench](https://arxiv.org/html/2406.10290v1) - On-device benchmarking framework

### –°–æ–æ–±—â–µ—Å—Ç–≤–∞
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - Reddit community
- [Hugging Face Hub](https://huggingface.co/models) - Model repository
- [Awesome Mobile LLM](https://github.com/stevelaskaridis/awesome-mobile-llm) - Curated list

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏

- [[llm-fundamentals]] - –û—Å–Ω–æ–≤—ã LLM
- [[llm-inference-optimization]] - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
- [[ai-cost-optimization]] - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞—Ç—Ä–∞—Ç AI
- [[ai-devops-deployment]] - DevOps –¥–ª—è AI
- [[rag-systems]] - RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- [[mcp-model-context-protocol]] - MCP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
- [[tutorial-ai-agent]] - –°–æ–∑–¥–∞–Ω–∏–µ AI –∞–≥–µ–Ω—Ç–æ–≤

---

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
