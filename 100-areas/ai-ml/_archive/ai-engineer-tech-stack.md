---
title: "Техстек AI Engineer 2025"
created: 2025-11-24
modified: 2025-11-24
type: reference
status: verified
confidence: high
sources_verified: true
tags:
  - ai-ml/tools
  - ai-ml/tech-stack
related:
  - "[[ai-engineering-intro]]"
  - "[[rag-and-prompt-engineering]]"
  - "[[ai-engineer-roadmap]]"
---

# Техстек AI Engineer 2025

Минимум: Python + OpenAI API + Chroma. Продвинутый: LangChain/LlamaIndex + Pinecone + облако. AI-инструменты устаревают быстрее всего — понимай категории, не заучивай названия.

---

## Терминология

| Термин | Значение |
|--------|----------|
| **OpenAI API** | GPT-4, GPT-4o, o1 через HTTP |
| **Anthropic API** | Claude 3.5 Sonnet, Claude 3 Opus |
| **LangChain** | Фреймворк для оркестрации LLM-вызовов |
| **LlamaIndex** | Фреймворк для RAG и индексации данных |
| **Vector DB** | Chroma, Pinecone, Weaviate, Qdrant |
| **Embedding model** | Превращает текст в вектор (ada-002, cohere) |
| **Self-hosted** | Локальный запуск моделей (Ollama, llama.cpp) |
| **Context window** | Сколько токенов модель обрабатывает за раз |

---

## Важное предупреждение

*AI-инструменты устаревают быстрее, чем в любой другой области. LangChain год назад был стандартом, сейчас многие считают его переусложнённым. Фокусируйся на понимании категорий инструментов, а не на заучивании конкретных названий.*

---

## Обзор стека

```
┌─────────────────────────────────────────────────────────┐
│                    ПРИЛОЖЕНИЕ                           │
│         (твой код, API, интерфейс пользователя)         │
├─────────────────────────────────────────────────────────┤
│                   ОРКЕСТРАЦИЯ                           │
│            LangChain │ LlamaIndex │ свой код            │
├─────────────────────────────────────────────────────────┤
│    ВЕКТОРНЫЕ БД          │         LLM ПРОВАЙДЕРЫ       │
│  Chroma │ Pinecone       │    OpenAI │ Anthropic        │
│  Weaviate │ Qdrant       │    Google │ Open-source      │
├─────────────────────────────────────────────────────────┤
│                   ИНФРАСТРУКТУРА                        │
│         AWS │ GCP │ Azure │ локальный сервер            │
└─────────────────────────────────────────────────────────┘
```

**Как читать эту схему:**
- **Приложение** — то, что видит пользователь (чат-бот, API, веб-интерфейс)
- **Оркестрация** — код, который связывает компоненты (промпты, RAG-логика, цепочки вызовов)
- **Векторные БД** — хранят [[rag-and-prompt-engineering|embeddings]] для поиска по смыслу
- **LLM-провайдеры** — API к языковым моделям
- **Инфраструктура** — где всё это работает

---

## 1️⃣ LLM-провайдеры

**LLM (Large Language Model)** — большая языковая модель. Основа любого AI-приложения. Подробнее: [[ai-engineering-intro]]

### Закрытые API (managed)

Платишь за использование, не нужно думать об инфраструктуре.

| Провайдер | Флагманские модели | Когда использовать |
|-----------|-------------------|-------------------|
| **OpenAI** | GPT-4o, GPT-4-turbo, o1 | Стандарт индустрии, лучшая документация, широкая экосистема |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus | Длинный контекст (200k токенов), лучше для кода и анализа |
| **Google** | Gemini 1.5 Pro, Gemini 2.0 | Multimodal (текст + изображения + видео), интеграция с Google |

**Что значит "context window":**
- GPT-4o: 128k токенов ≈ 100 страниц текста
- Claude 3: 200k токенов ≈ 150 страниц текста
- Больше контекст = больше информации модель может обработать за раз

**Пример выбора:**
```
Задача: Чат-бот для поддержки клиентов
→ OpenAI GPT-4o-mini (дешевле, достаточно для простых задач)

Задача: Анализ длинных юридических документов
→ Claude 3.5 Sonnet (длинный контекст, хорош для анализа)

Задача: Обработка изображений + текст
→ GPT-4o или Gemini (multimodal)
```

### Open-source модели

Можно запускать на своих серверах. Полный контроль, но нужна инфраструктура.

| Модель | Размеры | Особенности |
|--------|---------|-------------|
| **Llama 3.1** | 8B, 70B, 405B | Лучший open-source, от Meta, коммерческая лицензия |
| **Mistral** | 7B, 8x7B (Mixtral) | Отличное соотношение качество/размер |
| **Qwen 2** | 7B-72B | Хорош для китайского + английского, от Alibaba |

**Что значит "8B", "70B":**
- B = миллиарды параметров
- 8B — можно запустить на хорошей видеокарте (RTX 3090, 24GB VRAM)
- 70B — нужно несколько GPU или cloud
- 405B — только cloud с серьёзными ресурсами

### Где запускать open-source

| Платформа | Модель использования | Когда выбрать |
|-----------|---------------------|---------------|
| **Together.ai** | API как у OpenAI | Хочешь простоту, но с open-source моделями |
| **Replicate** | Pay-per-use | Много разных моделей, включая image/audio |
| **Hugging Face Inference** | Pay-per-use | Уже используешь HF экосистему |
| **Ollama** | Локально, бесплатно | Разработка, эксперименты, приватность |

**Когда open-source:**
- Критична приватность данных (данные не уходят на внешние серверы)
- Нужен [[rag-and-prompt-engineering|fine-tuning]]
- Большой объём запросов (closed API дорого)
- Регуляторные требования (данные в своём контуре)

*Для большинства задач начинай с закрытых API. Open-source — когда есть конкретная причина.*

---

## 2️⃣ Векторные базы данных

### Что это и зачем

**Векторная БД** хранит embeddings — числовые представления текста. Ключевой компонент [[rag-and-prompt-engineering|RAG-систем]].

**Как работает:**
```
1. Текст → Embedding-модель → Вектор [0.023, -0.041, 0.089, ...]
2. Вектор сохраняется в векторную БД
3. При поиске: запрос → вектор → найти ближайшие векторы в БД
```

**Почему не обычная БД:**
Обычная БД ищет по точному совпадению: `WHERE title = 'cats'`
Векторная БД ищет по смыслу: запрос "домашние питомцы" найдёт документы про кошек и собак.

### Популярные решения

| База | Тип развёртывания | Когда использовать |
|------|-------------------|-------------------|
| **Chroma** | Локально / облако | Прототипы, обучение. Запускается в 3 строки кода |
| **FAISS** | Библиотека (Python) | Нужен полный контроль, встраивание в свой код |
| **Pinecone** | Managed cloud | Production. Не думаешь об инфраструктуре |
| **Weaviate** | Self-hosted / облако | Сложные запросы, multimodal, GraphQL API |
| **Qdrant** | Self-hosted / облако | Высокая производительность, Rust-based |
| **Milvus** | Self-hosted | Enterprise, очень большие объёмы (миллиарды векторов) |

### Как выбрать

```
Прототип/обучение?
    ↓ Да
    → Chroma (pip install chromadb, работает сразу)

    ↓ Нет, нужен production

Хочешь managed service (не думать об инфраструктуре)?
    ↓ Да
    → Pinecone (платишь за использование)

    ↓ Нет, будем хостить сами
    → Qdrant или Weaviate (зависит от требований)
```

**Пример запуска Chroma (для старта):**
```python
import chromadb

# Создаём клиент (хранит данные локально)
client = chromadb.Client()

# Создаём коллекцию
collection = client.create_collection("my_docs")

# Добавляем документы (Chroma сам создаст embeddings!)
collection.add(
    documents=["Кошки любят спать", "Собаки любят гулять"],
    ids=["doc1", "doc2"]
)

# Ищем похожие
results = collection.query(query_texts=["домашние животные"], n_results=2)
```

*По данным LangChain State of AI 2024, топ-3: Chroma и FAISS для локальной разработки, Pinecone для production.*

---

## 3️⃣ Фреймворки оркестрации

**Оркестрация** — код, который связывает LLM, векторные БД, инструменты в единое приложение.

### LangChain

Самый популярный фреймворк. 600+ интеграций.

**Плюсы:**
- Огромное комьюнити, много туториалов
- Готовые интеграции почти со всем
- Быстрый старт для стандартных задач

**Минусы:**
- Избыточная абстракция — сложно понять что происходит внутри
- Сложно дебажить — ошибка глубоко в стеке
- API часто меняется — код ломается при обновлениях

**Когда использовать:** быстрые прототипы, стандартные use cases (Q&A, чат-бот), обучение.

```python
# LangChain пример: простой RAG
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

# Инициализация
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
llm = ChatOpenAI(model="gpt-4o")

# RAG chain в одну строку
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)

answer = qa_chain.invoke("Какая политика отпусков?")
```

### LlamaIndex

Специализируется на работе с документами и RAG.

**Плюсы:**
- Заточен под RAG — проще чем LangChain для этой задачи
- Хорошие data connectors (PDF, Notion, Google Drive, SQL)
- Более стабильный API

**Минусы:**
- Меньше интеграций (не критично для RAG)
- Меньше комьюнити

**Когда использовать:** RAG-системы, Q&A по документам.

### Свой код (без фреймворка)

**Плюсы:**
- Полный контроль — знаешь что происходит
- Проще дебажить
- Нет vendor lock-in

**Минусы:**
- Больше boilerplate кода
- Нужно самому писать интеграции

**Когда использовать:** production-системы, нестандартные задачи, когда важна надёжность.

*Тренд 2024-2025: опытные инженеры уходят от LangChain к своему коду. Для обучения LangChain полезен, для production — зависит от задачи.*

---

## 4️⃣ Embedding-модели

**Embeddings** превращают текст в векторы для поиска. Подробнее: [[rag-and-prompt-engineering]]

| Модель | Размерность | Стоимость | Когда использовать |
|--------|-------------|-----------|-------------------|
| **text-embedding-3-small** | 1536 | $0.02/1M tokens | Стандартный выбор, баланс качество/цена |
| **text-embedding-3-large** | 3072 | $0.13/1M tokens | Нужно лучшее качество поиска |
| **Cohere embed-v3** | 1024 | ~$0.1/1M tokens | Multilingual, хорош для русского |
| **BGE (BAAI)** | varies | Бесплатно | Open-source, можно запускать локально |

**Размерность** — сколько чисел в векторе. Больше = точнее, но дороже хранить.

**Практический совет:** Начни с text-embedding-3-small. Переходи на large только если качество поиска критично и ты измерил разницу.

---

## 5️⃣ AI-агенты и инструменты

**AI-агент** — LLM, которая может использовать инструменты: искать в интернете, вызывать API, выполнять код. Подробнее: [[ai-engineering-intro]]

### Фреймворки для агентов

| Фреймворк | От кого | Особенности |
|-----------|---------|-------------|
| **LangGraph** | LangChain | Для сложных workflows с состоянием, графы вызовов |
| **AutoGen** | Microsoft | Multi-agent: несколько агентов общаются друг с другом |
| **CrewAI** | Open-source | Role-based: агенты с разными "профессиями" |
| **Semantic Kernel** | Microsoft | Enterprise-grade, .NET + Python |

### Инструменты для агентов

**Веб-поиск:**
- **Tavily** — оптимизирован для AI, выдаёт структурированные результаты
- **SerpAPI** — парсинг Google результатов
- **Brave Search API** — альтернатива Google

**Выполнение кода:**
- **E2B** — sandboxed environments для выполнения кода
- **Modal** — serverless Python execution

**Браузер:**
- **Playwright** — автоматизация браузера, скриншоты, навигация

*Агенты — главный хайп 2025. По прогнозу Gartner, к 2028 году 33% enterprise-софта будет включать agentic AI.*

---

## 6️⃣ MLOps и инфраструктура

### Observability (наблюдаемость)

**Что это:** Возможность понять, что происходит внутри AI-системы — какие промпты отправляются, какие ответы приходят, где ошибки.

| Инструмент | Что делает | Когда нужен |
|------------|-----------|-------------|
| **LangSmith** | Tracing LangChain приложений | Используешь LangChain, нужен debugging |
| **Langfuse** | Open-source LLM observability | Хочешь контроль над данными |
| **Helicone** | Логирование всех LLM-запросов | Нужна аналитика использования |
| **Weights & Biases** | Experiment tracking | Fine-tuning, ML эксперименты |

**Пример проблемы без observability:**
```
Пользователь: "Ответ неправильный!"
Ты: "Какой был запрос? Какой промпт? Какой контекст из RAG?"
→ Без логов — не узнаешь
```

### Inference engines

**Inference** — процесс получения ответа от модели. Engine — софт, который это делает эффективно.

| Engine | Когда использовать |
|--------|-------------------|
| **vLLM** | Production serving, нужен высокий throughput (много запросов/сек) |
| **TGI** | Hugging Face модели, простой деплой |
| **Ollama** | Локальная разработка, один клик запуска |
| **llama.cpp** | CPU inference, edge devices, максимальная оптимизация |

**Throughput** — сколько запросов в секунду система может обработать.

### Облачные платформы

| Платформа | AI-сервисы | Когда выбрать |
|-----------|-----------|---------------|
| **AWS** | SageMaker, Bedrock, Lambda | Уже используешь AWS, нужен Bedrock (managed LLM) |
| **GCP** | Vertex AI, Cloud Run | Тесная интеграция с Google AI, Gemini |
| **Azure** | Azure OpenAI, Azure ML | Enterprise, нужен OpenAI с гарантиями SLA |

**AWS Bedrock** — managed API к Claude, Llama, и другим моделям. Как OpenAI API, но в инфраструктуре AWS.

---

## 7️⃣ Fine-tuning

**Fine-tuning** — дообучение модели на своих данных. Когда RAG и промпты не дают нужного качества.

| Техника | Что это | Когда использовать |
|---------|---------|-------------------|
| **LoRA** | Обучает только небольшую часть параметров | Стандарт, баланс качества и ресурсов |
| **QLoRA** | LoRA + квантизация | Мало GPU памяти, бюджетный вариант |
| **Full fine-tuning** | Обновляет все параметры | Редко, нужны огромные ресурсы |

**Почему не всегда fine-tuning:**
- Дорого (время GPU, данные, эксперименты)
- Сложно (нужны навыки ML)
- Часто [[rag-and-prompt-engineering|RAG + хороший промпт]] достаточно

**Когда fine-tuning нужен:**
- Специфический стиль ответов (юридический язык, медицина)
- Узкая доменная область с особой терминологией
- Нужна очень высокая точность

### Инструменты для fine-tuning

- **Hugging Face PEFT** — библиотека для LoRA, QLoRA
- **Axolotl** — упрощённый fine-tuning, yaml-конфиги
- **OpenAI Fine-tuning API** — managed, простой интерфейс

---

## Минимальный стек для старта

### Для обучения (первые проекты)

```
Python 3.10+
├── openai (pip install openai)
├── chromadb (pip install chromadb)
├── jupyter (pip install jupyter)
└── python-dotenv (для API ключей)
```

**Стоимость:** ~$5-20 на эксперименты с OpenAI API

### Для первого production-проекта

```
Python + FastAPI
├── openai или anthropic
├── Pinecone (managed) или Chroma (self-hosted)
├── Docker
└── AWS/GCP/Vercel для деплоя
```

**Пример структуры проекта:**
```
my-ai-app/
├── app/
│   ├── main.py          # FastAPI app
│   ├── rag.py           # RAG логика
│   └── prompts.py       # Промпты
├── Dockerfile
├── requirements.txt
└── .env                 # API ключи (не в git!)
```

---

## Тренды 2025

1️⃣ **Упрощение стека** — меньше абстракций, больше своего кода. LangChain критикуют за сложность.

2️⃣ **AI-агенты** — главный хайп года. Агенты, которые могут планировать, использовать инструменты, работать автономно.

3️⃣ **Multimodal** — работа с текстом, изображениями, аудио, видео в одном приложении.

4️⃣ **Smaller models** — компактные модели (7-8B) достигают качества GPT-3.5. Можно запускать локально.

5️⃣ **Structured outputs** — JSON mode, function calling становятся стандартом для надёжной интеграции.

---

## Связи

- Введение в профессию: [[ai-engineering-intro]]
- Основные техники: [[rag-and-prompt-engineering]]
- Как начать: [[ai-engineer-roadmap]]
- Docker для деплоя: [[docker-for-developers]]

---

## Источники

- [LangChain State of AI 2024](https://blog.langchain.com/langchain-state-of-ai-2024/) — проверено 2025-11-24
- [Medium: AI Engineer Tech Stack 2025](https://medium.com/@alinaqishaheen/the-ai-engineers-tech-stack-in-2025-what-you-need-to-know-89e76dea89a8) — проверено 2025-11-24
- [Zilliz: 10 Open-Source LLM Frameworks 2025](https://zilliz.com/blog/10-open-source-llm-frameworks-developers-cannot-ignore-in-2025) — проверено 2025-11-24
- [Analytics Vidhya: Top Vector Databases 2025](https://www.analyticsvidhya.com/blog/2023/12/top-vector-databases/) — проверено 2025-11-24

---

**Последняя верификация**: 2025-11-24
**Уровень достоверности**: high

---

*Проверено: 2026-01-09*
