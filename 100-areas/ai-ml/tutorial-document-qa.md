---
title: "–ü—Ä–∞–∫—Ç–∏–∫—É–º: Document Q&A System"
tags: [ai, rag, documents, pdf, qa, tutorial, python, pydantic, extraction]
category: ai-engineering
date: 2025-12-24
status: complete
level: intermediate
related:
  - "[[tutorial-rag-chatbot]]"
  - "[[embeddings-vector-databases]]"
  - "[[langchain-ecosystem]]"
---

# –ü—Ä–∞–∫—Ç–∏–∫—É–º: Document Q&A —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

> **TL;DR**: –°–æ–∑–¥–∞–µ–º production-ready —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PDF, structured outputs —Å Pydantic AI, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –û–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ 2025: Docling, LlamaParse, PyMuPDF4LLM, Mistral OCR.

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **Python –æ—Å–Ω–æ–≤—ã** | –í–µ—Å—å –∫–æ–¥ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ Python | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **RAG –æ—Å–Ω–æ–≤—ã** | Document QA ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG | [[tutorial-rag-chatbot]] |
| **Pydantic** | Structured outputs –∏—Å–ø–æ–ª—å–∑—É—é—Ç Pydantic —Å—Ö–µ–º—ã | [Pydantic docs](https://docs.pydantic.dev/) |
| **Vector databases** | –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ chunks –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | [[vector-databases-guide]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫** | ‚ö†Ô∏è –° –ø–æ–¥–≥–æ—Ç–æ–≤–∫–æ–π | –°–Ω–∞—á–∞–ª–∞ –∏–∑—É—á–∏ RAG –æ—Å–Ω–æ–≤—ã |
| **Intermediate** | ‚úÖ –î–∞ | –û—Å–Ω–æ–≤–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è |
| **Advanced** | ‚úÖ –î–∞ | –§–æ–∫—É—Å –Ω–∞ production-–ø–∞—Ç—Ç–µ—Ä–Ω—ã |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Document Q&A** = —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç **—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –ø—Ä–æ—Å—Ç–æ "–ø—Ä–∏–º–µ—Ä–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç", –∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—É–º–º—ã, –¥–∞—Ç—ã, —É—Å–ª–æ–≤–∏—è)

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Structured Output** | –û—Ç–≤–µ—Ç LLM –≤ —Å—Ç—Ä–æ–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (JSON, Pydantic) | **–ê–Ω–∫–µ—Ç–∞ vs –ø–∏—Å—å–º–æ** ‚Äî –∞–Ω–∫–µ—Ç–∞ –∏–º–µ–µ—Ç –ø–æ–ª—è, –ø–∏—Å—å–º–æ —Å–≤–æ–±–æ–¥–Ω–æ–µ |
| **Docling** | IBM –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF | **–°–∫–∞–Ω–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤** ‚Äî –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç PDF –≤ —Ç–µ–∫—Å—Ç |
| **LlamaParse** | SaaS —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ª–æ–∂–Ω—ã—Ö PDF | **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π OCR** ‚Äî –ø–ª–∞—Ç–Ω—ã–π, –Ω–æ –æ—á–µ–Ω—å —Ç–æ—á–Ω—ã–π |
| **Pydantic AI** | Framework –¥–ª—è LLM —Å —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤—ã—Ö–æ–¥–∞–º–∏ | **Strict TypeScript –¥–ª—è LLM** ‚Äî LLM –Ω–µ –º–æ–∂–µ—Ç –æ—Ç–∫–ª–æ–Ω–∏—Ç—å—Å—è –æ—Ç —Å—Ö–µ–º—ã |
| **GraphRAG** | RAG —Å knowledge graph –¥–ª—è —Å–≤—è–∑–µ–π | **–ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π** ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –∫–∞–∫ —Å—É—â–Ω–æ—Å—Ç–∏ —Å–≤—è–∑–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π |
| **Hybrid Search** | Vector + keyword –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–µ | **Google + —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫** ‚Äî –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª –ò –∏—â–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã |
| **Reranking** | –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ cross-encoder'–æ–º | **–í—Ç–æ—Ä–æ–µ –º–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞** ‚Äî —Ç–æ—á–Ω–µ–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å |
| **Citation** | –°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ –æ—Ç–≤–µ—Ç–µ | **–°–Ω–æ—Å–∫–∞ –≤ –∫–Ω–∏–≥–µ** ‚Äî –æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è |
| **Chunking** | –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ | **–ù–∞—Ä–µ–∑–∫–∞ –ø–∏—Ü—Ü—ã** ‚Äî –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–µ–ª–∏–º –Ω–∞ –∫—É—Å–∫–∏ |

---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö

| –ü—Ä–æ–±–ª–µ–º–∞ | –ü—Ä–∏–º–µ—Ä | –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è |
|----------|--------|-------------|
| **–†—É—á–Ω–æ–π –≤–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö** | –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏–∑ PDF-—Å—á–µ—Ç–æ–≤ –≤ ERP | –û—à–∏–±–∫–∏, –ø–æ—Ç–µ—Ä—è –≤—Ä–µ–º–µ–Ω–∏ |
| **–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏** | "–ö–∞–∫–∏–µ —É—Å–ª–æ–≤–∏—è –æ–ø–ª–∞—Ç—ã –≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–µ?" | –ß–∞—Å—ã –Ω–∞ —á—Ç–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ |
| **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π** | –î–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ ‚Äî —á—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å? | –†–∏—Å–∫ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–∞–∂–Ω–æ–µ |
| **–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º** | –ü—Ä–æ–≤–µ—Ä–∫–∞ compliance –≤ 100+ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö | –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—Ä—É—á–Ω—É—é |

**80% –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö** ‚Äî –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (PDF, DOCX, —Å–∫–∞–Ω—ã). –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π RAG –æ—Ç–≤–µ—á–∞–µ—Ç "–ø—Ä–∏–º–µ—Ä–Ω–æ" ‚Äî –∞ –±–∏–∑–Ω–µ—Å—É –Ω—É–∂–Ω—ã **—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –¥–∞—Ç—ã, —Å—É–º–º—ã, —Å—Ç–æ—Ä–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–∞.

### –ß—Ç–æ –¥–∞—ë—Ç Document Q&A

```
–û–±—ã—á–Ω—ã–π RAG:                    Document Q&A:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "–°—É–º–º–∞ –æ–∫–æ–ª–æ        ‚îÇ         ‚îÇ {                   ‚îÇ
‚îÇ  10000 –¥–æ–ª–ª–∞—Ä–æ–≤"    ‚îÇ         ‚îÇ   "total": 10450.00,‚îÇ
‚îÇ                     ‚îÇ         ‚îÇ   "currency": "USD",‚îÇ
‚îÇ (—Å–≤–æ–±–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç)   ‚îÇ         ‚îÇ   "tax": 450.00     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ }                   ‚îÇ
                                ‚îÇ + —Ü–∏—Ç–∞—Ç–∞ + —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –¢–æ—á–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≥–æ—Ç–æ–≤—ã–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å ERP, CRM, –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.

### –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å 2024-2025

| –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –ß—Ç–æ –Ω–æ–≤–æ–≥–æ |
|------------|--------|------------|
| **Pydantic AI** | ‚úÖ Production-ready | Structured outputs –∫–∞–∫ first-class feature |
| **Docling (IBM)** | ‚úÖ Open-source | 97.9% —Ç–æ—á–Ω–æ—Å—Ç—å —Ç–∞–±–ª–∏—Ü, MIT –ª–∏—Ü–µ–Ω–∑–∏—è |
| **LlamaParse** | ‚úÖ SaaS | ~99% —Ç–æ—á–Ω–æ—Å—Ç—å, multimodal parsing |
| **GraphRAG** | ‚úÖ Microsoft | Knowledge graphs –¥–ª—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ |
| **Mistral OCR** | üÜï 2024 | 2000 —Å—Ç—Ä/–º–∏–Ω, –æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |

**–¢—Ä–µ–Ω–¥—ã 2025:**
- Structured outputs ‚Äî killer feature –¥–ª—è production LLM
- Vision-language –º–æ–¥–µ–ª–∏ (Gemini 2.0, GPT-4o) –¥–ª—è document understanding
- Hybrid search (vector + keyword) —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º

---

## –ß—Ç–æ —Ç–∞–∫–æ–µ Document Q&A

**Document Q&A** (Document Question Answering) - —ç—Ç–æ –∑–∞–¥–∞—á–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ RAG, –∑–¥–µ—Å—å —Ñ–æ–∫—É—Å –Ω–∞:

- **–¢–æ—á–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö** - –¥–∞—Ç—ã, —Å—É–º–º—ã, –∏–º–µ–Ω–∞, —É—Å–ª–æ–≤–∏—è –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
- **–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤** - –∫–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω —Å—Å—ã–ª–∫–æ–π –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤—ã–≤–æ–¥–µ** - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON/Pydantic, –∞ –Ω–µ —Å–≤–æ–±–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
- **–°—Ä–∞–≤–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤** - –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –≤–µ—Ä—Å–∏—è–º–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤

```
+------------------------------------------------------------------+
|                    Document Q&A System                            |
+------------------------------------------------------------------+
|                                                                   |
|  Documents ---> Extraction ---> Chunking ---> Vector Store        |
|  (PDF, DOCX)        |              |              |               |
|                     v              v              v               |
|               +----------+   +----------+   +----------+          |
|               | Tables   |   | Markdown |   | ChromaDB |          |
|               +----------+   +----------+   +----------+          |
|                                                   |               |
|  User Query --------------------------------------+               |
|       |                                                           |
|       v                                                           |
|  +--------------------------------------------------------+       |
|  |              Structured QA Chain                        |       |
|  |  - Answer with citations                                |       |
|  |  - Extract structured data (Pydantic)                   |       |
|  |  - Compare documents                                    |       |
|  +--------------------------------------------------------+       |
|                          |                                        |
|                          v                                        |
|              JSON / Pydantic Response                             |
|                                                                   |
+------------------------------------------------------------------+
```

---

## –õ–∞–Ω–¥—à–∞—Ñ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ 2025

### PDF Extraction: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤

–ö–∞—á–µ—Å—Ç–≤–æ RAG –Ω–∞–ø—Ä—è–º—É—é –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í 2025 –≥–æ–¥—É —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤:

| –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç | –¢–∏–ø | –¢–æ—á–Ω–æ—Å—Ç—å —Ç–∞–±–ª–∏—Ü | –°–∫–æ—Ä–æ—Å—Ç—å | OCR | –õ–∏—Ü–µ–Ω–∑–∏—è |
|------------|-----|-----------------|----------|-----|----------|
| **[Docling](https://docling-project.github.io/docling/)** | Open-source (IBM) | 97.9% | –°—Ä–µ–¥–Ω—è—è | –î–∞ | MIT |
| **[LlamaParse](https://www.llamaindex.ai/llamaparse)** | SaaS | ~99% | ~6 —Å–µ–∫/–¥–æ–∫ | –î–∞ | Proprietary |
| **[PyMuPDF4LLM](https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/)** | Open-source | –•–æ—Ä–æ—à–∞—è | –ë—ã—Å—Ç—Ä–∞—è | –ù–µ—Ç | AGPL/Commercial |
| **[Mistral OCR](https://docs.mistral.ai/)** | API | –û—Ç–ª–∏—á–Ω–∞—è | ~2000 —Å—Ç—Ä/–º–∏–Ω | –î–∞ | Proprietary |
| **Unstructured.io** | Open-source/SaaS | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è | –î–∞ | Apache 2.0 |

#### Docling (IBM Granite-Docling)

[Docling](https://github.com/DS4SD/docling) - –æ—Ç–∫—Ä—ã—Ç—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ—Ç IBM Research, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è enterprise RAG:

```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()
result = converter.convert("contract.pdf")

# Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
markdown = result.document.export_to_markdown()

# –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON
json_output = result.document.export_to_dict()
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ç–∞–±–ª–∏—Ü—ã, –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç AI-–º–æ–¥–µ–ª–∏ DocLayNet –∏ TableFormer
- 258M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –æ–±—ã—á–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangChain –∏ LlamaIndex

**[Granite-Docling](https://www.ibm.com/new/announcements/granite-docling-end-to-end-document-conversion)** (2025) - —ç–≤–æ–ª—é—Ü–∏—è —Å vision-language –º–æ–¥–µ–ª—å—é, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∞—è —Å–≤—è–∑—å —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º.

#### LlamaParse

[LlamaParse](https://www.llamaindex.ai/llamaparse) - –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å –æ—Ç LlamaIndex –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

```python
from llama_parse import LlamaParse

parser = LlamaParse(
    api_key="...",
    result_type="markdown",
    parsing_instruction="Extract all tables with full precision"
)

documents = parser.load_data("financial_report.pdf")
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ü–æ—á—Ç–∏ 99% —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
- Multimodal parsing (–¥–∏–∞–≥—Ä–∞–º–º—ã, –≥—Ä–∞—Ñ–∏–∫–∏)
- Markdown —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è chunking
- 1000 —Å—Ç—Ä–∞–Ω–∏—Ü/–¥–µ–Ω—å –±–µ—Å–ø–ª–∞—Ç–Ω–æ

#### PyMuPDF4LLM

[PyMuPDF4LLM](https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/) - –±—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤ Markdown, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–ª—è LLM:

```python
import pymupdf4llm

# –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
md_text = pymupdf4llm.to_markdown("input.pdf")

# –° —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
pages = pymupdf4llm.to_markdown("input.pdf", page_chunks=True)

# –° –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
md_with_images = pymupdf4llm.to_markdown(
    "input.pdf",
    write_images=True,
    image_path="./images"
)
```

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–Ω–µ—Ç OCR).

---

## Structured Output: –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏

### –ü–æ—á–µ–º—É structured output –≤–∞–∂–µ–Ω

Structured output - "killer app –¥–ª—è LLM" ([Simon Willison, 2025](https://simonwillison.net/2025/Feb/28/llm-schemas/)). –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:

1. **–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç** - LLM –Ω–µ –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON
2. **–¢–∏–ø–∏–∑–∞—Ü–∏—è** - Pydantic –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ runtime
3. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - Downstream —Å–∏—Å—Ç–µ–º—ã –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π –≤–≤–æ–¥
4. **–°–Ω–∏–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π** - –ñ–µ—Å—Ç–∫–∞—è —Å—Ö–µ–º–∞ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç "—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ" –º–æ–¥–µ–ª–∏

### Pydantic AI Framework

[Pydantic AI](https://ai.pydantic.dev/) - agent framework –æ—Ç —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π Pydantic, –ø—Ä–∏–Ω–æ—Å—è—â–∏–π "FastAPI feeling" –≤ GenAI —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É:

```python
from pydantic_ai import Agent
from pydantic import BaseModel, Field
from datetime import date

class Invoice(BaseModel):
    """–°—Ö–µ–º–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—á–µ—Ç–∞."""
    invoice_number: str = Field(description="–ù–æ–º–µ—Ä —Å—á–µ—Ç–∞")
    invoice_date: date = Field(description="–î–∞—Ç–∞ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è")
    total_amount: float = Field(description="–ò—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞")
    currency: str = Field(default="USD", description="–í–∞–ª—é—Ç–∞")
    vendor_name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞")
    items: list[dict] = Field(default_factory=list, description="–ü–æ–∑–∏—Ü–∏–∏ —Å—á–µ—Ç–∞")

# –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ —Å —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º
agent = Agent(
    "openai:gpt-4o",
    output_type=Invoice,
    system_prompt="Extract invoice data from the document. Be precise with numbers and dates."
)

# –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
result = await agent.run(document_text)
invoice: Invoice = result.output  # Pydantic –º–æ–¥–µ–ª—å, –≥–æ—Ç–æ–≤–∞—è –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
```

**–ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Pydantic AI:**

- **Model-agnostic** - OpenAI, Anthropic, Gemini, DeepSeek, Ollama –∏ –¥—Ä.
- **Streamed outputs** - –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- **Tool use** - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∫–∞–∫ typed functions
- **Dependency injection** - –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ RunContext
- **MCP integration** - Model Context Protocol –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: LangChain with_structured_output

LangChain —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç structured output —á–µ—Ä–µ–∑ `with_structured_output()`:

```python
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

class ContractSummary(BaseModel):
    parties: list[str] = Field(description="–°—Ç–æ—Ä–æ–Ω—ã –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞")
    effective_date: str = Field(description="–î–∞—Ç–∞ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ —Å–∏–ª—É")
    term_months: int = Field(description="–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –≤ –º–µ—Å—è—Ü–∞—Ö")
    total_value: float = Field(description="–°—É–º–º–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞")
    key_obligations: list[str] = Field(description="–û—Å–Ω–æ–≤–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞")

llm = ChatOpenAI(model="gpt-4o", temperature=0)
structured_llm = llm.with_structured_output(ContractSummary)

result = structured_llm.invoke(f"Extract contract details:\n\n{contract_text}")
```

### Instructor: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥

[Instructor](https://github.com/jxnl/instructor) - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è structured extraction —Å –ª—é–±—ã–º LLM:

```python
import instructor
from openai import OpenAI
from pydantic import BaseModel

client = instructor.from_openai(OpenAI())

class Citation(BaseModel):
    quote: str
    page: int
    document: str

class AnswerWithCitations(BaseModel):
    answer: str
    citations: list[Citation]
    confidence: float

response = client.chat.completions.create(
    model="gpt-4o",
    response_model=AnswerWithCitations,
    messages=[
        {"role": "system", "content": "Answer based on documents. Include citations."},
        {"role": "user", "content": f"Context: {context}\n\nQuestion: {question}"}
    ]
)
```

---

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ RAG-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è Document QA

### GraphRAG: —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏

[GraphRAG](https://microsoft.github.io/graphrag/) (Microsoft) —Å—Ç—Ä–æ–∏—Ç knowledge graph –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

```
Traditional RAG: Query -> Vector Search -> Chunks -> LLM -> Answer

GraphRAG: Query -> Graph Traversal -> Related Entities -> Context -> LLM -> Answer
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è Document QA:**
- –ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∏, –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –ø—Ä–∏ vector search
- –õ—É—á—à–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ "–ö–∞–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–≤—è–∑–∞–Ω—ã —Å X?"
- Explainability - –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –ø—É—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π

```python
from graphrag import GraphRAG

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
graph = GraphRAG()
graph.build_from_documents(documents)

# –ó–∞–ø—Ä–æ—Å —Å traversal
result = graph.query(
    "What are the payment obligations of Party A?",
    traversal_depth=2  # –î–æ 2 —Å–≤—è–∑–µ–π –æ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö entities
)
```

### Hybrid Search: –ª—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤

–ö–æ–º–±–∏–Ω–∞—Ü–∏—è vector search (—Å–µ–º–∞–Ω—Ç–∏–∫–∞) –∏ keyword search (BM25):

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_chroma import Chroma

# Vector retriever
vector_store = Chroma.from_documents(documents, embeddings)
vector_retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# BM25 retriever
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 5

# Ensemble —Å –≤–µ—Å–∞–º–∏
ensemble = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7]  # 70% —Å–µ–º–∞–Ω—Ç–∏–∫–∞, 30% keywords
)
```

### Reranking: —É–ª—É—á—à–µ–Ω–∏–µ top-k

–ü–æ—Å–ª–µ –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ retrieval –ø—Ä–∏–º–µ–Ω—è–µ–º cross-encoder –¥–ª—è reranking:

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank

# Base retriever –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
base_retriever = vector_store.as_retriever(search_kwargs={"k": 20})

# Reranker –æ—Å—Ç–∞–≤–ª—è–µ—Ç –ª—É—á—à–∏–µ
reranker = CohereRerank(model="rerank-v3.5", top_n=5)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=base_retriever
)
```

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
document-qa/
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ documents/
‚îÇ   ‚îú‚îÄ‚îÄ contracts/
‚îÇ   ‚îú‚îÄ‚îÄ invoices/
‚îÇ   ‚îî‚îÄ‚îÄ reports/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docling_extractor.py   # IBM Docling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pymupdf_extractor.py   # PyMuPDF4LLM
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llama_extractor.py     # LlamaParse
‚îÇ   ‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py             # Smart chunking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ qa/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structured.py          # Pydantic AI extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ citations.py           # QA with citations
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ invoice.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contract.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.py
‚îÇ   ‚îî‚îÄ‚îÄ vectorstore.py
‚îú‚îÄ‚îÄ api.py                         # FastAPI
‚îî‚îÄ‚îÄ cli.py                         # CLI interface
```

### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```toml
[project]
name = "document-qa"
version = "2.0.0"
requires-python = ">=3.11"
dependencies = [
    # Extraction
    "docling>=2.15.0",
    "pymupdf4llm>=0.0.17",
    "llama-parse>=0.5.0",

    # LLM & RAG
    "pydantic-ai>=0.0.39",
    "langchain>=0.3.14",
    "langchain-openai>=0.2.14",
    "langchain-chroma>=0.2.0",

    # Vector Store
    "chromadb>=0.5.23",

    # Utilities
    "pydantic>=2.10.0",
    "python-dotenv>=1.0.0",
    "rich>=13.9.0",

    # API
    "fastapi>=0.115.0",
    "uvicorn>=0.34.0",
]
```

### Extractor —Å –≤—ã–±–æ—Ä–æ–º backend

```python
"""
src/extractors/factory.py
–§–∞–±—Ä–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
"""
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
import hashlib


class ExtractorType(Enum):
    DOCLING = "docling"
    PYMUPDF = "pymupdf"
    LLAMAPARSE = "llamaparse"
    AUTO = "auto"


@dataclass
class ExtractedDocument:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è."""
    filename: str
    file_hash: str
    markdown: str
    pages: list[dict]  # [{"page": 1, "content": "..."}, ...]
    tables: list[str]
    metadata: dict


class BaseExtractor(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤."""

    @abstractmethod
    def extract(self, file_path: Path) -> ExtractedDocument:
        pass

    def _compute_hash(self, file_path: Path) -> str:
        hasher = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hasher.update(chunk)
        return hasher.hexdigest()[:16]


class DoclingExtractor(BaseExtractor):
    """IBM Docling - –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è enterprise –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

    def extract(self, file_path: Path) -> ExtractedDocument:
        from docling.document_converter import DocumentConverter

        converter = DocumentConverter()
        result = converter.convert(str(file_path))

        doc = result.document
        markdown = doc.export_to_markdown()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        pages = []
        for i, page in enumerate(doc.pages):
            pages.append({
                "page": i + 1,
                "content": page.export_to_markdown() if hasattr(page, 'export_to_markdown') else ""
            })

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        tables = []
        for table in doc.tables:
            tables.append(table.export_to_markdown())

        return ExtractedDocument(
            filename=file_path.name,
            file_hash=self._compute_hash(file_path),
            markdown=markdown,
            pages=pages,
            tables=tables,
            metadata={"format": file_path.suffix, "extractor": "docling"}
        )


class PyMuPDFExtractor(BaseExtractor):
    """PyMuPDF4LLM - –±—ã—Å—Ç—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è –Ω–∞—Ç–∏–≤–Ω—ã—Ö PDF."""

    def extract(self, file_path: Path) -> ExtractedDocument:
        import pymupdf4llm

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_data = pymupdf4llm.to_markdown(
            str(file_path),
            page_chunks=True,
            show_progress=False
        )

        pages = []
        tables = []
        full_text_parts = []

        for i, page in enumerate(page_data):
            content = page.get("text", "")
            pages.append({"page": i + 1, "content": content})
            full_text_parts.append(content)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –∏–∑ markdown
            tables.extend(self._extract_tables(content))

        return ExtractedDocument(
            filename=file_path.name,
            file_hash=self._compute_hash(file_path),
            markdown="\n\n---\n\n".join(full_text_parts),
            pages=pages,
            tables=tables,
            metadata={"format": "PDF", "extractor": "pymupdf4llm"}
        )

    def _extract_tables(self, markdown: str) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç markdown —Ç–∞–±–ª–∏—Ü—ã."""
        tables = []
        lines = markdown.split("\n")
        table_lines = []
        in_table = False

        for line in lines:
            if line.startswith("|"):
                in_table = True
                table_lines.append(line)
            elif in_table:
                if table_lines:
                    tables.append("\n".join(table_lines))
                    table_lines = []
                in_table = False

        if table_lines:
            tables.append("\n".join(table_lines))

        return tables


class ExtractorFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤."""

    @staticmethod
    def create(
        extractor_type: ExtractorType = ExtractorType.AUTO,
        file_path: Path | None = None
    ) -> BaseExtractor:

        if extractor_type == ExtractorType.AUTO and file_path:
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–π–ª–∞
            suffix = file_path.suffix.lower()
            if suffix in [".pdf"]:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ OCR
                return PyMuPDFExtractor()  # –ë—ã—Å—Ç—Ä–µ–µ –¥–ª—è –Ω–∞—Ç–∏–≤–Ω—ã—Ö PDF
            elif suffix in [".docx", ".pptx", ".xlsx"]:
                return DoclingExtractor()  # –õ—É—á—à–µ –¥–ª—è Office

        extractors = {
            ExtractorType.DOCLING: DoclingExtractor,
            ExtractorType.PYMUPDF: PyMuPDFExtractor,
        }

        return extractors.get(extractor_type, DoclingExtractor)()
```

### Pydantic-—Å—Ö–µ–º—ã –¥–ª—è extraction

```python
"""
src/schemas/invoice.py
–°—Ö–µ–º–∞ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—á–µ—Ç–æ–≤.
"""
from pydantic import BaseModel, Field
from datetime import date
from typing import Optional


class MonetaryAmount(BaseModel):
    """–î–µ–Ω–µ–∂–Ω–∞—è —Å—É–º–º–∞ —Å –≤–∞–ª—é—Ç–æ–π."""
    amount: float = Field(description="–ß–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—É–º–º—ã")
    currency: str = Field(default="USD", description="–ö–æ–¥ –≤–∞–ª—é—Ç—ã ISO 4217")


class VendorInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Å—Ç–∞–≤—â–∏–∫–µ."""
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
    address: Optional[str] = Field(default=None, description="–ê–¥—Ä–µ—Å")
    tax_id: Optional[str] = Field(default=None, description="–ò–ù–ù/Tax ID")
    email: Optional[str] = Field(default=None, description="Email")


class InvoiceItem(BaseModel):
    """–ü–æ–∑–∏—Ü–∏—è –≤ —Å—á–µ—Ç–µ."""
    description: str = Field(description="–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞/—É—Å–ª—É–≥–∏")
    quantity: float = Field(default=1.0, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
    unit_price: MonetaryAmount = Field(description="–¶–µ–Ω–∞ –∑–∞ –µ–¥–∏–Ω–∏—Ü—É")
    total: MonetaryAmount = Field(description="–ò—Ç–æ–≥–æ –ø–æ –ø–æ–∑–∏—Ü–∏–∏")


class Invoice(BaseModel):
    """–ü–æ–ª–Ω–∞—è —Å—Ö–µ–º–∞ —Å—á–µ—Ç–∞ –¥–ª—è extraction."""
    # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
    invoice_number: str = Field(description="–ù–æ–º–µ—Ä —Å—á–µ—Ç–∞")
    invoice_date: date = Field(description="–î–∞—Ç–∞ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—á–µ—Ç–∞")
    due_date: Optional[date] = Field(default=None, description="–°—Ä–æ–∫ –æ–ø–ª–∞—Ç—ã")

    # –°—Ç–æ—Ä–æ–Ω—ã
    vendor: VendorInfo = Field(description="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Å—Ç–∞–≤—â–∏–∫–µ")
    customer_name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è")

    # –ü–æ–∑–∏—Ü–∏–∏
    items: list[InvoiceItem] = Field(
        default_factory=list,
        description="–°–ø–∏—Å–æ–∫ –ø–æ–∑–∏—Ü–∏–π —Å—á–µ—Ç–∞"
    )

    # –°—É–º–º—ã
    subtotal: MonetaryAmount = Field(description="–°—É–º–º–∞ –¥–æ –Ω–∞–ª–æ–≥–æ–≤")
    tax_rate: Optional[float] = Field(default=None, description="–°—Ç–∞–≤–∫–∞ –Ω–∞–ª–æ–≥–∞ %")
    tax_amount: Optional[MonetaryAmount] = Field(default=None, description="–°—É–º–º–∞ –Ω–∞–ª–æ–≥–∞")
    total: MonetaryAmount = Field(description="–ò—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞ –∫ –æ–ø–ª–∞—Ç–µ")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
    payment_terms: Optional[str] = Field(default=None, description="–£—Å–ª–æ–≤–∏—è –æ–ø–ª–∞—Ç—ã")
    notes: Optional[str] = Field(default=None, description="–ü—Ä–∏–º–µ—á–∞–Ω–∏—è")

    class Config:
        json_schema_extra = {
            "example": {
                "invoice_number": "INV-2025-001",
                "invoice_date": "2025-01-15",
                "vendor": {"name": "Acme Corp", "tax_id": "12-3456789"},
                "customer_name": "Client LLC",
                "items": [
                    {
                        "description": "Consulting services",
                        "quantity": 10,
                        "unit_price": {"amount": 150.0, "currency": "USD"},
                        "total": {"amount": 1500.0, "currency": "USD"}
                    }
                ],
                "subtotal": {"amount": 1500.0, "currency": "USD"},
                "tax_rate": 10.0,
                "tax_amount": {"amount": 150.0, "currency": "USD"},
                "total": {"amount": 1650.0, "currency": "USD"}
            }
        }
```

```python
"""
src/schemas/common.py
–û–±—â–∏–µ —Å—Ö–µ–º—ã –¥–ª—è QA —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
"""
from pydantic import BaseModel, Field
from typing import Optional


class Citation(BaseModel):
    """–¶–∏—Ç–∞—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    quote: str = Field(description="–¢–æ—á–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    document: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    page: Optional[int] = Field(default=None, description="–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã")


class AnswerWithCitations(BaseModel):
    """–û—Ç–≤–µ—Ç —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–º–∏ —Ü–∏—Ç–∞—Ç–∞–º–∏."""
    answer: str = Field(description="–ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å")
    citations: list[Citation] = Field(
        default_factory=list,
        description="–¶–∏—Ç–∞—Ç—ã, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –æ—Ç–≤–µ—Ç"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ –æ—Ç 0 –¥–æ 1"
    )

    def format_answer(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        result = [self.answer, "", "Sources:"]
        for i, c in enumerate(self.citations, 1):
            page_info = f", p.{c.page}" if c.page else ""
            result.append(f"[{i}] \"{c.quote}\" - {c.document}{page_info}")
        return "\n".join(result)


class DocumentComparison(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    document1: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    document2: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    key_differences: list[str] = Field(description="–ö–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è")
    similarities: list[str] = Field(description="–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è")
    risk_assessment: str = Field(description="–û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤")
    recommendation: str = Field(description="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è")
```

### Structured QA —Å Pydantic AI

```python
"""
src/qa/structured.py
Q&A —Å–∏—Å—Ç–µ–º–∞ —Å Pydantic AI –¥–ª—è structured outputs.
"""
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel
from typing import Type, TypeVar
import os

from ..schemas.common import AnswerWithCitations, Citation, DocumentComparison
from ..schemas.invoice import Invoice
from ..vectorstore import VectorStoreManager


T = TypeVar('T', bound=BaseModel)


class StructuredQA:
    """
    Q&A —Å–∏—Å—Ç–µ–º–∞ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ structured outputs.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pydantic AI –¥–ª—è —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.
    """

    def __init__(
        self,
        vectorstore: VectorStoreManager,
        model: str = "openai:gpt-4o",
        temperature: float = 0.0
    ):
        self.vectorstore = vectorstore
        self.model = model
        self.temperature = temperature

    async def ask_with_citations(
        self,
        question: str,
        k: int = 5
    ) -> AnswerWithCitations:
        """
        –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Returns:
            AnswerWithCitations —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ chunks
        results = self.vectorstore.search(question, k=k)

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []
        for doc, score in results:
            source = doc.metadata.get("filename", "unknown")
            page = doc.metadata.get("page_number", "?")
            context_parts.append(
                f"[Source: {source}, Page: {page}]\n{doc.page_content}"
            )
        context = "\n\n---\n\n".join(context_parts)

        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ —Å —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º
        agent = Agent(
            self.model,
            output_type=AnswerWithCitations,
            system_prompt="""You are a document analysis assistant.
Answer questions based ONLY on the provided context.

RULES:
1. Use only information from the provided documents
2. Include direct quotes as citations
3. If information is not in documents, say "Not found in documents"
4. Rate confidence based on how well documents support your answer
5. Always cite the source document and page number

Context:
{context}
""".format(context=context)
        )

        result = await agent.run(question)
        return result.output

    async def extract_structured(
        self,
        document_text: str,
        schema: Type[T],
        instructions: str = ""
    ) -> T:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

        Args:
            document_text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            schema: Pydantic —Å—Ö–µ–º–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            instructions: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏

        Returns:
            –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–∞—è Pydantic –º–æ–¥–µ–ª—å
        """
        system_prompt = f"""You are an expert document data extractor.
Extract information from the document into the specified structure.
If a field cannot be found, use null/None.

{instructions}

Document:
{document_text}
"""

        agent = Agent(
            self.model,
            output_type=schema,
            system_prompt=system_prompt
        )

        result = await agent.run("Extract the data from this document.")
        return result.output

    async def compare_documents(
        self,
        doc1_text: str,
        doc1_name: str,
        doc2_text: str,
        doc2_name: str
    ) -> DocumentComparison:
        """
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
        """
        system_prompt = f"""You are a legal/business document comparison expert.
Compare the two documents and identify differences, similarities, risks.

Document 1 ({doc1_name}):
{doc1_text}

---

Document 2 ({doc2_name}):
{doc2_text}
"""

        agent = Agent(
            self.model,
            output_type=DocumentComparison,
            system_prompt=system_prompt
        )

        result = await agent.run("Compare these documents and provide analysis.")
        output = result.output
        output.document1 = doc1_name
        output.document2 = doc2_name
        return output

    async def extract_invoice(self, document_text: str) -> Invoice:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—á–µ—Ç–∞."""
        return await self.extract_structured(
            document_text,
            Invoice,
            "Focus on extracting all line items with precise amounts. "
            "Pay attention to tax calculations and totals."
        )
```

### Vector Store —Å ChromaDB

```python
"""
src/vectorstore.py
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ ChromaDB –¥–ª—è Document QA.
"""
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from pathlib import Path
import os


class VectorStoreManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ChromaDB.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multi-document search –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é.
    """

    def __init__(
        self,
        collection_name: str = "documents",
        persist_directory: str | None = None,
        embedding_model: str = "text-embedding-3-small"
    ):
        self.collection_name = collection_name
        self.persist_directory = persist_directory or os.getenv(
            "CHROMA_PERSIST_DIR", "./chroma_db"
        )

        Path(self.persist_directory).mkdir(parents=True, exist_ok=True)

        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory
        )

    def add_documents(
        self,
        documents: list[Document],
        doc_id: str | None = None
    ) -> list[str]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º doc_id –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏."""
        if doc_id:
            for doc in documents:
                doc.metadata["doc_id"] = doc_id

        return self.vectorstore.add_documents(documents)

    def search(
        self,
        query: str,
        k: int = 5,
        filter_dict: dict | None = None,
        score_threshold: float | None = None
    ) -> list[tuple[Document, float]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π."""
        results = self.vectorstore.similarity_search_with_relevance_scores(
            query, k=k, filter=filter_dict
        )

        if score_threshold:
            results = [(doc, score) for doc, score in results if score >= score_threshold]

        return results

    def search_in_document(self, query: str, doc_id: str, k: int = 5) -> list[Document]:
        """–ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        results = self.search(query, k=k, filter_dict={"doc_id": doc_id})
        return [doc for doc, _ in results]

    def get_retriever(self, search_kwargs: dict | None = None):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç retriever –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ chains."""
        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs=search_kwargs or {"k": 5}
        )

    def list_documents(self) -> list[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö doc_id."""
        results = self.vectorstore.get()
        if results and results.get("metadatas"):
            return list({m["doc_id"] for m in results["metadatas"] if m and "doc_id" in m})
        return []

    def delete_document(self, doc_id: str) -> bool:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID."""
        try:
            self.vectorstore.delete(filter={"doc_id": doc_id})
            return True
        except Exception:
            return False
```

### Smart Chunking

```python
"""
src/processing/chunker.py
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
"""
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter
from langchain_core.documents import Document
from dataclasses import dataclass
import re


@dataclass
class ChunkingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è chunking."""
    chunk_size: int = 1000
    chunk_overlap: int = 200
    preserve_tables: bool = True
    preserve_headers: bool = True


class SmartChunker:
    """
    Chunker, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    –¢–∞–±–ª–∏—Ü—ã –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ—Å–æ–±—ã–º –æ–±—Ä–∞–∑–æ–º.
    """

    def __init__(self, config: ChunkingConfig | None = None):
        self.config = config or ChunkingConfig()

        self.md_splitter = MarkdownTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def chunk_document(
        self,
        text: str,
        metadata: dict | None = None
    ) -> list[Document]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ chunks —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

        Args:
            text: Markdown —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            –°–ø–∏—Å–æ–∫ Document chunks
        """
        metadata = metadata or {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –æ—Ç–¥–µ–ª—å–Ω–æ
        if self.config.preserve_tables:
            text, tables = self._extract_tables(text)
        else:
            tables = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        if self._is_markdown(text):
            chunks = self.md_splitter.split_text(text)
        else:
            chunks = self.text_splitter.split_text(text)

        # –°–æ–∑–¥–∞–µ–º Documents
        documents = []
        current_header = ""

        for i, chunk in enumerate(chunks):
            if self.config.preserve_headers:
                header = self._extract_header(chunk)
                if header:
                    current_header = header

            chunk_meta = {
                **metadata,
                "chunk_index": i,
                "total_chunks": len(chunks) + len(tables),
                "section": current_header,
                "content_type": "text"
            }

            documents.append(Document(page_content=chunk, metadata=chunk_meta))

        # –¢–∞–±–ª–∏—Ü—ã –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ chunks
        for j, table in enumerate(tables):
            table_meta = {
                **metadata,
                "chunk_index": len(chunks) + j,
                "content_type": "table",
                "table_index": j
            }
            documents.append(Document(page_content=table, metadata=table_meta))

        return documents

    def _extract_tables(self, text: str) -> tuple[str, list[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç markdown —Ç–∞–±–ª–∏—Ü—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        tables = []
        pattern = r'(\|[^\n]+\|\n(?:\|[-:| ]+\|\n)?(?:\|[^\n]+\|\n)+)'

        def replace(match):
            tables.append(match.group(1))
            return f"\n[TABLE_{len(tables)}]\n"

        text_without_tables = re.sub(pattern, replace, text)
        return text_without_tables, tables

    def _extract_header(self, chunk: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ chunk."""
        for line in chunk.strip().split("\n"):
            if line.startswith("#"):
                return line.lstrip("#").strip()
        return ""

    def _is_markdown(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç markdown."""
        patterns = [r'^#{1,6}\s', r'\*\*[^*]+\*\*', r'^\|.+\|$']
        return any(re.search(p, text, re.MULTILINE) for p in patterns)
```

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### CLI –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

```python
"""
cli.py
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π CLI –¥–ª—è Document QA.
"""
import asyncio
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt

from src.extractors.factory import ExtractorFactory, ExtractorType
from src.processing.chunker import SmartChunker
from src.vectorstore import VectorStoreManager
from src.qa.structured import StructuredQA

console = Console()


async def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    chunker = SmartChunker()
    vectorstore = VectorStoreManager(collection_name="doc_qa")
    qa = StructuredQA(vectorstore)

    console.print(Panel.fit(
        "[bold]Document Q&A System[/bold]\n\n"
        "Commands:\n"
        "  load <file>      - Load document\n"
        "  ask <question>   - Ask question\n"
        "  invoice <file>   - Extract invoice data\n"
        "  compare <f1> <f2> - Compare documents\n"
        "  exit             - Exit"
    ))

    while True:
        try:
            cmd = Prompt.ask("\n[green]>[/green]").strip()
            if not cmd:
                continue

            parts = cmd.split(maxsplit=1)
            command = parts[0].lower()
            args = parts[1] if len(parts) > 1 else ""

            if command == "exit":
                break

            elif command == "load":
                path = Path(args)
                extractor = ExtractorFactory.create(ExtractorType.AUTO, path)
                doc = extractor.extract(path)

                chunks = chunker.chunk_document(
                    doc.markdown,
                    {"filename": doc.filename, "hash": doc.file_hash}
                )

                vectorstore.add_documents(chunks, doc_id=doc.file_hash)
                console.print(f"[green]Loaded: {doc.filename} ({len(chunks)} chunks)[/green]")

            elif command == "ask":
                result = await qa.ask_with_citations(args)

                console.print(f"\n[bold]Answer:[/bold] {result.answer}")
                console.print(f"[dim]Confidence: {result.confidence:.0%}[/dim]")

                if result.citations:
                    console.print("\n[bold]Sources:[/bold]")
                    for c in result.citations:
                        console.print(Panel(
                            f'"{c.quote}"',
                            title=f"[{c.document}, p.{c.page or '?'}]",
                            border_style="dim"
                        ))

            elif command == "invoice":
                path = Path(args)
                extractor = ExtractorFactory.create(ExtractorType.AUTO, path)
                doc = extractor.extract(path)

                invoice = await qa.extract_invoice(doc.markdown)

                table = Table(title=f"Invoice {invoice.invoice_number}")
                table.add_column("Field", style="cyan")
                table.add_column("Value")

                table.add_row("Number", invoice.invoice_number)
                table.add_row("Date", str(invoice.invoice_date))
                table.add_row("Vendor", invoice.vendor.name)
                table.add_row("Customer", invoice.customer_name)
                table.add_row("Total", f"{invoice.total.amount} {invoice.total.currency}")

                console.print(table)

            else:
                console.print(f"[red]Unknown command: {command}[/red]")

        except KeyboardInterrupt:
            break
        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")


if __name__ == "__main__":
    asyncio.run(main())
```

### FastAPI —Å–µ—Ä–≤–µ—Ä

```python
"""
api.py
REST API –¥–ª—è Document Q&A.
"""
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from pathlib import Path
import tempfile
import shutil

from src.extractors.factory import ExtractorFactory, ExtractorType
from src.processing.chunker import SmartChunker
from src.vectorstore import VectorStoreManager
from src.qa.structured import StructuredQA
from src.schemas.invoice import Invoice
from src.schemas.common import AnswerWithCitations

app = FastAPI(title="Document Q&A API", version="2.0.0")

# Globals
chunker = SmartChunker()
vectorstore = VectorStoreManager(collection_name="api_docs")
qa = StructuredQA(vectorstore)


class QuestionRequest(BaseModel):
    question: str
    k: int = 5


@app.post("/documents/upload")
async def upload_document(file: UploadFile = File(...)):
    """Upload and index a document."""
    suffix = Path(file.filename).suffix.lower()

    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        shutil.copyfileobj(file.file, tmp)
        tmp_path = Path(tmp.name)

    try:
        extractor = ExtractorFactory.create(ExtractorType.AUTO, tmp_path)
        doc = extractor.extract(tmp_path)

        chunks = chunker.chunk_document(
            doc.markdown,
            {"filename": doc.filename}
        )

        vectorstore.add_documents(chunks, doc_id=doc.file_hash)

        return {
            "doc_id": doc.file_hash,
            "filename": doc.filename,
            "chunks": len(chunks)
        }
    finally:
        tmp_path.unlink()


@app.post("/qa/ask", response_model=AnswerWithCitations)
async def ask_question(request: QuestionRequest):
    """Ask a question about uploaded documents."""
    return await qa.ask_with_citations(request.question, k=request.k)


@app.post("/extract/invoice", response_model=Invoice)
async def extract_invoice(file: UploadFile = File(...)):
    """Extract structured invoice data."""
    suffix = Path(file.filename).suffix.lower()

    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        shutil.copyfileobj(file.file, tmp)
        tmp_path = Path(tmp.name)

    try:
        extractor = ExtractorFactory.create(ExtractorType.AUTO, tmp_path)
        doc = extractor.extract(tmp_path)
        return await qa.extract_invoice(doc.markdown)
    finally:
        tmp_path.unlink()


@app.get("/documents")
async def list_documents():
    """List all indexed documents."""
    return {"documents": vectorstore.list_documents()}


@app.get("/health")
async def health():
    return {"status": "healthy"}
```

---

## Best Practices

### 1. –í—ã–±–æ—Ä extractor –ø–æ–¥ –∑–∞–¥–∞—á—É

| –°—Ü–µ–Ω–∞—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|----------|--------------|
| Enterprise PDF —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏ | Docling |
| –ë—ã—Å—Ç—Ä—ã–π parsing –Ω–∞—Ç–∏–≤–Ω—ã—Ö PDF | PyMuPDF4LLM |
| –°–ª–æ–∂–Ω—ã–µ layouts, –¥–∏–∞–≥—Ä–∞–º–º—ã | LlamaParse |
| –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã | Mistral OCR / Docling |
| Office –¥–æ–∫—É–º–µ–Ω—Ç—ã | Docling |

### 2. Chunking strategy

- **–†–∞–∑–º–µ—Ä chunk**: 500-1500 —Ç–æ–∫–µ–Ω–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
- **Overlap**: 10-20% –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ chunk
- **–¢–∞–±–ª–∏—Ü—ã**: –í—Å–µ–≥–¥–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ chunks
- **–ó–∞–≥–æ–ª–æ–≤–∫–∏**: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ metadata –¥–ª—è context

### 3. Structured output tips

- –ù–∞—á–∏–Ω–∞–π —Å –ø—Ä–æ—Å—Ç—ã—Ö —Å—Ö–µ–º, —É—Å–ª–æ–∂–Ω—è–π –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
- –ò—Å–ø–æ–ª—å–∑—É–π `Field(description=...)` - —ç—Ç–æ —á–∞—Å—Ç—å prompt –¥–ª—è –º–æ–¥–µ–ª–∏
- –í–∞–ª–∏–¥–∏—Ä—É–π –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–æ–ª—è —Å `@field_validator`
- –¢–µ—Å—Ç–∏—Ä—É–π –Ω–∞ edge cases (–ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)

### 4. RAG evaluation

–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ Document QA ([Evidently AI](https://www.evidentlyai.com/llm-guide/rag-evaluation)):

- **Retrieval metrics**: Precision@k, Recall@k, MRR
- **Generation metrics**: Faithfulness, Answer relevancy
- **End-to-end**: Human evaluation, LLM-as-judge

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

1. **–ü–æ—á–µ–º—É Docling –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ–≥–æ OCR –¥–ª—è RAG?**
   > Docling —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ç–∞–±–ª–∏—Ü—ã, –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–µ–∫—Ü–∏–∏), –∏—Å–ø–æ–ª—å–∑—É—è AI-–º–æ–¥–µ–ª–∏ DocLayNet –∏ TableFormer. –û–±—ã—á–Ω—ã–π OCR –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–ª–æ—Å–∫–∏–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.

2. **–ß—Ç–æ –¥–∞–µ—Ç with_structured_output() / Pydantic AI?**
   > –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ LLM –≤–µ—Ä–Ω–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ç–æ—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ Pydantic —Å—Ö–µ–º—ã. –ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç "–æ—Ç–∫–ª–æ–Ω–∏—Ç—å—Å—è" –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–º.

3. **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GraphRAG –≤–º–µ—Å—Ç–æ vector RAG?**
   > –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä: "–ö–∞–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–≤—è–∑–∞–Ω—ã —Å —ç—Ç–∏–º –¥–æ–≥–æ–≤–æ—Ä–æ–º?" —Ç—Ä–µ–±—É–µ—Ç traversal –≥—Ä–∞—Ñ–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏–π.

4. **–ó–∞—á–µ–º —Ç–∞–±–ª–∏—Ü—ã —Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ chunks?**
   > –¢–∞–±–ª–∏—Ü—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Ä—è—é—Ç —Å–º—ã—Å–ª –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏. –¶–µ–ª–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–∞–∫ chunk —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫ –∏ –∫–æ–ª–æ–Ω–æ–∫.

5. **–ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å extraction –∏–∑ —Å—á–µ—Ç–æ–≤?**
   > –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Pydantic —Å—Ö–µ–º—ã —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –ø–æ–ª–µ–π, –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã —á–µ—Ä–µ–∑ json_schema_extra, –∑–∞–ø—Ä–æ—Å–∏—Ç—å confidence score, –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–æ–ª—è.

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

| # | –ò—Å—Ç–æ—á–Ω–∏–∫ | –¢–∏–ø | –ö–ª—é—á–µ–≤–æ–π –≤–∫–ª–∞–¥ |
|---|----------|-----|----------------|
| 1 | [Docling (IBM)](https://docling-project.github.io/docling/) | Docs | Open-source document extraction |
| 2 | [PyMuPDF4LLM](https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/) | Docs | –ë—ã—Å—Ç—Ä—ã–π PDF ‚Üí Markdown |
| 3 | [LlamaParse](https://www.llamaindex.ai/llamaparse) | Tool | ~99% —Ç–æ—á–Ω–æ—Å—Ç—å parsing |
| 4 | [Pydantic AI](https://ai.pydantic.dev/) | Docs | Structured outputs framework |
| 5 | [LLM Schemas ‚Äî Simon Willison](https://simonwillison.net/2025/Feb/28/llm-schemas/) | Blog | –ü–æ—á–µ–º—É structured output –≤–∞–∂–µ–Ω |
| 6 | [GraphRAG (Microsoft)](https://microsoft.github.io/graphrag/) | Docs | Knowledge graphs –¥–ª—è RAG |
| 7 | [RAG Evaluation ‚Äî Evidently AI](https://www.evidentlyai.com/llm-guide/rag-evaluation) | Guide | –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ RAG |
| 8 | [PaperQA2](https://github.com/Future-House/paper-qa) | GitHub | Scientific document QA |
| 9 | [Instructor](https://github.com/jxnl/instructor) | GitHub | Structured extraction library |
| 10 | [PDF Extraction Benchmark 2025](https://procycons.com/en/blogs/pdf-data-extraction-benchmark/) | Blog | –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ |

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

- [[tutorial-rag-chatbot]] ‚Äî –±–∞–∑–æ–≤–∞—è RAG —Å–∏—Å—Ç–µ–º–∞
- [[embeddings-complete-guide]] ‚Äî —Ä–∞–±–æ—Ç–∞ —Å embeddings
- [[vector-databases-guide]] ‚Äî vector stores
- [[structured-outputs-tools]] ‚Äî structured outputs –≤ –¥–µ—Ç–∞–ª—è—Ö
- [[rag-advanced-techniques]] ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ RAG –ø–∞—Ç—Ç–µ—Ä–Ω—ã

---

*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2025-12-28*

---

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
