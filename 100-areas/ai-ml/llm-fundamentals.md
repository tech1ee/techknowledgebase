# LLM Fundamentals: –û—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–æ—Ä–º—É–ª attention, softmax | –õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞, –º–∞—Ç—Ä–∏—Ü—ã |
| **Python** | –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞, —Ä–∞–±–æ—Ç–∞ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ | –õ—é–±–æ–π –∫—É—Ä—Å Python |
| **–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ—Å–µ—Ç—å** | –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ ML | [[ai-ml-overview-v2]] |
| **–û—Å–Ω–æ–≤—ã NLP** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ | –õ—é–±–æ–π NLP –∫—É—Ä—Å |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –±—ç–∫–≥—Ä–∞—É–Ω–¥–∞** | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ | –ù–∞—á–Ω–∏—Ç–µ —Å [[ai-ml-overview-v2]] |
| **–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –±–µ–∑ ML –æ–ø—ã—Ç–∞** | ‚úÖ –î–∞ | –ß–∏—Ç–∞–π—Ç–µ –∞–Ω–∞–ª–æ–≥–∏–∏, –ø—Ä–æ–ø—É—Å–∫–∞–π—Ç–µ —Ñ–æ—Ä–º—É–ª—ã |
| **ML –∏–Ω–∂–µ–Ω–µ—Ä** | ‚úÖ –î–∞ | –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É |
| **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å** | ‚úÖ –î–∞ | –§–æ–∫—É—Å –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö (MoE, RoPE) |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **LLM** = –≥–∏–≥–∞–Ω—Ç—Å–∫–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ. –î–µ–ª–∞–µ—Ç —ç—Ç–æ —Ç–∞–∫ —Ö–æ—Ä–æ—à–æ, —á—Ç–æ –∫–∞–∂–µ—Ç—Å—è "—É–º–Ω–æ–π".

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Transformer** | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ | **–ö–æ–Ω–≤–µ–π–µ—Ä** ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –∞ –Ω–µ –ø–æ –æ–¥–Ω–æ–º—É |
| **Self-Attention** | –ú–µ—Ö–∞–Ω–∏–∑–º "–≤–Ω–∏–º–∞–Ω–∏—è" –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ | **–í–∑–≥–ª—è–¥ —á–∏—Ç–∞—Ç–µ–ª—è** ‚Äî –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç |
| **Token** | –ï–¥–∏–Ω–∏—Ü–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤–æ –∏–ª–∏ —á–∞—Å—Ç—å —Å–ª–æ–≤–∞) | **–ö—É—Å–æ—á–µ–∫ –ø–∞–∑–ª–∞** ‚Äî —Ç–µ–∫—Å—Ç —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ |
| **Embedding** | –ß–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π —Å–ª–æ–≤–æ | **–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–º—ã—Å–ª–æ–≤** ‚Äî "–∫–æ—Ç" –∏ "–∫–æ—Ç—ë–Ω–æ–∫" –±–ª–∏–∑–∫–æ, "–∫–æ—Ç" –∏ "–∞–≤—Ç–æ–±—É—Å" –¥–∞–ª–µ–∫–æ |
| **Pre-training** | –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Å–ª–æ–≤ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ | **–ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–Ω–∏–≥ –º–∏—Ä–∞** ‚Äî –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —è–∑—ã–∫—É |
| **Fine-tuning** | –î–æ–æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É | **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–∞—á–∞** ‚Äî –∏–∑ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∞ –≤ –∫–∞—Ä–¥–∏–æ–ª–æ–≥–∞ |
| **RLHF** | –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π | **–ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ** ‚Äî –ª—é–¥–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –ª—É—á—à–µ |
| **DPO** | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RLHF, –ø—Ä–æ—â–µ –∏ –¥–µ—à–µ–≤–ª–µ | **–£–ø—Ä–æ—â—ë–Ω–Ω–æ–µ –Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ** ‚Äî —Ç–æ—Ç –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –º–µ–Ω—å—à–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ |
| **MoE** | Mixture of Experts ‚Äî –º–Ω–æ–≥–æ "—ç–∫—Å–ø–µ—Ä—Ç–æ–≤" –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ | **–ö–æ–Ω—Å–∏–ª–∏—É–º –≤—Ä–∞—á–µ–π** ‚Äî –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã |
| **Context Window** | –°–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–¥–µ–ª—å "–≤–∏–¥–∏—Ç" –∑–∞ —Ä–∞–∑ | **–†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å** ‚Äî —á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ |
| **Hallucination** | –ú–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ–ø—Ä–∞–≤–¥—É | **–§–∞–Ω—Ç–∞–∑–∏—è** ‚Äî –Ω–µ—Ç –∑–Ω–∞–Ω–∏–π = –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç |
| **RoPE** | Rotary Position Embedding | **–ß–∞—Å—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–µ–ª–∫–∞–º–∏** ‚Äî –ø–æ–∑–∏—Ü–∏—è —Ç–æ–∫–µ–Ω–∞ –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è —É–≥–ª–∞–º–∏ –≤—Ä–∞—â–µ–Ω–∏—è |

---
created: 2025-12-24
modified: 2025-12-24
type: foundational
status: verified
confidence: high
tags:
  - ai
  - llm
  - transformer
  - attention
  - tokenization
  - fundamentals
  - deep-learning
  - training
  - rlhf
related:
  - "[[models-landscape-2025]]"
  - "[[prompt-engineering-masterclass]]"
  - "[[ai-ml-overview]]"
  - "[[embeddings-complete-guide]]"
sources_verified: true
---

## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ

### –ü—Ä–æ–±–ª–µ–º–∞: "–ú–∞–≥–∏—á–µ—Å–∫–∏–π —á—ë—Ä–Ω—ã–π —è—â–∏–∫"

| –°–∏–º–ø—Ç–æ–º | –ü—Ä–∏—á–∏–Ω–∞ | –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è |
|---------|---------|-------------|
| **"–ü–æ—á–µ–º—É –º–æ–¥–µ–ª—å –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É–µ—Ç?"** | –ù–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ next-token prediction | –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–±–æ—Ä —Ä–µ—à–µ–Ω–∏–π |
| **"–ö–∞–∫—É—é –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞—Ç—å?"** | –ù–µ–∑–Ω–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä MoE, GQA | –ü–µ—Ä–µ–ø–ª–∞—Ç–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| **"–ü–æ—á–µ–º—É 128K –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–æ–∏—Ç –¥–æ—Ä–æ–∂–µ?"** | –ù–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ attention O(n¬≤) | –ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã |
| **"–ó–∞—á–µ–º –Ω—É–∂–µ–Ω RLHF?"** | –ü—É—Ç–∞–Ω–∏—Ü–∞ pre-training vs alignment | –†–∞–±–æ—Ç–∞ —Å base vs instruct –º–æ–¥–µ–ª—è–º–∏ |

**80% –ø—Ä–æ–±–ª–µ–º —Å LLM** ‚Äî –æ—Ç –Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–≥–æ, –∫–∞–∫ –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç. –í—ã –Ω–µ –º–æ–∂–µ—Ç–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –µ—Å–ª–∏ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ –µ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø—ã.

### –ß—Ç–æ –¥–∞—ë—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–æ–≤

```
–ë–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏—è:                  –° –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "–ü—Ä–æ—Å—Ç–æ –ø–æ–ø—Ä–æ–±—É–µ–º"  ‚îÇ         ‚îÇ –û—Å–æ–∑–Ω–∞–Ω–Ω—ã–π –≤—ã–±–æ—Ä    ‚îÇ
‚îÇ Trial & Error       ‚îÇ         ‚îÇ –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚îÇ
‚îÇ –°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã   ‚îÇ         ‚îÇ –ù–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥      ‚îÇ
‚îÇ –ü–µ—Ä–µ–ø–ª–∞—Ç–∞ –∑–∞ API    ‚îÇ         ‚îÇ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è costs   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –í—ã –ø–æ–Ω–∏–º–∞–µ—Ç–µ, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª—å –≤–µ–¥—ë—Ç —Å–µ–±—è –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –∏ –º–æ–∂–µ—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–æ–∏—Ö —Ä–µ—à–µ–Ω–∏–π.

### –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å 2024-2025

| –¢—Ä–µ–Ω–¥ | –°—Ç–∞—Ç—É—Å | –ß—Ç–æ –≤–∞–∂–Ω–æ –∑–Ω–∞—Ç—å |
|-------|--------|-----------------|
| **MoE –º–æ–¥–µ–ª–∏** | ‚úÖ Mainstream | DeepSeek R1 (671B/37B active), Mixtral |
| **Reasoning models** | üÜï 2024-2025 | o1, o3, DeepSeek R1 ‚Äî test-time compute |
| **Long context** | ‚úÖ –î–æ 10M —Ç–æ–∫–µ–Ω–æ–≤ | Llama 4 Scout 10M, Gemini 2M |
| **Open-source SOTA** | ‚úÖ –°—Ä–∞–≤–Ω–∏–º–æ —Å GPT-4 | DeepSeek V3/R1, Llama 4 |
| **Edge deployment** | üÜï –†–∞—Å—Ç—ë—Ç | Gemma 3 1B-27B, Llama 3.2 1B-3B |

**–ö–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è 2024-2025:**
- DeepSeek R1 ‚Äî open-source reasoning model —É—Ä–æ–≤–Ω—è o1
- Llama 4 ‚Äî –¥–æ 10M –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- Gemini 2.0 ‚Äî native multimodal, 1M context
- Claude 4.5 Opus ‚Äî extended thinking, 1M context

---

## –í–≤–µ–¥–µ–Ω–∏–µ

Large Language Models (LLM) - —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –æ–±—ä–µ–º–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –í –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –ª–µ–∂–∏—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –≤ 2017 –≥–æ–¥—É –≤ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ "Attention Is All You Need".

–≠—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ - –æ—Ç –±–∞–∑–æ–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è.

---

## TL;DR

- **LLM** - –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∞—è —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
- **Transformer** - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ self-attention, –∑–∞–º–µ–Ω–∏–≤—à–∞—è RNN –≤ 2017 –≥–æ–¥—É
- **Self-Attention** - –º–µ—Ö–∞–Ω–∏–∑–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É "—Å–º–æ—Ç—Ä–µ—Ç—å" –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
- **BPE Tokenization** - —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å—É–±—Å–ª–æ–≤–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- **Pre-training** - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ next token prediction
- **RLHF/DPO** - –º–µ—Ç–æ–¥—ã –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏
- **MoE** - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è

---

## –ß–∞—Å—Ç—å 1: –≠–≤–æ–ª—é—Ü–∏—è NLP - –æ—Ç RNN –∫ Transformer

### 1.1 –†–∞–Ω–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —è–∑—ã–∫–∞

–î–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è NLP –ø–æ–ª–∞–≥–∞–ª—Å—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã:

**Bag of Words (1954)** - –ø–æ–¥—Å—á–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ. –ü—Ä–æ—Å—Ç–æ–π, –Ω–æ —Ç–µ—Ä—è—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤.

**TF-IDF (1972)** - –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å–ª–æ–≤ –ø–æ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏: —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –ø–æ–ª—É—á–∞—é—Ç –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏, —á–∞—Å—Ç—ã–µ - –Ω–∏–∑–∫–∏–µ.

**Word2Vec (2013)** - –ø—Ä–æ—Ä—ã–≤ –æ—Ç Google: –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (word embedding), –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —É—á–∏–ª–∏—Å—å –Ω–∞—Ö–æ–¥–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–∞—Ö —Ç–µ–∫—Å—Ç–∞.

### 1.2 –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN)

**RNN (–æ–∫–æ–ª–æ 1986)** - –ø–µ—Ä–≤—ã–µ —Å–µ—Ç–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Å–æ—Ö—Ä–∞–Ω—è—è "–ø–∞–º—è—Ç—å" –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö.

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã —á–∏—Ç–∞–µ—Ç–µ –∫–Ω–∏–≥—É –∏ –ø–æ–º–Ω–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü. RNN –¥–µ–ª–∞–µ—Ç –Ω–µ—á—Ç–æ –ø–æ—Ö–æ–∂–µ–µ - –∫–∞–∂–¥–æ–µ –Ω–æ–≤–æ–µ —Å–ª–æ–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Å —É—á–µ—Ç–æ–º "–ø–∞–º—è—Ç–∏" –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö.

**–ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞ - vanishing gradient:** –ü–æ –º–µ—Ä–µ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã "–∑–∞—Ç—É—Ö–∞—é—Ç", –∏ —Å–µ—Ç—å "–∑–∞–±—ã–≤–∞–µ—Ç" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –∫–∞–∫ –ø—ã—Ç–∞—Ç—å—Å—è –≤—Å–ø–æ–º–Ω–∏—Ç—å –Ω–∞—á–∞–ª–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ - –¥–µ—Ç–∞–ª–∏ —Ç–µ—Ä—è—é—Ç—Å—è.

### 1.3 Long Short-Term Memory (LSTM)

**LSTM (1997)** - —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã vanishing gradient —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ "–≤–æ—Ä–æ—Ç–∞" (gates):

- **Forget Gate** - —Ä–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∑–∞–±—ã—Ç—å
- **Input Gate** - —Ä–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
- **Output Gate** - —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥
- **Cell State** - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å, –ø—Ä–æ—Ö–æ–¥—è—â–∞—è —á–µ—Ä–µ–∑ –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å

**–ê–Ω–∞–ª–æ–≥–∏—è:** LSTM –ø–æ—Ö–æ–∂ –Ω–∞ —Ä–∞–±–æ—á–∏–π —Å—Ç–æ–ª —Å —è—â–∏–∫–∞–º–∏. –í—ã –º–æ–∂–µ—Ç–µ –ø–æ–ª–æ–∂–∏—Ç—å –≤–∞–∂–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —è—â–∏–∫ (cell state), –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞—Å—ã–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω–æ–µ (forget gate) –∏ –¥–æ—Å—Ç–∞–≤–∞—Ç—å –Ω—É–∂–Ω–æ–µ (output gate).

LSTM —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ 2017 –≥–æ–¥–∞, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É: **–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** –Ω–µ –ø–æ–∑–≤–æ–ª—è–ª–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ GPU.

### 1.4 Encoder-Decoder –∏ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è

**Encoder-Decoder RNN (2014)** - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∑–∞–¥–∞—á "–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å" (seq2seq):
- Encoder —Å–∂–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
- Decoder —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç —ç—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä –≤ –≤—ã—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å

**–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention, 2014)** - –∫–ª—é—á–µ–≤–æ–π –ø—Ä–æ—Ä—ã–≤ Bahdanau et al.:
- –í–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ decoder –ø–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫–æ –≤—Å–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º encoder
- –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è "–æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ" –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –º–æ–∂–µ—Ç –∑–∞–≥–ª—è–Ω—É—Ç—å –≤ –ª—é–±–æ–µ –º–µ—Å—Ç–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–º–Ω–∏—Ç—å –æ–±—â–∏–π —Å–º—ã—Å–ª.

---

## –ß–∞—Å—Ç—å 2: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer

### 2.1 –†–µ–≤–æ–ª—é—Ü–∏—è 2017 –≥–æ–¥–∞

–°—Ç–∞—Ç—å—è "Attention Is All You Need" (Vaswani et al., 2017) –ø—Ä–æ–∏–∑–≤–µ–ª–∞ —Ä–µ–≤–æ–ª—é—Ü–∏—é, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É **–ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è**, –±–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Å–≤–µ—Ä—Ç–æ–∫.

**–ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏:**
- WMT 2014 English-German: 28.4 BLEU (–ø—Ä–µ–≤–∑–æ—à–µ–ª –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 2+ BLEU)
- WMT 2014 English-French: 41.8 BLEU (–Ω–æ–≤—ã–π state-of-the-art)
- –û–±—É—á–µ–Ω–∏–µ: 3.5 –¥–Ω—è –Ω–∞ 8 GPU P100 (–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤)

**–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç:** –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ - –æ—Ç—Å—ã–ª–∫–∞ –∫ –ø–µ—Å–Ω–µ Beatles "All You Need Is Love". –û–¥–∏–Ω –∏–∑ –∞–≤—Ç–æ—Ä–æ–≤, Jakob Uszkoreit, –≤—ã–±—Ä–∞–ª —Å–ª–æ–≤–æ "Transformer", –ø–æ—Ç–æ–º—É —á—Ç–æ –µ–º—É –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –µ–≥–æ –∑–≤—É—á–∞–Ω–∏–µ.

### 2.2 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Encoder-Decoder

```
[Input Embeddings] + [Positional Encoding]
           |
           v
    +--------------+
    |   ENCODER    |  x N (–æ–±—ã—á–Ω–æ N=6)
    |  (N layers)  |
    +--------------+
           |
           v
    +--------------+
    |   DECODER    |  x N
    |  (N layers)  |
    +--------------+
           |
           v
    [Linear + Softmax]
           |
           v
    [Output Probabilities]
```

**Encoder** —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ N –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Å–ª–æ–µ–≤, –∫–∞–∂–¥—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç:
1. Multi-Head Self-Attention
2. Feed-Forward Neural Network
3. Residual connections –∏ Layer Normalization –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–ª–æ—è

**Decoder** –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç—Ä–µ—Ç–∏–π –ø–æ–¥—Å–ª–æ–π:
1. Masked Multi-Head Self-Attention (–º–∞—Å–∫–∏—Ä—É–µ—Ç –±—É–¥—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏)
2. Multi-Head Cross-Attention (–≤–Ω–∏–º–∞–Ω–∏–µ –∫ –≤—ã—Ö–æ–¥–∞–º encoder)
3. Feed-Forward Neural Network

### 2.3 Self-Attention: —Å–µ—Ä–¥—Ü–µ Transformer

Self-attention –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É "–æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ" –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è.

**–¢—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∞: Query, Key, Value**

–î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —Å–æ–∑–¥–∞—é—Ç—Å—è —Ç—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∞ —á–µ—Ä–µ–∑ —É–º–Ω–æ–∂–µ–Ω–∏–µ embedding –Ω–∞ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤:

- **Query (Q)** - "–≤–æ–ø—Ä–æ—Å", –∫–æ—Ç–æ—Ä—ã–π –∑–∞–¥–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω: "–ù–∞ —á—Ç–æ –º–Ω–µ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ?"
- **Key (K)** - "–∫–ª—é—á" –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞: "–í–æ—Ç —á—Ç–æ —è –∏–∑ —Å–µ–±—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é"
- **Value (V)** - "–∑–Ω–∞—á–µ–Ω–∏–µ": "–í–æ—Ç –º–æ—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –µ—Å–ª–∏ —Ç—ã —Ä–µ—à–∏—à—å –Ω–∞ –º–µ–Ω—è –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å"

**–ê–Ω–∞–ª–æ–≥–∏—è —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π:**
- Query - —ç—Ç–æ –≤–∞—à –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
- Key - –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥ –Ω–∞ –ø–æ–ª–∫–∞—Ö
- Value - —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–Ω–∏–≥
- Attention - –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–Ω–∏–≥ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Attention:**

```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
```

**–®–∞–≥–∏:**
1. –í—ã—á–∏—Å–ª–∏—Ç—å dot product –º–µ–∂–¥—É Query –∏ –≤—Å–µ–º–∏ Key (–ø–æ–ª—É—á–∞–µ–º "–æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
2. –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ sqrt(d_k) –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)
3. –ü—Ä–∏–º–µ–Ω–∏—Ç—å softmax –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
4. –£–º–Ω–æ–∂–∏—Ç—å –Ω–∞ Value –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º—ã

**–ü–æ—á–µ–º—É –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ sqrt(d_k)?** –ë–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è dot products —Ä–∞—Å—Ç—É—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ—Å–ª–µ softmax –∏ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º.

**–ò–Ω—Ç—É–∏—Ü–∏—è:** –ï—Å–ª–∏ Query –∏ Key –ø–æ—Ö–æ–∂–∏ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω—ã), –∏—Ö dot product –±—É–¥–µ—Ç –±–æ–ª—å—à–∏–º, softmax –¥–∞—Å—Ç –≤—ã—Å–æ–∫–∏–π –≤–µ—Å, –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π Value –≤–Ω–µ—Å–µ—Ç –±–æ–ª—å—à–∏–π –≤–∫–ª–∞–¥ –≤ –≤—ã—Ö–æ–¥.

### 2.4 Multi-Head Attention: –ø–æ—á–µ–º—É –º–Ω–æ–≥–æ –≥–æ–ª–æ–≤ –ª—É—á—à–µ –æ–¥–Ω–æ–π

–í–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ attention Transformer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö "–≥–æ–ª–æ–≤" (–æ–±—ã—á–Ω–æ 8 –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ):

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O

–≥–¥–µ head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)
```

**–ó–∞—á–µ–º –Ω—É–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤?**

–û–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã—á–∏—Å–ª—è–µ—Ç –æ–¥–∏–Ω –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π average - —ç—Ç–æ —Å–ª–∏—à–∫–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ. –†–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö:

- –û–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ - —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ (–ø–æ–¥–ª–µ–∂–∞—â–µ–µ-—Å–∫–∞–∑—É–µ–º–æ–µ)
- –î—Ä—É–≥–∞—è - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ (–∞–Ω—Ç–æ–Ω–∏–º—ã, —Å–∏–Ω–æ–Ω–∏–º—ã)
- –¢—Ä–µ—Ç—å—è - –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –ß–µ—Ç–≤–µ—Ä—Ç–∞—è - —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∞–Ω–∞—Ñ–æ—Ä (–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è -> —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ)

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –∫–æ–º–∏—Å—Å–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–≤–æ–µ–º –∞—Å–ø–µ–∫—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ - –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∏—Ö –º–Ω–µ–Ω–∏–π.

**–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:** –ì–æ–ª–æ–≤—ã –Ω–µ —Å—Ç–æ–ª—å–∫–æ "—Å–º–æ—Ç—Ä—è—Ç –Ω–∞ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏", —Å–∫–æ–ª—å–∫–æ "—Å–º–æ—Ç—Ä—è—Ç —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è –Ω–∞ —Ç–æ –∂–µ —Å–∞–º–æ–µ". Attention —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º.

### 2.5 Positional Encoding: –∫–∞–∫ Transformer –∑–Ω–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤

–ü–æ—Å–∫–æ–ª—å–∫—É self-attention –Ω–µ –∏–º–µ–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω—è—Ç–∏—è –ø–æ—Ä—è–¥–∫–∞ (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN), –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —è–≤–Ω–æ —á–µ—Ä–µ–∑ positional encoding.

**Sinusoidal Positional Encoding (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Transformer):**

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

–ì–¥–µ:
- `pos` - –ø–æ–∑–∏—Ü–∏—è —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- `i` - –∏–Ω–¥–µ–∫—Å –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤ embedding
- `d_model` - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏

**–ü–æ—á–µ–º—É —Å–∏–Ω—É—Å –∏ –∫–æ—Å–∏–Ω—É—Å?**

1. **–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å:** –ö–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è –ø–æ–ª—É—á–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π "–æ—Ç–ø–µ—á–∞—Ç–æ–∫"
2. **–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏:** PE(pos+k) –º–æ–∂–Ω–æ –≤—ã—Ä–∞–∑–∏—Ç—å –∫–∞–∫ –ª–∏–Ω–µ–π–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ—Ç PE(pos), —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
3. **–û–±–æ–±—â–µ–Ω–∏–µ:** –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–Ω–µ–µ, —á–µ–º –≤ –æ–±—É—á–µ–Ω–∏–∏

**–ê–Ω–∞–ª–æ–≥–∏—è —Å —á–∞—Å–∞–º–∏:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ü–∏—Ñ–µ—Ä–±–ª–∞—Ç —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–µ–ª–∫–∞–º–∏, –≤—Ä–∞—â–∞—é—â–∏–º–∏—Å—è —Å —Ä–∞–∑–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é. –ü–æ–ª–æ–∂–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Ç—Ä–µ–ª–æ–∫ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Ä–µ–º—è. –†–∞–∑–Ω—ã–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è - —ç—Ç–æ —Å—Ç—Ä–µ–ª–∫–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏ –≤—Ä–∞—â–µ–Ω–∏—è.

**–ü–æ—á–µ–º—É 10000?** –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏—é –¥–ª–∏–Ω –≤–æ–ª–Ω –æ—Ç 2*pi –¥–æ 10000*2*pi, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –±–ª–∏–∑–∫–∏—Ö, —Ç–∞–∫ –∏ –¥–∞–ª–µ–∫–∏—Ö –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π.

### 2.6 Feed-Forward Networks –∏ Residual Connections

**Position-wise Feed-Forward Network:**
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```

–î–≤–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –º–µ–∂–¥—É –Ω–∏–º–∏. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è **–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –∫ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏** (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π).

**–ó–∞—á–µ–º –Ω—É–∂–µ–Ω FFN –ø–æ—Å–ª–µ attention?** Attention —Å–º–µ—à–∏–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–µ–∂–¥—É –ø–æ–∑–∏—Ü–∏—è–º–∏, –Ω–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –ª–∏–Ω–µ–π–Ω–æ. FFN –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, —É–≤–µ–ª–∏—á–∏–≤–∞—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.

**Residual Connections (Skip Connections):**
```
output = LayerNorm(x + Sublayer(x))
```

–ö–∞–∂–¥—ã–π –ø–æ–¥—Å–ª–æ–π –æ–∫—Ä—É–∂–µ–Ω residual connection: –≤—ã—Ö–æ–¥ = –≤—Ö–æ–¥ + —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–¥—Å–ª–æ—è.

**–ó–∞—á–µ–º?**
- –£–ø—Ä–æ—â–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç —Ç–µ—á—å –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ skip connections
- –ü–æ–∑–≤–æ–ª—è–µ—Ç —Å–ª–æ—è–º "–¥–æ–æ–±—É—á–∞—Ç—å—Å—è" –ø–æ–≤–µ—Ä—Ö identity mapping

---

## –ß–∞—Å—Ç—å 3: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

### 3.1 –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è?

–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —á–∏—Å–ª–∞–º–∏, –∞ –Ω–µ —Ç–µ–∫—Å—Ç–æ–º. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–ª–æ–≤—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ (token IDs).

**–ü—Ä–æ–±–ª–µ–º–∞ —É—Ä–æ–≤–Ω—è —Å–ª–æ–≤:**
- –û–≥—Ä–æ–º–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (—Å–æ—Ç–Ω–∏ —Ç—ã—Å—è—á —Å–ª–æ–≤)
- Out-of-vocabulary (OOV) —Å–ª–æ–≤–∞
- –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã–µ —è–∑—ã–∫–∏ —Å–æ–∑–¥–∞—é—Ç –µ—â–µ –±–æ–ª—å—à–µ –ø—Ä–æ–±–ª–µ–º

**–ü—Ä–æ–±–ª–µ–º–∞ —É—Ä–æ–≤–Ω—è —Å–∏–º–≤–æ–ª–æ–≤:**
- –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –ü–æ—Ç–µ—Ä—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**–†–µ—à–µ–Ω–∏–µ - subword tokenization:** —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∑–Ω–∞—á–∏–º—ã–µ –ø–æ–¥—Å–ª–æ–≤–∞.

### 3.2 Byte Pair Encoding (BPE)

BPE - –∞–ª–≥–æ—Ä–∏—Ç–º —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–∞, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ LLM. –í–ø–µ—Ä–≤—ã–µ –æ–ø–∏—Å–∞–Ω Philip Gage –≤ 1994 –≥–æ–¥—É, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω OpenAI –¥–ª—è GPT –≤ 2016 –≥–æ–¥—É.

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç BPE:**

1. –ù–∞—á–∞—Ç—å —Å –±–∞–∑–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è (–≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã)
2. –ù–∞–π—Ç–∏ —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É —Å–æ—Å–µ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
3. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —ç—Ç—É –ø–∞—Ä—É –≤ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω, –¥–æ–±–∞–≤–∏—Ç—å –≤ —Å–ª–æ–≤–∞—Ä—å
4. –ü–æ–≤—Ç–æ—Ä—è—Ç—å –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è

**–ü—Ä–∏–º–µ—Ä:**
```
–ù–∞—á–∞–ª–æ: l o w e r _ l o w e s t _ n e w e r
–ò—Ç–µ—Ä–∞—Ü–∏—è 1: —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞ "e r" -> "er"
         l o w er _ l o w e s t _ n e w er
–ò—Ç–µ—Ä–∞—Ü–∏—è 2: —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞ "l o" -> "lo"
         lo w er _ lo w e s t _ n e w er
... –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ
```

**–ö–ª—é—á–µ–≤–æ–µ —Å–≤–æ–π—Å—Ç–≤–æ:** –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ —Ü–µ–ª–∏–∫–æ–º –≤ —Å–ª–æ–≤–∞—Ä–µ, —Ä–µ–¥–∫–∏–µ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞.

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:** GPT, GPT-2, RoBERTa, BART, DeBERTa

### 3.3 Byte-Level BPE

GPT-2 –∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç byte-level BPE:
- –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å - 256 –±–∞–π—Ç (–∞ –Ω–µ Unicode —Å–∏–º–≤–æ–ª—ã)
- –õ—é–±–æ–π —Ç–µ–∫—Å—Ç –º–æ–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å (–Ω–µ—Ç unknown tokens)
- –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º —è–∑—ã–∫–æ–º –∏ –¥–∞–∂–µ –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

### 3.4 SentencePiece vs tiktoken

| –ê—Å–ø–µ–∫—Ç | SentencePiece (Google) | tiktoken (OpenAI) |
|--------|------------------------|-------------------|
| **–ü–æ–¥—Ö–æ–¥** | –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ code points | –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤ (UTF-8) |
| **–ü—Ä–æ–±–µ–ª—ã** | –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ —Ä–∞–≤–Ω–æ–ø—Ä–∞–≤–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã | Pre-tokenization –ø–æ –ø—Ä–æ–±–µ–ª–∞–º |
| **–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å** | –õ—É—á—à–µ –¥–ª—è non-Latin —è–∑—ã–∫–æ–≤ | –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ |
| **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ** | Training + Inference | –¢–æ–ª—å–∫–æ Inference |
| **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤** | Llama 2, T5, XLNet | GPT-3, GPT-4, Llama 3 |

**–ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ:** –ê–ª–≥–æ—Ä–∏—Ç–º BPE —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–æ-—Ä–∞–∑–Ω–æ–º—É. tiktoken —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –Ω–∞—Ä—É—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π BPE, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ.

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** Llama 3 –ø–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è —Å SentencePiece –Ω–∞ tiktoken.

### 3.5 –í–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è

| –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è | –ü—Ä–∏–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π | Trade-offs |
|----------------|-----------------|------------|
| 32,000 | Mistral 7B | –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ embedding —Å–ª–æ–µ |
| 50,257 | GPT-2 | –ë–∞–ª–∞–Ω—Å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ |
| ~100,000 | GPT-4 | –õ—É—á—à–µ –¥–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏ |
| 128,256 | Llama 3 | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å |
| 256,128 | Gemma | –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ |

**Trade-offs:**
- **–ë–æ–ª—å—à–æ–π —Å–ª–æ–≤–∞—Ä—å:** –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç (—ç–∫–æ–Ω–æ–º–∏—è context window), –Ω–æ –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ embedding —Å–ª–æ–µ
- **–ú–∞–ª–µ–Ω—å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å:** –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –¥–ª–∏–Ω–Ω–µ–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:**
- –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ—Ç 8K –¥–æ 32K –¥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ perplexity
- –°–≤—ã—à–µ ~100K - diminishing returns
- –î–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–∏–π —Å–ª–æ–≤–∞—Ä—å –∫—Ä–∏—Ç–∏—á–µ–Ω (–¥–æ 68% –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ English-centric —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ:** ~50,000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–Ω–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏.

---

## –ß–∞—Å—Ç—å 4: –û–±—É—á–µ–Ω–∏–µ LLM

### 4.1 Pre-training: –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞

**–¶–µ–ª—å pre-training:** –ù–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è–º —è–∑—ã–∫–∞ –Ω–∞ –æ–≥—Ä–æ–º–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–∞.

**Next Token Prediction (Causal Language Modeling):**

```
–í—Ö–æ–¥: "I do not like green eggs"
–¶–µ–ª—å: "do not like green eggs and"

–ü—Ä–∏–º–µ—Ä—ã –æ–±—É—á–µ–Ω–∏—è:
(I) -> do
(I do) -> not
(I do not) -> like
(I do not like) -> green
(I do not like green) -> eggs
(I do not like green eggs) -> and
```

–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞:

```
P(next_token | previous_tokens)
```

**Loss Function:** Cross-Entropy Loss –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º —Å–ª–µ–¥—É—é—â–∏–º —Ç–æ–∫–µ–Ω–æ–º.

**–ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?** –ß—Ç–æ–±—ã —Ö–æ—Ä–æ—à–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–Ω–∏–º–∞—Ç—å:
- –ì—Ä–∞–º–º–∞—Ç–∏–∫—É
- –°–µ–º–∞–Ω—Ç–∏–∫—É
- –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è –æ –º–∏—Ä–µ
- –õ–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏

**–ú–∞—Å—à—Ç–∞–± pre-training:**
- GPT-3: 570 GB —Ç–µ–∫—Å—Ç–∞ (~300B —Ç–æ–∫–µ–Ω–æ–≤)
- Llama 2: 2T —Ç–æ–∫–µ–Ω–æ–≤
- Llama 3.1: 15+ —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤
- –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: –Ω–µ–¥–µ–ª–∏/–º–µ—Å—è—Ü—ã –Ω–∞ —Ç—ã—Å—è—á–∞—Ö GPU
- –°—Ç–æ–∏–º–æ—Å—Ç—å: $100M+ –¥–ª—è frontier –º–æ–¥–µ–ª–µ–π (GPT-4)

**Law of Equi-Learning:** –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM —É–ª—É—á—à–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ - –∫–∞–∂–¥—ã–π —Å–ª–æ–π —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å.

### 4.2 Encoder-only vs Decoder-only –º–æ–¥–µ–ª–∏

–ü–æ—Å–ª–µ Transformer –≤–æ–∑–Ω–∏–∫–ª–∏ –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:

**Encoder-only (BERT, 2018):**
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ encoder —á–∞—Å—Ç—å Transformer
- Bidirectional: –≤–∏–¥–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –æ–±–µ–∏—Ö —Å—Ç–æ—Ä–æ–Ω
- –û–±—É—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ Masked Language Modeling (MLM): –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- –õ—É—á—à–µ –¥–ª—è –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è (classification, NER, QA)

**Decoder-only (GPT, 2018):**
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ decoder —á–∞—Å—Ç—å Transformer
- Autoregressive: –≤–∏–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã
- –û–±—É—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ Next Token Prediction
- –õ—É—á—à–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞

**–ü–æ—á–µ–º—É –ø–æ–±–µ–¥–∏–ª decoder-only?**
- –ü—Ä–æ—â–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å
- –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- Zero-shot –∏ few-shot capabilities
- –í—Å–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ frontier LLM - decoder-only

### 4.3 Supervised Fine-Tuning (SFT)

–ü–æ—Å–ª–µ pre-training –º–æ–¥–µ–ª—å - –æ—Ç–ª–∏—á–Ω—ã–π "–∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—å", –Ω–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.

**–ü—Ä–∏–º–µ—Ä –ø—Ä–æ–±–ª–µ–º—ã:**
```
Base Model (Llama 3 Base):
–ü—Ä–æ–º–ø—Ç: "–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏?"
–û—Ç–≤–µ—Ç: "–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏? –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —á–∞—Å—Ç–æ –∑–∞–¥–∞—é—Ç —Ç—É—Ä–∏—Å—Ç—ã..."
(–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç, –∞ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç)
```

**SFT** –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:

```
–í—Ö–æ–¥: <|user|>–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏<|assistant|>
–¶–µ–ª—å: <—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ>
```

**–î–∞–Ω–Ω—ã–µ –¥–ª—è SFT:**
- –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –æ—Ç–≤–µ—Ç)
- –û–±—ã—á–Ω–æ 10K-100K –ø—Ä–∏–º–µ—Ä–æ–≤
- –°–æ–∑–¥–∞—é—Ç—Å—è –ª—é–¥—å–º–∏ –∏–ª–∏ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏

**–ß—Ç–æ –¥–∞–µ—Ç SFT:**
- –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º
- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã
- –û—Ç–≤–µ—á–∞—Ç—å –∫—Ä–∞—Ç–∫–æ –∏–ª–∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É
- –û—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç –≤—Ä–µ–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

**–ö–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ (Catastrophic Forgetting):** SFT –º–æ–∂–µ—Ç "–∏—Å–ø–æ—Ä—Ç–∏—Ç—å" –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏–∑ pre-training.

**–†–µ—à–µ–Ω–∏—è:**
- LoRA, QLoRA (–æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –ø—Ä–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –≤–µ—Å–∞—Ö)
- –°–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö SFT —Å pre-training –¥–∞–Ω–Ω—ã–º–∏
- Functionally Invariant Paths (FIP)

**–¢—Ä–∏ —à–∞–≥–∞ alignment (InstructGPT):**
1. Pre-training –Ω–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. SFT –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö
3. RLHF –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏

### 4.4 RLHF: Reinforcement Learning from Human Feedback

SFT —É—á–∏—Ç –º–æ–¥–µ–ª—å —Ñ–æ—Ä–º–∞—Ç—É, –Ω–æ –Ω–µ —É—á–∏—Ç –±—ã—Ç—å **–ø–æ–ª–µ–∑–Ω–æ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π**. RLHF —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É.

**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** "–ß—Ç–æ –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞ –≤—ã–¥–∞–µ—Ç —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞? –¢–æ–≥–¥–∞ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Å–∫–æ—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è LLM –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏."

**–¢—Ä–∏ —Ñ–∞–∑—ã RLHF:**

**–§–∞–∑–∞ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π**
- –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç
- –õ—é–¥–∏ —Ä–∞–Ω–∂–∏—Ä—É—é—Ç –æ—Ç–≤–µ—Ç—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
- –°–æ–∑–¥–∞–µ—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç: (–ø—Ä–æ–º–ø—Ç, –ª—É—á—à–∏–π_–æ—Ç–≤–µ—Ç, —Ö—É–¥—à–∏–π_–æ—Ç–≤–µ—Ç)

**–§–∞–∑–∞ 2: –û–±—É—á–µ–Ω–∏–µ Reward Model**
- –û—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
- –í—Ö–æ–¥: (–ø—Ä–æ–º–ø—Ç, –æ—Ç–≤–µ—Ç) -> –í—ã—Ö–æ–¥: —Å–∫–∞–ª—è—Ä (–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞)
- –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä–Ω—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏—è—Ö (Bradley-Terry model)

**–§–∞–∑–∞ 3: RL –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–æ–±—ã—á–Ω–æ PPO)**
- LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã
- Reward Model –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö
- PPO (Proximal Policy Optimization) –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ LLM —á—Ç–æ–±—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å reward
- KL-divergence penalty –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —É—Ö–æ–¥ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ –æ—Ç SFT –º–æ–¥–µ–ª–∏

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã —É—á–∏—Ç–µ —Å–æ–±–∞–∫—É —Ç—Ä—é–∫–∞–º. SFT - —ç—Ç–æ –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Ç—Ä—é–∫–æ–≤. RLHF - —ç—Ç–æ –¥–∞–≤–∞—Ç—å —É–≥–æ—â–µ–Ω–∏–µ –∑–∞ —Ö–æ—Ä–æ—à–µ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞ –æ—à–∏–±–∫–∏.

**–£—Å–ø–µ—Ö–∏ RLHF:**
- ChatGPT - –ø–µ—Ä–≤—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç —Å RLHF
- Google Gemini —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç RLHF
- –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç helpfulness –∏ harmlessness

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ RLHF:** –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –Ω—é–∞–Ω—Å—ã –∏ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–∑–∏—Ç–∏–≤–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ª—é–¥–µ–π –≤–º–µ—Å—Ç–æ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π.

### 4.5 DPO: Direct Preference Optimization

**–ü—Ä–æ–±–ª–µ–º–∞ RLHF:**
- –°–ª–æ–∂–Ω–∞—è pipeline: reward model + RL training
- RL –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω –∏ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- –ë–æ–ª—å—à–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã (–Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∏ policy, –∏ reward model, –∏ value function)

**DPO (2023)** - —ç–ª–µ–≥–∞–Ω—Ç–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–æ—Ç –∂–µ objective —á—Ç–æ RLHF
- –ù–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ supervised learning
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π reward model

**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** Reward model –º–æ–∂–Ω–æ –≤—ã—Ä–∞–∑–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ policy (LLM), –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é.

**Loss DPO:**
```
L_DPO = -log(sigmoid(beta * (log(pi(y_w|x)/pi_ref(y_w|x)) - log(pi(y_l|x)/pi_ref(y_l|x)))))
```

–ì–¥–µ y_w - –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, y_l - –º–µ–Ω–µ–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π.

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**

| –ê—Å–ø–µ–∫—Ç | RLHF | DPO |
|--------|------|-----|
| **Reward Model** | –¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–∞—è | –ù–µ –Ω—É–∂–Ω–∞ |
| **–ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è** | Reinforcement Learning (PPO) | Supervised Learning |
| **–°–ª–æ–∂–Ω–æ—Å—Ç—å** | –í—ã—Å–æ–∫–∞—è, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è | –ü—Ä–æ—Å—Ç–∞—è, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è |
| **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã** | –ú–Ω–æ–≥–æ | –ú–∞–ª–æ |
| **–í—ã—á–∏—Å–ª–µ–Ω–∏—è** | –ë–æ–ª—å—à–µ | –ú–µ–Ω—å—à–µ |

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã DPO:**
- –ù–∞ TL;DR summarization: win rate ~61% –ø—Ä–æ—Ç–∏–≤ 57% —É PPO
- –°—Ä–∞–≤–Ω–∏–º–æ–µ –∏–ª–∏ –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è DPO:**
- Bradley-Terry model –Ω–µ –≤—Å–µ–≥–¥–∞ –∏–¥–µ–∞–ª—å–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
- –°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é –∏–∑-–∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏

### 4.6 Constitutional AI (Anthropic)

**–ü—Ä–æ–±–ª–µ–º–∞ RLHF:** –ù—É–∂–Ω–æ –º–Ω–æ–≥–æ –ª—é–¥–µ–π –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

**Constitutional AI** –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ —É—á–∞—Å—Ç–∏–µ:

**–§–∞–∑–∞ 1: Supervised (Self-Critique)**
1. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã (–≤–∫–ª—é—á–∞—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω—ã–µ)
2. –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –∫—Ä–∏—Ç–∏–∫—É–µ—Ç —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ "–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏" (–Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤)
3. –ú–æ–¥–µ–ª—å —Ä–µ–≤–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã
4. Fine-tuning –Ω–∞ —Ä–µ–≤–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö

**–§–∞–∑–∞ 2: RLAIF (RL from AI Feedback)**
- –í–º–µ—Å—Ç–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è AI-–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
- AI —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏:**
- –í—Å–µ–æ–±—â–∞—è –¥–µ–∫–ª–∞—Ä–∞—Ü–∏—è –ø—Ä–∞–≤ —á–µ–ª–æ–≤–µ–∫–∞ –û–û–ù
- –ü—Ä–∞–≤–∏–ª–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º (Apple, Google)
- –ü—Ä–∏–Ω—Ü–∏–ø—ã AI-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–π
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–≤–æ–¥–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (Collective Constitutional AI)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –õ—é–¥–∏ –Ω–µ –≤–∏–¥—è—Ç –≤—Ä–µ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
- –õ–µ–≥–∫–æ –∏–Ω—Å–ø–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø—ã
- Pareto improvement: –º–æ–¥–µ–ª—å –∏ –ø–æ–ª–µ–∑–Ω–µ–µ, –∏ –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ
- –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å: –º–æ–∂–Ω–æ –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–º –ø—Ä–∏–Ω—Ü–∏–ø–∞–º —Å–ª–µ–¥—É–µ—Ç AI

---

## –ß–∞—Å—Ç—å 5: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LLM

### 5.1 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ GPT –∏ Llama

–û–±–µ —Å–µ–º–µ–π—Å—Ç–≤–∞ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ decoder-only Transformer, –Ω–æ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –æ—Ç–ª–∏—á–∏—è–º–∏:

| –ê—Å–ø–µ–∫—Ç | GPT (OpenAI) | Llama (Meta) |
|--------|--------------|--------------|
| **–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å** | –ó–∞–∫—Ä—ã—Ç—ã–π | Open-source |
| **Positional Encoding** | Learned / Absolute | RoPE (Rotary) |
| **Normalization** | LayerNorm (post-norm) | RMSNorm (pre-norm) |
| **Activation** | GELU | SwiGLU |
| **Attention** | Standard MHA | Grouped-Query Attention (GQA) |

**–ü–æ—á–µ–º—É pre-norm –ª—É—á—à–µ?** –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ self-attention –∏ FFN (–≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ) —É–ª—É—á—à–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–æ–∫ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π.

### 5.2 RoPE: Rotary Position Embedding

RoPE - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤ Llama, Mistral, PaLM, CodeGen –∏ –º–Ω–æ–≥–∏—Ö –¥—Ä—É–≥–∏—Ö.

**–ò–¥–µ—è:** –í–º–µ—Å—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ - –≤—Ä–∞—â–∞—Ç—å query –∏ key.

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- Embedding —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ø–∞—Ä—ã –∏–∑–º–µ—Ä–µ–Ω–∏–π
- –ö–∞–∂–¥–∞—è –ø–∞—Ä–∞ –≤—Ä–∞—â–∞–µ—Ç—Å—è –Ω–∞ —É–≥–æ–ª, –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–∑–∏—Ü–∏–∏
- –†–∞–∑–Ω—ã–µ –ø–∞—Ä—ã –≤—Ä–∞—â–∞—é—Ç—Å—è —Å —Ä–∞–∑–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π

**–ê–Ω–∞–ª–æ–≥–∏—è —Å —á–∞—Å–∞–º–∏:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–µ–ª–æ–∫ —á–∞—Å–æ–≤, –∫–∞–∂–¥–∞—è –≤—Ä–∞—â–∞–µ—Ç—Å—è —Å–æ —Å–≤–æ–µ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é. –ü–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —É–≥–ª–∞–º–∏ –≤—Å–µ—Ö —Å—Ç—Ä–µ–ª–æ–∫.

**–ö–ª—é—á–µ–≤–æ–µ —Å–≤–æ–π—Å—Ç–≤–æ:** Dot product –º–µ–∂–¥—É query –∏ key –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç **–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ** —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ RoPE:**
- –ù–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∫–æ–¥–∏—Ä—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
- –õ–µ–≥–∫–æ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —É–≥–ª–æ–≤)
- –†–∞–±–æ—Ç–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è regular, linear –∏ local self-attention

### 5.3 Grouped-Query Attention (GQA)

**–ü—Ä–æ–±–ª–µ–º–∞:** –í —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º Multi-Head Attention –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –∏–º–µ–µ—Ç —Å–≤–æ–∏ Key –∏ Value. –ü—Ä–∏ inference —ç—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–π KV-cache.

**–†–µ—à–µ–Ω–∏–µ GQA:** –ù–µ—Å–∫–æ–ª—å–∫–æ Query –≥–æ–ª–æ–≤ –¥–µ–ª—è—Ç –æ–¥–Ω—É –ø–∞—Ä—É Key-Value.

```
Standard MHA: 32 Q heads, 32 K heads, 32 V heads
GQA:          32 Q heads, 8 K heads, 8 V heads
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- KV-cache —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –≤ 4-8 —Ä–∞–∑
- Inference –±—ã—Å—Ç—Ä–µ–µ
- –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ—á—Ç–∏ –Ω–µ —Å—Ç—Ä–∞–¥–∞–µ—Ç

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:** Llama 2/3, Mistral, Gemma

### 5.4 Mixture of Experts (MoE)

MoE –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
- FFN –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ N "—ç–∫—Å–ø–µ—Ä—Ç–æ–≤" (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö FFN)
- Router (gating network) –≤—ã–±–∏—Ä–∞–µ—Ç top-k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
- –¢–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è

**–ü—Ä–∏–º–µ—Ä Mixtral 8x7B:**
- 8 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ ~7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–∂–¥—ã–π
- –û–±—â–µ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~47B (–Ω–µ 56B! attention —Å–ª–æ–∏ –æ–±—â–∏–µ)
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è 2 —ç–∫—Å–ø–µ—Ä—Ç–∞
- "–ê–∫—Ç–∏–≤–Ω—ã–µ" –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ~13B (FLOPs –∫–∞–∫ —É 13B –º–æ–¥–µ–ª–∏)
- d_model: 4096, 32 layers, hidden_dim: 14,336
- Context window: 32K —Ç–æ–∫–µ–Ω–æ–≤

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ö–∞—á–µ—Å—Ç–≤–æ –∫–∞–∫ —É –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ (47B)
- Inference –∫–∞–∫ —É –º–∞–ª–µ–Ω—å–∫–æ–π (13B)
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- –í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ (–≤—Å–µ 47B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ RAM)
- –°–ª–æ–∂–Ω–æ—Å—Ç–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

**–ò–∑–≤–µ—Å—Ç–Ω—ã–µ MoE –º–æ–¥–µ–ª–∏:**
| –ú–æ–¥–µ–ª—å | Total Params | Active Params | Experts | –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å |
|--------|--------------|---------------|---------|-------------------|
| GPT-4 | ~1.8T (est.) | N/A | ~8 (est.) | SOTA |
| Mixtral 8x7B | 47B | 13B | 8 | = Llama 2 70B |
| Mixtral 8x22B | ~141B | ~39B | 8 | SOTA open-source |
| DeepSeek-V2/R1 | 671B | 37B | 128 | = GPT-4o/o1 |
| DBRX | 132B | 36B | 16 | Strong open-source |

### 5.5 Context Window: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

Context window - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞ —Ä–∞–∑.

**–≠–≤–æ–ª—é—Ü–∏—è:**
| –ú–æ–¥–µ–ª—å | –ì–æ–¥ | Context Window | –ü—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü |
|--------|-----|----------------|-----------------|
| GPT-2 | 2019 | 1,024 | ~1 |
| GPT-3 | 2020 | 2,048 | ~2 |
| GPT-3.5 | 2022 | 4,096 | ~3 |
| GPT-4 | 2023 | 8K / 32K | ~6 / ~24 |
| GPT-4 Turbo | Nov 2023 | 128K | ~100 |
| GPT-4o | May 2024 | 128K | ~100 |
| Claude 3.5 Sonnet | Jun 2024 | 200K | ~150 |
| Gemini 1.5 Pro | 2024 | 2M | ~1500 |
| Claude 4/4.5 | 2025 | 1M | ~750 |
| Llama 4 Scout | 2025 | 10M | ~7500 |

**–ü–æ—á–µ–º—É —Å–ª–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç?**

Self-attention –∏–º–µ–µ—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å: O(n^2) –ø–æ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º. –£–¥–≤–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = 4x –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

**–ú–µ—Ç–æ–¥—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è:**

1. **Sparse Attention (Longformer, BigBird):** –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤–Ω–∏–º–∞–µ—Ç —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

2. **Sliding Window Attention:** –õ–æ–∫–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ + –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã

3. **RoPE Position Interpolation:** –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö encoding –¥–ª—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏

4. **SelfExtend:** Bi-level attention - grouped attention –¥–ª—è –¥–∞–ª–µ–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, neighbor attention –¥–ª—è –±–ª–∏–∑–∫–∏—Ö

5. **Semantic Compression:** –°–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (–¥–æ 6x)

**Trade-offs –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:**
- –ú–µ–¥–ª–µ–Ω–Ω–µ–µ inference (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞)
- Lost-in-the-Middle problem: –º–æ–¥–µ–ª—å —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ
- –ë–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏

---

## –ß–∞—Å—Ç—å 6: –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

### 6.1 –ò—Å—Ç–æ—Ä–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è GPT

| –ú–æ–¥–µ–ª—å | –î–∞—Ç–∞ —Ä–µ–ª–∏–∑–∞ | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –î–∞–Ω–Ω—ã–µ |
|--------|-------------|-----------|----------|--------|
| GPT-1 | –ò—é–Ω—å 2018 | 117M | 512 | BookCorpus |
| GPT-2 | –§–µ–≤—Ä–∞–ª—å 2019 | 1.5B | 1,024 | 40GB (WebText) |
| GPT-3 | –ò—é–Ω—å 2020 | 175B | 2,048 | 570GB (~300B —Ç–æ–∫–µ–Ω–æ–≤) |
| GPT-4 | –ú–∞—Ä—Ç 2023 | ~1.76T (MoE)* | 8K/32K/128K | ~13T —Ç–æ–∫–µ–Ω–æ–≤* |
| GPT-4 Turbo | –ù–æ—è–±—Ä—å 2023 | ~1.76T* | 128K | - |
| GPT-4o | –ú–∞–π 2024 | N/A | 128K | - |
| GPT-4.1 | –ê–ø—Ä–µ–ª—å 2025 | N/A | 1M | - |

*–ù–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, OpenAI –Ω–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏

**–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ:**
- GPT-2 -> GPT-3: 100x –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- GPT-3 -> GPT-4: ~10x –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (MoE)
- –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è GPT-4: >$100M (–ø–æ —Å–ª–æ–≤–∞–º Sam Altman)

### 6.2 –°–µ–º–µ–π—Å—Ç–≤–æ Llama

| –ú–æ–¥–µ–ª—å | –î–∞—Ç–∞ | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –¢–æ–∫–µ–Ω—ã | –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä |
|--------|------|-----------|----------|--------|-------------|
| Llama 1 | –§–µ–≤ 2023 | 7B/13B/33B/65B | 2K | 1T | SentencePiece |
| Llama 2 | –ò—é–ª—å 2023 | 7B/13B/70B | 4K | 2T | SentencePiece |
| Llama 3 | –ê–ø—Ä 2024 | 8B/70B | 8K | - | tiktoken |
| Llama 3.1 | –ò—é–ª—å 2024 | 8B/70B/405B | 128K | 15T+ | tiktoken (128K vocab) |

**Llama 3.1 405B —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
- Largest open-source LLM
- Standard decoder-only (–Ω–µ MoE) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- Grouped-Query Attention
- 8 —è–∑—ã–∫–æ–≤: EN, DE, FR, IT, PT, HI, ES, TH
- –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 16,000+ H100 GPU

### 6.3 Claude –æ—Ç Anthropic

| –ú–æ–¥–µ–ª—å | –î–∞—Ç–∞ | –ö–æ–Ω—Ç–µ–∫—Å—Ç | Output limit | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ |
|--------|------|----------|--------------|-------------|
| Claude 3 Haiku | –ú–∞—Ä—Ç 2024 | 200K | 4K | –ë—ã—Å—Ç—Ä—ã–π, –¥–µ—à–µ–≤—ã–π |
| Claude 3 Sonnet | –ú–∞—Ä—Ç 2024 | 200K | 4K | –ë–∞–ª–∞–Ω—Å |
| Claude 3 Opus | –ú–∞—Ä—Ç 2024 | 200K | 4K | –°–∞–º—ã–π —É–º–Ω—ã–π |
| Claude 3.5 Sonnet | –ò—é–Ω—å 2024 | 200K | 8K (beta) | SOTA –Ω–∞ –º–Ω–æ–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö |
| Claude 4/4.5 | 2025 | 1M | - | Extended context |

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Claude 3.5 Sonnet:**
- Knowledge cutoff: April 2024
- –¶–µ–Ω–∞: $3/1M input, $15/1M output
- Vision capabilities
- Tool use (function calling)
- ASL-2 safety level

---

## –ß–∞—Å—Ç—å 7: Inference –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 7.1 Autoregressive Generation

LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç **–ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É –∑–∞ —Ä–∞–∑**:

```
Prompt: "Write a poem about"

Step 1: model("Write a poem about") -> "the" (p=0.23)
Step 2: model("Write a poem about the") -> "sea" (p=0.18)
Step 3: model("Write a poem about the sea") -> "," (p=0.45)
...
```

**–í–∞–∂–Ω–æ:** –ú–æ–¥–µ–ª—å –Ω–µ "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç" –æ—Ç–≤–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ. –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω - –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ.

### 7.2 –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

**Temperature:**
```
logits_scaled = logits / temperature

Temperature = 0.1: –ø–æ—á—Ç–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–±–æ—Ä
Temperature = 1.0: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
Temperature = 2.0: –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ, –∫—Ä–µ–∞—Ç–∏–≤–Ω–µ–µ
```

**Top-P (Nucleus Sampling):**
- –ë–µ—Ä–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ–∫–∞ —Å—É–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π < P
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ: –º–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

**Top-K:**
- –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ç–æ–∫–µ–Ω–æ–≤

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
| –ó–∞–¥–∞—á–∞ | Temperature | Top-P |
|--------|-------------|-------|
| –§–∞–∫—Ç—ã, –∫–æ–¥, JSON | 0.0-0.3 | 1.0 |
| –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ | 0.7 | 0.95 |
| –ö—Ä–µ–∞—Ç–∏–≤, brainstorm | 0.9-1.2 | 0.95 |

### 7.3 KV Cache

**–ü—Ä–æ–±–ª–µ–º–∞ –±–µ–∑ –∫—ç—à–∞:** O(n^2) - –ø–µ—Ä–µ—Å—á–µ—Ç –≤—Å–µ—Ö K, V –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ.

**–° KV Cache:** O(n) - —Å–æ—Ö—Ä–∞–Ω—è–µ–º K, V –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤.

**–ü–∞–º—è—Ç—å KV Cache:**
```
= 2 * num_layers * d_model * seq_len * bytes_per_param

Llama 70B (80 layers, d=8192, seq=4096, fp16):
= 2 * 80 * 8192 * 4096 * 2 = ~10.7 GB –Ω–∞ –∑–∞–ø—Ä–æ—Å!
```

### 7.4 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

**Quantization:** FP32 -> FP16 -> INT8 -> INT4

**Speculative Decoding:** –ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–µ—Ä–Ω–æ–≤–∏–∫, –±–æ–ª—å—à–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç. –£—Å–∫–æ—Ä–µ–Ω–∏–µ 2-3x.

**Batching:** –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.

**PagedAttention (vLLM):** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é KV cache.

---

## –ß–∞—Å—Ç—å 8: –ü—Ä–æ–±–ª–µ–º—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

### 8.1 Hallucinations

**–¢–∏–ø—ã:**
1. **Factual** - –Ω–µ–≤–µ—Ä–Ω—ã–µ —Ñ–∞–∫—Ç—ã
2. **Fabricated citations** - –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
3. **Confident nonsense** - —É–≤–µ—Ä–µ–Ω–Ω–∞—è —á–µ–ø—É—Ö–∞
4. **Context infidelity** - –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

**–ü—Ä–∏—á–∏–Ω—ã:**
- –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç—å, –Ω–µ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å
- –ù–µ—Ç "–∑–Ω–∞–Ω–∏—è" - —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- Knowledge cutoff
- Incentive to please

**–ú–µ—Ç–æ–¥—ã –±–æ—Ä—å–±—ã:**
| –ú–µ—Ç–æ–¥ | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å |
|-------|---------------|
| RAG | –í—ã—Å–æ–∫–∞—è |
| Grounding/Citations | –°—Ä–µ–¥–Ω—è—è |
| Chain-of-Thought | –°—Ä–µ–¥–Ω—è—è |
| Low Temperature | –ß–∞—Å—Ç–∏—á–Ω–∞—è |
| Refusal training | –ó–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ |

### 8.2 Lost in the Middle

–ú–æ–¥–µ–ª–∏ —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

**–†–µ—à–µ–Ω–∏—è:**
- –í–∞–∂–Ω–æ–µ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ –ø—Ä–æ–º–ø—Ç–∞
- Chunking –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- Structured format (JSON, markdown)

### 8.3 Knowledge Cutoff

–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–Ω–∞—é—Ç —Å–æ–±—ã—Ç–∏–π –ø–æ—Å–ª–µ –¥–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è.

**–ü—Ä–∏–º–µ—Ä—ã cutoff:**
- Claude 3.5 Sonnet: April 2024
- GPT-4 Turbo: December 2023

**–†–µ—à–µ–Ω–∏–µ:** RAG —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

<details>
<summary>1. –ü–æ—á–µ–º—É Transformer –∑–∞–º–µ–Ω–∏–ª RNN –¥–ª—è NLP?</summary>

**–û—Ç–≤–µ—Ç:**
- **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å:** RNN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, Transformer - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
- **Vanishing gradient:** RNN —Ç–µ—Ä—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö
- **Attention:** –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π –¥–ª—è —Å–≤—è–∑–∏ –ª—é–±—ã—Ö –¥–≤—É—Ö –ø–æ–∑–∏—Ü–∏–π (vs O(n) –¥–ª—è RNN)
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ:** Transformer —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU
</details>

<details>
<summary>2. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ—Ä–º—É–ª–∞ Attention?</summary>

**–û—Ç–≤–µ—Ç:**
```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
```
1. QK^T - dot product –º–µ–∂–¥—É Query –∏ Key (–æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)
2. / sqrt(d_k) - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
3. softmax - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
4. * V - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ Values
</details>

<details>
<summary>3. –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É SFT –∏ RLHF?</summary>

**–û—Ç–≤–µ—Ç:**
- **SFT:** Supervised learning –Ω–∞ –ø–∞—Ä–∞—Ö (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –æ—Ç–≤–µ—Ç). –£—á–∏—Ç —Ñ–æ—Ä–º–∞—Ç—É –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.
- **RLHF:** Reinforcement learning –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö. –£—á–∏—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —á–µ—Ä–µ–∑ reward model.

SFT - –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –ø–µ—Ä–≤—ã–π —à–∞–≥. RLHF - "–ø–æ–ª–∏—Ä–æ–≤–∫–∞" –¥–ª—è alignment.
</details>

<details>
<summary>4. –ß—Ç–æ —Ç–∞–∫–æ–µ MoE –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?</summary>

**–û—Ç–≤–µ—Ç:**
Mixture of Experts –∑–∞–º–µ–Ω—è–µ—Ç FFN –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ "—ç–∫—Å–ø–µ—Ä—Ç–æ–≤" —Å router.
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ 2-4 —ç–∫—Å–ø–µ—Ä—Ç–∞ –∏–∑ 8-128
- Total params >> Active params
- –ö–∞—á–µ—Å—Ç–≤–æ –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏, inference –º–∞–ª–µ–Ω—å–∫–æ–π
- –ü—Ä–∏–º–µ—Ä: Mixtral 8x7B –∏–º–µ–µ—Ç 47B total, –Ω–æ 13B active params
</details>

<details>
<summary>5. –ü–æ—á–µ–º—É DPO –ø—Ä–æ—â–µ RLHF?</summary>

**–û—Ç–≤–µ—Ç:**
- RLHF —Ç—Ä–µ–±—É–µ—Ç: reward model + PPO training + value function
- DPO: –ø—Ä—è–º–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ supervised learning
- –¢–æ—Ç –∂–µ objective, –Ω–æ –±–µ–∑ RL
- –ú–µ–Ω—å—à–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ, –±—ã—Å—Ç—Ä–µ–µ
</details>

---

## –°–≤—è–∑–∏

- [[models-landscape-2025]] - –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- [[prompt-engineering-masterclass]] - –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å LLM
- [[embeddings-complete-guide]] - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
- [[rag-advanced-techniques]] - RAG –¥–ª—è –±–æ—Ä—å–±—ã —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏
- [[ai-ml-overview]] - –æ–±—â–∏–π –æ–±–∑–æ—Ä AI Engineering

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

| # | –ò—Å—Ç–æ—á–Ω–∏–∫ | –¢–∏–ø | –í–∫–ª–∞–¥ |
|---|----------|-----|-------|
| 1 | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. 2017 | Paper | –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer |
| 2 | [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al. 2018 | Paper | Encoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ |
| 3 | [RoFormer: Rotary Position Embedding](https://arxiv.org/abs/2104.09864) - Su et al. 2021 | Paper | RoPE –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ |
| 4 | [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) - Rafailov et al. 2023 | Paper | DPO –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RLHF |
| 5 | [Mixtral of Experts](https://arxiv.org/abs/2401.04088) - Jiang et al. 2024 | Paper | MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ |
| 6 | [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Jay Alammar | Tutorial | –í–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ Transformer |
| 7 | [Understanding and Coding Self-Attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) - Sebastian Raschka | Tutorial | –†–µ–∞–ª–∏–∑–∞—Ü–∏—è attention —Å –Ω—É–ª—è |
| 8 | [MoE LLMs](https://cameronrwolfe.substack.com/p/moe-llms) - Cameron R. Wolfe | Blog | –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ MoE |
| 9 | [RLHF Illustrated](https://huggingface.co/blog/rlhf) - Hugging Face | Guide | RLHF pipeline |
| 10 | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) - OpenAI 2023 | Paper | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ frontier –º–æ–¥–µ–ª–∏ |

---

*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2025-12-28*

---

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
