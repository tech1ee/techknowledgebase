---
title: "Mobile AI/ML: Complete Guide to On-Device Inference (2024-2025)"
date: 2025-12-29
type: guide
status: published
tags:
  - mobile
  - topic/ai-ml
  - ml
  - on-device
  - npu
  - quantization
  - production
  - type/guide
  - level/intermediate
related:
  - "[[ai-ml-overview-v2]]"
  - "[[local-llms-self-hosting]]"
  - "[[android-overview]]"
  - "[[ios-development]]"
  - "[[llm-inference-optimization]]"
---

# Mobile AI/ML: Complete Guide to On-Device Inference

Comprehensive guide covering SDKs, hardware acceleration, mobile-optimized models, quantization techniques, and production patterns for on-device AI/ML as of late 2025.

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ ML** | –ß—Ç–æ —Ç–∞–∫–æ–µ –º–æ–¥–µ–ª–∏, inference | [[ai-ml-overview-v2]] |
| **Mobile Development** | iOS –∏–ª–∏ Android —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ | [[android-overview]], iOS docs |
| **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** | –ü–æ–Ω–∏–º–∞–Ω–∏–µ trade-offs | [[llm-inference-optimization]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **Mobile Developer** | ‚úÖ –î–∞ | –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è AI –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è |
| **AI/ML Engineer** | ‚úÖ –î–∞ | On-device deployment |
| **Product Manager** | ‚úÖ –î–∞ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π |
| **–ù–æ–≤–∏—á–æ–∫ –≤ ML** | ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ | –°–Ω–∞—á–∞–ª–∞ [[ai-ml-overview-v2]] |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **On-Device AI** = AI —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä—è–º–æ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ, –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ API

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **On-Device Inference** | –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ | **–û—Ñ–ª–∞–π–Ω –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä** ‚Äî –≤—Å—ë —Å—á–∏—Ç–∞–µ—Ç —Å–∞–º —Ç–µ–ª–µ—Ñ–æ–Ω |
| **NPU** | Neural Processing Unit | **AI-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä** ‚Äî —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —á–∏–ø –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π |
| **TOPS** | Trillions of Operations Per Second | **–ú–æ—â–Ω–æ—Å—Ç—å NPU** ‚Äî —á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –±—ã—Å—Ç—Ä–µ–µ AI |
| **LiteRT** | Lite Runtime (–±—ã–≤—à–∏–π TensorFlow Lite) | **–õ—ë–≥–∫–∏–π –¥–≤–∏–∂–æ–∫** ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö |
| **ExecuTorch** | PyTorch –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ | **PyTorch –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ** ‚Äî –ø—Ä–∏–≤—ã—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –º–æ–±–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç |
| **MLX** | Apple ML framework | **ML –¥–ª—è iPhone/Mac** ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ Apple Silicon |
| **Quantization** | –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏ | **–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞** ‚Äî –º–æ–¥–µ–ª—å –º–µ–Ω—å—à–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ |
| **GGUF** | –§–æ—Ä–º–∞—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π | **–§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞** ‚Äî –∫–∞–∫ MP3 –¥–ª—è –º—É–∑—ã–∫–∏, GGUF –¥–ª—è –º–æ–¥–µ–ª–µ–π |
| **tok/s** | –¢–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É | **–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞** ‚Äî —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞ —Å–µ–∫—É–Ω–¥—É |

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [SDK Comparison](#sdk-comparison)
3. [Hardware Acceleration](#hardware-acceleration)
4. [Mobile-Optimized Models](#mobile-optimized-models)
5. [Quantization Techniques](#quantization-techniques)
6. [Production Patterns](#production-patterns)
7. [Platform Integration](#platform-integration)
8. [Decision Guide](#decision-guide)
9. [References](#references)

---

## Executive Summary

### State of Mobile AI in 2025

Mobile AI has matured from experimental to production-ready:

| Metric | 2023 | 2025 |
|--------|------|------|
| LLM on device | Experimental | Production (Gemma 3n, Llama 3.2) |
| NPU TOPS | 15-20 | 35-73 |
| Model size | 1-2GB max | 4-8GB viable |
| Inference speed | 5-10 tok/s | 20-45 tok/s |

### Key Developments (2024-2025)

1. **LiteRT** (Sep 2024) ‚Äî TensorFlow Lite rebranded, multi-framework support
2. **ExecuTorch 1.0** (Oct 2024) ‚Äî Native PyTorch deployment, 50KB runtime
3. **Gemma 3n** (2025) ‚Äî 2-4GB effective memory, multimodal
4. **NNAPI Deprecated** ‚Äî Migration to GPU delegates and NPU SDKs
5. **On-device LoRA** ‚Äî Fine-tuning now possible on mobile GPUs

---

## SDK Comparison

### Overview Matrix

| SDK | Platforms | Format | NPU | GPU | Best For |
|-----|-----------|--------|-----|-----|----------|
| **LiteRT** | Android, iOS, Web | .tflite | Via delegates | Yes | Cross-platform, general |
| **CoreML** | iOS/macOS | .mlmodel | ANE | Metal | iOS-exclusive |
| **ONNX Runtime** | All | .onnx | Via EPs | Yes | Framework agnostic |
| **MediaPipe** | Android, iOS, Web | .task | Via TFLite | Yes | Pre-built tasks |
| **ExecuTorch** | Android, iOS, Embedded | .pte | QNN, CoreML | Vulkan, Metal | PyTorch native |

### LiteRT (TensorFlow Lite)

**Status:** Production-ready, actively developed
**Rebrand:** TensorFlow Lite ‚Üí LiteRT (September 2024)

```kotlin
// Android: LiteRT inference
val interpreter = Interpreter(loadModelFile("model.tflite"))
interpreter.run(inputBuffer, outputBuffer)
```

**Key Features:**
- Multi-framework: PyTorch, JAX, TensorFlow, Keras
- Minimal runtime: ~300KB
- Hardware delegates: GPU, NNAPI (deprecated), CoreML, Hexagon

**Delegates:**
| Delegate | Platform | Acceleration |
|----------|----------|--------------|
| GPU | Android/iOS | OpenGL, Metal |
| CoreML | iOS | Apple Neural Engine |
| Hexagon | Qualcomm | DSP/NPU |
| XNNPACK | All | CPU optimization |

### CoreML

**Status:** iOS-exclusive, best for Apple devices

```swift
// iOS: CoreML inference
let config = MLModelConfiguration()
config.computeUnits = .all  // Use ANE
let model = try MyModel(configuration: config)
let prediction = try model.prediction(input: inputData)
```

**Optimization Tips:**
1. Use `.all` compute units (best performance)
2. Weight palettization (1-8 bits) for ANE
3. W8A8 quantization for A17 Pro/M4
4. EnumeratedShapes over dynamic inputs

### ONNX Runtime Mobile

**Status:** Cross-platform, framework agnostic

```java
// Android: ONNX Runtime
OrtEnvironment env = OrtEnvironment.getEnvironment();
OrtSession.SessionOptions opts = new OrtSession.SessionOptions();
opts.addXnnpack();  // CPU optimization
OrtSession session = env.createSession(modelPath, opts);
```

**Execution Providers:**
| EP | Platform | Status |
|----|----------|--------|
| CPU | All | Default |
| XNNPACK | Android/iOS | Best for float |
| CoreML | iOS | Stable |
| NNAPI | Android | **DEPRECATED** |
| QNN | Qualcomm | New recommended |

**Known Issue:** Operator coverage gaps ‚Äî some models need CPU fallback for unsupported ops.

### MediaPipe

**Status:** Best for pre-built ML tasks, experimental for LLM

**Supported Tasks:**
- Vision: Object detection, segmentation, pose
- Audio: Speech recognition
- Text: Classification
- GenAI: LLM inference (experimental)

```kotlin
// Android: MediaPipe object detection
val detector = ObjectDetector.createFromOptions(context, options)
val results = detector.detect(image)
```

**LLM API (Experimental):**
- Models: Gemma 3n, Gemma 2B, Phi-2
- Features: Multimodal, LoRA adapters
- Devices: Pixel 8+, Samsung S23+ recommended

### ExecuTorch

**Status:** Production-ready (v1.0 Oct 2024), PyTorch native

```python
# Export model for mobile
import torch
from executorch.exir import to_edge

model = MyModel()
exported = torch.export.export(model, example_inputs)
edge_program = to_edge(exported)
et_program = edge_program.to_executorch()
```

**Key Advantages:**
- No format conversion needed
- 50KB base runtime
- 12+ hardware backends
- Production-proven at Meta

**Backends:**
| Backend | Vendor | Acceleration |
|---------|--------|--------------|
| XNNPACK | ARM | CPU |
| CoreML | Apple | ANE |
| MPS | Apple | Metal GPU |
| QNN | Qualcomm | Hexagon NPU |
| Vulkan | Cross-platform | GPU |
| NeuroPilot | MediaTek | APU |

---

## Hardware Acceleration

### NPU Comparison Matrix

| SoC | NPU | TOPS | LLM Speed | Power Efficiency |
|-----|-----|------|-----------|------------------|
| Apple A18 Pro | Neural Engine | 35 | ~33 tok/s (8B) | Best |
| Snapdragon 8 Gen 3 | Hexagon | 73 | 15 tok/s (10B) | Excellent |
| MediaTek D9300+ | APU 790 | 46 | 22 tok/s (7B) | Very Good |
| Samsung Exynos 2400 | Custom | 17 | - | Good |
| Google Tensor G4 | Edge TPU | ~15 | 45 tok/s (3.5B) | Good |

### Apple Neural Engine

**Optimization for ANE:**

```swift
// Best practices for CoreML on ANE
let config = MLModelConfiguration()

// 1. Use all compute units
config.computeUnits = .all

// 2. For specific ANE targeting
config.computeUnits = .cpuAndNeuralEngine  // Skip GPU

// 3. Fixed shapes for better optimization
let constraint = MLImageConstraint(
    pixelsWide: 224,
    pixelsHigh: 224
)
```

**Key Techniques:**
1. **Weight palettization** ‚Äî Best for ANE (1-8 bits)
2. **W8A8 quantization** ‚Äî int8-int8 compute path on A17+
3. **EnumeratedShapes** ‚Äî Fixed inputs run faster
4. **Avoid custom ops** ‚Äî Fall back to CPU

### Qualcomm Hexagon NPU

**QNN Integration:**

```kotlin
// Android: LiteRT with QNN delegate
dependencies {
    implementation("com.qualcomm.qnn:qnn-runtime:2.34.0")
    implementation("com.qualcomm.qnn:qnn-litert-delegate:2.34.0")
}

// Usage
val delegate = QnnDelegate(options)
interpreter.modifyGraphWithDelegate(delegate)
```

**Performance (llama.cpp QNN backend):**
- 7-10x improvement for quantized LLM inference
- Optimized for MULMAT operations

### NPU vs GPU Power Consumption

| Metric | NPU | GPU |
|--------|-----|-----|
| Peak power | 35W | 75W |
| Power savings | 35-70% less | Baseline |
| Latency | Sub-ms | Higher |
| Batch throughput | Lower | Higher |

**When to Use:**
- **NPU:** LLM inference, sustained workloads, battery-critical
- **GPU:** Batch processing, training, graphics-heavy

---

## Mobile-Optimized Models

### LLM Models

| Model | Params | Memory | Performance | Multimodal |
|-------|--------|--------|-------------|------------|
| Gemma 3n E2B | 5B (2B eff) | ~2GB | 1300+ Elo | Yes |
| Gemma 3n E4B | 8B (4B eff) | ~3GB | GPT-4.1-nano | Yes |
| Llama 3.2 1B Q4 | 1B | ~600MB | Basic | No |
| Llama 3.2 3B Q4 | 3B | ~1.5GB | Good | No |
| Phi-3-mini | 3.8B | ~2GB | 69% MMLU | No |
| Qwen3-0.6B | 0.6B | ~400MB | Strong | No |

### Gemma 3n (Google, 2025)

**Architecture: MatFormer**
- Nested smaller models within larger
- Per-Layer Embeddings (PLE)
- Selective parameter activation

**Features:**
- Multimodal: text, image, audio, video
- 140+ languages
- E2B: 2GB RAM, E4B: 3GB RAM

### Llama 3.2 Quantized (Meta, Oct 2024)

**Quantization Techniques:**
1. QAT with LoRA (QLoRA)
2. SpinQuant (post-training)

**Performance:**
| Metric | Improvement |
|--------|-------------|
| Decode speed | 2-4x |
| Prefill speed | 5x |
| Model size | -56% |
| Memory | -41% |

**Device Testing:**
- OnePlus 12: Full support
- Samsung S24+: 1B and 3B
- Samsung S22: 1B only

### Vision Models

| Model | Size | Task | SDK |
|-------|------|------|-----|
| MobileViT | ~6M | Classification | CoreML, TFLite |
| EfficientNet-Lite | 5-25M | Classification | TFLite |
| YOLO Nano/Small | 3-10M | Detection | CoreML, TFLite |
| MobileSAM | ~10M | Segmentation | ExecuTorch |

### Audio Models

| Model | Size | Task | Latency |
|-------|------|------|---------|
| Whisper Tiny | 39M | STT | Real-time |
| Whisper Base | 74M | STT | Real-time |
| OpenWakeWord | ~10M | Wake word | <50ms |

---

## Quantization Techniques

### GGUF Format

**Naming Convention:**
```
Q + [bits] + [type] + [variant]

Q4_0    ‚Äî 4-bit legacy
Q4_K_M  ‚Äî 4-bit K-quant medium (recommended)
Q5_K_M  ‚Äî 5-bit K-quant medium
Q8_0    ‚Äî 8-bit legacy
```

### Recommended Formats

| Device | RAM | Format | Use Case |
|--------|-----|--------|----------|
| iPhone 15 Pro+ | 8GB | Q5_K_M | Quality priority |
| iPhone 13/14 | 6GB | Q4_K_M | General |
| High-end Android | 12GB+ | Q5_K_M/Q8_0 | Quality |
| Mid-range Android | 8GB | Q4_K_M | General |
| Entry-level | 4-6GB | Q4_K_M (small) | Basic |

### Quality vs Size Trade-offs

| Format | Compression | Quality Loss | Recommendation |
|--------|-------------|--------------|----------------|
| FP16 | 1x | 0% | Development |
| Q8_0 | 2x | ~1% | Quality-critical |
| Q6_K | 2.7x | ~2% | Good balance |
| Q5_K_M | 3.2x | ~3% | Mac/iPad sweet spot |
| Q4_K_M | 4x | ~5% | Best for iPhone |
| Q2_K | 8x | ~20%+ | Not recommended |

### K-Quant vs Legacy

**K-Quant Advantages:**
- Adaptive precision by layer importance
- Super blocks reduce memory overhead
- Better quality at same bit width

### Mobile Quantization Best Practices

1. **Start with Q4_K_M** ‚Äî Best balance for mobile
2. **Test on real device** ‚Äî Simulator differs
3. **Measure latency** ‚Äî Not just accuracy
4. **Consider memory** ‚Äî Leave room for app
5. **Profile power** ‚Äî Battery drain matters

---

## Production Patterns

### Hybrid Cloud+Edge Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Request      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Confidence Check   ‚îÇ
‚îÇ  (threshold: 0.8)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ           ‚îÇ
   High        Low
     ‚îÇ           ‚îÇ
     ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇOn-Device‚îÇ ‚îÇ Cloud   ‚îÇ
‚îÇInference‚îÇ ‚îÇ Fallback‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fallback Strategies:**
- Confidence-based: prediction < threshold
- Out-of-domain: input doesn't match training
- Timeout-based: local inference too slow

### Model Caching

**Cache Types:**
| Type | Purpose | Implementation |
|------|---------|----------------|
| Model Cache | Loaded models | Keep warm |
| KV Cache | LLM context | Token chunks |
| Prompt Cache | Repeated queries | Hash lookup |
| Semantic Cache | Similar queries | Embedding |

**KV Cache Optimization:**
- 2 orders of magnitude latency reduction
- Token-level granularity
- LCTRU eviction policy

### Model Preloading

```kotlin
// Android: Background preloading
class MyApplication : Application() {
    override fun onCreate() {
        super.onCreate()
        CoroutineScope(Dispatchers.IO).launch {
            ModelManager.preload("model.tflite")
        }
    }
}
```

```swift
// iOS: Async preloading
class AppDelegate: UIApplicationDelegate {
    func application(...) -> Bool {
        Task.detached {
            try await ModelManager.shared.preload()
        }
        return true
    }
}
```

### OTA Model Updates

**A/B Partition Strategy:**
```
Partition A (active, v1.0)
Partition B (staging, v1.1)
          ‚Üì
     Validation
          ‚Üì
    Success ‚Üí Swap A‚ÜîB
    Failure ‚Üí Rollback
```

**Best Practices:**
1. Staged rollouts: 1% ‚Üí 10% ‚Üí 100%
2. Automatic rollback on failure
3. Version compatibility checks
4. Device capability tracking

### Privacy & Federated Learning

**Industry Adoption:**
- Google: Gboard, "Hey Google"
- Apple: Siri, Photos
- Meta: AI assistants with FL-DP

**Privacy Techniques:**
| Technique | Protection |
|-----------|------------|
| Differential Privacy | Individual data |
| Secure Aggregation | Model updates |
| Local-only Inference | No data transfer |

### Observability

**Key Metrics:**
| Category | Metrics |
|----------|---------|
| Performance | Latency, throughput |
| Accuracy | Confidence, corrections |
| Resource | Memory, battery, CPU/GPU |
| Health | Drift, staleness, version |

**Tools:**
- Arize AI: LLM observability
- Dynatrace: A/B testing
- Custom: OpenTelemetry integration

---

## Platform Integration

### Android (Kotlin)

```kotlin
// LiteRT setup
class AIService(context: Context) {
    private val interpreter: Interpreter

    init {
        val options = Interpreter.Options().apply {
            addDelegate(GpuDelegate())
            setNumThreads(4)
        }
        interpreter = Interpreter(loadModel(context), options)
    }

    fun predict(input: FloatArray): FloatArray {
        val output = FloatArray(outputSize)
        interpreter.run(input, output)
        return output
    }
}
```

### iOS (Swift)

```swift
// CoreML setup
class AIService {
    private var model: MLModel?

    func load() async throws {
        let config = MLModelConfiguration()
        config.computeUnits = .all
        model = try await MyModel.load(configuration: config)
    }

    func predict(input: MLFeatureProvider) throws -> MLFeatureProvider {
        return try model!.prediction(from: input)
    }
}
```

### Flutter

```dart
// TensorFlow Lite Flutter
import 'package:tflite_flutter/tflite_flutter.dart';

class AIService {
  late Interpreter _interpreter;

  Future<void> load() async {
    _interpreter = await Interpreter.fromAsset('model.tflite');
  }

  List<double> predict(List<double> input) {
    var output = List<double>.filled(outputSize, 0);
    _interpreter.run(input, output);
    return output;
  }
}
```

### React Native

```javascript
// react-native-fast-tflite
import { loadModel } from 'react-native-fast-tflite';

const model = await loadModel('model.tflite');
const output = model.run([inputData]);
```

### Kotlin Multiplatform

```kotlin
// Shared code with expect/actual
expect class AIService() {
    suspend fun load()
    fun predict(input: FloatArray): FloatArray
}

// Android actual
actual class AIService {
    private lateinit var interpreter: Interpreter
    // TFLite implementation
}

// iOS actual
actual class AIService {
    private var model: MLModel? = null
    // CoreML implementation
}
```

---

## Decision Guide

### SDK Selection Flowchart

```
Need mobile AI?
‚îú‚îÄ‚îÄ iOS only?
‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí CoreML (best ANE)
‚îÇ
‚îú‚îÄ‚îÄ Android only?
‚îÇ   ‚îî‚îÄ‚îÄ YES ‚Üí LiteRT (best ecosystem)
‚îÇ
‚îú‚îÄ‚îÄ Cross-platform?
‚îÇ   ‚îú‚îÄ‚îÄ PyTorch source? ‚Üí ExecuTorch
‚îÇ   ‚îú‚îÄ‚îÄ Max compatibility? ‚Üí ONNX Runtime
‚îÇ   ‚îî‚îÄ‚îÄ Pre-built tasks? ‚Üí MediaPipe
‚îÇ
‚îî‚îÄ‚îÄ LLM specifically?
    ‚îú‚îÄ‚îÄ Production? ‚Üí ExecuTorch/CoreML
    ‚îî‚îÄ‚îÄ Prototype? ‚Üí MediaPipe LLM API
```

### Model Selection by Device

| Device Class | Max Memory | Recommended Model |
|--------------|------------|-------------------|
| iPhone 15 Pro+ | 4-6GB | Gemma 3n E4B, Phi-3 |
| iPhone 13/14 | 2-3GB | Gemma 3n E2B, Llama 3.2 3B |
| Flagship Android | 6-8GB | Most models |
| Mid-range Android | 3-4GB | Llama 3.2 3B, Qwen3-1.7B |
| Entry Android | 1-2GB | Qwen3-0.6B, Llama 3.2 1B |

### Quantization Selection

```
Quality priority?
‚îú‚îÄ‚îÄ YES ‚Üí Q8_0 or Q5_K_M
‚îî‚îÄ‚îÄ NO (size/speed priority)
    ‚îú‚îÄ‚îÄ iPhone ‚Üí Q4_K_M
    ‚îú‚îÄ‚îÄ High-end Android ‚Üí Q5_K_M
    ‚îî‚îÄ‚îÄ Mid-range ‚Üí Q4_K_M
```

### Hardware Acceleration Selection

```
Model type?
‚îú‚îÄ‚îÄ Quantized (INT8/4)
‚îÇ   ‚îî‚îÄ‚îÄ NPU (best efficiency)
‚îÇ
‚îú‚îÄ‚îÄ Float32
‚îÇ   ‚îî‚îÄ‚îÄ GPU or XNNPACK
‚îÇ
‚îú‚îÄ‚îÄ LLM
‚îÇ   ‚îî‚îÄ‚îÄ NPU (sustained low power)
‚îÇ
‚îî‚îÄ‚îÄ Batch processing
    ‚îî‚îÄ‚îÄ GPU (throughput)
```

---

## References

### Official Documentation

- [LiteRT (TensorFlow Lite)](https://ai.google.dev/edge/litert)
- [CoreML](https://developer.apple.com/documentation/coreml)
- [ONNX Runtime Mobile](https://onnxruntime.ai/docs/tutorials/mobile/)
- [MediaPipe](https://ai.google.dev/edge/mediapipe)
- [ExecuTorch](https://pytorch.org/executorch/)

### Research Papers

- [Gemma 3n Architecture](https://ai.google.dev/gemma/docs/gemma-3n)
- [Llama 3.2 Quantization](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- [On-Device Language Models Survey](https://arxiv.org/html/2409.00088v1)

### Research Reports (This Guide)

- [Mobile AI SDKs Research](../docs/research/2025-12-29-mobile-ai-sdks.md)
- [NPU Hardware Acceleration](../docs/research/2025-12-29-mobile-npu-acceleration.md)
- [Mobile Models & Quantization](../docs/research/2025-12-29-mobile-models-quantization.md)
- [Production Mobile AI Patterns](../docs/research/2025-12-29-mobile-ai-production.md)

---

*Last updated: 2025-12-29*
*Research depth: 100+ sources across 4 deep research sessions*

---

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
