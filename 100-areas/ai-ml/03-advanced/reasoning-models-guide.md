---
title: "Reasoning Models: o1, o3, DeepSeek R1, Claude Extended Thinking"
created: 2025-12-24
updated: 2025-12-24
author: AI Assistant
level: advanced
type: guide
topics:
  - reasoning
  - o1
  - o3
  - deepseek-r1
  - chain-of-thought
  - extended-thinking
  - test-time-compute
  - GRPO
  - QwQ
  - Gemini
status: published
tags:
  - topic/ai-ml
  - type/guide
  - level/intermediate
related:
  - "[[llm-fundamentals]]"
  - "[[models-landscape-2025]]"
  - "[[ai-api-integration]]"
---

# Reasoning Models: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò

---

## Prerequisites

| –¢–µ–º–∞ | –ó–∞—á–µ–º –Ω—É–∂–Ω–æ | –ì–¥–µ –∏–∑—É—á–∏—Ç—å |
|------|-------------|-------------|
| **–ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ LLM** | –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ | [[llm-fundamentals]] |
| **Chain-of-Thought** | –û—Å–Ω–æ–≤–∞ reasoning | [[prompt-engineering-masterclass]] |
| **API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** | –†–∞–±–æ—Ç–∞ —Å o1, R1, Claude | [[ai-api-integration]] |

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª

| –£—Ä–æ–≤–µ–Ω—å | –ü–æ–¥—Ö–æ–¥–∏—Ç? | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-----------|--------------|
| **–ù–æ–≤–∏—á–æ–∫ –≤ AI** | ‚úÖ –î–∞ | –ü–æ–Ω—è—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã |
| **AI Engineer** | ‚úÖ –î–∞ | –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ reasoning –º–æ–¥–µ–ª–µ–π |
| **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å** | ‚úÖ –î–∞ | –ì–ª—É–±–æ–∫–∏–π —Ä–∞–∑–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä |
| **Tech Lead** | ‚úÖ –î–∞ | –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∑–∞–¥–∞—á—É |

### –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤

> üí° **Reasoning Model** = AI, –∫–æ—Ç–æ—Ä—ã–π "–¥—É–º–∞–µ—Ç" –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º, –∞ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ

| –¢–µ—Ä–º–∏–Ω | –ó–Ω–∞—á–µ–Ω–∏–µ | –ê–Ω–∞–ª–æ–≥–∏—è –¥–ª—è –Ω–æ–≤–∏—á–∫–∞ |
|--------|----------|---------------------|
| **Reasoning** | –ü–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º | **–î—É–º–∞—Ç—å –≤—Å–ª—É—Ö** ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —Ö–æ–¥ –º—ã—Å–ª–µ–π, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç |
| **Chain-of-Thought (CoT)** | –¶–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π | **–ß–µ—Ä–Ω–æ–≤–∏–∫ —Ä–µ—à–µ–Ω–∏—è** ‚Äî –∑–∞–ø–∏—Å–∞—Ç—å —à–∞–≥–∏, –ø–æ—Ç–æ–º –¥–∞—Ç—å –æ—Ç–≤–µ—Ç |
| **Test-Time Compute** | –í—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ | **–í—Ä–µ–º—è –Ω–∞ —Ä–∞–∑–¥—É–º—å—è** ‚Äî —á–µ–º –¥–æ–ª—å—à–µ –¥—É–º–∞–µ—Ç, —Ç–µ–º –ª—É—á—à–µ –æ—Ç–≤–µ—Ç |
| **Thinking Tokens** | –¢–æ–∫–µ–Ω—ã "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è" –º–æ–¥–µ–ª–∏ | **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–æ–Ω–æ–ª–æ–≥** ‚Äî –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç —Å–∞–º–∞ —Å —Å–æ–±–æ–π |
| **Extended Thinking** | –†–µ–∂–∏–º –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è (Claude) | **–†–µ–∂–∏–º —ç–∫—Å–ø–µ—Ä—Ç–∞** ‚Äî –º–æ–¥–µ–ª—å —Ç—Ä–∞—Ç–∏—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ |
| **GRPO** | Group Relative Policy Optimization (DeepSeek) | **–û–±—É—á–µ–Ω–∏–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º** ‚Äî –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è, —Å—Ä–∞–≤–Ω–∏–≤–∞—è —Ä–∞–∑–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è |
| **Self-Correction** | –ú–æ–¥–µ–ª—å –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Å–µ–±—è | **–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞** ‚Äî –ø–µ—Ä–µ—á–∏—Ç–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫–∏ |
| **Knowledge Cutoff** | –î–∞—Ç–∞, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–µ—Ç —Å–æ–±—ã—Ç–∏–π | **–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏** ‚Äî –º–æ–¥–µ–ª—å –Ω–µ —á–∏—Ç–∞–ª–∞ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ—Å–ª–µ —ç—Ç–æ–π –¥–∞—Ç—ã |

---

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ü—Ä–æ–ª–æ–≥: –î–µ–∫–∞–±—Ä—å 2024](#–ø—Ä–æ–ª–æ–≥-–¥–µ–∫–∞–±—Ä—å-2024---–º–æ–º–µ–Ω—Ç-–∫–æ–≥–¥–∞-–≤—Å—ë-–∏–∑–º–µ–Ω–∏–ª–æ—Å—å)
2. [–ß—Ç–æ —Ç–∞–∫–æ–µ Reasoning](#—á—Ç–æ-—Ç–∞–∫–æ–µ-reasoning-–∫–æ–≥–¥–∞-–º–æ–¥–µ–ª—å-–¥—É–º–∞–µ—Ç-–ø–µ—Ä–µ–¥-–æ—Ç–≤–µ—Ç–æ–º)
3. [–ú–µ—Ö–∞–Ω–∏–∫–∞ —Ä–∞–±–æ—Ç—ã](#–º–µ—Ö–∞–Ω–∏–∫–∞-—Ä–∞–±–æ—Ç—ã-–∫–∞–∫-—É—Å—Ç—Ä–æ–µ–Ω—ã-reasoning-models)
4. [–û—Å–Ω–æ–≤–Ω—ã–µ –∏–≥—Ä–æ–∫–∏](#–æ—Å–Ω–æ–≤–Ω—ã–µ-–∏–≥—Ä–æ–∫–∏-reasoning-—Ä—ã–Ω–∫–∞)
5. [–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑](#—Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π-–∞–Ω–∞–ª–∏–∑-–º–æ–¥–µ–ª–µ–π)
6. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ](#–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
7. [Prompt Engineering –¥–ª—è Reasoning](#prompting-–¥–ª—è-reasoning-models-–º–µ–Ω—å—à–µ-–∑–Ω–∞—á–∏—Ç-–ª—É—á—à–µ)
8. [–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã](#–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è-–∏-–ø—Ä–æ–±–ª–µ–º—ã-reasoning-–º–æ–¥–µ–ª–µ–π)
9. [–ë—É–¥—É—â–µ–µ Reasoning](#–±—É–¥—É—â–µ–µ-reasoning-–º–æ–¥–µ–ª–µ–π)

---

## –ü—Ä–æ–ª–æ–≥: –î–µ–∫–∞–±—Ä—å 2024 - –ú–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –≤—Å–µ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å

–î–µ–∫–∞–±—Ä—å 2024 –≥–æ–¥–∞ –≤–æ–π–¥–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é –ò–ò –∫–∞–∫ –º–µ—Å—è—Ü, –∫–æ–≥–¥–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—è –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –Ω–∞–≤—Å–µ–≥–¥–∞.

–î–æ —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –≤—Å–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É: –ø–æ–ª—É—á–∏ –≤–æ–ø—Ä–æ—Å - –≤—ã–¥–∞–π –æ—Ç–≤–µ—Ç. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ. –ö–∞–∫ —Ä–µ—Ñ–ª–µ–∫—Å. GPT-4, Claude, Gemini - –≤—Å–µ –æ–Ω–∏ –±—ã–ª–∏ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ —É–º–Ω—ã, –Ω–æ –¥—É–º–∞–ª–∏ "–Ω–∞ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–µ". –°–ø—Ä–æ—Å–∏—Ç–µ –∏—Ö —Å–ª–æ–∂–Ω—É—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É, –∏ –æ–Ω–∏ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ –≤—ã–¥–∞–¥—É—Ç –æ—Ç–≤–µ—Ç. –ò–Ω–æ–≥–¥–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π. –ß–∞—Å—Ç–æ - –Ω–µ—Ç. –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∏ –Ω–µ **–¥—É–º–∞–ª–∏** –Ω–∞–¥ –∑–∞–¥–∞—á–µ–π - –æ–Ω–∏ **—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏** –Ω–∞ –Ω–µ–µ.

–ê –ø–æ—Ç–æ–º –ø–æ—è–≤–∏–ª—Å—è o1.

–ü–µ—Ä–≤—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä —Å o1 –±—ã–ª —Å—Ç—Ä–∞–Ω–Ω—ã–º. –¢—ã –∑–∞–¥–∞–µ—à—å –≤–æ–ø—Ä–æ—Å... –∏ –∂–¥–µ—à—å. 5 —Å–µ–∫—É–Ω–¥. 10 —Å–µ–∫—É–Ω–¥. 30 —Å–µ–∫—É–Ω–¥. –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? –ú–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–ª–∞? –ù–µ—Ç. –û–Ω–∞ **–¥—É–º–∞–µ—Ç**. –í–ø–µ—Ä–≤—ã–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –ò–ò –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∞—Å—å –∏ –∑–∞–¥—É–º–∞–ª–∞—Å—å –ø–µ—Ä–µ–¥ —Ç–µ–º, –∫–∞–∫ –æ—Ç–≤–µ—á–∞—Ç—å.

–ò –∫–æ–≥–¥–∞ –æ—Ç–≤–µ—Ç –ø—Ä–∏—à–µ–ª - –æ–Ω –±—ã–ª –¥—Ä—É–≥–∏–º. –ù–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º, –∞ **–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º**. –ú–æ–¥–µ–ª—å –Ω–µ —É–≥–∞–¥–∞–ª–∞ –æ—Ç–≤–µ—Ç - –æ–Ω–∞ –µ–≥–æ –≤—ã–≤–µ–ª–∞. –®–∞–≥ –∑–∞ —à–∞–≥–æ–º. –ö–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –±–µ—Ä–µ—Ç —Ä—É—á–∫—É –∏ –±—É–º–∞–≥—É, —á—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É.

**–•—Ä–æ–Ω–æ–ª–æ–≥–∏—è —Å–æ–±—ã—Ç–∏–π –¥–µ–∫–∞–±—Ä—è 2024:**

- **12 –¥–µ–∫–∞–±—Ä—è 2024** - OpenAI –≤—ã–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é o1
- **19 –¥–µ–∫–∞–±—Ä—è 2024** - Google –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç Gemini 2.0 Flash Thinking
- **20 –¥–µ–∫–∞–±—Ä—è 2024** - OpenAI –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç o3 –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å "12 Days of Shipmas"
- **–Ø–Ω–≤–∞—Ä—å 2025** - DeepSeek –≤—ã–ø—É—Å–∫–∞–µ—Ç R1 –ø–æ–¥ MIT –ª–∏—Ü–µ–Ω–∑–∏–µ–π, –ø–æ—Ç—Ä—è—Å–∞—è –∏–Ω–¥—É—Å—Ç—Ä–∏—é

–≠—Ç–æ –±—ã–ª–∞ —Ä–µ–≤–æ–ª—é—Ü–∏—è. –¢–∏—Ö–∞—è, –Ω–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è.

---

## –ß—Ç–æ —Ç–∞–∫–æ–µ Reasoning: –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥—É–º–∞–µ—Ç –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º

### –ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ

–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –¥–≤—É—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.

**–ü–µ—Ä–≤—ã–π —Å—Ç—É–¥–µ–Ω—Ç** (–æ–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å) —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –∑–∞–¥–∞—á—É –∏ —Å—Ä–∞–∑—É –ø–∏—à–µ—Ç –æ—Ç–≤–µ—Ç. –û–Ω –≤–∏–¥–µ–ª —Ç—ã—Å—è—á–∏ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á –∏ "—á—É–≤—Å—Ç–≤—É–µ—Ç", –∫–∞–∫–∏–º –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–≤–µ—Ç. –ò–Ω–æ–≥–¥–∞ —É–≥–∞–¥—ã–≤–∞–µ—Ç. –ò–Ω–æ–≥–¥–∞ –Ω–µ—Ç. –ù–æ –æ–Ω –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ–±—ã **–ø–æ–¥—É–º–∞—Ç—å**.

**–í—Ç–æ—Ä–æ–π —Å—Ç—É–¥–µ–Ω—Ç** (reasoning –º–æ–¥–µ–ª—å) —á–∏—Ç–∞–µ—Ç –∑–∞–¥–∞—á—É, –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ä—É—á–∫—É –∏ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å. "–¢–∞–∫, —á—Ç–æ —Ç—É—Ç —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç? –ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —É –º–µ–Ω—è –µ—Å—Ç—å? –ö–∞–∫–æ–π –º–µ—Ç–æ–¥ –ø–æ–¥–æ–π–¥–µ—Ç? –î–∞–≤–∞–π –ø–æ–ø—Ä–æ–±—É—é... –Ω–µ—Ç, —ç—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ê –µ—Å–ª–∏ —Ç–∞–∫? –î–∞, —ç—Ç–æ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª. –ü—Ä–æ–≤–µ—Ä—é –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ... –°—Ö–æ–¥–∏—Ç—Å—è. –û–∫–µ–π, —Ç–µ–ø–µ—Ä—å –º–æ–≥—É –∑–∞–ø–∏—Å–∞—Ç—å –æ—Ç–≤–µ—Ç."

–†–∞–∑–Ω–∏—Ü–∞ –∫–æ–ª–æ—Å—Å–∞–ª—å–Ω–∞—è. –ü–µ—Ä–≤—ã–π —Å—Ç—É–¥–µ–Ω—Ç –±—ã—Å—Ç—Ä–µ–µ. –í—Ç–æ—Ä–æ–π - —Ç–æ—á–Ω–µ–µ.

### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ

Reasoning models - —ç—Ç–æ **–Ω–æ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π**, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–∞ –º–µ–Ω—å—à–∏–µ, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —à–∞–≥–∏ –∏ —Ä–µ—à–µ–Ω–∏—è –∏—Ö —á–µ—Ä–µ–∑ —è–≤–Ω–æ–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ (—Ç–∞–∫–∂–µ –Ω–∞–∑—ã–≤–∞–µ–º–æ–µ "thinking").

–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—ã—á–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ø—Ä—è–º—ã–µ –æ—Ç–≤–µ—Ç—ã, reasoning –º–æ–¥–µ–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω—ã **–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ—é —Ä–∞–±–æ—Ç—É** –∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É.

### –ê–Ω–∞–ª–æ–≥–∏—è: –°–∏—Å—Ç–µ–º–∞ 1 –∏ –°–∏—Å—Ç–µ–º–∞ 2

–ü—Å–∏—Ö–æ–ª–æ–≥ –î–∞–Ω–∏—ç–ª—å –ö–∞–Ω–µ–º–∞–Ω –æ–ø–∏—Å–∞–ª –¥–≤–∞ —Ç–∏–ø–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è:

- **–°–∏—Å—Ç–µ–º–∞ 1**: –ë—ã—Å—Ç—Ä–æ–µ, –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ. "–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?" - "4" (–º–≥–Ω–æ–≤–µ–Ω–Ω–æ).
- **–°–∏—Å—Ç–µ–º–∞ 2**: –ú–µ–¥–ª–µ–Ω–Ω–æ–µ, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ, –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ. "–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 127x89?" - –Ω—É–∂–Ω–æ —Å–µ—Å—Ç—å –∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å.

–û–±—ã—á–Ω—ã–µ LLM - —ç—Ç–æ –°–∏—Å—Ç–µ–º–∞ 1. –û–Ω–∏ —Ä–µ–∞–≥–∏—Ä—É—é—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

Reasoning models - —ç—Ç–æ –°–∏—Å—Ç–µ–º–∞ 2. –û–Ω–∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç, —Ä–∞—Å—Å—É–∂–¥–∞—é—Ç –∏ —Ç–æ–ª—å–∫–æ –ø–æ—Ç–æ–º –æ—Ç–≤–µ—á–∞—é—Ç.

```
–û–ë–´–ß–ù–ê–Ø –ú–û–î–ï–õ–¨ (System 1):
+------------------+        +------------------+
|  "127 x 89 = ?" | -----> |     "11293"      |  <- –ë—ã—Å—Ç—Ä–æ, –Ω–æ –æ—à–∏–±–∫–∞!
+------------------+        +------------------+
       Input            –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π output

REASONING MODEL (System 2):
+------------------+        +--------------------------------+        +-----------+
|  "127 x 89 = ?" | -----> | "–¢–∞–∫, 127 x 89...              | -----> |  "11303"  |
+------------------+        | –ú–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å: 127 x (90-1)    |        +-----------+
       Input               | = 127 x 90 - 127               |         –ü—Ä–∞–≤–∏–ª—å–Ω–æ!
                           | = 11430 - 127                  |
                           | = 11303. –ü—Ä–æ–≤–µ—Ä—é: ..."         |
                           +--------------------------------+
                                –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
                                 (thinking tokens)
```

---

## –ú–µ—Ö–∞–Ω–∏–∫–∞ —Ä–∞–±–æ—Ç—ã: –ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω—ã Reasoning Models

### Test-Time Compute: –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è

**Test-Time Compute (TTC)** - —ç—Ç–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –≤–æ –≤—Ä–µ–º—è inference (–∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç), –∞ –Ω–µ –≤–æ –≤—Ä–µ–º—è training (–∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è).

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π: —É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏, –¥–∞–Ω–Ω—ã–µ, compute –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –≠—Ç–æ –¥–æ—Ä–æ–≥–æ –∏ –∏–º–µ–µ—Ç –ø—Ä–µ–¥–µ–ª—ã.

–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥: –ø–æ–∑–≤–æ–ª—å—Ç–µ –º–æ–¥–µ–ª–∏ **–¥—É–º–∞—Ç—å –¥–æ–ª—å—à–µ** –Ω–∞–¥ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –¢—Ä–∞—Ç—å—Ç–µ –±–æ–ª—å—à–µ compute —Ç–∞–º, –≥–¥–µ –Ω—É–∂–Ω–æ.

**–ö–ª—é—á–µ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Google DeepMind** (–∞–≤–≥—É—Å—Ç 2024):
> "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏:
- –ù–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö test-time compute –º–æ–∂–µ—Ç –¥–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç, —Å—Ä–∞–≤–Ω–∏–º—ã–π —Å 14x —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏
- Compute-optimal —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ 4x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å best-of-N baseline
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å TTC –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏

### –¢–∏–ø—ã Test-Time Scaling

```
+------------------------------------------------------------------+
|                    –¢–ò–ü–´ TEST-TIME SCALING                         |
+------------------------------------------------------------------+
|                                                                    |
| PARALLEL SCALING:                                                  |
| - –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ                        |
| - –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ majority voting –∏–ª–∏ reward model               |
| - –ü—Ä–∏–º–µ—Ä: Best-of-N sampling                                       |
|                                                                    |
| SEQUENTIAL SCALING:                                                |
| - –ú–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç —à–∞–≥ –∑–∞ —à–∞–≥–æ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ                  |
| - –ö–∞–∂–¥—ã–π —à–∞–≥ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ                               |
| - –ü—Ä–∏–º–µ—Ä: Chain of Thought reasoning                               |
|                                                                    |
| HYBRID SCALING:                                                    |
| - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ                    |
| - –ü—Ä–∏–º–µ—Ä: Tree of Thoughts                                         |
|                                                                    |
| INTERNAL SCALING:                                                   |
| - –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∫–æ–ª—å–∫–æ compute –≤—ã–¥–µ–ª–∏—Ç—å                 |
| - –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤–Ω—É—Ç—Ä–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏                           |
| - –ü—Ä–∏–º–µ—Ä: Claude Extended Thinking —Å dynamic budget                |
|                                                                    |
+------------------------------------------------------------------+
```

### Reasoning Tokens: –ö–∞–∫ –º–æ–¥–µ–ª—å "–¥—É–º–∞–µ—Ç"

**Reasoning tokens** (thinking tokens) - —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ö–∞–Ω–∏–∑–º, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –¥–ª—è reasoning –º–æ–¥–µ–ª–µ–π.

**–ö–∞–∫ –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç:**

1. –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç input prompt
2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç **reasoning tokens** - –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
3. –≠—Ç–∏ —Ç–æ–∫–µ–Ω—ã "—Ä–∞–∑–±–∏–≤–∞—é—Ç" –∑–∞–¥–∞—á—É –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
4. –û—Ü–µ–Ω–∏–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ—à–µ–Ω–∏—é
5. –ò–º–∏—Ç–∏—Ä—É—é—Ç –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —á–µ–ª–æ–≤–µ–∫–∞
6. –ì–µ–Ω–µ—Ä–∏—Ä—É—é—Ç **output tokens** - —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
7. Reasoning tokens –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–Ω–æ –æ–ø–ª–∞—á–∏–≤–∞—é—Ç—Å—è!)

**–í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å:**
- Reasoning tokens –∑–∞–Ω–∏–º–∞—é—Ç –º–µ—Å—Ç–æ –≤ context window
- –û–Ω–∏ –æ–ø–ª–∞—á–∏–≤–∞—é—Ç—Å—è –∫–∞–∫ output tokens
- –û—Ç —Å–æ—Ç–µ–Ω –¥–æ –¥–µ—Å—è—Ç–∫–æ–≤ —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
- OpenAI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ summary, –Ω–µ —Å—ã—Ä–æ–π reasoning

### –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π o1

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (arxiv 2410.13639) –≤—ã—è–≤–∏–ª–æ 6 —Ç–∏–ø–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π o1:

| –ü–∞—Ç—Ç–µ—Ä–Ω | –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ | –û–ø–∏—Å–∞–Ω–∏–µ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è |
|---------|--------------|----------|---------------------|
| **Systematic Analysis** | SA | –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ | –û–±—â–∏–π –ø–æ–¥—Ö–æ–¥ |
| **Method Reuse** | MR | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ | –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –∫–æ–¥ |
| **Divide and Conquer** | DC | –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏ | –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ |
| **Self-Refinement** | SR | –°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ | –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—à–µ–Ω–∏—è |
| **Context Identification** | CI | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–¥–∞—á–∏ | Commonsense |
| **Emphasizing Constraints** | EC | –§–æ–∫—É—Å –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö | –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ |

**–ö–ª—é—á–µ–≤–æ–π –≤—ã–≤–æ–¥**: DC (Divide and Conquer) –∏ SR (Self-Refinement) - —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–ª—é—á–æ–º –∫ —É—Å–ø–µ—Ö—É o1.

### –û–±—É—á–µ–Ω–∏–µ Reasoning –º–æ–¥–µ–ª–µ–π

#### Reinforcement Learning with Verifiable Rewards (RLVR)

RLVR - –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö reasoning –º–æ–¥–µ–ª–µ–π:

```
+------------------------------------------------------------------+
|                           RLVR PIPELINE                           |
+------------------------------------------------------------------+
|                                                                    |
| 1. BASE MODEL                                                      |
|    - Pretrained LLM (GPT, Llama, DeepSeek-V3)                     |
|                                                                    |
| 2. REWARD SIGNAL                                                   |
|    - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π/–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (verifiable)                   |
|    - –ù–µ —Ç—Ä–µ–±—É–µ—Ç reward model –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª—É—á–∞–µ–≤                  |
|    - –î–ª—è DeepSeek R1: –±—É–∫–≤–∞–ª—å–Ω–æ regex matching!                   |
|                                                                    |
| 3. RL OPTIMIZATION                                                 |
|    - PPO (Proximal Policy Optimization) - OpenAI –ø–æ–¥—Ö–æ–¥           |
|    - GRPO (Group Relative Policy Optimization) - DeepSeek –ø–æ–¥—Ö–æ–¥  |
|                                                                    |
| 4. EMERGENT REASONING                                              |
|    - –ú–æ–¥–µ–ª—å —Å–∞–º–∞ "–∏–∑–æ–±—Ä–µ—Ç–∞–µ—Ç" —Å–ø–æ—Å–æ–±—ã —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å                  |
|    - Self-verification, reflection, long CoT –ø–æ—è–≤–ª—è—é—Ç—Å—è —Å–∞–º–∏      |
|                                                                    |
+------------------------------------------------------------------+
```

#### GRPO: –ü—Ä–æ—Ä—ã–≤ DeepSeek

**Group Relative Policy Optimization (GRPO)** - –∞–ª–≥–æ—Ä–∏—Ç–º RL, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤ DeepSeekMath –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è R1.

**–û—Ç–ª–∏—á–∏—è –æ—Ç PPO:**

| –ê—Å–ø–µ–∫—Ç | PPO | GRPO |
|--------|-----|------|
| Value Function | –¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å | –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è |
| Advantage Calculation | –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π | –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≥—Ä—É–ø–ø—ã |
| Memory Usage | –í—ã—Å–æ–∫–∏–π | –ù–∞ ~50% –º–µ–Ω—å—à–µ |
| KL Divergence | –í reward | –í loss function |
| Compute Requirements | –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π | –ù–∞ ~50% –º–µ–Ω—å—à–µ |

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã DeepSeekMath —Å GRPO:**
- GSM8K: 82.9% -> 88.2% (+5.3%)
- MATH: 46.8% -> 51.7% (+4.9%)
- CMATH: 84.6% -> 88.8% (+4.2%)

#### Process Reward Models vs Outcome Reward Models

**Outcome Reward Models (ORM):**
- –û—Ü–µ–Ω–∏–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
- –ü—Ä–æ—â–µ –≤ –æ–±—É—á–µ–Ω–∏–∏
- –ú–æ–≥—É—Ç –Ω–∞–≥—Ä–∞–∂–¥–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π reasoning –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç

**Process Reward Models (PRM):**
- –û—Ü–µ–Ω–∏–≤–∞—é—Ç –∫–∞–∂–¥—ã–π —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
- –°–ª–æ–∂–Ω–µ–µ –≤ –æ–±—É—á–µ–Ω–∏–∏ (–Ω—É–∂–Ω–∞ —Ä–∞–∑–º–µ—Ç–∫–∞ —à–∞–≥–æ–≤)
- –õ—É—á—à–µ alignment: –Ω–∞–≥—Ä–∞–∂–¥–∞—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å

**OpenAI Research (2023):**
> "Process supervision can train much more reliable reward models than outcome supervision"

–†–µ–∑—É–ª—å—Ç–∞—Ç: 78.2% –Ω–∞ MATH test set —Å PRM vs –º–µ–Ω–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å ORM.

---

## –û—Å–Ω–æ–≤–Ω—ã–µ –∏–≥—Ä–æ–∫–∏ Reasoning —Ä—ã–Ω–∫–∞

### OpenAI o-series

#### o1 (–°–µ–Ω—Ç—è–±—Ä—å-–î–µ–∫–∞–±—Ä—å 2024)

**o1-preview** (—Å–µ–Ω—Ç—è–±—Ä—å 2024) - –ø–µ—Ä–≤–∞—è –ø—É–±–ª–∏—á–Ω–∞—è reasoning –º–æ–¥–µ–ª—å.

**o1** (–¥–µ–∫–∞–±—Ä—å 2024) - –ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è:
- AIME 2024: 74.3%
- Codeforces: 1891 ELO
- SWE-Bench Verified: 48.9%

**–ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- –°–∫—Ä—ã—Ç—ã–π chain of thought (—Ç–æ–ª—å–∫–æ summary)
- –ù–µ—Ç system prompt –ø–æ–¥–¥–µ—Ä–∂–∫–∏
- –ù–µ—Ç streaming
- –ù–µ—Ç vision
- –í—ã—Å–æ–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: $15/M input, $60/M output

#### o3 (–î–µ–∫–∞–±—Ä—å 2024 –∞–Ω–æ–Ω—Å, –ê–ø—Ä–µ–ª—å 2025 —Ä–µ–ª–∏–∑)

–ê–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–Ω 20 –¥–µ–∫–∞–±—Ä—è 2024 –∫–∞–∫ —Ñ–∏–Ω–∞–ª "12 Days of Shipmas".

**–ü–æ—á–µ–º—É o3, –∞ –Ω–µ o2?**
> Sam Altman: "–†–µ—à–µ–Ω–∏–µ –±—ã–ª–æ —á–∏—Å—Ç–æ –∏–∑-–∑–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å —Ç–æ–≤–∞—Ä–Ω—ã–º –∑–Ω–∞–∫–æ–º O2 (–º–æ–±–∏–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä)"

**Benchmark —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
- AIME 2024: 91.6% (vs o1: 74.3%)
- AIME 2025: 88.9%
- Codeforces: 2706 ELO (vs o1: 1891)
- SWE-Bench Verified: 69.1% (vs o1: 48.9%)
- ARC-AGI: 3x —Ç–æ—á–Ω–µ–µ —á–µ–º o1
- EpochAI Frontier Math: 25.2% (–¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ < 2%)

**–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (–∞–ø—Ä–µ–ª—å 2025):**
- Vision (–∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
- Tool use (–≤–µ–±-–ø–æ–∏—Å–∫, Python, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
- Streaming
- Deliberative Alignment (—Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ safety)
- Self-fact-checking

**–í–∞–∂–Ω–æ–µ:**
> "O3 and o4-mini are both smarter AND cheaper than o1 and o3-mini respectively for most real-world usage"

#### o3-mini

–ë—ã—Å—Ç—Ä–∞—è, —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º reasoning effort:

```python
# –ö–æ–Ω—Ç—Ä–æ–ª—å –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
reasoning_effort = "low"     # –ë—ã—Å—Ç—Ä–æ, –¥–µ—à–µ–≤–æ
reasoning_effort = "medium"  # –ë–∞–ª–∞–Ω—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
reasoning_effort = "high"    # –ú–∞–∫—Å–∏–º—É–º –∫–∞—á–µ—Å—Ç–≤–∞
```

**–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å o3-mini-high:**
- MATH-500: 96.7% (vs o1: 83.3%)

#### o4-mini (–ê–ø—Ä–µ–ª—å 2025)

–°–ª–µ–¥—É—é—â–µ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ o3-mini —Å –µ—â–µ –ª—É—á—à–∏–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ.

### Anthropic Claude Extended Thinking

#### –§–∏–ª–æ—Å–æ—Ñ–∏—è: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥

> "We've developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market. Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely."

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** –û–¥–Ω–∞ –º–æ–¥–µ–ª—å, –¥–≤–∞ —Ä–µ–∂–∏–º–∞ - –Ω–µ –Ω—É–∂–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏.

#### Claude 3.7 Sonnet (–§–µ–≤—Ä–∞–ª—å 2025)

**–ü–µ—Ä–≤–∞—è hybrid reasoning –º–æ–¥–µ–ª—å:**
- Standard mode: —É–ª—É—á—à–µ–Ω–Ω—ã–π Claude 3.5 Sonnet
- Extended thinking mode: –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑

**Benchmarks —Å Extended Thinking:**
- SWE-bench Verified: 63.7% (–±–µ–∑ scaffolding), 70.3% (—Å enhanced TTC)
- GPQA Physics: 96.5%
- –û–±—â–∏–π reasoning: 68% -> 84.8% —Å extended thinking

**Thinking Budget:**
```python
thinking = {
    "type": "enabled",
    "budget_tokens": 10000  # –û—Ç 1,024 –¥–æ 128,000
}
```

**–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ:**
> "Performance on mathematics improves logarithmically as thinking tokens increase"

#### Claude 4.5 Sonnet –∏ Opus 4.5 (2025)

–ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º extended thinking:
- –ò—Å–ø–æ–ª—å–∑—É—é—Ç –¥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏ –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –∏–ª–∏ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- "Less backtracking, less redundant exploration, less verbose reasoning"

#### –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Claude

**Visible Thinking:**
- –ü–æ–ª–Ω—ã–π chain of thought –≤–∏–¥–µ–Ω (–Ω–µ summary –∫–∞–∫ —É o1)
- –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑—É—á–∞—Ç—å reasoning –º–æ–¥–µ–ª–∏
- –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è alignment research

**Interleaved Thinking:**
- Claude –¥—É–º–∞–µ—Ç **–º–µ–∂–¥—É** tool calls
- –ë–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è reasoning –∏ actions

**Safety:**
> "In rare cases, Claude's thought process might include potentially harmful content. In such cases, the thought process will be encrypted."

### DeepSeek R1: Open Source —Ä–µ–≤–æ–ª—é—Ü–∏—è

#### –ü—Ä–æ—Ä—ã–≤ —è–Ω–≤–∞—Ä—è 2025

DeepSeek R1 –ø–æ—Ç—Ä—è—Å –∏–Ω–¥—É—Å—Ç—Ä–∏—é, –¥–æ—Å—Ç–∏–≥–Ω—É–≤ —É—Ä–æ–≤–Ω—è o1 –ø—Ä–∏:
- **96% –¥–µ—à–µ–≤–ª–µ** –≤ API
- **MIT –ª–∏—Ü–µ–Ω–∑–∏—è** - –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–π
- **–û—Ç–∫—Ä—ã—Ç—ã–µ –≤–µ—Å–∞** - –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
- 671B total parameters
- 37B active parameters (MoE)
- 128K context window

#### –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞

**DeepSeek-R1-Zero:**
- –û–±—É—á–µ–Ω **—Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ RL**, –±–µ–∑ SFT
- Reasoning capabilities emerged naturally
- –î–∞–∂–µ reward model –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ - regex matching!

> "It is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT."

**Emergent behaviors:**
- Self-verification
- Reflection
- Dynamic strategy adaptation
- Long chains of thought

#### Benchmark —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å o1

| Benchmark | DeepSeek R1 | OpenAI o1-1217 |
|-----------|-------------|----------------|
| AIME 2024 | 79.8% | 79.2% |
| MATH-500 | 97.3% | 96.4% |
| GPQA Diamond | 71.5% | 75.7% |
| MMLU | 90.8% | 91.8% |
| Codeforces | 96.3% | 96.6% |

**–í—ã–≤–æ–¥:** –°–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ –Ω–∏–∂–µ —Ü–µ–Ω–∞.

#### Pricing —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –ú–æ–¥–µ–ª—å | Input/1M | Output/1M | Cached Input |
|--------|----------|-----------|--------------|
| DeepSeek R1 | $0.55 | $2.19 | $0.14 |
| OpenAI o1 | $15.00 | $60.00 | $7.50 |
| **–†–∞–∑–Ω–∏—Ü–∞** | **27x** | **27x** | **54x** |

#### Distilled –≤–µ—Ä—Å–∏–∏

–î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã distilled –º–æ–¥–µ–ª–∏:
- DeepSeek-R1-Distill-Qwen-1.5B
- DeepSeek-R1-Distill-Qwen-7B
- DeepSeek-R1-Distill-Llama-8B
- DeepSeek-R1-Distill-Qwen-14B
- DeepSeek-R1-Distill-Qwen-32B
- DeepSeek-R1-Distill-Llama-70B

**–ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Ollama:**
```bash
ollama run deepseek-r1:7b
ollama run deepseek-r1:32b
```

### Qwen QwQ (Alibaba)

#### QwQ-32B-Preview (–ù–æ—è–±—Ä—å 2024)

**QwQ = "Qwen with Questions"** - –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è reasoning –º–æ–¥–µ–ª—å –æ—Ç Alibaba.

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- 32B parameters
- 32K context (–ø–æ–∑–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω)
- Apache 2.0 license (–∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)

**Benchmarks:**
- GPQA: 65.2%
- AIME: 50.0%
- MATH-500: 90.6%
- LiveCodeBench: 50.0%

#### QwQ-32B (2025)

–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞—è —Å DeepSeek-R1:
> "While DeepSeek-R1 operates with 671 billion parameters, QwQ-32B achieves comparable performance with a much smaller footprint."

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –û—Ç–∫—Ä—ã—Ç—ã–π thinking process (–Ω–µ —Å–∫—Ä—ã—Ç –∫–∞–∫ —É o1)
- –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ compute requirements
- Apache 2.0 license

**–ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- Language Mixing (—Å–º–µ—à–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤)
- Recursive Reasoning Loops (–∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ)

### Google Gemini Thinking

#### Gemini 2.0 Flash Thinking (–î–µ–∫–∞–±—Ä—å 2024)

–ü–µ—Ä–≤–∞—è reasoning –º–æ–¥–µ–ª—å –æ—Ç Google, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ 19 –¥–µ–∫–∞–±—Ä—è 2024.

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- Multimodal (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç thought process –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- –î–æ—Å—Ç—É–ø–µ–Ω –≤ Google AI Studio –∏ Vertex AI

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è thinking:**
```python
# Dynamic thinking - –º–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ –¥—É–º–∞—Ç—å
thinking_budget = -1

# –û—Ç–∫–ª—é—á–∏—Ç—å thinking
thinking_budget = 0

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π budget (—Ç–æ–∫–µ–Ω—ã)
thinking_budget = 10000
```

**Benchmarks:**
- AIME 2024: 73.3% (vs Flash –±–µ–∑ thinking: 35.5%)

### –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π

| –ú–æ–¥–µ–ª—å | –ö–æ–º–ø–∞–Ω–∏—è | Open Source | Vision | Tools | Visible CoT | –¶–µ–Ω–∞* |
|--------|----------|-------------|--------|-------|-------------|-------|
| o1 | OpenAI | –ù–µ—Ç | –ù–µ—Ç | –ù–µ—Ç | Summary | $$$$$ |
| o3 | OpenAI | –ù–µ—Ç | –î–∞ | –î–∞ | Summary | $$$$$ |
| o3-mini | OpenAI | –ù–µ—Ç | –î–∞ | –î–∞ | –ë–æ–ª—å—à–µ | $$ |
| DeepSeek R1 | DeepSeek | –î–∞ (MIT) | –ù–µ—Ç | –î–∞ | –ü–æ–ª–Ω—ã–π | $ |
| Claude ET | Anthropic | –ù–µ—Ç | –î–∞ | –î–∞ | –ü–æ–ª–Ω—ã–π | $$$ |
| QwQ-32B | Alibaba | –î–∞ (Apache) | –ù–µ—Ç | –ù–µ—Ç | –ü–æ–ª–Ω—ã–π | $ |
| Gemini Thinking | Google | –ù–µ—Ç | –î–∞ | –î–∞ | –ü–æ–ª–Ω—ã–π | $$ |

*–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

---

## –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π

### Claude 3.7 vs o1 vs DeepSeek R1

**Reasoning –∫–∞—á–µ—Å—Ç–≤–æ:**
```
o3 > o1 ‚âà Claude ET (high budget) ‚âà DeepSeek R1 > Claude ET (low budget) > o3-mini
```

**Coding (SWE-Bench Verified):**
| –ú–æ–¥–µ–ª—å | Score |
|--------|-------|
| Claude 3.7 ET | 70.3% (with enhanced TTC) |
| o3 | 69.1% |
| Claude 3.5 Sonnet | 50.8% |
| DeepSeek R1 | 49.2% |
| o1 | 48.9% |

**Instruction Following:**
- Claude 3.7: 93.2%
- DeepSeek R1: 83.3%

**Cost-Performance –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á:**
```
DeepSeek R1 >>> Claude ET > o3-mini > o3 > o1
```

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫—É—é –º–æ–¥–µ–ª—å

**–í—ã–±–µ—Ä–∏—Ç–µ o3**, –µ—Å–ª–∏:
- –ù—É–∂–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ reasoning
- –ë—é–¥–∂–µ—Ç –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω
- –í–∞–∂–Ω—ã vision –∏ tool use –≤ reasoning
- Enterprise —Å Microsoft/OpenAI partnership

**–í—ã–±–µ—Ä–∏—Ç–µ o3-mini**, –µ—Å–ª–∏:
- –ù—É–∂–µ–Ω –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ/—Ü–µ–Ω–∞
- –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å reasoning effort
- Streaming –≤–∞–∂–µ–Ω

**–í—ã–±–µ—Ä–∏—Ç–µ DeepSeek R1**, –µ—Å–ª–∏:
- –ë—é–¥–∂–µ—Ç –∫—Ä–∏—Ç–∏—á–µ–Ω (27x –¥–µ—à–µ–≤–ª–µ o1)
- –ù—É–∂–µ–Ω open source
- –•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ (privacy)
- –•–æ—Ç–∏—Ç–µ –∏–∑—É—á–∞—Ç—å –ø–æ–ª–Ω—ã–π chain of thought

**–í—ã–±–µ—Ä–∏—Ç–µ Claude Extended Thinking**, –µ—Å–ª–∏:
- –ù—É–∂–Ω–∞ –≥–∏–±–∫–æ—Å—Ç—å (–æ–¥–∏–Ω API, –¥–≤–∞ —Ä–µ–∂–∏–º–∞)
- –í–∞–∂–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å tools
- –•–æ—Ç–∏—Ç–µ —Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å thinking budget
- –ó–∞–¥–∞—á–∏ –ø–æ —Ñ–∏–∑–∏–∫–µ –∏ –Ω–∞—É–∫–µ (96.5% GPQA Physics!)

**–í—ã–±–µ—Ä–∏—Ç–µ QwQ-32B**, –µ—Å–ª–∏:
- –ù—É–∂–µ–Ω open source
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã compute —Ä–µ—Å—É—Ä—Å—ã
- –ù–µ –Ω—É–∂–Ω–∞ multimodality

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ö–æ–≥–¥–∞ Reasoning vs –æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏

#### Decision Framework

```
                        –ó–∞–¥–∞—á–∞ —Å–ª–æ–∂–Ω–∞—è?
                              |
              +---------------+---------------+
              |               |               |
             –ù–ï–¢           –°–†–ï–î–ù–ï             –î–ê
              |               |               |
              v               v               |
         GPT-4o /        –û—à–∏–±–∫–∞ –¥–æ—Ä–æ–≥–æ        |
         Claude          —Å—Ç–æ–∏—Ç?               |
         Sonnet               |               |
                    +---------+---------+     |
                    |                   |     |
                   –ù–ï–¢                 –î–ê     |
                    |                   |     |
                    v                   v     |
              GPT-4o +             o3-mini    |
              CoT prompt            (low)     |
                                              |
                              +---------------+
                              v
                    –ö—Ä–∏—Ç–∏—á–Ω–∞ –ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å?
                              |
              +---------------+---------------+
              |               |               |
        –û–ß–ï–ù–¨ –í–ê–ñ–ù–ê      –í–ê–ñ–ù–ê         –ù–ï –û–ß–ï–ù–¨
              |               |               |
              v               v               v
           o3-pro           o3 /         o3-mini
              –∏–ª–∏      Claude ET          (high)
         Claude ET         (high              –∏–ª–∏
           (max)          budget)        DeepSeek R1
                                         (–ª–æ–∫–∞–ª—å–Ω–æ)
```

#### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á

**–ò–°–ü–û–õ–¨–ó–£–ô–¢–ï REASONING –ú–û–î–ï–õ–ò:**

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ü—Ä–∏–º–µ—Ä—ã |
|-----------|---------|
| **–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ STEM** | –û–ª–∏–º–ø–∏–∞–¥–Ω—ã–µ –∑–∞–¥–∞—á–∏, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º, –Ω–∞—É—á–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã |
| **–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ** | Security review, —Å–ª–æ–∂–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ |
| **–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤** | –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã, –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, due diligence |
| **LLM-as-Judge** | –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π, –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π |
| **Multi-step –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ** | Agentic workflows, —Å–ª–æ–∂–Ω—ã–µ pipelines |

**–ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –û–ë–´–ß–ù–´–ï –ú–û–î–ï–õ–ò:**

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ü—Ä–∏–º–µ—Ä—ã |
|-----------|---------|
| **Conversational AI** | –ß–∞—Ç-–±–æ—Ç—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã |
| **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞** | –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã, —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, –ø–µ—Ä–µ–≤–æ–¥ |
| **–†—É—Ç–∏–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏** | –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, extraction, –ø—Ä–æ—Å—Ç–æ–π Q&A |
| **Real-time** | –ê–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç –≤ IDE, streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è |

### API –ø—Ä–∏–º–µ—Ä—ã

#### OpenAI o3

```python
from openai import OpenAI

client = OpenAI()

# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ o3
response = client.chat.completions.create(
    model="o3",
    messages=[
        {
            "role": "user",
            "content": "Prove that there are infinitely many prime numbers."
        }
    ],
    max_completion_tokens=16000  # –í–∫–ª—é—á–∞–µ—Ç thinking tokens
)

print(response.choices[0].message.content)
```

#### OpenAI o3-mini —Å reasoning effort

```python
# –ö–æ–Ω—Ç—Ä–æ–ª—å –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
response = client.chat.completions.create(
    model="o3-mini",
    messages=[
        {"role": "user", "content": "Analyze this algorithm complexity..."}
    ],
    reasoning_effort="high"  # "low", "medium", "high"
)
```

#### DeepSeek R1

```python
from openai import OpenAI

# –ß–µ—Ä–µ–∑ API DeepSeek
client = OpenAI(
    api_key="your-deepseek-api-key",
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-reasoner",
    messages=[
        {"role": "user", "content": "Solve this logic puzzle..."}
    ]
)

# –í–∏–¥–∏–º –≤–µ—Å—å chain of thought!
print("Reasoning:", response.choices[0].message.reasoning_content)
print("Answer:", response.choices[0].message.content)
```

#### DeepSeek R1 –ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ Ollama

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
ollama pull deepseek-r1:7b

# –ó–∞–ø—É—Å–∫
ollama run deepseek-r1:7b
```

```python
# –ß–µ—Ä–µ–∑ OpenAI-compatible API
client = OpenAI(
    api_key="ollama",
    base_url="http://localhost:11434/v1"
)

response = client.chat.completions.create(
    model="deepseek-r1:7b",
    messages=[...]
)
```

#### Claude Extended Thinking

```python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000  # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞
    },
    messages=[
        {
            "role": "user",
            "content": "Analyze this contract for risks..."
        }
    ]
)

# –í–∏–¥–∏–º –∏ thinking, –∏ –æ—Ç–≤–µ—Ç
for block in response.content:
    if block.type == "thinking":
        print(f"Claude's reasoning:\n{block.thinking}\n")
    elif block.type == "text":
        print(f"Final answer:\n{block.text}")
```

#### –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π thinking budget

```python
# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ budget_tokens:
THINKING_BUDGETS = {
    "simple": 1024,      # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏
    "moderate": 5000,    # –°—Ä–µ–¥–Ω–∏–µ
    "complex": 10000,    # –°–ª–æ–∂–Ω—ã–µ
    "very_complex": 20000,  # –û—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–µ
    "maximum": 128000    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
}

def get_thinking_budget(task_complexity: str) -> int:
    return THINKING_BUDGETS.get(task_complexity, 5000)
```

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

#### –ü—Ä–∏–º–µ—Ä 1: –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ

**–ó–∞–¥–∞—á–∞:** –î–æ–∫–∞–∂–∏—Ç–µ, —á—Ç–æ sqrt(2) –∏—Ä—Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ.

**GPT-4o** (—Ç–∏–ø–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç):
```
–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ sqrt(2) = p/q, –≥–¥–µ p –∏ q - —Ü–µ–ª—ã–µ...
[–ú–æ–∂–µ—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —à–∞–≥–∏ –∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É]
```

**o3** (—Å reasoning):
```
[Internal thinking - summary shown]
The model is carefully constructing a proof by contradiction,
checking each logical step and verifying the conclusion.

Final answer:
–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º –ø—Ä–æ—Ç–∏–≤–Ω–æ–µ: sqrt(2) = p/q –≤ –Ω–µ—Å–æ–∫—Ä–∞—Ç–∏–º–æ–π –¥—Ä–æ–±–∏.
–¢–æ–≥–¥–∞ 2 = p^2/q^2, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ p^2 = 2q^2.
–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç p^2 —á–µ—Ç–Ω–æ, –∑–Ω–∞—á–∏—Ç p —á–µ—Ç–Ω–æ.
–ü—É—Å—Ç—å p = 2k. –¢–æ–≥–¥–∞ 4k^2 = 2q^2, —Ç.–µ. q^2 = 2k^2.
–ó–Ω–∞—á–∏—Ç q —Ç–æ–∂–µ —á–µ—Ç–Ω–æ.
–ù–æ —Ç–æ–≥–¥–∞ –¥—Ä–æ–±—å —Å–æ–∫—Ä–∞—Ç–∏–º–∞ - –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ.
–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, sqrt(2) –∏—Ä—Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ.
```

#### –ü—Ä–∏–º–µ—Ä 2: Security Code Review

```python
def get_user_data(user_id):
    query = f"SELECT * FROM users WHERE id = {user_id}"
    return db.execute(query)
```

**Claude Extended Thinking** (budget 10000):
```
[Thinking visible]
–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–¥ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ...

1. SQL Injection: f-string + user input = –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å
   –ê—Ç–∞–∫–∏: "1 OR 1=1", "1; DROP TABLE users;--"

2. –ù–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–∏–ø–∞ user_id

3. SELECT * –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç sensitive –¥–∞–Ω–Ω—ã–µ

4. –ù–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

5. –ù–µ—Ç rate limiting

6. –ù–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

[Final answer]
–ù–∞–π–¥–µ–Ω–æ 6 —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π...
[–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏]
```

#### –ü—Ä–∏–º–µ—Ä 3: –ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥:**
```python
def find_duplicates(arr):
    duplicates = []
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j] and arr[i] not in duplicates:
                duplicates.append(arr[i])
    return duplicates
```

**DeepSeek R1** (—Å –≤–∏–¥–∏–º—ã–º thinking):
```
<think>
–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å:
- –í–Ω–µ—à–Ω–∏–π —Ü–∏–∫–ª: O(n)
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ü–∏–∫–ª: O(n)
- –ü—Ä–æ–≤–µ—Ä–∫–∞ 'not in duplicates': O(n) –≤ —Ö—É–¥—à–µ–º —Å–ª—É—á–∞–µ
- –ò—Ç–æ–≥–æ: O(n^3)

–ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–æ O(n):
1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å set –¥–ª—è seen —ç–ª–µ–º–µ–Ω—Ç–æ–≤
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å set –¥–ª—è duplicates
3. Single pass

Edge cases:
- –ü—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤
- –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
- –í—Å–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
</think>

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è:
def find_duplicates(arr):
    seen = set()
    duplicates = set()
    for item in arr:
        if item in seen:
            duplicates.add(item)
        seen.add(item)
    return list(duplicates)

–°–ª–æ–∂–Ω–æ—Å—Ç—å: O(n) time, O(n) space
```

---

## Prompting –¥–ª—è Reasoning Models: –ú–µ–Ω—å—à–µ –∑–Ω–∞—á–∏—Ç –ª—É—á—à–µ

### –ì–ª–∞–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ

> **–î–ª—è reasoning –º–æ–¥–µ–ª–µ–π –ù–ï –ù–£–ñ–ï–ù prompt engineering –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–º —Å–º—ã—Å–ª–µ.**

Chain of Thought —É–∂–µ –≤—Å—Ç—Ä–æ–µ–Ω. –ú–æ–¥–µ–ª—å —É–∂–µ –∑–Ω–∞–µ—Ç, —á—Ç–æ –Ω—É–∂–Ω–æ –¥—É–º–∞—Ç—å –ø–æ—à–∞–≥–æ–≤–æ. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ "Let's think step by step" –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç - –º–æ–∂–µ—Ç –¥–∞–∂–µ –Ω–∞–≤—Ä–µ–¥–∏—Ç—å.

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:**
> "When using explicit Chain-of-Thought prompting with reasoning models, it can hurt the instruction-following performance of the model."

### –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

```python
# –•–û–†–û–®–û: –ü—Ä–æ—Å—Ç–æ–π, –ø—Ä—è–º–æ–π prompt
prompt = """
Solve the integral: integrate(e^x * sin(x) dx)

Show the final answer clearly.
"""

# –•–û–†–û–®–û: –ß–µ—Ç–∫–∞—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
prompt = """
Analyze this code for security vulnerabilities:

[–∫–æ–¥]

List all vulnerabilities with severity ratings.
"""

# –•–û–†–û–®–û: –£–∫–∞–∑–∞–Ω–∏–µ –∂–µ–ª–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ output
prompt = """
Prove that sqrt(2) is irrational.

Format: Step-by-step proof with numbered steps.
"""
```

### –ß—Ç–æ –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç

```python
# –ü–õ–û–•–û: –ò–∑–±—ã—Ç–æ—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
prompt = """
You are an expert mathematician. Think carefully step by step.
First, identify the type of problem. Then, list possible approaches.
For each approach, evaluate its feasibility.
Consider edge cases. Verify your solution...

[–∫—É—á–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π]

Now solve: What is 2 + 2?
"""
# –ú–æ–¥–µ–ª—å —É–∂–µ –∑–Ω–∞–µ—Ç –∫–∞–∫ –¥—É–º–∞—Ç—å! –ù–µ —É—á–∏—Ç–µ –µ–µ.

# –ü–õ–û–•–û: Few-shot –ø—Ä–∏–º–µ—Ä—ã
prompt = """
Example 1: 2+2=4
Example 2: 3+3=6

Now solve: 127 x 89 = ?
"""
# Few-shot –º–æ–∂–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å reasoning –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º

# –ü–õ–û–•–û: Detailed step-by-step instructions
# –í—Ä–µ–¥–∏—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–±–∏–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π reasoning –ø—Ä–æ—Ü–µ—Å—Å
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤

| –¢–µ—Ö–Ω–∏–∫–∞ | GPT-4o | o1/o3 | Claude Extended |
|---------|--------|-------|-----------------|
| System prompt | –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ | –ò–∑–±–µ–≥–∞–π—Ç–µ | –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ |
| "Think step by step" | –ü–æ–º–æ–≥–∞–µ—Ç | –ù–µ –Ω—É–∂–µ–Ω/–≤—Ä–µ–¥–∏—Ç | –ù–µ –Ω—É–∂–µ–Ω |
| Few-shot examples | –ü–æ–º–æ–≥–∞–µ—Ç | –ú–æ–∂–µ—Ç –Ω–∞–≤—Ä–µ–¥–∏—Ç—å | –û—Å—Ç–æ—Ä–æ–∂–Ω–æ |
| Temperature tuning | –†–∞–±–æ—Ç–∞–µ—Ç | –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è | –û—Å—Ç–æ—Ä–æ–∂–Ω–æ |
| Detailed instructions | –ü–æ–º–æ–≥–∞–µ—Ç | –ò–∑–±–µ–≥–∞–π—Ç–µ | –£–º–µ—Ä–µ–Ω–Ω–æ |

### –ê–Ω–∞–ª–æ–≥–∏—è

> "A reasoning model is like a senior co-worker. You can give them a goal to achieve and trust them to work out the details. A GPT model is like a junior coworker. They'll perform best with explicit instructions to create a specific output."

### Best practices –¥–ª—è reasoning –º–æ–¥–µ–ª–µ–π

1. **Use minimal, high-level prompting** - zero-shot –∏–ª–∏ single-instruction
2. **Be explicit about goals, not steps** - —á—Ç–æ –Ω—É–∂–Ω–æ, –Ω–µ –∫–∞–∫ –¥–µ–ª–∞—Ç—å
3. **Structure inputs and outputs** - delimiters, JSON/XML –¥–ª—è output
4. **Add constraints when needed** - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –Ω–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
5. **Encourage more reasoning for complex tasks** - "Please analyze thoroughly"

---

## –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã Reasoning –º–æ–¥–µ–ª–µ–π

### Hallucinations –≤ reasoning

Reasoning –º–æ–¥–µ–ª–∏ –≤—Å–µ –µ—â–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É—é—Ç, —Ö–æ—Ç—è —Ä–µ–∂–µ:

> "Language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty."

**–¢–∏–ø—ã –æ—à–∏–±–æ–∫:**
1. **Factual errors** –≤ reasoning steps
2. **Logical leaps** - –ø—Ä–æ–ø—É—Å–∫ —à–∞–≥–æ–≤
3. **Self-reinforcing errors** - –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —à–∞–≥ –≤–µ–¥–µ—Ç –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –≤—ã–≤–æ–¥—É
4. **Reversal Curse** - –∑–Ω–∞–µ—Ç "A is B", –Ω–æ –Ω–µ "B is A"

### Tool-Induced Myopia (TIM)

–ù–æ–≤—ã–π —Ç–∏–ø –æ—à–∏–±–æ–∫, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –¥–ª—è reasoning + tools:

> "Access to an external tool causes the model to narrow its reasoning to what the tool can compute, rather than utilizing its full internal reasoning abilities."

**–°–∏–º–ø—Ç–æ–º—ã:**
- Overreliance –Ω–∞ tool outputs
- –ë–æ–ª–µ–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ reasoning chains –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ tools
- Degradation —É—Å–∏–ª–∏–≤–∞–µ—Ç—Å—è —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º tool calls

### –°—Ç–æ–∏–º–æ—Å—Ç—å –∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å

| –ú–æ–¥–µ–ª—å | –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å | –°—Ç–æ–∏–º–æ—Å—Ç—å vs GPT-4o |
|--------|-------------|---------------------|
| GPT-4o | ~1-3 —Å–µ–∫ | 1x |
| o3-mini | ~5-30 —Å–µ–∫ | 2-5x |
| o3 | ~10-60 —Å–µ–∫ | 10-20x |
| o1 | ~20-120 —Å–µ–∫ | 15-30x |

> "Reasoning models can be 5-10x more expensive per request than standard models."

### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è

**OpenAI o-series:**
- –ù–µ—Ç temperature control
- –ù–µ—Ç system prompt (–∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω)
- –°–∫—Ä—ã—Ç—ã–π chain of thought
- Billing –∑–∞ –Ω–µ–≤–∏–¥–∏–º—ã–µ —Ç–æ–∫–µ–Ω—ã

### "Faithful" –ª–∏ reasoning?

Anthropic Research (2024):
> "Reasoning models don't always say what they think"

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç:
- –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å "–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–µ" reasoning traces
- –†–∞–±–æ—Ç–∞—Ç—å backwards –æ—Ç –æ—Ç–≤–µ—Ç–∞ –∫ "–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—é"
- –ù–µ –≤—Å–µ–≥–¥–∞ —Å–ª–µ–¥–æ–≤–∞—Ç—å —Ç–æ–º—É, —á—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ CoT

---

## –ë—É–¥—É—â–µ–µ Reasoning –º–æ–¥–µ–ª–µ–π

### Trends 2025+

1. **Hybrid models —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–æ—Ä–º–æ–π**
   - –û–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –∏ –≥–ª—É–±–æ–∫–∏—Ö –∑–∞–¥–∞—á
   - Claude —É–∂–µ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ, –¥—Ä—É–≥–∏–µ –ø–æ—Å–ª–µ–¥—É—é—Ç

2. **Open source –¥–æ–≥–æ–Ω—è–µ—Ç**
   - DeepSeek R1 –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ
   - –û–∂–∏–¥–∞–π—Ç–µ –±–æ–ª—å—à–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö reasoning –º–æ–¥–µ–ª–µ–π

3. **Multimodal reasoning**
   - o3 —É–∂–µ —É–º–µ–µ—Ç reasoning –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
   - Gemini Thinking - multimodal by design

4. **Agentic reasoning**
   - o3/o4-mini: –ø–µ—Ä–≤—ã–µ reasoning –º–æ–¥–µ–ª–∏ —Å full tool use
   - Reasoning –º–µ–∂–¥—É actions, –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –Ω–∞—á–∞–ª–µ

5. **Efficient architectures**
   - Hierarchical Reasoning Model (HRM): 27M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–æ—á—Ç–∏ perfect –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö
   - State Space Models –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### –ù–µ—Ä–µ—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

1. **Verifiable reasoning** - –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ reasoning –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π?
2. **Faithful CoT** - –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –º–æ–¥–µ–ª—å —Å–ª–µ–¥—É–µ—Ç —Å–≤–æ–µ–º—É CoT?
3. **Cost optimization** - reasoning –¥–æ—Ä–æ–≥–æ–π, –∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å –¥–µ—à–µ–≤–ª–µ?
4. **Safety** - –∫–∞–∫ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å reasoning –∫ –≤—Ä–µ–¥–Ω—ã–º –≤—ã–≤–æ–¥–∞–º?

---

## –ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è

### –í–æ–ø—Ä–æ—Å 1: –ß—Ç–æ —Ç–∞–∫–æ–µ Test-Time Compute?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

Test-Time Compute (TTC) - —ç—Ç–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç **–≤–æ –≤—Ä–µ–º—è inference** (–∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç), –∞ –Ω–µ –≤–æ –≤—Ä–µ–º—è training.

–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è: –ø–æ–∑–≤–æ–ª–∏—Ç—å –º–æ–¥–µ–ª–∏ "–¥—É–º–∞—Ç—å –¥–æ–ª—å—à–µ" –Ω–∞–¥ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, —Ç—Ä–∞—Ç—è –±–æ–ª—å—à–µ compute —Ç–∞–º, –≥–¥–µ –Ω—É–∂–Ω–æ.

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ TTC –º–æ–∂–µ—Ç –¥–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç, —Å—Ä–∞–≤–Ω–∏–º—ã–π —Å 14x —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.
</details>

### –í–æ–ø—Ä–æ—Å 2: –ß–µ–º GRPO –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç PPO?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**GRPO (Group Relative Policy Optimization):**
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π value function –º–æ–¥–µ–ª–∏
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä—É–ø–ø–æ–≤–æ–π advantage –≤–º–µ—Å—Ç–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–≥–æ
- –ù–∞ ~50% –º–µ–Ω—å—à–µ memory –∏ compute requirements
- KL divergence –≤ loss function, –Ω–µ –≤ reward

–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω DeepSeek, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è R1.
</details>

### –í–æ–ø—Ä–æ—Å 3: –ü–æ—á–µ–º—É "Let's think step by step" –≤—Ä–µ–¥–∏—Ç reasoning –º–æ–¥–µ–ª—è–º?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

Chain of Thought **—É–∂–µ –≤—Å—Ç—Ä–æ–µ–Ω** –≤ reasoning –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ RL –æ–±—É—á–µ–Ω–∏–µ.

–î–æ–±–∞–≤–ª–µ–Ω–∏–µ CoT prompt:
- –ù–µ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
- –ú–æ–∂–µ—Ç —Å–±–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π reasoning process
- –¢—Ä–∞—Ç–∏—Ç —Ç–æ–∫–µ–Ω—ã input
- –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏ —É—Ö—É–¥—à–µ–Ω–∏–µ instruction-following

Best practice: –ø—Ä–æ—Å—Ç—ã–µ, –ø—Ä—è–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.
</details>

### –í–æ–ø—Ä–æ—Å 4: –ß—Ç–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –≤ DeepSeek R1-Zero?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

DeepSeek R1-Zero –æ–±—É—á–µ–Ω **—Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ RL**, –±–µ–∑ Supervised Fine-Tuning:

1. –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è reasoning
2. Reward signal - –ø—Ä–æ—Å—Ç–æ regex matching –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
3. Reasoning capabilities emerged naturally
4. –ü–µ—Ä–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –¥–æ–∫–∞–∑–∞–≤—à–µ–µ —á—Ç–æ reasoning –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ RL
</details>

### –í–æ–ø—Ä–æ—Å 5: –ö–æ–≥–¥–∞ –≤—ã–±—Ä–∞—Ç—å Claude Extended Thinking vs o3?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**Claude Extended Thinking:**
- –ù—É–∂–Ω–∞ –≥–∏–±–∫–æ—Å—Ç—å (–æ–¥–∏–Ω API, –¥–≤–∞ —Ä–µ–∂–∏–º–∞)
- –•–æ—Ç–∏—Ç–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å thinking budget
- –í–∞–∂–µ–Ω visible chain of thought
- –ó–∞–¥–∞—á–∏ –ø–æ —Ñ–∏–∑–∏–∫–µ (96.5% –Ω–∞ GPQA Physics!)
- Interleaved reasoning —Å tools

**o3:**
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ math/code benchmarks
- –ù—É–∂–Ω—ã vision capabilities –≤ reasoning
- Enterprise —Å OpenAI partnership
- Agentic workflows —Å tool use
</details>

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

- [[llm-fundamentals]] - –û—Å–Ω–æ–≤—ã —Ä–∞–±–æ—Ç—ã LLM
- [[models-landscape-2025]] - –û–±–∑–æ—Ä –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- [[prompt-engineering-masterclass]] - –ü—Ä–æ–º–ø—Ç –∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- [[ai-agents-advanced]] - –ê–≥–µ–Ω—Ç—ã –∏ reasoning

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏

### OpenAI
1. [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/)
2. [Introducing o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)
3. [Reasoning Models Guide](https://platform.openai.com/docs/guides/reasoning)
4. [Reasoning Best Practices](https://platform.openai.com/docs/guides/reasoning-best-practices)
5. [Improving Mathematical Reasoning with Process Supervision](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)

### Anthropic
6. [Claude's Extended Thinking](https://www.anthropic.com/news/visible-extended-thinking)
7. [Claude 3.7 Sonnet and Claude Code](https://www.anthropic.com/news/claude-3-7-sonnet)
8. [Building with Extended Thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
9. [Reasoning Models Don't Always Say What They Think](https://www.anthropic.com/research/reasoning-models-dont-say-think)

### DeepSeek
10. [DeepSeek-R1 Paper](https://arxiv.org/abs/2501.12948)
11. [DeepSeek-R1 GitHub](https://github.com/deepseek-ai/DeepSeek-R1)
12. [DeepSeek-R1 on HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)
13. [DeepSeekMath: GRPO Paper](https://arxiv.org/abs/2402.03300)

### –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
14. [Scaling LLM Test-Time Compute](https://arxiv.org/abs/2408.03314)
15. [A Comparative Study on Reasoning Patterns of o1](https://arxiv.org/html/2410.13639v1)
16. [Chain-of-Thought Prompting Elicits Reasoning](https://arxiv.org/abs/2201.11903)
17. [Survey on Test-Time Scaling](https://testtimescaling.github.io/)
18. [RLHF Book by Nathan Lambert](https://rlhfbook.com/)
19. [The State of LLM Reasoning Model Training](https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training)

### –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
20. [QwQ: Reflect Deeply on the Boundaries of the Unknown](https://qwenlm.github.io/blog/qwq-32b-preview/)
21. [Gemini Thinking Documentation](https://ai.google.dev/gemini-api/docs/thinking)

### –°—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑
22. [Claude 3.7 Sonnet vs OpenAI o1 vs DeepSeek R1](https://www.vellum.ai/blog/claude-3-7-sonnet-vs-openai-o1-vs-deepseek-r1)
23. [OpenAI o3 Released: Benchmarks and Comparison to o1](https://www.helicone.ai/blog/openai-o3)
24. [DeepSeek R1 vs OpenAI o1](https://www.analyticsvidhya.com/blog/2025/01/deepseek-r1-vs-openai-o1/)

---

*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2025-12-24*

---

## –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏

**[[llm-fundamentals]]** ‚Äî Reasoning-–º–æ–¥–µ–ª–∏ (o1, o3, DeepSeek R1) —Å—Ç—Ä–æ—è—Ç—Å—è –ø–æ–≤–µ—Ä—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ –æ—Å–Ω–æ–≤–∞—Ö LLM. –ü–æ–Ω–∏–º–∞–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –æ—Å–æ–∑–Ω–∞–Ω–∏—è —Ç–æ–≥–æ, –∫–∞–∫ chain-of-thought –∏ test-time compute —Ä–∞—Å—à–∏—Ä—è—é—Ç –±–∞–∑–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. Reasoning ‚Äî —ç—Ç–æ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∞ –Ω–∞–¥—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞–¥ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ LLM-–º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏, –æ–±—É—á–µ–Ω–Ω–∞—è —á–µ—Ä–µ–∑ RLHF –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**[[models-landscape-2025]]** ‚Äî –õ–∞–Ω–¥—à–∞—Ñ—Ç –º–æ–¥–µ–ª–µ–π 2025 –≥–æ–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–æ–µ –º–µ—Å—Ç–æ reasoning-–º–æ–¥–µ–ª–∏ –∑–∞–Ω–∏–º–∞—é—Ç —Å—Ä–µ–¥–∏ –¥—Ä—É–≥–∏—Ö —Å–µ–º–µ–π—Å—Ç–≤ (GPT, Claude, Gemini, open-source). –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, —Ü–µ–Ω–æ–≤–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏. Reasoning-–º–æ–¥–µ–ª–∏ –æ—Å–æ–±–µ–Ω–Ω–æ —Å–∏–ª—å–Ω—ã –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –∫–æ–¥–∏–Ω–≥–µ –∏ —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–µ, –Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –æ–±—ã—á–Ω—ã–º –º–æ–¥–µ–ª—è–º.

**[[ai-api-integration]]** ‚Äî –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è reasoning-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ API –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –∏–∑-–∑–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ ¬´–¥—É–º–∞–Ω–∏—è¬ª, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (reasoning_effort, thinking budget), –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ reasoning-—Ç–æ–∫–µ–Ω–æ–≤. –ü–æ–Ω–∏–º–∞–Ω–∏–µ API-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è reasoning-–º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ, –≤–∫–ª—é—á–∞—è fallback-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏.

---

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —á—Ç–µ–Ω–∏–µ

- **Russell S., Norvig P. (2020). Artificial Intelligence: A Modern Approach. 4th edition.** ‚Äî –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —É—á–µ–±–Ω–∏–∫, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ reasoning –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ AI, —á—Ç–æ –¥–∞—ë—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è reasoning-–º–æ–¥–µ–ª–µ–π
- **Goodfellow I., Bengio Y., Courville A. (2016). Deep Learning. MIT Press.** ‚Äî –≥–ª—É–±–æ–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è RLHF –∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è reasoning-–º–æ–¥–µ–ª–µ–π
- **Jurafsky D., Martin J.H. (2023). Speech and Language Processing. 3rd edition.** ‚Äî —Ä–∞–∑–¥–µ–ª –æ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–∞—ë—Ç –±–∞–∑—É –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ chain-of-thought –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–æ–Ω–æ–ª–æ–≥–æ–≤ reasoning-–º–æ–¥–µ–ª–µ–π

---

*–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 2026-01-09*
